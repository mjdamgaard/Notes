\documentclass{report}
\usepackage[utf8]{inputenc}


\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{braket}

%\usepackage{stix}

%\usepackage[toc, page]{appendix}
%\usepackage[nottoc, numbib]{tocbibind}
\usepackage[nottoc]{tocbibind}
\usepackage[bookmarks=true]{hyperref}
\usepackage[numbered]{bookmark}

%%%\usepackage{hyperref}     
%%%\usepackage{amsthm}     
%%\usepackage{cleveref}
%\usepackage{silence}
%\WarningFilter{pdftex}{destination with the same}

\hypersetup{
	colorlinks	= true,
	urlcolor	= blue,
	linkcolor	= black,
	citecolor	= black
}

\usepackage{comment}

\usepackage{lipsum}


\title{
	Note collection (2023--20xx)
	\author{Mads J.\ Damgaard%
		%\footnote{
		%	See https://www.github.com/mjdamgaard/notes for potential updates, additional points, and other work.
		%}
		%\footnote{
		%	B.Sc.\ at the Niels Bohr Institute, University of Copenhagen.
		%	B.Sc.\ at the Department of Computer Science, University of Copenhagen.
		%	E-mail: fxn318@alumni.ku.dk.
		%	GitHub folder: https://www.github.com/mjdamgaard/notes.
		%}
	}
}

\usepackage[margin=1.5in]{geometry}

\begin{document}
\maketitle

\section*{Foreword}
{\centering\noindent
	\vspace{-\baselineskip}
	\hspace{-0.7em}
	{\hspace{-4.em}$|$\hspace{\linewidth}\hspace{8em}$|$}
}
Note collection from 10.01.23--???. 

%Kopieret nedenfra:
	%"(10.01.23) Okay, jeg har lige påbegyndt dette notesæt. Jeg er pt. i gang med at skrive version 2 (en meget mere simpel udgave) af min SRC-artikel, og så har jeg ikke kunne lade være med at bruge en del af tiden (som jeg "burde" have brugt på at skrive om SRC) på at tænke mere over, hvordan jeg ville starte mine hjemmesider, jeg har i tankerne, hvis jeg selv skulle gå i gang (hvad jeg faktisk stærkt overvejer). Særligt har jeg tænkt nogle tanker om en alternativ opsætning end den med fanerne, når det kommer til hele den applikation der. De tanker vil jeg skrive ned, når jeg vender tilbage her, sikkert senere i dag/aften. (For inden da vil jeg lige fortsætte lidt mere med SRC-arbejdet.) (17:17)"
%

\ 

%\lipsum[1]


\chapter{Web ideas}

%(10.01.23) Okay, jeg har lige påbegyndt dette notesæt. Jeg er pt. i gang med at skrive version 2 (en meget mere simpel udgave) af min SRC-artikel, og så har jeg ikke kunne lade være med at bruge en del af tiden (som jeg "burde" have brugt på at skrive om SRC) på at tænke mere over, hvordan jeg ville starte mine hjemmesider, jeg har i tankerne, hvis jeg selv skulle gå i gang (hvad jeg faktisk stærkt overvejer). Særligt har jeg tænkt nogle tanker om en alternativ opsætning end den med fanerne, når det kommer til hele den applikation der. De tanker vil jeg skrive ned, når jeg vender tilbage her, sikkert senere i dag/aften. (For inden da vil jeg lige fortsætte lidt mere med SRC-arbejdet.) (17:17)

\section{Nye tanker om, hvordan Web 2.0--3.0-siden kunne være til at starte med (10.01.23)}

(17:58, 10.01.23) Jeg har nogle nye tanker om, hvad jeg gør mht.\ mine hjemmeside-idéer. Sidst jeg skrev om det var for ret kort tid siden i mine 22--23-huskenoter, og inden da har jeg skrevet nogle tanker ned i det, der endte med at blive udkommenterede noter under SRC-artiklen (version 1). Men jeg har nu skiftet mening siden de noter. Jeg tænker således ikke længere på at starte med det der fane-værk. Nu har jeg en anden indledende opsætning i tankerne.

Men inden jeg går i gang med at beskrive den opsætning, så lad mig også lige sige, at jeg nu faktisk tænker at fokusere mest på web-applikationen, der har med semantiske strukturer, tag-ratings, opdelte kommentarer, ``automatiske point'' (og ``brugergrupper'') osv.\ at gøre, og derudover så også min debatside-applikation. Og jeg tænker så altså lidt at starte med førstnævnte emne, og så også tænke på at tilføje en debatside-applikation til den hjemmeside også inden for en nær fremtid efter, at jeg har fået den første applikation i gang. Desuden tænker jeg nu at gøre det open source. Jeg tænker altså ikke længere på at fokusere på at starte en SRC omkring det. I forhold til det monetære, så synes jeg nemlig, det er bedre at gøre dette i et separat lag oven over det applikationsmæssige lag. Så jeg tænker dermed altså, at den monetære del bare skal implementeres via donationer, og muligvis særligt via en donationsforening, ligesom.\,.

Ok. Nej, vent. Og fordi jeg vil gøre det helt open source, så tænker jeg altså også bare at tage med arme og ben fra andre open source-hjemmesider. Særligt kunne det være smart at tage fra en YouTube-agtig open source applikation og en Reddit/Twitter-agtig (måske Mastodon) applikation.\,. Og så altså derfra påsætte min.\,. hm, lad mig bare kalde det min `semantik-applikation' for nu; det er nemmere end, hvad jeg ellers har haft gang i (så som min ``Web 2.0--3.0-hjemmeside''). Jeg vil altså prøve at påsætte den applikation oven på de andre --- og efterfølgende påsætte en debatside-applikation oveni, så det bliver altså lidt af en Schweizerkniv-hjemmeside, men hvorfor ikke?\,. (sagde han helt naivt.\,. ej, jeg håber, det kommer til at give god mening sådan, 7, 9, 13).\,. 

Nå, og nu til at forklare, hvordan jeg tænker min semantiske applikation nu --- og det er ved at blive småsent, så jeg ser bare, hvor langt jeg når (og hvor sammenhængende det bliver), og så må jeg samle tråden op igen en af de kommende dage.

Som jeg forestiller mig min semantiske applikation nu (altså hvordan den kunne starte med at se ud rettere, for i fremtiden kan den så få alle mulige former, det er jo en del af det (altså at brugerne selv skal have frihed til at ændre udseendet)), så forestiller jeg mig altså bare en enkelt HTML-side, hvor at når man ser på lister over ressourcer, hvilket bliver en rigtig central del af applikationens brug, så er dette altså bare en liste midt på HTML-siden. Dog skal der så være en menu i siden, som gerne skal kunne være fold-ud. Denne menu bliver så til, hvad vi kan betragte som brugerens ``workspace.'' (Lad mig kalde det `arbejdsbord' på dansk.) .\,.\,Hm, lad mig lige holde en pause, og så ser jeg lige på, om jeg vender tilbage, eller om jeg bare holder for i dag.\,. (18:22)

(12.01.23, 16:58) I forgårs aften fik jeg tænkt lidt flere gode idéer omkring opbygningen, særligt af fold-ud-menuen. Men lad mig lige starte med at gie en helt grundlæggende beskrivelse, og så kan jeg føje flere detaljer til bagefter.

Lad os forestille os at en bruger kigger på kategorien `film,' og har valgt, ikke at se på en liste over film, men valgt at se en liste over underkategorier til kategorien `film.' Bemærk altså at `film' er implementeret, ikke som et er\_film-prædikat, men som et term simpelthen. Lad os så sige, at brugeren vælger `komediefilm.' Brugeren føjer hermed dette term til sit arbejdsbord, ved siden af `film.' Dette term ligger altså nu i fold-ud-menuen. Brugeren kan så nu i princippet gå til fold-ud-menuen og vælge en knap, der skifter visningen fra underkategorier\_af(term) til termer\_af\_kategorien(term). Og ved så at vælge `komediefilm' fra menuen (der også indeholder brugeren arbejdsbord nemlig), så får brugeren altså nu en liste over alle de termer, som passer på termer\_af\_kategorien(komediefilm). Dette er altså en mulig vej for brugeren i princippet, men allerede i prototypen af hjemmesiden skal det altså gerne være indbygget som en standard ting, at når brugeren har en liste over kategorier, så er der også en knap der gør disse ting på én gang, nemlig tilføjer kategorien til arbejdsbordet og går direkte hen til visningen af termer\_af\_kategorien(term), hvor inputtet så er den valgte kategori. 

Nu ser brugeren så en liste af ressourcer (komediefilm i dette tilfælde). Nu vil brugeren så måske gerne lægge et specifikt filter over denne liste, så kun visse ting bliver vist og i en vis rækkefølge. Nu kan brugeren så i fold-ud-menuen vælge en knap til at se, hvad vi kunne kalde filter-prædikater. Lad os sige, at brugeren allerede har browset film mange gange før og derfor har alle de almindelige filter-prædikater klar såsom `populær,' `godt bedømt' osv., men af en eller anden grund mangler prædikatet `sjov' i listen. Nu vil brugeren så gerne finde et sådant prædikat frem. Dette kan så gøres af to primære vejen, foruden at brugeren selvfølgelig kan lave en normal, ikke-semantisk søgning på prædikatet, hvilket man i praksis ofte vil gøre med tiden, men jeg har dog ikke i sinde at fokusere særligt på den mulighed med min prototype. I stedet kan brugeren enten gå til en grundlæggende visning over prædikat-kategorier, og så navigere frem til det ønskede prædikat, nemlig på samme måde som da brugeren navigerede til `komediefilm,' bare hvor brugeren altså nu søger i prædikat-kategorier (og til sidst -termer) og ikke i ressource-kategorier (og til sidst -termer). Når brugeren så har fundet prædikatet `sjov,' kan denne så tilføje det til arbejdsbordet, hvorved det så bliver vist i filter-prædikat-delmenuen. Ellers kan brugeren også forsøge at vælge en liste over prædikater\_relaterede\_til(term), hvor brugeren så kan vælge et term fra arbejdsbordet at putte ind her som input. Og her vil det jo så være oplagt, at brugeren inputter `komediefilm.' Den resulterende liste over prædikater kan så forestilles at indeholde det søgte prædikat (rimeligt tidligt i listen), og brugen kan så vælge det fra denne liste direkte. 

Når brugeren nu har tilføjet `sjov' til sine filter-prædikater, så kan brugeren nu gå tilbage og se termer\_af\_kategorien(komediefilm). Her kunne brugeren med fordel have valgt at navigere væk fra denne visning i første omgang med et midterklik, eller med ctrl + klik, således at den midlertidige søgning på prædikatet foregik i en ny fane. Pointen er nemlig, at arbejdsbordet skal ændres på tværs af faner (hvis brugeren altså bliver i det samme workspace, men det kommer vi til). I så fald kan brugeren bare klikke tilbage til browserfanen, hvor han/hun kom fra, og får herved nu vist `sjov' i filter-prædikat-menuen. 

Nu kan brugeren så tilpasse filteret mht.\ dette prædikat, hvilket indebærer at basalt set indstille en kurve over en akse for, hvor `sjov' filmen er. Jeg forstiller mig således en todelt gaussisk kurve, som altså næsten er en gaussfunktion, bortset fra at den kan have to forskellige spredninger til hver side (så altså en asymmetrisk version af en gausskurve). I dette tilfælde vil brugeren så sandsynligvis ønske at sætte toppunktet for kurven i enden, hvor `sjov' er mest gældende. Herefter kan brugeren så bestemme spredningen til den eneste af de to sider, der her er relevante. Og sidst men ikke mindst skal brugeren også kunne indstille en min- og en maks-værdi til filteret, således at for eksempel ingen film bliver vist, der har under en vis `sjov'-score. 

Jeg forestiller mig mere specifikt dermed at brugeren får vist tre barer, når brugeren folder indstillingerne ud for et filter-prædikat: Den første har én knap, der kan bevæges over hele baren, og som bestemmer kurvens toppunkt. Den næste bar kunne så være til at indstille de to spredninger og kunne så være en todelt bar, hvor knappen i hver siden kan sættes tæt på midten eller tæt på den respektive ende af baren, alt efter om spredningen til den side skal være lille eller stor (helt flad kurve, hvis man sætter dem i endepunkterne). Den sidste bar kan så være to knapper, der også kan sættes hvor som helst, og som så angiver min- og maks-værdierne for filteret. Imens brugeres finindstiller filter-prædikatet således, forestiller jeg mig, at brugeren også for vist den pågældende kurve i en lille graf over barerne. 

Nu kunne man så tro, at brugeren bare vil tage en virkelig lille spredning for kun at vise de film, der er har fået de allerhøjeste `sjov'-scorer, men i så fald ville de andre filter-prædikater, såsom f.eks.\ `godt bedømt' *(`god,' rettere) blive ubetydende, og derfor kan det altså ofte give god mening ikke at gøre kurverne alt for smalle. 

Cool, lad mig lige tænke over, om der er mere jeg skal sige til denne første lille del-rundgang, og ellers vil jeg gå over til at opsummere detaljerne, som jeg har tænkt mig nu her omkring denne nye opsætning (efter en lille pause.\,.). (17:55)

(18:09) Jo, jeg skal selvfølgelig også lige omkring det her med at rate ressourcer. Lad os sige, at brugeren undrer sig over, at en vis komediefilm kommer enten tidligt eller sent i listen, og så kigger på `sjov'-ratingen og ser, at den ikke ser ud, som brugeren ville forvente/synes. Så bør brugeren så kunne klikke sig ind på en rating-visning, nu hvor.\,. hm, jeg skulle til at skrive, at det er hvor alle andre spredninger er sat til uendeligt og at alle andre kurver end det relevante prædikat dermed bliver til firkantede trinkurver mellem deres valgte min- og maks-værdier i stedet, men er det nu også virkelig ideelt.\,.\,? 

\ldots Ah, man skal selvfølgelig bare have nogle separate filter-indstillinger for at vise rating-lister, og i starten vil de fleste så sikkert kunne nøjes med at bruge et enkelt prædikat udover det pågældende man rater i forhold til, og det er så popularitetsprædikatet. Senere kan man så prøve at implementere nogle indstillinger, hvor brugerne også får prioriteret at vise termer, der er bekendte for brugeren i listen, men det vil så være mere kompliceret at få dette op at køre. Så ja, når brugeren trykker, at denne vil ind og rate termerne, f.eks.\ ift.\ sjovhed i dette tilfælde, så får brugeren altså bare en ny rating-visning, hvor der også er særlige filter-indstillinger til, og hvor brugeren så kan rate ressourcerne/termerne. Og som jeg har forklaret i tidligere noter, så forestiller jeg mig altså her, at brugeren så særligt skal kunne rate termer/ressourcer (ift.\ pågældende prædikat) ved at trække termen op eller ned i listen --- og hvor brugeren kan zoome ind og ud i, hvor mange termer, der vises i listen, hvilket så altså meget vel ofte kunne indebære at indstille på, hvor mange termer vises i listen efter popularitet (altså mere præcist stille på, hvad min-værdien er for popularitet i listen). (Og jeg forestiller mig altså, at brugeren endda kan ``zoome ind og ud'' imens denne trækker en ressource/et term også.) 

Når brugeren så har rykket lidt rundt på termer i denne liste og derved givet dem rating i henhold til pågældende prædikat, kan brugeren jo så gå tilbage til, hvad brugeren egentligt var interesseret i, nemlig at se på en liste over sjove komediefilm. Slut på denne lille overordnede rundgang. Og så kan jeg skrive nogle flere detaljer om disse seneste idéer en anden dag (måske i morgen). (18:58) 

.\,.\,Hm, lad mig lige nævne, at når brugeren skifter type af visning, altså hvis denne f.eks. skifter fra ressource- til prædikat-visninger.\,. hm.\,. .\,.\,Hm, jeg skulle til at skrive noget med, at det ofte gerne må være standarden, at når brugeren går til en anden visning, at der åbnes en ny fane i browseren, men på den anden side er dette svært at sige præcis, hvad der er smartest her. Og måske er det fint, hvis brugeren bare husker at trykke ctrl (eller midterklik) på de rette tidspunkter og, nå ja, så kan brugeren jo altid gå tilbage i browseren --- især idet listerne som regel er simpel HTML, hvilket sikkert vil sige, at browseren endda tit vil kunne huske, hvor i listen brugeren var og dermed kan vende tilbage til samme sted.\,.\,:) 


(13.01.23, 16:40) Nu vil jeg så prøve at tilføje lidt flere detaljer om den opsætning, jeg nu tænker, at prototypen på den semantiske hjemmeside kunne have. Ja, og jeg kan jo starte med at sige, at jeg jo tænker at hjemmesiden kunne have flere applikationer i sig, bl.a.\ en YouTube-agtig applikation og en Reddit-/Twitter-/9gag-agtig applikation, hvor brugere så kan uploade ressourcer i forbindelse med de applikationer, som så derefter også kan findes i den semantiske applikation.

Og hvis vi så ser på den semantiske applikation, så forestiller jeg mig altså en ret central fold-ud-menu, hvor brugerne kan skifte visningen som beskrevet lige her ovenfor. Og når brugeren skifter visningen skal det så bare i prototypen føre til en ændret URL, hvilket så gør at brugerne kan gå frem og tilbage i deres seneste visninger, og åbne flere faner for visninger, ved simpelthen bare at bruge deres browsers velkendte funktionalitet hertil.

Fold-ud-menuen kan så have en liste af faneblade i toppen, der afgør, hvilket workspace brugeren arbejder i. Som sagt, så skal det være sådan, at hvis brugeren føjer en ting til et givent workspace i én fane, så skal ændringen ske i alle faner (som opdateres, når brugeren klikker ind på den igen (eller når brugeren går frem og tilbage i sin sessions browserhistorik)). Men ændringen skal dog kun ske i pågældende workspace.

Nedenunder denne liste kan så være endnu en liste over overordnede undermenuer i fold-ud-menuen. Den mest centrale undermenu er så den, hvor alle de tilføjede termer i workspace'et findes. Jeg forestiller mig, at en mulig løsning her for prototypen kunne være en træ-struktureret menu a la den jeg har her ude i venstre side af min TeXstudio-editor over sektioner og undersektioner (og hvor man så kan folde oversektionerne ind og ud). Sådanne oversigter har sikkert et teknisk navn, men det kender jeg ikke lige på stående fod. Og en endnu simplere løsning, måske til en tidlig version af prototypen, kunne bare være at have en enkelt liste ordnet kronologisk ift.\ hvornår termerne sidst blev brugt/tilføjet. I øvrigt kunne man også tænke sig en blanding af disse muligheder, sådan at man tilføjer endnu en fane af muligheder, hvor den ene så er omtalte træ-struktur-oversigt, og hvor den/de andre bare er lister over seneste brugte og/eller seneste valgte og/eller seneste tilføjet.\,. Ok.

Men udover at se lister over termer (som altså også inkluderer prædikater og relationer --- og også relationer og input-termer sat sammen til prædikater), som så kan bruges til at vælge nye visninger med, så skal der også være en (overordnet) undermenu med filter-prædikater, som nævnt i overordnede tekst. Og der skal være en menu specifikt beregnet til rating-visningerne (altså når brugeren iagttager en liste med udgangspunkt i et specifikt prædikat, som termerne i listen så kan rates i forhold til (gerne ved at brugeren kan flytte op og ned på termer)). 

En anden undermenu, der faktisk bliver ret væsentlig selv på nogenlunde kort sigt, er en hvor brugerne kan vælge indstillinger for, hvordan de gerne vil have diverse ressourcer og lister vist. Her bør man så faktisk ret hurtigt gøre det til en del af applikationen, at brugere kan aktivere CSS styles for diverse ressource-typer. (Og ja, det er en god idé, at man fra start af indfører diverse typer for de forskellige, ja, typer af ressourcer. Så applikation skal altså have et helt typesystem, hvor brugere kan tilføje nye typer, og hvor man så også har indbyggede list-constructor're til at danne list-typer. Her kan der så endda være forskellige typer alt efter om vi f.eks.\ snakker en standard oversigtsliste, eller om vi snakker om en liste beregnet til en rating-visning. Når en bruger opretter en ny ressource-type, så sker dette ved at definere et HTML-template for typen.\,. Tja nej, faktisk to HTML-templates: Et der udgør selve ressourcen, og et der udgør den data, der skal vises, når ressourcen indgår i en listeoversigt over flere ressourcer (f.eks.\ thumbnail og kort beskrivelse og sådan). Indholds-HTML-skabelonen bør så også have datafelter i sig med f.eks.\ titel, tilhørende tekst, og hvad man ellers kan finde på, og hvis ressourcen indeholder et billede eller en video af en art (måske i et eller andet fast format), så skal skabelonen selvfølgelig også indeholde de datafelter. Når så en type er oprettet, og brugere skal uploade andre instanser af typen, så skal de så bare uploade data af passende formater, der passer til felterne i HTML-skabelonen. Og når brugere så får vist ressourcen, så sættes alle disse datafelter altså ind i HTML-skabelonen. Og når brugerne for vist en liste over ressourcer af den pågældende type, så vil elementerne i listen vises i form af den anden HTML-skabelon, hvor det passende data (så som f.eks. thumbnail og kort beskrivelse) er loadet ind. Og for så at vende tilbage til den undermenu, som vi startede med at snakke om, så skal brugere altså gerne kunne indstille, hvordan diverse typer helt præcist skal vises, bl.a. ved at vælge CSS-indstillinger. Og på sigt (gerne kort sigt) skal brugerne endda og også få mulighed for at ændre i og lave tilføjelser til selve HTML'en (ligesom browserudvidelser fungerer), sådan at de kan få vist det lige som de vil have det. 

I øvrigt, og det gælder både, når brugere opretter HTML-skabelonerne, og når brugere opretter udvidelser til eksisterende HTML-skabeloner, så vil det være godt på sigt at få det sådan, at brugere kan lave (div-)felter i skabelonerne, som faktisk får mulighed for at query'e selve hjemmesidens database, og altså vise ting fra denne database. Dette kommer til at gøre, at man kan få levende ressourcer, hvis udseende og struktur kan afhænge af databasens tilstand. For eksempel kunne man lave HTML-skabeloner, der automatisk henter lister over mest relevante ressourcer til pågældende ressource, eller henter kommentarer, ratings, annotationer osv.\ osv.


Det skal så dog nævnes, at den semantiske applikation allerede indeholder en måde, hvorpå brugere kan kommentere ressourcer, nemlig ved at tage termen, der udgør ressourcens reference, og så vælge en relation i den forbindelse, således at man danner et prædikat: er\_kommentarer\_tilhørende\_ressourcen(term), hvor `term' så her er en placeholder for på-gældende ressource. Herved kan man altså gøre kommentarer til ressourcer til en del af den samlede (semantisk strukturerede).\,. database, eller rettere: selve den underlæggende database bør jo (nok) bare være en relationel database, men udefra set får man altså en semantisk struktureret database, og det er altså det jeg mener, når jeg siger en semantisk struktureret database; det er sådan, den ser ud fr brugerne. 


Nå, tilbage til fold-ud-menuen: Derudover kunne det nok også være smart (på sigt) med menuer, mest for de avancerede brugere, til at vælge opsætningsindstillingerne for selve fold-ud-menuen, sådan at selv denne bliver en del af alt det, som brugerne i sidste ende selv kan bestemme opsætningen for.

Ok, så det var altså den overordnede struktur.\,. Nå ja, på nær at jeg også lige kan nævne, at man på sigt også kan åbne op for, at selve applikationen får sin egen måde at have gang i flere visninger på én gang, sådan at dette ikke bare kun opnås via browserens funktionalitet (og ved hele tiden at skifte URL for hver visning). Så på længere sigt skal man måske også gøre, så at brugere kan implementere sådan navigation, og også gerne mulighed for f.eks.\ split-screen-visning, i applikation selv. 

Okay. Det var den gennemgang. Så havde jeg vist lige en eller flere ekstra punkter, som jeg også gerne ville nævne (men som jeg dog har talt om i tidligere noter), hvis jeg ellers kan huske, hvad de var.\,. (Og hvis det altså ikke bare var dem, jeg allerede har nævnt nu her.\,.) .\,.\,Nå, hvis der ar noget mere, så er det lige glippet, så jeg vil bare gå og summe lidt over det, og så ellers bare vende tilbage, når jeg finder nogen tilføjelser, jeg også bør nævne. (17:44)

(14.01.23, 11:48) Okay, der er nogle tilføjelser, jeg mangler, og jeg har også tænkt på lidt nye ting. Jeg mangler at nævne, at al data gerne skal gemmes via, hvad der svarer lidt til tripletter, men hvor de dog ikke behøver at være begrænset til 2-nære relationer; de kan også være 1-ære (altså prædikater) og 3-nære. Der må så vidt jeg kan se også være relationer med endnu flere input en tre, hvis brugerne får behov for dette. Og foruden relation-ID og ID på alle input objekterne (hvilke også kan være placeholders, for man skal f.eks.\ gerne kunne danne prædikater ud fra (f.eks.) en 2-ær relation og en inputterm, so vi har set ovenfor), så skal der også i alle disse udvidede tripletter være bruger-ID for hvem, der siger/har uploadet udsagnet, samt også et tal, der bestemmer hvor meget brugeren mener at udsagnet er sandt, og som i øvrigt også kan være negativt, således at brugeren kan negere udsagnet uden at skulle skifte relation. Dette med at alle udvidede tripletter, som jeg fra nu af vil kalde udsagn, har sådan en floating point rating med sig, er faktisk en super vigtig ting for hele idéen. Uden dette ville applikationen ikke blive nær den samme. .\,.\,Og endda selv for mange relationer, hvor man umiddelbart tror, at man kun er interesseret i at høre en sandt-eller-falsk vurdering fra brugere, kan det alligevel være gavnligt med et floating point-tal --- ikke for alle tilfælde (og så må man bare omfortolke tallet til en binær størrelse), men for mange. For eksempel hvis vi tænker på et udsagn: ``hører denne term til en vis kategori eller ej?'' (som underkategori eller som genstand, der hører til kategorien). Her skulle man tro, at man bare var interesseret i et ja-nej-svar fra brugerne, men faktisk kan det her være rigtigt gavnligt, hvis udsagnene alligevel kan gradbøjes, for nogle underkategorier (eller ressourcer) bare mere relevante end andre, og hermed kunne man altså få en nem måde at gøre, så at når en bruger ser på underkategorier til en kategori, så er det de mest relevante underkategorier, der popper frem øverst på listen. 

Jeg fik heller ikke nævnt noget, jeg ville sige om mine ``brugergrupper,'' som jeg har skrevet en del om i tidligere noter (se disse). Angående dette emne, så tror jeg bare man skal starte med den type ``brugergrupper,'' hvor en vis gruppe af brugere (muligvis bare én) starter med en lige mængde af nogle delelige tokens, der giver dem stemmevægt ift.\ at bedømme udsagn. .\,.\,Hov vent, jamen \emph{skal} vurderingerne så komme med udsagnene, eller skal udsagnene gemmes for sig, og så kan brugeres vurderinger af dem gemmes for sig også.\,.\,? Hm.\,.\,. (12:12) .\,.\,Hm, og brugeren, der uploadede udsagnet, bør i så fald også bare gemmes som et separat udsagn, oprettet automatisk af serveren.\,. hm, så man også kan slette det igen uden at slette brugeren, men hvordan ser man så.\,. Nå jo, i princippet kan serveren så oprette to udsagn for at gemme, at brugeren var ophavsmanden; udsagnet selv og så et udsagn om, at denne server siger, at det var den bruger (med 100\,\% sikkerhed, hvorfor ikke?\,.), der uploadede det originalt (eller rettere set med pågældendes servers øjne (men den siger så ikke noget om, hvorvidt andre brugere var først på andre servere)). Ja, sådan kunne det sagtens være.\,. Hm.\,. \ldots Ja, så nu går jeg faktisk ind for tripletter igen (har jeg ellers ikke gjort i lang tid, mener jeg).\,. .\,.\,Hm, og man kan så faktisk bare droppe de der server-vurderinger,
hvis man bare i stedet gør sådanne, at visse relationer er off-limits ift. hvad brugerne selv kan uploade, nemlig de relationer som er beregnet til at blive ``uploadet''af serveren(erne). 

Nå, men tilbage til ``brugergrupper.'' Omtalte tokens kan så deles i flere, og de kan så efterfølgende gives (delvist) ud til andre brugere. En giver af tokens i en brugergruppe må i reglen godt altid annullere en overførsel af tokens, medmindre.\,. Ja, lad mig sige det sådan her: Tokens kan lånes ud til andre brugere, som kan låne dem videre ad libitum, og hvis så en udlåner af tokens skifter mening, kan denne hive sine tokens tilbage med det samme (uanset hvor mange gange, de er lånt videre). Brugere kan også \emph{give} tokens til andre brugere, nemlig hvis de gerne vil pensioneres som ansvarshaver i brugergruppen. Hver token har så en et floating point number, der bestemmer stemmevægten, som gives af denne. Den samlede stemmevægt summer så op til 1. Og bum, så har man allerede et ret effektivt system til at danne diverse grupper.

Pointen med ``brugergrupper'' er så, at brugere skal have mulighed for, når de indstiller et filter-prædikat, at vælge hvilken/hvilke brugergrupper, som vurderingen skal beregnes ud fra. Så i pågældende menu skal der altså gerne for hver filter-prædikat være en knap til at folde en liste af brugergrupper ud (inkl.\ den basale, hvor alle brugere bare har én stemme), og hvor man så, muligvis ved at indstille vertikale barer tilhørende hver brugergruppe, kan indstille sin vægt til hver brugergruppe i filteret. 

Ok, det var allerede rimeligt dækkende, men jeg har stadig en del flere ting, der skal nævnes, og faktisk også som skal overvejes. Men lad mig starte med en positiv ting, og det er, at brugerne via de ovenfor omtalte HTML-skabelon-udvidelser kan implementere diverse knapper og fold-ud-menuer, når ressourcer af en vis type vises i en liste. For eksempel skal der som en standard være en ``udvidelse'' (som dog altså er standard) til visningen af kategorier i lister.\,. lad mig sige `visning af kategori-\emph{referencer}' fra nu af, som giver mindst to knapper: én hvor brugeren bliver ledt hen til underkategorier (og får kategorien tilføjet til workspacet, om ikke andet så i menuen af `seneste termer' (af typen `kategori')), og én hvor de brugeren bliver ledt hen til en visning over ressourcer/termer i kategorien. .\,.\,Nå ja, og der skal så også gerne være en knap, der leder brugeren hen til en visning over prædikater, der relaterer sig specifikt til den kategori. Denne ``udvidelse'' skal så gerne oprettes, inden applikationen åbnes op for almindelige brugere. 

Og lad mig lige indskyde, at filter-menuen gerne skal have nyligt tilføjede prædikater vist i toppen, da man må regne med, at brugere ikke nær så ofte vil behøve at stille på de typiske prædikater. Men hvis brugere alligevel har nogle typiske prædikater, de gerne tit vil stille på, så skal de så bare have lov til at ``pinne'' dem til toppen af menuen, som man siger. 

\ldots Hm, nu overvejer jeg, om der overhovedet er så mange flere ting, der skal siges i denne omgang, for jeg synes egentligt, at løsningen med hurtigt at arbejde i HTML-udvidelser til referencevisningerne faktisk løser meget af det problem, jeg havde, med at det virkede for indviklet at bruge applikationen i udgangspunket. Jeg tænker lige lidt mere over det, men nu vil jeg ellers bare lige tilføje den idé, at der også skal være en (overordnet) undermenu i fold-ud-menuen (som mange brugere i øvrigt sikkert vil have konstant foldet ud, hvis de arbejder på en computer (ikke en telefon)), som simpelthen er en konsol, hvor brugere kan skrive de udsagn (med placeholders), de gerne vil søge på, og også dem de gerne vil uploade, og hvor der så bør være automatiske forslag til udfyldning af ordene, hvor applikationen altså så (primært) søger i termerne i brugerens workspace, når den skal give forslag til udfyldning af ordene (altså `word completion'). (13:39)  

\ldots\ Der vil også være behov for tokens, der ikke kan videregives af modtager (og som modtager i øvrigt ikke skal acceptere eller afslå, men bare får tildelt sig af en anden). Dette kan f.eks.\ bruges til at flagge spammere; så kan de styrede brugere uddele tokens, enten til alle spammere, eller alle ikke-spammere (som man ikke har mistanke til). Brugere kan så bruge disse bruger-grupper i et er\_spammer- eller et kommer\_fra\_en\_spammer-filterprædikat. (18:03)

(16.01.23, 16:30) Der skal også være en slags automatiske brugergruppe tokens, men nærmere bestemt kan man også kalde dette for `automatiske point.' Vi snakker altså point, som kan gives til brugere --- eller til alle mulige andre termer i databasen, nemlig (og bruger-ID'er indgår nemlig også i databasen som termer) --- ud fra data i databasen om dem. Dette kan så specifikt bruges til at implementere, hvad jeg også i mine tidligere noter har kaldt `brugerdrevet machine learning (ML).' Hvis vi så tager en vis korrelationsegenvektor, når man har lavet ML-statistik over brugerne af applikationen, så kan man altså nu, via disse `automatiske point,' tildele brugere point alt efter, hvor stor projektionen af deres brugerdata ind på på gældende egenvektor er. Og dette kan man jo så bruge videre i diverse filter-indstillinger. For eksempel kunne man forestille sig, at man kunne sige: `sorter disse film ud fra `sjovhed,' med særlig vægt på folks vurderinger, som følger den og den korrelationsvektor. Og ja, man kan i øvrigt også gøre mere simple ting, så som bare at sige: `sorter disse film ud fra `sjovhed,' med særlig vægt på folks vurderinger, der også synes at det og det var sjovt --- altså en mere simpel form for statistisk brug a brugerdata i brugerlavede filteralgoritmer. Mulighederne er virkeligt åbne.

Og måden man så kan query'e databasen om sådanne point kan så bare være ud fra en syntaks, der følger det logiske programmeringsparadigme, nemlig således at pointen både gemmes som tripletter i databasen for hver bruger, og hvor man så kan query'e disse point via den relation, der nu hører til pointene. (16:45) .\,.\,Og lige for at præcisere, så er det altså brugerne selv, der kan uploade forslag til nye automatiske point, nemlig ved at de så uploader den relevante metadata, samt den formel, som det hele handler om, nemlig den formel ud fra hvilken de automatiske point bliver givet til termerne (som i øvrigt i reglen vil være af en bestemt type, som så også defineres som en del af omtalte metadata (eller `header-data' er nok mere rigtigt at kalde det.\,.)). 

(17:13) Jeg havde også i sinde at skrive lidt om, hvordan jeg så forestiller mig, at man også kunne implementere en ``debatside-applikation'' i dette system, hvor brugerne kan bruge dette brugergruppesystem, og jeg kunne måske også finde på nogle små nye tilføjelser i denne forbindelse, men jeg tror nu, jeg bare vil lade emnet være for nu. Jeg har skrevet fint om det, i mine tidligere noter, og selvom jeg sikkert kunne finde på nogle små ting at tilføje, så tror jeg ikke, det vil ændre så meget på idéen overordnet set. Så lad mig lade det emne være for nu. Det ville alligevel også skulle implementeres \emph{efter}, at man får den semantiske applikation op og køre.

Jeg bør også på et tidspunkt vende tilbage her og overveje nærmere, hvad jeg skal begrænse en prototype til, for lige nu har jeg nævnt væsentligt flere features, end man bør prøve at få med fra start af. Hm, jeg mener dog, det vil være en god idé, hvis man så hurtigt som muligt gør, så at brugerne selv kan lave omtalte HTML-udvidelser. Men ellers er det nu godt bare at starte simpelt, og så bygge på derfra. (17:20)

\ 

(24.01.23, 11:39) Man kan sagtens bruge triplet-konstruktioner, man kan også sagtens tillade +2-ære relationer, hvis man vil, det betyder ikke så meget, for man kan altid omstrukturere, hvis man finder ud af, at der er en bedre standard. Så længe brugerne har frihed til at skabe de konstruktioner, de vil, og at grammatikken i disse konstruktioner er veldefineret, så går det fint. To gode muligheder er derfor, 1, kun at tillade 3-term-konstruktioner (tripletter), eller at gøre det helt frit ligesom i en logisk database.

Nu kommer det mere vigtige, dog: I første omgang handler det hele om at konstruere `udsagn' (som brugerne så efterfølgende kan rate (bedømme)). Men disse udsagn skal så \emph{ikke} tolkes som i standard formel logik, hvor har en binær (boolsk) værdi, så at sige. I stedet skal de ses som termer, der beholder informationen om hele deres indre konstruktion, når de indgår i en kontekst som sammensat formular. Hermed for man nemlig særligt mulighed/lov til at sige ting som: ``Jeg synes udsagnet er\_sjov(film) er sand til en grad af 9/10 rating score.'' Man åbner altså herved op for en meget mere intuitiv måde at kontruere sætninger på, en hvis man skulle beskrive den samme sætninger ud fra mere formel logik, hvor er\_sjov(film) altså bare ville være en binær værdi over alt, hvor den indgår, og hvor denne sætning så ikke ville give nogen mening. Man kan så også konstruere mange volapyksætninger med denne semantisk, og kan altså lave ugyldige og eller paradoksale sætninger, men det er kun et sundt tegn, ift.\ hvad sproget skal bruges til. 

De helt centrale udsang, som nemlig er de eneste udsagn, der repræsenterer direkte \emph{sandheder} i systemet, det er så rating-udsagnene. Disse består af et bruger(-ID)-subjekt, et udsang-term (så som `er\_sjov(film)') og så en rating score. Tja, eller dvs., der mangler én mere information her, og det er informationen om, hvad konteksten er for ratingscorens talværdi. Her kan man så eksempelvis, hvis man tillader +2-ære relationer, som jeg så forresten i virkeligheden er funktioner, fordi alle udsagn er termer (og disse funktioner vil så i øvrigt også som regel være bijektive, i henhold til hvad jeg lige nævnte om, at information ikke bør gå tabt i udsagnene (indmaden forsvinder ikke, med andre ord; hvert udsagn ``kender'' sit eget udsende på papiret)).\,. hvis man tillader 2-ære relationer, så kan man så tilføje en funktion, der sender et udsagn så som `er\_sjov(film)' til et nyt udsagn, der siger ``er\_sjov(film) ud fra ratingscoren $x$,'' hvor $x$ så også er input til omtalte funktion. Nå ja, så lige med dette eksempel er der så ikke behov for +2-ære relationer, eller funktioner rettere, men hvis man skal kunne sådanne ting, så bør man dog kunne have funktioner af alle mulige typer (og ordner). *(Hm, så never mind, at tripletter er fine for systemet. Jeg synes faktisk, at databasen bare mere bør være som en logisk database.)

*Hov, jeg skal forresten lige nævne, at de eneste restriktioner i databasen, udover på hvor meget data og hvor mange termer hver bruger må uploade hver dag/uge/måned, hvilket i øvrigt vil være en god idé at have, nok ellers bare skal være, at det kun er brugeren selv, der må uploade rating-udsagn, hvor brugerens bruger-ID indgår. Så hvis en bruger-udsagn-rating-instans forekommer i databasen, så er det fordi pågældende bruger har givet den rating. Hver bruger skal også have mulighed for at slette sine egne ratings igen fra databasen (hvorfor det faktisk ikke dur med et decentralt system, synes jeg). I den forbindelse kan jeg så også nævne, at man godt kan få det sådan på sigt, at ressourcer faktisk automatisk sender rating-information til databasen på brugerens vegne (hvis brugeren har valgt de ``HTML-udvidelser,'' der gør dette), hvilket f.eks.\ så kan bruges til at sende, om brugeren har set en vis ressource eller ej (hvilket kan bruges til at fjerne gengangere i et feed, f.eks.), men så kan brugeren altid bare selv fjerne sådant data igen efter eget behov. Nå ja, og herved er det jo så også vigtigt lige at nævne, at brugerne sagtens skal kunne uploade data til deres egen private del af databasen, sådan at f.eks.\ ikke alle kan se den data om, hvad brugereb har set og ikke set, eller kan se alt hvad brugeren har ratet for den sags skyld. Brugerne kan sagtens rate rent anonymt, og så bare kun bruge den data til at forbedre egen oplevelse, hvis de vil. Men de kan så selvfølgelig også vælge at offentliggøre noget af denne data (dog med mulighed for at slette det igen fra den offentlige del af databasen), hvis de gerne vil bidrage til, hvad andre brugere ser af brugerratings (hvad de fleste brugere gerne vil; det er typisk en vigtig del af, hvorfor vi normalt afgiver ratings rundtomkring på internettet). (12:42)

Men kommer brugerne ikke bare til at bruge den samme type rating hele tiden? Nej, jeg har nemlig tænkt på, at det vil være rigtigt gavnligt, hvis brugerne selv kan vælge, hver gang de rater, om de bare vil rate ud fra en treværdi-score, nemlig negativ, neutral, positiv, eller om de vil bruge flere muligheder, f.eks. fem stjerner eller ti stjerner, eller hvad det kunne være. Og her snakker vi så ikke den rating, jeg har beskrevet, hvor brugerne rater ud fra en liste med en masse andre relevante ressourcer i. Her snakker jeg, hvis brugerne bare skal rate ressourcen alene i dens egen kontekst, enten på ressourcens egen side, eller måske når den vises i en liste. 
\ldots(12:42) Her er det jo så vigtigt, at brugerne så kan sætte dette i system, så de alle kan kende forskel på, hvad den pågældende ratingscore repræsenterer. 

Jeg har skrevet ovenfor et sted, at man kun skal have termerne ordnet ud fra det (filter-)prædikat, man rater med hensyn til, når man rater ressourcer ud fra en liste, og at alle andre filter-prædikater så kun skal være et spørgsmål om at sortere ressourcer fra i listen. Men dette behøver ikke nødvendigvis at være sandt. Man kunne således godt tænke sig, at.\,. Tja, eller rettere, det skal nok være det samme prædikat, så som eksempelvis `er\_sjov()', men det behøver ikke nødvendigvis kun at være én type ratingscore, man så bruger til ordningen. Men kunne således godt forestille sig en ordning, hvor både folks negativ-neutral-positiv-ratings er med, hvor femstjernede og tistjernede (etc.) ratings tæller med, og hvor selvfølgelig folks rating-ud-fra-en-liste-ratings også tæller med. Så kan de forskellige typer scorer indgå på forskellige måder ud fra den endelige sortering efter brugerens eget behov, men når brugeren så afgiver sine egne svar, jamen så er det så bare rating-ud-fra-en-liste-scoretypen, der bliver valgt for den rating, der uploades til databasen herved. .\,.\,Hvordan man så blander de forskellige ratingtyper sammen til en enkelt sortering på en fornuftig måde, det er så en noget mere kompliceret sag, men det er også lige meget her, for det er nemt at overbevise sig selv om, at det må kunne lade sig gøre på en fornuftig måde (især hvis man finder en god måde at implementere ``usikkerheder (statistiske) / spredninger'' på i forbindelse med diverse ratings). 

.\,.\,Lad mig lige tænke over, hvad jeg ellers gerne vil sige noget om.\,. (13:07) .\,.\,Hm, jeg kunne sige, at hvis man så bruger, hvad der svarer til en logisk database (i hvert fald i interfacet med brugerne), så vil man så skulle lave en query-API, som brugerne kan bruge i ``HTML-udvidelserne,'' der svarer til logiske query-sprog, men det giver jo næsten sig selv.\,. .\,.\,Hm, det var faktisk muligvis alle de tekniske ting, jeg ville nævne for denne omgang. Så har jeg også nogle mere overordnede ting, som jeg tror, jeg vil skrive på engelsk under en ny sektion. Så kan jeg jo derfor også bare vende tilbage hertil, hvis jeg har nogle tilføjelser til nogen af disse tekniske ting. (13:18)

*Jo, der er faktisk lige den tilføjelse, at min SRC-idé jo også kunne virke godt til denne idé, nemlig især hvis man gerne vil have et fastansat hold af programmører til siden --- og i det hele taget når det kommer til, at man jo gerne, efter min mening, skal have nogle centraliserede databaser, som så kan underskrive kontrakter om ikke at offentliggøre eller videresælge privat data fra brugerne, samt at brugerne skal kunne bede om at få deres data slettet.

*(17:05) Dette er ikke en teknisk tilføjelse, men en bare en kort tilføjelse omkring min ``debatside-applikation.'' Jeg nævner det nok igen i Sektion \ref{Some_hopes_in_terms_of_my_ideas}, men angående min debatsideidé, så kan det godt være, at den ikke rigtigt vil.\,. take off.\,. få god ind i sejlene, før at den videnskabelige verden virkeligt kommer med og deltager i hele projektet med at strukturere, i dette tilfælde viden og videnskabelige diskussioner, i semantiske grafer. Så jeg håber altså på, at den videnskabelige verden vil benytte denne semantiske teknologi mere og mere, og herved vil det jo så selvsagt være relevant at afholde sådanne diskussioner, som jeg tænker dem i forbindelse med, hvad jeg kalder min debatside-idé. Et alternativ ville være, hvis mine tanker omkring e-demokrati fik god vind i sejlene forinden, så tror jeg også, at dette kunne blive en vej til, at få godt gang i sådanne graf-diskussioner, som jeg tænker dem.\,.

*(26.01.23) Vil lige for det første kort nævne, at jeg synes, det giver rigtig god mening, hvis man i bruger-udsagn-rating-entiteterne også har et flag / en reference til, hvordan ratingværdien skal tolkes. For det synes jeg nemlig giver bedst mening rent fortolkningsmæssigt: Så kan man nemlig se udsagnet for sig som sin egen ting, og se ratingen samt dennes fortolkning som sin egen ting også. (12:52)

*(12:52) Nå, den næste, mere vigtige ting, jeg vil tilføje, er, at jeg nu er kommet på, at brugerne måske også skal kunne tilføje URL-RegEx'er til at scrape indhold fra andre sider. Og når man så har disse scrape-formler, så kan brugere så tilføje specifikke inputs til disse RegEx'er, der giver en gyldig URL til en eksisterende Web-ressource. Og så kan databasen i princippet bare gemme selve inputtet, og når brugerne så iagttager pågældende ressource, så kan hjemmesiden bare hente de relevante URI'er fra sættet af de fuldendte URL-RegEx'er og indsætte dem på hjemmesiden. Så dette kunne altså være en hurtig måde at få en hel masse indhold på siden.

*(12:59) Samtidigt åbner dette så også op for, at man ret nemt kan lave et ``overlay,'' som jeg før har kaldt det i mine noter, nemlig en slags annotationsudvidelse til browseren, således at en bruger kan folde en menu ud og se relevant data, særligt ``rating-tags'', som jeg har kaldt det, for en ressource de browser på en anden hjemmeside. Så når brugerne browser andre hjemmesider, kan de altså få adgang til den ``rating folksonomy,'' der tilhører semantik-hjemmesiden. De kan også få adgang til anden data, så som relevante links, inklusiv den/de oprindelige kilde(r) til ressourcen, og også diverse advarsler, f.eks.\ om NSFW/NSFL, og ikke mindst også om `misinformation.' Disse muligheder kunne muligvis være med til at gøre semantik-applikationen/hjemmesiden populær lynhurtig i sammenligning med, hvis brugerne kun kunne få alle disse muligheder, når de browser semantik-hjemmesiden selv. Man kunne i øvrigt også sælge hele denne idé denne vej rundt, altså hvor man starter med at sælge idéen om et tværgående system, hvor brugere kan få adgang til en ``rating folksonomy,'' der fungerer på tværs af alle mulige hjemmesider, og hvor brugerne også kan få andre links og advarsler herved, og så derfra pointere, at denne applikation så også bør have sin egen hjemmeside, hvor brugere kan se alle tingene samlet, endda på en semantisk struktureret måde, der giver et meget effektivt overblik over alting. 
*(RegEx'erne skal jo også gerne hente al mulig gavnlig metadata såsom titler osv., hvis de kan, og dermed bliver det også relativt nemt at finde en ny passede URL (og måske med nye RegEx'er) til ressourcen, hvis nu hjemmesiden, hvor den er hentet fra i første omgang, skulle ændre sig.)

*(13:11) Nå, og den tredje/fjerde ting, jeg også lige ville nævne her, er omkring trust-fordeling. Når det kommer til brugere, der tilføjer nye ting i applikationen, jamen så vil det være rimeligt nemt at sortere skidt fra kanel, for så kan brugere bare stige i tillid, jo mere de tilføjer, der bliver godkendt af andre brugere, men ellers vil det altid også bare være sådan, at sensitive og/eller utålmodige brugere bare kan have et ret skrapt filter for nye tilføjelser, der altså kræver mange up-votes, før de selv får tilføjelsen at se, og mere engagerede brugere kan så have mindre skrappe filtre og dermed blive mere aktivt med til at godkende og afvise nye tilføjelser. Og angående tillid ift.\ bruger-ratings, så kan man sikkert komme rigtig langt i starten ved bare at antage, at alle konti repræsenterer en unik bruger, og så snart siden vokser bare lit, så kan man begynde at implementere et friend-of-a-friend-system, således at man kan begynde at skille bots fra. Så jeg tror dermed faktisk ikke, at dette bliver så svært, og jeg tror således heller ikke, at mine simple, token-baserede ``brugergrupper'' beskrevet ovenfor bliver særligt nødvendige.\,. .\,.\,Og det er rart at man applikationen ikke er tvunget til at vante på, at brugerne for oprettede sådanne halvavancerede systemer (som kræver noget brugerengagement) til at uddele trust, før at applikationen kan blive god og gavnlig. 



\section{Overall selling points for my Web 2.0--3.0 ideas}

(13:19, 24.01.23) This section is best understood if one has read my previous notes on the subject, including the ones above in the previous section (in Danish). But the notes might make some sense still, even without having read my previous notes.

%(13:22) Jeg tager lige en hurtig pause og tænker lidt over, hvad jeg vil sige i denne sektion... ...(13:52) Okay, lad mig prøve at fortsætte.

One little idea, which can be implemented on any conventional Web 2.0 site as well, is my idea for ``rating folksonomies.'' The idea is basically to attach a rating bar to all tags. Even if the site does nothing further about this technology, it will still be nice for users to be able to see a rating score along all the tags that they observe on the site. On from there it is a very short step to start using this extra data to make searching better on the site, and to make feed algorithms better. Thus, it is a very simple, and I believe very useful, idea that is easy to get going with.

The next idea is to have user-driven search filter / feed algorithms. This idea is more complicated, but I have described how it could work in the notes of the previous section. The big selling points here, are that users will then be able to not just get a single feed algorithm (as well as some very simple search filters) that depends on the user's data, but can always choose from an array of algorithms. This gives the users much more freedom of customization on the site, and in a way such that they can shift between different customizations depending on what they are interested at in the moment. Now, if we think about YouTube in its current state, this is a very good example of a quite terrible user experience with the feed algorithm (in my opinion as well as some other's). I am actually afraid to click on anything that I don't recognize, since I am afraid that that will then trigger a whole bunch of stupid recommendation in the future, where I have to repeatedly click `not interested' in order to get that ``contamination'' out of my feed again. Such ridiculous situations will all be past with a more user-driven system such as the one that I imagine. Here users will instead be able to control their own algorithms quite effectively, and will be able to use a wide array of different sittings depending on what the are interested in at the moment.

User-driven algorithms also means that the users don't even have to feed the machine a lot of their personal data in order to get the feed/search result that they desire. They can instead stand on the back of other users' commitment and simply use their preferences. A
Or they can keep their own data completely private and still use it to find out about, where they lie in the general space of user preferences. They can also at anytime open up completely anonymous accounts and import preferences from other accounts (without having to reveal this data to the public). The users will thus be able to get much more freedom in how the use their data, and will have much more control and ownership over it.

And continuing on the topic of feed/search algorithms, I certainly believe that these will get a lot better for the individual user when there is a large open source community behind them (and when the user has so much ability to use a wide range of settings). This is finally a point where the open source part of the idea starts to get important. Once suc a site takes off, the sheer number of users who want to contribute to better making better search/feed algorithms will make the possibilities far exceed what any private team of web developers can muster for their users. This is just my belief, but yeah, I believe it quite strongly (perhaps with the only exception if AI is going to come in and help provide super good and varied feed algorithms somehow.\,.). .\,.\,But yeah, most likely, a big open source community around such an open source site as what I have in mind will be able to achieve much better things than what we know currently.

Another idea where the open source part is really important, is about having the users be able to change the appearance of the site themselves. Again, we are then talking about a case where the site has already taken off and has gained a very big user network around it. One it has this, this open source community will be able to achieve much more in terms of the usability and nt least the ability for customization than what a limited team of web developers can achieve. And here I should mention that part of my ideas related to this subject is that users are able to choose different settings for different types of resources. Each users can thus customize the appearance of the various interfaces on the site in a modular way. (See my previous notes for more details.)

Moving on, another big selling point about my ideas has to do with having a semantic (I guess) structure of all the resources contained (and referenced) on the site such that all resources can be found in a tree (or graph, rather) structure of categories and subcategories and so on. Hm, well perhaps `semantic' is actually not the right term to use here.\,. %(14:35)
.\,.\,Hm well, yes, I guess it is, cause that is essentially what I achieve with having users being able to also choose more and more filter predicates at the same time as they browse the category tree, such that the end up choosing both a subcategory for the resource they are interested in as well as a bunch of predicates which can specify the resource they are interested in further. %(14:39) ..Går lige en lille tur, før jeg fortsætter. (14:46)

%(16:06)
If keyword searches was never invented, such semantic catalogs would probably have been how we would have ordered resources on the web. Such a resource graph might then have been centralized to begin with, but at one point, an open and user-driven implementation would have become popular. I am glad we keyword searches was invested, but I really think we have been missing out on something big all this time. I really think that such user-driven resource graphs over the contents of the web would have been such a useful thin to have. Whenever a website has content added to it, the author of that content would have wanted it to add a reference to this content in whatever semantic ressource graph would be popular at the time, such that more people would see that content. And since the Web of Trust ideas are quite similar to my ``(user-driven) user groups'' in a lot of ways, I am pretty sure that we would have also quickly implemented ways for users to choose different ways of applying trust when it comes to rating useful contributions to said semantic resource graph. 

Anyway, I think that having such (user-driven) structured graphs over all the content on the web will be such a useful thing to get in the future, and I see (as I have described in the previous section) how I site such as the one I have in mind could be one way to get there. So to add a selling point to the list: Having such a semantic structure of content like the one I have described in the previous section could become a massive thing in the future. 

And the last point I want to mention, which is related to the last point, is that I think it will also be very useful to have content and data related to any specific resource ordered with a similar semantic structure. Having ordered comment sections would thus potentially be quite a nice thing to have, and on top of this, there is also related data/content such as related links, annotations, source material, etc., which could benefit from a similar user-driven, semantic structure. (See my previous notes for more details.)

So that concludes the list of selling points that I have in mind. The first ones are something that a regular Web 2.0 site might also be able to implement to some degree. This is kinda unfortunate for someone like me who thinks that the open source way is the ultimate way to go, for if these selling points could also only be realized on a much more user-driven site, it would sell the idea of such user-driven sites all the more. However, my hope is that other people will see the big potential in starting up an open source site which sets out to realize more and more of all these mentioned ideas, and that we can thus get a big community going around starting up such a site. This community will then consist of mainly open source programmers initially, and there will be a time where the site has yet to attract users from the general public, but once some of the first points mentioned in this list of ideas starts to become realized, users will slowly migrate from other Web 2.0 sites and start using this open source version more and more. The following network effect will then mean (as I foresee it) that some of the last points on this list of ideas will start to become realized, and from there, the sky is the limit. %(16:32) 

(13:22, 26.01.23) Let me also quickly mention the point that I think my system where users rate things by moving them around in a list will be good at encouraging users to give a lot of (valuable) rating data to the system. 

(13:38) In accordance with what I have just added in the previous section, there is now also the selling point, that the application will be able to be used on all kinds of other websites as well, namely such that users can see semantic data about resources they are viewing while browsing other sites. 

(15:10) And just to make clear, there is also another great point, which might not be so easy to ``sell'' since it is hard to argue that things will go according to how I imagine them, but which is really the big underlying reason why I'm so interested in all this. The point is that I believe that this technology can get us to a point where all of science can also be structured in a great semantically linked graph such that is becomes easy to look at all point and counterpoints to a given question, and to look at all existing solutions to a problem (and see arguments for their benefits and drawbacks). The same can also be said for open source programming: I believe we can get to a point where all programming solutions (modular) can be ordered in a great semantically linked graph. I believe that my ``Web 2.1'' ideas here, as we can call them, potentially might be able to bring about such a future, and I really think that this will mean so much for our scientific (and societal) advancement.\,.\,! %(Let me by the way mention here in the comments that I have thought about this today and reconsidered if I still really believe that my Web 2.1 ideas can lead to this, and luckily I have sort of arrived at the point where I think I will double down on that belief. For the way I see it, having a semantic graph over web content can very well become very popular, and this might very well further lead to the scientific --- and open source programming --- community/ties also making use of this technology to structure all scientific knowledge and discussion (each individual scientist (or programmer or amateur) taking part partly of selfish reasons to make their work reach a larger audience). And once such a well-structured graph becomes a reality, I believe this will... Hm, let me actually write this in the rendered text instead.. )
Let me by the way mention that I have thought about this today and reconsidered if I still really believe that my Web 2.1 ideas can lead to this, and luckily I have sort of arrived at the point where I think I will double down on that belief. For the way I see it, having a semantic graph over web content can very well become very popular, and this might very well further lead to the scientific --- and open source programming --- community/ties also making use of this technology to structure all scientific knowledge and discussion (each individual scientist (or programmer or amateur) taking part partly of selfish reasons to make their work reach a larger audience). And once such a well-structured graph becomes a reality, I believe this will greatly increase people's --- scientists/programmers as well as all other people --- ability to look up specific knowledge and to engage in discussions and innovation/solution-finding processes. I thus see that this technology can maybe sort of create a giant online collective intelligence --- not an artificial intelligence, but metaphorically speaking still a big collective brain. These are large words, but I really do think that such technology will give us intellectual powers as a civilization that is many times greater than what we have now. Anyway, I hope so. Hm, I guess that this paragraph belongs more in section \ref{Some_hopes_in_terms_of_my_ideas} below than here, but let me keep it here and simply copy (not cut) and paste it below there as well.\,.




\section{Flere tanker om min semantik-applikation}

(30.01.23, 9:51) Jeg var lidt begyndt at second guess'e, hvor stor en gennemslagskraft min hjemmeside/applikation vil kunne have, men nu føler jeg faktisk virkeligt, at jeg har fundet den røde tråd igen, som gør, at jeg igen virkeligt tror, at det vil kunne bliv kæmpe stort, og på ikke så lang tid endda. 

En stor pointe er, at nu har jeg godt nok snakket med om et prædikat såsom `er\_sjov,' og sådanne prædikater er også vigtige, nemlig prædikater der bruges mere til bedømmelse af en ressource frem for en kategorisering af den. Et relateret prædikat til er\_sjov, der i stedet bruges til bedømmelse, kunne være er\_komedie, når vi snakker film, og ellers kunne det være noget såsom har\_humor\_som\_et\_vigtigt\_fokus. Der vil så generelt være meget mere tværgående enighed omkring sidstnævnte typer af prædikater, hvor bedømmelserne af førstnævnte vil afhænge meget mere af personlige holdninger --- og dermed altså også hvilken `brugergruppe,' man ``spørger'' i forbindelse med diverse filterindstillinger. 

Nå, den store pointe er så, at kategoriseringsprædikater vil være mindst ligeså vigtige for brugerne, og jeg tror på, at disse i høj grad også vil være villige til at bedømme kategorier, som vi kan kalde det, frem for bare at bedømme, hvor godt ressourcen lever op til sine kategorier, osv. Og på den måde, så bliver applikationens brugbarhed altså set ikke så todelt, som jeg egentligt lidt har gået og tænkt på det sidste, for det vil i stedet være sådan, at hele den del af applikationen, der handler om at kategorisere ressourcer, i høj grad også vil hænge sammen med tag-rating-delen. Og dermed tror jeg altså lynhurtigt, man vil få brugerne godt i gang med at tilføje og bedømme kategorier. 

For når man som bruger af gængse hjemmesider afgiver bedømmelser, så er det ofte i høj grad for at støtte skaberen og ikke mindst hjælpe til at andre med samme interesser kan finde frem til det samme. Og dette vil kategori-rating tags jo lige netop kunne hjælpe gevaldigt med. Jeg er altså ret overbevist om, at brugerne i høj grad vil benytte sig af disse.

Og oveni, hvis vi går tilbage og ser på, hvad der generelt vil få applikationen til at blive en stor succes efter min mening, så vil applikationen jo hurtigt kunne bruges vildt bredt. Vi snakker jo således slet ikke bare film og videoer, men også tekster (bl.a.\ fra Wikipedia, men også fra alle mulige andre steder), varer af alverdens afskygninger, bøger, spil og alverdens andre ting omkring fritidsinteresser. Og ja, jeg tror altså nu, at der ikke vil gå særligt lang tid, før at brugerne får udbygget en omfattende kategoriserings-semantik-graf over alle sådanne ting. .\,.\,Nå ja, og i øvrigt forestiller jeg mig også, at politiske holdninger og andre meninger også kunne blive en vigtig type ressource. Altså tekster, der opsummerer en eller anden form for mening om noget, og som folk så kan bedømme efter enighed. Dette kan blive en rigtig god måde for folk at udtrykke sig på (politisk og i andre sammenhænge), f.eks.\ hvis nu de hører i fjernsynet eller i radioen om, at ``der har været en stor bevægelse/underskriftindsamling/shitstorm / et stort backlash/.\,.\,you name it.\,. hvor de selv føler, at de ikke selv er repræsenteret i disse reaktioner. Så kan det være rart at kunne gå ind at give sin mening til kende ved at stemme på de relaterede `meninger' på hjemmesiden/applikationen, og også så at kunne se et mere klart billede af, hvor mange mener det ene og hvor mange mener det andet. Og ja, det kan det selvfølgelig også i mange, mange andre sammenhænge (og også i tilfælde, hvor `meningen' ikke er vildt aktuel i nyhederne, men det er den måske for brugeren selv). Men ja, og hvis vi går et skridt tilbage til at kategorisere (og bedømme!) varer *(og servicer forresten) af alverdens typer, så vil det også blive en kæmpe stor ting. Hvis man f.eks.\ ser på Trust Pilot, så vil den hjemmeside/applikations muligheder være vand ift., hvad man kan på min semantik-hjemmeside. For det første vil man kunne kategorisere alle varer i en semantisk grafstruktur, så det er let at finde frem til, hvad man leder efter, og let at sammenligne, og for det andet vil man så også have mange flere parametre, man kan bedømme varerne (og servicerne) ift.\ (og se bedømmelserne for). 

Og ja, angående vidensressourcer, så tror jeg nu også, at applikation lynhurtigt vil blive super gavnlig, og at brugere således vil kunne bruge den til meget nemmere at finde frem til tekster omkring ligepræcis det, de er interesserede i. 

En lille teknisk tilføjelse omkring videns-ressourcerne, hvilket jo bare er tekster, eventuelt hypertekster, plus kilde-URL'en, jamen så ville jeg vælge bare at gemme (hyper)teksterne direkte i databasen fra start af (i modsætning til tungere ting såsom videoer og billeder). Hvis så kildesiden ændrer struktur en anelse, eller hvis selve teksten bliver skrevet en anelse om, så må man alligevel kunne finde tilbage til det eksakte sted, hvor teksten stammer fra, nemlig ved bare at lave en automatisk søgning og finde det bedste match til teksten på kildesiden. *(Jeg glemte lige at nævne her, at man dog nok ikke bør lade brugerne se disse tekster direkte, da de jo let kan være copyright'ede. .\,.\,Hm, men vent, for vil det så egentligt være særligt smart sådan? Måske på sigt, men det vil jo kræve ret meget programmering, måske man kan finde på noget smartere.\,.\,? .\,.\,Ah, mon ikke hvis man bare gemmer de første par sætninger i teksten i stedet (foruden overskriften, hvis der er en), så kan man vel nok ikke komme i klemme. .\,.\,Nej, det skulle undre mig meget, hvis man kan det.)

Men ja, så det korte af det lange er altså, at jeg virkeligt tror, at hjemmesiden/applikati-onen ift.\ hver enkelt type ressource vil gå fra 0 til `nu er siden brugbar' til 100 på virkeligt kort tid, for når først den semantiske struktur er brugbar for en del brugere, så tror jeg udviklingen vil accelerere helt vildt derfra, nemlig fordi flere og flere brugere vil komme til, og disse vil føje flere og flere ressourcer og bedømmelser (inklusiv kategori-bedømmelser) til. (10:43)


(10:54) Jeg har også tænkt på noget andet, som jeg passende lige kan nævne her, inden jeg går videre, og det er, at følgende donationsidé nok virkeligt også vil være gavnlig, nemlig at brugerne i fællesskab (og hver for sig og, ikke mindst, i grupper) kan udarbejde donationsfordelingsopskrifter, som så altså er opskrifter/planer, der bestemmer, hvordan penge doneret til opskriften/planen skal fordeles mellem skabere/bidragsydere (og altså ikke bare skabere, der har uploadet ressourcer specifikt til webapplikationen; det kan også være skabere, som ikke selv har tilføjet deres indhold specifikt til webapplikationen (men til en anden hjemmeside/applikation), men som dog har registreret sig selv og tilføjet en konto man kan donere til). Hver doner kan så selv vælge, hvilken plan/opskrift, de vil bruge. I princippet kan de så vælge en opskrift, der bare giver pengene tilbage til den selv (ved også at registrere sig selv som en skaber), men hele pointen er så, at andre brugere så også kan se, at du ikke er donor til en af de mest populære planer/opskrifter, men i stedet er donor til en måske meget obskur opskrift. Dermed vil andre brugere altså ikke have nær så stor tendens til at bruge dig særligt meget i diverse filteralgoritmer, mere end hvordan de bruger ikke-donerende brugere. (Og det er nemlig en rigtig gavnlig ting, at brugere kan vægte filteralgoritmer ud fra, hvor meget brugere har givet til visse populære og fornuftige donationsopskrifter. 

En lille anden tanke omkring bruger--skaber-økonomien, som jeg lige kom på nu her (de fleste andre idéer jeg nedskriver nu her stammer ellers fra weekenden, hvor jeg var på Fyn), er at man i stedet for at tænke på at starte en SRC over webapplikationsvirksomheden kunne starte den over en IP-fællespulje i stedet. Her har jeg jo før tænkt, at en sådan IP-organisation kunne indgå som et modul i den samlede SRC omkring en sådan hjemmeside/webapplikation, men nu tænker jeg altså: Hvorfor ikke bare nøjes med at foreslå, at skaberne kan gå sammen (efterfølgende) og oprette en SRC til at forene sig omkring deres samlede IP-rettigheder? Så det tror jeg faktisk er min plan nu: Jeg vil oprette en virksomhed omkring denne semantik-webapplikation, som altså skal stå for de grundlæggende ting, ikke mindst at passe databasen (og som tiltrækker penge ved at vise reklamer (muligvis hvor brugerne selv kan slå dem fra (altså uden at bruge en add blocker)) og så ikke mindst via donationer, og som i øvrigt ikke prøver på at ekspandere særligt meget), og så er tanken, at diverse skabere (indholdsskabere og ``ramme-skabere,'' som jeg har kaldt det) selv kan oprette en SRC omkring en sammensat IP-pulje, hvis de vil. 

Ok, og har jeg andet, jeg vil nævne, inden jeg går videre til mine nye idéer omkring webapplikationens interface og alt det.\,.\,? (11:22)


(13:25) Okay, jeg tillod lige mig selv at tænke noget mere over det hele. Lad mig starte med det vigtigste først. Nu tænker jeg ikke længere at fokusere på den der fold-ud-menu, jeg har snakket om ovenfor, ikke i starten i hvert fald. %Lad mig prøve at opsummere, hvad jeg tænker, man bør prøve at konstruere i starten i stedet for.
I stedet forestiller jeg mig, at det bare er helt standard, at hver ressource i en liste får en lille liste af muligheder, ja, som essentielt set så vil være list-prædikater, der hver er dannet af en relation (2 inputs), hvor pågældende ressource automatisk er sat ind som det ene input. Denne lille liste af muligheder må så gerne kunne afhænge af ressource-typen, men er ellers rimeligt konstant. Det vil sige, at brugeren derfor i princippet vælger en liste for hver ressourcetype (f.eks.\ `kategori,' ress.\,.).\,. Nå nej, lad mig kalde det et term i stedet for en ressource, og så kan jeg bruge ressource som en over-termtype, der indeholder (ressource-)termer såsom `videoressourcer,' tekstressourcer,' `meninger' (som jeg har skrevet om ovenfor), `varer' osv. Okay, og for at fortsætte, så vælger brugeren altså derfor i princippet en liste for hver termtype, nemlig i form af en lille liste af relationer. Disse relationer kan så f.eks.\ være `er\_overkategori\_til\_x' (hvor x så er den automatisk indsatte term), `er\_underkategori\_til\_x,' `hører\_under\_kategorien\_x,' `er\_et\_relateret\_link\_til\_x,' `er\_en\_relateret\_ressource\_til\_x,' osv.\ (hvor disse nævnte relationer så ikke alle vil være relevante for alle termtyper (de to første vil nemlig nok bare være relevante kategorier, hvorimod de tre sidste vil være relevante for diverse ressource-termtyper)). 

Selvfølgelig vil de fleste brugere i starten bare bruge de lister her, som webudviklerne (som jeg så forestiller mig at være en del af) vælger (og justerer løbende), men med tiden vil brugerne selv tage over og gøre det for hinanden, som de vil have det (men her snakker vi altså på langt længere sigt). 

.\,.\,Nå ja, men brugerne kan dog fra starten altså vælge imellem nogle forskellige muligheder for hver termtype, og selv oprette nye liste for hver af disse, hvor de selv tilføjer eller fjerne relationer herfra (ift.\ til standardmulighederne). *(Og de skal selvfølgelig også gerne selv kunne vælge brugervægtningen ift.\ ratingen af relationerne i listen.)

Når brugerne så iagttager en given liste over termer, der er blevet bedømt positivt i forhold til et givent prædikat, så kan de så ved at klikke på en term folde denne lidt ud for at få nogle valgmuligheder med den, og her skal den nævnte lille liste altså så også findes. Her kan brugerne så skifte imellem de forskellige valgmuligheder i listen, hvilket så bestemmer, hvilken liste af termer, som vises hvis brugeren vælger at folde en ny liste ud for det givne term. Jeg forestiller mig så, at brugeren kan folde en ny liste ud på to forskellige måder, via to forskellige tilhørende knapper: èn knap med en pil nedad og én knap med en pil til højre. Hvis brugeren trykker på knappen med pilen pegende nedad, vil en liste foldes ud under termet selv (muligvis med et lille indryk), imellem dette og så den næste term i listen. I øvrigt foldes listen kun lidt ud med de øverste fem-ti valgmuligheder til at starte med, og ved endnu et klik på en nedadpil i bunden kan brugeren så få endnu flere valgmuligheder i listen. Alternativt kan brugeren klikke på pilen pegende mod højre, hvorved samme liste så bliver udfoldet i større format i en kolonne ved siden af den forrige kolonne, hvor det givne term fandtes i. Brugeren kan så også skifte frit imellem valgmulighederne over relationer i den lille liste, jeg har snakket om her, hvorved indholdet i listen, hvad end den er foldet ud under termen eller i en kolonne til højre for den forrige, skifter til den nye relation.

Når brugeren trykker på et hvilket som helst givent term, skal der også enten foldes en rating bar (eller to.\,.) ud til at starte med, eller også skal der være en knap til at folde ratingen/erne ud. For alle termer i en liste skal der så i første omgang være en rating bar, som lige præcis handler om prædikatet om, hvor godt pågældende term passer til den liste, den vises i. Eller med andre ord, for enhver liste afhænger jo af et specifikt prædikat, så skal man altså kunne rate termen i forhold til lige netop dette prædikat. .\,.\,Hm, for prædikat-termer skal man så kunne gøre noget mere, men lad mig lige tænke lidt mere, inden jeg fortsætter omkring dette.\,. (14:09)

(14:32) Okay, når det kommer til prædikater, så skal der også være endnu en rating bar, nemlig hvor brugeren kan rate andre termer ud fra pågældende prædikatet (altså i stedet for at rate pågældende prædikat-term ud fra det prædikat, der danner den liste, som pågældende prædikat-term vises i). Og her skal brugeren så kunne skifte subjekttermen til denne rating bar. Titlen (og måske et tilhørende billede/ikon på længere sigt) kan så vises over eller ved siden af denne rating bar, og ved at klikke på en knap nær ved denne titel, skal brugen så kunne cykle imellem alle de aktuelle\,.\,. hm, de aktuelle termer, men det vil jo i høj grad være ressource-termer, vi taler om her, ikke.\,.\,? Jo.\,. .\,.\,Ja, ok, så i første omgang (som noget der vises allerførst i listen over termer, som brugeren kan rate på denne måde --- hvis altså ikke det bare \emph{kun} er ressource-termer, som brugeren skal kunne rate på denne måde, det kan faktisk godt være.\,.) skal alle de aktuelle ressource-termer altså stå i listen over, hvad brugeren kan rate her. Og disse ressource-termer er så altså nærmere bestemt alle de ressourcer, som brugeren har klikket på for nylig. 

For hver eneste rating bar, der vises, hvad så vi snakker den ene eller den anden af de to nævne ratingbarer (hvor den anden kun giver mening, når pågældende term selv er et prædikat), så skal der kunne udfoldes en lille beskrivelse af, hvordan det relevante prædikat (som rates ift.) er defineret mere eksakt (end bare ved at læse dens titel), og når brugeren peger forskellige steder på ratingbaren, så skal der også gerne vises en lille tekst til det specifikke interval, som brugeren holder over, hvor der given en forklaring på, hvordan en rating i dette interval generelt bør fortolkes (ifølge forfatteren). Det kan godt være, at disse tekster vises, når brugeren indstiller knappen på ratingbaren, måske særligt hvis vi snakker et mobile device, men der skal så selvfølgelig altid være en bekræftelse af hver afgivne rating (og hver ændring af en tidligere afgiven rating), hvor brugeren skal trykke bekræft eller annuller. (14:47)

Okay, jeg har det som om, jeg mangler at nævne noget, men lad mig bare lige gå lidt videre for nu. Hvis vi tænker på de filter-indstillinger, jeg har snakket om ovenfor, så kan dette fungere ret meget ligesom, jeg har beskrevet før, måske endda med en fold-ud-menu fra venstre. Jeg forestiller mig dog, at brugeren skal gå til hjemmesiden for applikationen selv for at lave nye filterindstillinger, og at brugeren defor bare eventuelt har en række yndlingsfiltre, som denne kan vælge fra, hvis brugeren tilgår applikationen via et overlay på en anden hjemmeside.\,.

.\,.\,For jeg har jo allerede nævnt ovenfor, at jeg nu forestiller mig, at applikationen ikke bare skal fungere på sin egen hjemmeside, men at brugerne også kan tilgå den, når de browser ressourcer på andre hjemmesider. Her forestiller jeg mig så, at dette overlay (eller hvad man kalder det) skal kunne rulles ind fra højre, hvis brugeren trykker på en knap, og i øvrigt at denne knap skifter udseende, hvis den ressource, som brugeren betragter på den anden hjemmeside allerede er tilføjet til databasen (altså den semantiske), og hvis der så findes relevant data (så som ratings og relevante links m.m.) til ressourcen i databasen. Men jeg forestiller mig dog lidt, at overlay-applikationen kan være en tand mere simpel, end hvad man kan tilgå på webapplikationens egen hjemmeside. Brugeren skal dog selvfølgelig stadig være logget ind i overlay-applikationen og hermed have adgang til de relation-liste-indstillinger m.m., som jeg har nævnt nu her i dag, og jeg synes også nok gerne, at brugeren må kunne folde underlister ud til højre for forrige kolonne i en ny kolonne ved at trykke på pilen til højre, som beskrevet her ovenfor, men ud over disse ting, så kan det dog godt være, at overlayet bare skal være en tand mere begrænset. Men til gengæld bliver brugeren så bare simpelthen dirigeret om til applikationens egen hjemmeside, hvis denne trykker på en valgmulighed, som ikke er implementeret i overlayet, og så skal brugerens nuværende position i hele semantik-grafen, samt de ressourcer brugeren har valgt, bare overføres, så brugeren starter med helt det samme sted, som denne kom fra i overlayet (nu bare med nogle flere valgmuligheder). Og i øvrigt skal denne omdirigering altså ske via åbning af et nyt vindue, så brugeren ikke forlader den side, han/hun var på. 

Så lad os forestille os, at brugeren betragter en ressource på en vis hjemmeside, og nu ser ude i siden, at der findes data til denne ressource i semantik-databasen. Brugeren kan så folde overlayet som en menu fra højre. Her skal brugeren så.\,. Nå ja, det har jeg ikke nævnt! Jeg vil gerne have.\,. Hm.\,. .\,.\,Jo okay, jeg vil også gerne have en standardvisning for en given ressource, hvor alle relationsmulighederne vises som overskrifter i en sammensat liste, hvor der så kun lige vises de allermest populære termer i hver liste, men hvor man så også får mulighed for at folde disse lister mere ud, både ved at trykke pil nedad eller ved at trykke pil til højre (som beskrevet ovenfor). Hm, vi kan så se denne mulighed som en standardmulighed for hver relationsliste (vi snakker altså denne ``lille liste af muligheder,'' som jeg startede med at forklare om i dag), hvilket altså for lige at gentage det giver en liste, som er sammensat (i rækkefølge) over alle de egentlige relationer, som er i denne liste. Så brugeren kan altså se denne standardliste, hvor alle relation-mulighederne kommer efter hinanden, eller brugeren kan også trykke på en specifik mulighed (ikke i den nye søjle med omtalte standardliste men i den forrige søjle med objekttermen selv), således at hele den nye liste så bare \emph{kun} kommer til at indeholde en specifik mulighed i form af en af de pågældende relationer i relation-listen. (Håber dette giver mening, hvis man læser det et par gange.)

Ok. Brugeren kan herved så trykke overlayet frem fra højre og med det samme se en lille oversigt over de mest relevante ratings og links m.m.\ til pågældende ressource. Og ved bare ét klik mere, nemlig på en nedad-pil, kan brugeren så folde en af disse dellister længere ud og for så at se nogle flere muligheder (i.e.\ en række af de næstmest relevante prædikater/links m.m.). (15:20) .\,.\,Og herfra kan brugeren så endda navigere endnu videre i grafen, navnligt måske hvis denne hurtigt lige vil finde en passende kategori og/eller et passende prædikat som ikke indgår i listen (.\,.\,eller f.eks.\ hvis brugeren lige hurtigt vil tilføje et relevant link). Og hvis brugeren så gerne vil bruge webapplikationen endnu mere end dette, såsom f.eks.\ at betragte ressourcen og/eller andre ressourcer i en liste sorteret ud fra visse filterindstillinger (og alt det jazz), så kan brugeren altid bare klikke på en knap, så applikationens hjemmeside åbner i en ny fane, og hvor brugeren så starter i helt samme tilstand (stort set), som denne var i i overlayet (nemlig de samme sted i grafen og med de samme midlertidigt valgte termer). (15:26)


(02.02.23, 9:32) Jeg har nogle få ændringer. På en måde tror jeg nu, at interfacet skal være lidt en blanding af, hvad jeg skrev om, dengang jeg introducerede og forklarede om fold-ud-menuen (fra venstre) ovenfor, nemlig hvor man har arbejdsbord, og hvor man så føjer flere og flere termer til et arbejdsbord, når man navigerer rundt i grafen, og ja, også så en blanding af det jeg lige har forklaret om, brugeren vælger en lille liste af muligheder for hver term type over, hvad man kan folde ud fra (ved at klikke nedad-pil eller højre-pil) en given term. 

Lige nogle lidt selvstændige tilføjelser, der er rare at få sagt med det samme, inden jeg fortsætter med det mere generelle: Nu forestiller jeg mig, at navigation i den semantiske graf primært kommer til (i det interface, jeg vil sigte mod at bygge) at foregå ved, at man folder flere og flere lister ud, ikke kun af over- og underkategorier, men sådan set af (kategori-)prædikater generelt. Og her skal brugeren så faktisk kunne aktivere flere kategori-prædikater på én gang, nemlig ved at klikke på dem (under navigationen/browsingen) og føje dem til sine aktive (kategori-)(filter-)prædikater. Sådanne prædikater behøver altså slet ikke at være disjunkte, og brugeren kan derfor altså sagtens få brug for at vælge flere på én gang i sin søgning. Jeg forestiller mig, at når brugeren klikker et kategori-prædikat aktivt i sin søgning, så skal den (skifte farve og) highlightes i den pågældende liste, samtidigt med at den også føjes til en speciel mappe i arbejdsbordet. 

Og den anden selvstændige tilføjelse er, at man også skal kunne have filter-prædikater, der har antecedenter foran sig, f.eks.\ til at gøre filtreringen afhængig af termtypen, men antecedenter bør også kunne bestå af eller indeholde andre filter-/kategori-prædikater. Hver bruger kan så endda have en hel lille preamble af faste filter-prædikater (sammensat med diverse antecedenter foran sig), som altid er aktive, når brugeren har gang i en søgning.

Nå, en anden ting er så, at jeg nu tror, at brugerne rigtigt gerne bare vil kunne søge efter nøgleord, når det kommer til bedømmelses-prædikater, som det eksempelvis gerne vil bedømme for en ressource de iagttager (måske på en anden hjemmeside og altså dermed via overlayet (som jeg stadig bare kalder det indtil videre)). Her kunne man sikkert komme langt med gængse søgemaskine-funktionaliteter, men derfor kan man stadig også godt gøre sådan, at brugerne kan tilføje nøgleord til prædikaterne. Så nu forestiller jeg mig altså, at forfatteren i første omgang får lov at tilføje nogle nøgleord, og at andre brugere så ellers også selv kan tilføje flere, samt rate tilføjede nøgleord op og ned. 

Så angående overlayet, så tror jeg bare man i staten kan nøjes med den standardvisning, jeg snakkede om her lige ovenfor i denne sektion, nemlig hvor lidt af alt bliver vist, og hvor brugerne så kan folde listerne mere ud (måske bare via nedad-pil i starten), og så ellers et søgefelt, så brugerne kan søge på prædikater.\,. eller andre termer såsom ressourcer.\,. de ikke finder i denne standardvisning. Hvis det så findes i databasen, kan de så rate det som relevant for pågældende ressource, og ellers kan de gå til hjemmesiden for at tilføje et nyt prædikat eller en ny ressource(-reference) selv. 

\ldots Nå ja, og jeg forestiller mig også nu, at når brugeren folder nye søjler ud via højre-pil, så ryger søjler, der allerede står til højre, ikke væk, men i stedet bliver den nye søjle bare indsat imellem de eksisterende søjler (lige til højre for dens forældersøjle), således at de gamle søjler til højre for bare rykker en tak længere til højre allesammen. Brugere skal så bare selv manuelt lukke de søjler, de ikke længere har behov for. 

Hvis jeg finder på nogle flere tilføjelser, der er relateret til de andre ting i denne sektion, i løbet af de næste par dage, så vender jeg nok tilbage og tilføjer dem her. Og ellers vil jeg nu gå i gang med at planlægge, hvad jeg vil begynde at prøve at programmere for min første prototype af applikationen.

*Hm, man kunne gøre sådan, at hvis brugeren holder shift inde, så vil en ny søjle erstatte den, der tidligere stod til højre, i stedet for bare at blive sat imellem dem.\,.

*Kopieret nedenfra: ``Der skal også gerne være en liste over ressourcer, der konstant ændrer sig, når man vægler flere og flere kategoriseringer til, hvilket vil sige, at vi måske faktisk nærmest skal tilbage til det der med at have navigationen i en fold-ud-menu, eller noget der svarer lidt til.\,. .\,.Hm, lad os sige, at (graf-)søjle-prædikatsøgningen og ressourceliste visningen kan foregå i to faner.\,. som dog godt også kan vises side om side i en tredje fanemulighed.''

*Kopieret nedenfra: ``Okay, jeg tror faktisk, at jeg vil gøre det sådan, at forfatteren til et prædikat faktisk kan tilføje et vilkårligt antal intervalbeskrivelser, endda med vilkårlige og muligvis overlappende intervaller. Disse skal så vises i en liste under ratingbaren, hvor teksten for kun de intervaller, hvor bar-knappen er indenfor, vises. På sigt vil jeg så faktisk også gerne have, at brugeren bare kan trykke på en tekst og se intervallet, og så endda vælge, om ratingen så skal gives ud fra bar-knappens position, eller i stedet bare ud fra den highlightede teksts interval, hvorved ratingen så bare forståes som, at brugerens rating ligger inden for dette interval! Så dette kan altså således også blive den måde, hvorpå brugerne basalt set giver hinanden mulighed for at lave mere diskrete ratings! Virker fornuftigt nok.\,:) Men ja, i første omgang til prototypen skal teksten bare vises, og ratingen skal bare gives ud fra bar-knappens position altid. :) ''


*(03.02.23, 10:34) Jeg skal også på sigt have implementeret et lille aritmetisk sprog (også med sammenligninger, og gerne inklusiv en if-then-(m.m.-)syntaks) til mine ``automatiske point,'' hvilket så må blive en slags ``automatiske prædikater og ($n$-ære) relationer.'' I dette sprog skal brugerne også kunne oprette deres egne funktioner, og ikke mindst skal de kunne tilgå rating distributioner via funktioner, hvor de altså kalder en funktion, der så har værdier ud fra den nuværende rating-distribution af et prædikat eller en relation.\,. ja, nej, eller af et udsagn rettere. Desuden skal brugerne også kunne kalde diverse almindelige descriptors (er det ikke det, det hedder?) (altså parametre så som gennemsnit, skewness osv. *(samt også hvor mange afgivne stemmer ratingen har)) fra nogle andre faste funktioner (hvor udsagnet er et input). ``Returværdien'' af et automatisk prædikat eller en automatisk relation bliver så typisk findes i form af en automatisk rating af pågældende.\,. ja, vi kan jo så passende kalde det et `automatisk udsagn' (altså et automatisk prædikat eller en automatisk relation taget på nogle inputs).\,. hm, hvis vi snakker $n$-ære relationer, så kan jeg jo bare sige at $n$ godt kan være 1, og dermed undgå at sige `prædikat' hele tiden.\,. Nå, men ``returværdien'' er så typisk en beregnet rating for den automatiske relation taget på givent input. Men på sigt kunne man også tillade, at automatiske relationer ligeledes kan give ``returværdier'' i form af input-placeholders, således at den automatiske relation altså ikke behøver at få visse dele af dets input men i stedet selv genererer, hvad dette input skal have af værdi i det endelige (automatiske) udsagn. (Det svarer altså lidt til at give en pointer med til en funktion i C, hvortil en returværdi så i sidste ende overskriver den oprindelige værdi på adressen, rent konceptuelt.) Og ja, så kan man så overveje, om man ligefrem så vil implementere et logisk sprog på baggrund af dette, men fordi brugerne jo allerede har mulighed for at implementere funktioner, så er dette nok ikke nødvendigt, og brugerne kan altså således bare nøjes med at have de automatiske relationer som noget, hvor det endelige returværdier findes, og altså ikke som noget, der kan indgå i selve sproget, der koder for returværdierne. (10:54)

*(10:59) Noget andet ret vigtigt er, at ressourcer i grunden bare skal være et begræsnet antal felter, der definerer ressourcen, nærmere bestemt ikke mht.\ \emph{hvor} den kan findes, men i forhold til hvad der definerer, den ting ressourc(e-referec)en refererer til.\,. Og hvis så duplianter findes på siden, så skal hjemmesiden og brugernetværket altså i reglen sigte efter at slå duplianterne sammen! Så brugerne kan altså hjælpe med at flagge, når der er duplianter (inklusiv når en ressourcetype er duplikeret), og hjemmeside-crewet skal så tjekke dette, og hvis flaggingen er korrekt, så skal duplianterne slås sammen i databasen. Det vil sige, at man ser på den hemmelige info om, hvilke bruger-ID'er ingår i diverse ratings for udsagn, hvor dublianterne indgår (i hver deres udsagn), og så merger databasen disse udsagn, sådan at alle brugere har stemt én gang. Brugere der har stemt på begge ressourcer til samme udsagn (hvilket nok ikke er så mange), de får så en notifikation om, at to af deres tidligere afstemninger er blevet merget til én (måske med gennemsnittet som det endelige svar, hvis det kan lade sig gøre), og kan så eventuelt rette denne rating, hvis de vil. Sådan noget som URL'er til en ressource, jamen det skal så gerne indgå som et 0-til-mange-felt for ressourcen, sådan at brugere altså selv kan føje flere URL'er til. Dette kan så foregå ved, at databasen opretter en ny relation til den pågældende ressourcetype, som så får et passende navn, der indeholder ressourcetypens navn som et slags efternavn, og hvor forfatteren til ressourcetypen så bestemmer fornavnet. Foreksempel kunne relationen komme til at hedde `Movie.hasLocation(),' og inputsne kan så være en URL og en dato for, hvornår URL'en virkede, samt selvfølgelig den pågældende ``Movie.'' Og ja, brugerne kan så efterfølgende rate diverse URL-forslag. Desuden skal der også være et Obsolete-prædikat, som brugerne bør bruge, hvis f.eks.\ en tidligere URL var gyldig (og derfor har en høj gyldighedsrating i udgangspunktet), men at den pludselig er blevet ugyldig. Således kan brugerne altså hurtigt signalere, at en URL er blevet ugyldig, uden at de skal ``kæmpe mod'' den oprindelige gyldighedsrating (som så i stedet bare bør fortolkes som `var URL'en gyldig ved den pågældende dato'), og samtidigt gør dette så også, at man efterfølgende vil kunne se, om en given URL var gyldig (og populær) dengang den blev oprettet (hvilket man ikke kan, hvis brugernetværket skulle ændre den oprindelige gyldighedsrating, nemlig hvis de ikke havde Obsolete-prædikatet). (11:23) .\,.\,Nå ja, og det at flagge dublianter kan (og bør) selvfølgelig også bare ske via en dertil indrettet relation i databasen. 


\section{My first prototype}

(10:30, 02.02.23) Jeg tror jeg vil kalde projektet (og applikationen/hjemmesiden) for SemDB, indtil videre.\,. Hm, lad mig lige hurtigt se, om det er taget.\,. .\,.\,Hm, kunne ikke finde nogen hits, der var særligt relaterede, så lad mig rigtignok bare bruge dette (SemDB) som det midlertidige navn.

For det første kan jeg nævne, at jeg vente med overlayet, så dette bliver altså ikke en del af den allerførste prototype. 

.\,.\,Lad mig også udskyde RegEx-halløjet og i stedet bare selv prøve at populere databasen med nogle eksempelressourcer.\,.

Jeg skal implementere, at man kan oprette sig og logge ind som bruger.\,. .\,.\,Brugere skal kunne tilføje nye prædikater.\,. .\,.\,Brugere skal kunne samle sig et ikke-struktureret (for jeg vil udskude, at de selv kan oprette mapper osv.) arbejdsbord over.\,. Hm, eller skal jeg prøve at føje struktur til.\,.\,? .\,.\,Nej, ikke med det allerførste.\,. De skal så bare have en enkelt liste over prædikater, og én over ressourcer (never mind relationer for nu), hvor de så bare kan fjerne elementer fra listen som den eneste struktur-ændrende handling her. 

Hm, lad mig nøjes med forfatter-tilføjede nøglefraser til prædikaterne.\,. .\,.\,Lad mig også nøjes med en enkelt type rating, nemlig bare den kontinuere type (fra et negativt tal til et positivt), og lad mig vente med at gøre sådan, at forfatterne kan tilføje intervalfortolkningsbeskrivelser. Så prædikaterne har altså bare en titel, nogle nøgleord/-fraser, en beskrivelse, og det er det for nu. .\,.\,(Så eventuelle intervalfortolkningsbeskrivelser føjes altså bare til beskrivelsen her for prototypen.)

I starten kan brugerne bare vælge imellem et lille antal af prædefinerede filterindstillinger for hvert prædikat (og med en knap til at flippe kurven horisontalt, så man ordner fra negativ til positiv i stedet).\,. 

.\,.\,Hm, lad mig lige tænke over, hvor meget jeg vil gøre ud af flersøjle-prædikatsøgningen i starten, før jeg begynder på oerlayet.\,.

I øvrigt skal filterindstillingerne kun foregå i en menu i højre side i starten, så når brugeren ser et prædikat i en liste, skal de altså kun kunne læse om det, og så vælge og tilføje det til arbejdsbordet --- og sikkert også kunne folde en ny søjle (eller underliste) ud fra den, men det vil jeg lige tænke over.\,. 

.\,.\,Jeg vil forresten bare bruge en PHP-server (og med den type SQL-database, der lige hører til den, jeg finder (.\,.\,hm, som sikkert bliver en Apache-server.\,.)). 

Okay, jeg går en tur i solskinnet og tænker videre over, hvor meget af flersøjle-navigati-onen, jeg skal tilføje her i starten, samt også hvordan brugeren skal finde frem til ressourcetermer og bedømmelsesprædikater.\,. (11:24)

(12:33) Okay, jeg tror vist bare, at jeg for prædikater skal have to muligheder i starten, når det kommer til at udfolde børnelister/-søjler, nemlig `relevante kategoriseringsprædikater' og `relevante bedømmelsesprædikater.' .\,.\,Hm, eller rettere `kategoriseringsprædikater som er relevante til brug for at lave en underinddeling, når givne prædikat er valgt.' .\,. .\,.\,Kortere sagt kunne man bare sige `relevante underkategoriseringsprædikater,' eller endnu kortere: `underkategoriseringer.' 

Der skal også gerne være en liste over ressourcer, der konstant ændrer sig, når man vægler flere og flere kategoriseringer til, hvilket vil sige, at vi måske faktisk nærmest skal tilbage til det der med at have navigationen i en fold-ud-menu, eller noget der svarer lidt til.\,. .\,.Hm, lad os sige, at (graf-)søjle-prædikatsøgningen og ressourceliste visningen kan foregå i to faner.\,. som dog godt også kan vises side om side i en tredje fanemulighed. 

Hm, udover at brugere skal kunne føje prædikater til listen over aktive prædikater, skal brugere så også kunne rate relevans for den pågældende søjle, direkte når brugeren har klikket på et prædikat i en søjle.

Lad mig forresten bare holde mig til søjler og dermed altså udskyde omtalte nedad-pils funktionalitet til et senere tidspunkt (og altså kun have højre-pilen). 

\ldots Ressourcer skal kunne rates efter hver enkelt af de valgte prædikater (som inkluderer de aktive prædikater), og barnesøjle-/fold-ud-mulighederne for ressourcer kan bare være `relevante bedømmelsesprædikater,' `relevante kategorier'.\,. Nå nej, never mind. Begge disse to ting er ikke nødvndige (af hver dere grund).\,. .\,.\,Men `relevante ressourcer' er selvfølgelig en god ting.\,. .\,.\,Nå jo forresten `relevante bedømmelsesprædikater' skal faktisk med.\,. Hm.\,. \ldots Der skal rigtignok være `relevante bedømmelsesprædikater' som en fast relation til ressourcetermer, men det er så bare vigtigt, at man sørger for at brugerne også kan få bedømmelsesprædikat-forslag fra ressourcens kategorier i stedet (altså fra når et prædikat er et `relevante bedømmelsesprædikat' til et kategoriprædikat, og hvor ressourcen så er dømt som inden for den kategori).\,. 


\ldots\ Okay, jeg tror faktisk, at jeg vil gøre det sådan, at forfatteren til et prædikat faktisk kan tilføje et vilkårligt antal intervalbeskrivelser, endda med vilkårlige og muligvis overlappende intervaller. Disse skal så vises i en liste under ratingbaren, hvor teksten for kun de intervaller, hvor bar-knappen er indenfor, vises. På sigt vil jeg så faktisk også gerne have, at brugeren bare kan trykke på en tekst og se intervallet, og så endda vælge, om ratingen så skal gives ud fra bar-knappens position, eller i stedet bare ud fra den highlightede teksts interval, hvorved ratingen så bare forståes som, at brugerens rating ligger inden for dette interval! Så dette kan altså således også blive den måde, hvorpå brugerne basalt set giver hinanden mulighed for at lave mere diskrete ratings! Virker fornuftigt nok.\,:) Men ja, i første omgang til prototypen skal teksten bare vises, og ratingen skal bare gives ud fra bar-knappens position altid. :) (16:14)

Hm, når brugeren tilføjer et prædikat til arbejdsbordet, kan denne tilføje det som aktivt eller ikke aktivt. Jeg tror ikke jeg vil lave to lister til aktive og ikke-aktive prædikater i prototypen (bare have én liste) i arbejdsbordet. I stedet skal de aktive prædikater bare highlightes, og brugeren kan så slå prædikater til og fra her, samt ændre på filterindstillingskurverne, og selvfølgelig også fjerne prædikater fra listen igen som nævnt. 






\section{Flere tanker om semantik-applikationen imens jeg arbejder på prototype}

(04.02.23, 9:54) Jeg føler virkeligt, at jeg har ramt noget rigtigt godt med mine seneste tanker. Førhen tænkte jeg jo mere på at starte med tag-ratings og et kategori(serings)træ (eller rettere en graf) som noget, der var hver for sig. Men nu bliver kategoriseringen meget mere bottom-up, også direkte ud fra rating-tags'ne, og det tror jeg altså bare kommer til at gøre så meget for, at applikationen kan komme hurtigt i gang (og at folk nemlig kommer godt i gang med at kategorisere ting på en rigtig god og naturlig måde). 

(9:59) Nå, men jeg har også nogle tekniske tilføjelser, jeg vil nævne. For det første skal URL'er ophæves til en ret vigtig termtype/datatype for systemet. Når hjemmesiden har godkendt en vis URL-RegEx, som kan hentes data fra, så skal brugerne være rimeligt frie til bare at sende URL'er til databasen, der matcher, gerne endda som en hel strøm af forslag, hvor databasen så bare kan time strømmen ud, hvis der er gået lang tid siden den sidst fik en ny URL --- og afbryde strømmen, hvis der er flere ikke-gyldige URL'er i den. 

Jeg vil også.\,. %Hm, jeg skal lige have nget at spise, før jeg kan fortsætte.. (10:04) 
%(10:16):
\ldots gerne have, at datafelter bliver en anden fast type, hvilket nærmere bestemt er relationsentiteter hver især mellem en ressource(-reference), et navn på datafeltet (f.eks.\ `medvirkende skuespiller'), en datatype (tekst, tal eller binær) og så et (andet) %(for datatypen kan også bestemmes af et byte-flag) 
byte-flag, der bestemmer, om det er en 1-, 0--1-, 0--mange- eller 1--mange-relation (f.eks.\ 0--mange hvis vi ser på `medvirkende skuespiller' i en film). Brugerne kan så rate, om et givent datafelt er relevant for en ressourcereference. Bemærk at disse felter så ikke er en del af, hvad der allerede er brugt til at karakterisere ressourcereference; alle felterne i en ressourcereference er nøglefelter, og alle datafelter er ikke nøglefelter, og er sågar nullable. .\,.\,(Så når jeg har skrevet f.eks.\ 1--mange, så er dette bare et signal om, at man ved at ressourcen har mindst én i virkeligheden, men der kan alligevel stadig godt stå null i den semantiske database.\,.) Nå, og brugerne kan så tilføje data til et givent datafelt, og her kan det bemærkes, at al sådan data kan implementeres i en relationel database som en binær relation, nemlig over et givent datafelts hash-nøgle (for sådan en mener jeg også, at hvert datafelt bør have) samt det givne data.\,. ja, og så skal der jo så også lige være en ny hash-nøgle her også.\,. .\,.\,Ja, jo.\,. Brugere kan herefter så rate disse datafeltinstanser, som vi kan kalde dem, op og ned, og herved kan brugerne altså føje data til eksisterende ressourcereferencer på semantisk vis. %(10:38)

URL'er er så et slags specielt datafelt, som en ressourcereference altid har 0--mange af.\,. Hm, og hvad med datareferencer.\,.\,? .\,.\,Nå ja, det var da egentligt lidt meningen, at der skulle have været en URL i stedet for den faktiske data.\,. Hm, men skal man ikke så bare inddele det i to typer datafelter, hvor den ene gemmer data i selve databasen, og hvor den anden i stedet bare ``derefererer'' dataen fra en URL i stedet.\,.\,? .\,.\,Jo, fint. .\,.\,Her skal det så nævnes, at applikationen dog kun rent faktisk vil hente data fra en URL, hvis det kommer fra en URL-RegEx, der allerede er tillid til (så brugere skal først have deres URL'er godkendt). (10:49)


\section{Nye tanker og idéer! (Attributter! Exisd! Idéer om query-sproget m.m.!)}

(09.02.23, 11:09) Jeg har en hel del gode nye idéer, som jeg har tænkt mig at skrive om (og tænke færdig om, når det kommer til visse dele af det). Men nu vil jeg lige starte med at sige, at jeg lige har fundet på et muligt (umiddelbart rimeligt awesome (men jeg skal selvfølgelig have lidt tid til at summe over det / tygge på det)) navn til web-applikationen! Jeg er lige kommet på, at jeg måske kunne kalde det Exisd. Extendable Interface for a Semantic Database. Umiddelbart ret nice, og sikkert ikke brugt til noget andet ellers. Lige inden jeg kom på det, tænkte jeg også på Exodus, nemlig ift.\ noget a la: Extentable.\,. interface for an Open Database of User Semantics. Umiddelbart ligger der bestemt også noget potentiale i denne idé, hvis man arbejder videre på den --- jeg kan især virkeligt godt lide slutningen (dus) --- men på den anden side synes jeg også, at to stavelser klart slår tre.\,. *(Og betydningen af `exodus' rammer også ret meget ved siden af, og i modsætning til Exisd, så synes jeg, at man tænker mere over betydningen af `Exodus,' når man hører/læser ordet.) .\,.\,Anyway, jeg vil tænke videre over det, men fedt endeligt at have nogle mere mundrette (og i det hele taget ret gode) bud på banen! .\,.\,(Og ja, så exisd.com kunne altså være et muligt domænenavn til hjemmesiden.\,.)

*(Nu tror jeg måske, at hjemmesiden skal hedde sema (.net og/eller .com) i stedet.\,. (14:17, 01.03.23))

(11:50) Nå, lad mig skrive om nogle flere af mine nye tanker, navnlig omkring de grundlæggende typer og den grundlæggende semantik. 

De grundlæggende typer, hvis vi ser på en abstraktion over, hvad databasen skal implementere, skal være diverse konstante grundlæggende datatyper såsom `int,' `float,' `date,' `time,' `date-time,' `binary' og `string.' Som jeg forestiller mig database-implementationen, så skal `int' og `float' altid stå på en plads i en databaserelation (i.e.\ en databasetabel), hvor der alligevel skal være plads til en reference også, så for simpelheden skyld (også fordi `floats' gerne må kunne være double precision minimum) så skal `int' og `float' samt alle reference-adresser bare være 8 bytes lange (altså long, double og long hhv.). Hvis brugerne vil snakke om andre datatyper såsom longint, og hvad har vi, så må de altså selv implementere dem via `object'-typere, som jeg kommer til lige om lidt. Angående `string,' så skal der bare være én type, men databasen kan dog selvfølgelig godt implementere mindst to typer, sådan at der både er en datatype 8 byte lange strings, og også en (eller flere) string-reference-type, hvor databasen slår op i en eller flere andre tabeller. Men som sagt: i abstraktionslaget lige over databasen skal der altså kun være én string-type. Denne skal i øvrigt have en fast encoding. Så igen, hvis brugere vil implementere andre encodings, så skal de selv gøre det via object-typerne. Denne faste encoding skal gerne være UTF-8, eller måske en HTML-escaped ændret udgave af UTF-8. .\,.\,Ja, det kommer lidt an på, hvorhenne saniteringsansvaret skal ligge (altså i hvilket lag), det skal jeg lige tænke over på et tidspunkt (når det passer sig). 

Der skal så som nævnt også være en `object'-type, som bliver den mest centrale type, kan man sige, på nær måske `string,' som også bliver ret central. Disse objekttyper bliver faktisk også defineret af (UTF-8-)strings, men forskellen er, at hvor en string-type (og vi er altså stadig i abstraktionen lige over databasen (.\,.\,tja, som man i princippet også kan kalde en database, for `database' er jo selv en abstraktion (og fås i mange udgaver, ikke bare relationel))) altid bør fortolkes som refererende til strengen selv, så skal en objekttype fortolkes som referere til den entitet som strengen taler om. I princippet kan formatet af, hvad vi kan kalde objektstregen, være alt muligt, men der skal dog gerne være en standard allerede til at begynde med for hvordan man formulerer et objekt via objektstrengen. Og her tænker jeg så simpelthen bare Javascripts syntaks for at definere Javascript-objekter, dog måske faktisk uden tuborgparenteserne, det ved jeg ikke. Men det skal altså gerne bare være en kommasepareret liste af attributdefinitioner, alle på formen ``$<$attribute name$>$=$<$attribute value$>$, hm, hvor attribute name så skal backslash-escape'e alle instanser af `=' samt af `\textbackslash,' og hvor attribute value skal backslash-escape alle instanser af `,' samt af `\textbackslash.'

Så alle objekter bliver altså defineret ud fra en række attributter, men dette er slet ikke alt, jeg hr at sige om attributter! For en anden ny idé går nemlig ud på, at brugerne stærkt skal anbefales generelt altid at prøve at (om-)formulere deres relationer, som de har tænkt sig at føje til databasen, som attributter! Så f.eks.: I stedet for at oprette en relation, der siger ``hasSubcategory,'' så bør de bare kalde relationen for ``Subcategory'' i stedet. Og et andet godt eksempel er, at ``hasRelatedArticle(WhichIsTheSecondInput)'' i stedet bare bliver til ``RelatedArticle.'' Jeg forudsiger, at det kommer til at forsimple tingene en hel del, både udseendesmæssigt, men faktisk også rent forståelses mæssigt, for så er det altid let at forstå, at subjektet i relationen er den der har noget, og at navneordende og tillægsordene, der forekommer i relationsnavnet altid har det med at beskrive objektet --- uden at dette behøver at specificeret via alle mulige andre små ord i en sætning, hvilket nemlig ofte kan være rigtigt kompliceret, især hvis sætningen er tvunget til at starte med ``has,'' og at man derfor i reglen vil være tvunget til at udskifte subjektet i sætningen, hvis man vil begynde at kvalificere objektet! Så ja, at holde sig til at prøve at formulere relationerne som attributter i stedet gør det bare SÅ meget nemmere (vil jeg forudsige)! Herved kan langt de fleste relationer kunne formuleres på formen: ``[Tillægsord, Sammensat tillægsord]$<$Navneord $|$ Sammensat navneord $>$.'' (12:49)

Prædikater må på den anden side gerne have formen ``is$<$Tillægsord$>$'' eller andet lignende, altså formen af lowerCamelCase sætninger, hvor subjektet implicit er det første (og eneste) input, og hvor første ord er et verbum. (Dette er i modsætning til attribut-formen, som altså efter min mening bør være UpperCamelCase og med et implicit verbum samt også implicit subjekt og objekt (og hvor attributnavnet så beskriver objektet).) 

Med disse standarder på plads for vi så et udgangspunkt, hvorfra vi kan definere alverdens semantik. Et passende spørgsmål er så: Hvilken type skal relationerne (som altså også kan (og bør!) fortolkes som attributter) og prædikaterne så have? De skal enten have string-typen eller objekt-typen! .\,.\,Hm, tja, vent lige, for jeg har ikke tænkt på, at det faktisk \emph{kan} lade sig gøre, kun at bruge objekttyper til dem, hvilket hænger sammen med, at attributterne i objektstrengene ikke behøver at følge de samme regler som relationsattributterne (for disse to ting skal faktisk bruges helt forskelligt, hvilket jeg vil vende tilbage til).\,. Hm.\,. (13:05) \ldots Hm, på den anden side var jeg jo egentligt alligevel nået frem til, at alle datatyper bliver til, hvad vi måske kunne kalde dynamiske objekter, så at sige.\,. så måske betyder denne overvejelse egentligt ikke så meget.\,.\,?\,.\,. .\,.\,Jo, så never mind.\,. Så alle datatyper kan fortolkes som objekter, nemlig ved altså at lade dem indgå i relationer, hvor relationen er formuleret som en attribut. I princippet kan relationer og prædikater dermed også bare have alle typer, men i praksis giver det selvfølgelig kun mening.\,. tja, hvis de er strenge eller konstante objekter, skulle jeg til at sige, men i princippet kunne man også vælge binær.\,. Tja, men det ville dog være meget mærkeligt at gøre, medmindre at en gruppe brugere på en eller anden måde kan finde gavn af dette på et tidspunkt. Men ja, i reglen bør relationer og prædikater altså enten være strings, og mere specifikt altså som følger den ovennævnte standard (på nær at brugerne også godt \emph{må} formulere relationer på mere konventionel vis, selvom de dog stærkt er opfordret til at formulere dem som attributter i stedet), og eller kan de være konstante objekter.\,. Hm, jeg skal forresten finde på en bedre måde at skelne mellem objektstreng-objekter og streng-objekter.\,. .\,.\,Hm.\,. .\,.\,Hm, jeg kunne måske bare kalde objektstrengene for `objektdefinitioner' i stedet.\,. .\,.\,Ja, det kommer vist til at give rigtig god mening.\,.(!) .\,.\,Klart. Ok, så alle datatyper bør fortolkes som objekter i princippet, og så kan man så dele alle objekter ind i to overordnede kategorier, nemlig i, hvad vi kunne kalde `datamonader' (hvilke så fås i flere underkategorier alt efter den pågældende datatype --- så vi har f.eks.\ `string-monader' og `int-monader' osv.), og så i, hvad vi kunne kalde.\,. .\,.\,hm, man kunne bl.a.\ kalde dem `attributdefinerede objekter.' (13:42) .\,.\,Yes! .\,.\,Hm, eller i stedet for `datamonader,' kan vi også bare kalde dem data\emph{instanser} i stedet, det virker simplere og bedre.\,:) 
.\,.\,Hm, ja, og faktisk burde man næsten kalde det for `attributdefinerede instanser' i stedet.\,. tja, eller det er så spørgsmålet, om vi skal sige ``objekter'' eller ``instanser''.\,. måske er det bedre at sige ``objekter'' i stedet faktisk.\,. .\,.\,Ja, yes, lad os gøre det!\,. 

Så langt, så godt! Lad mig se, hvad skal jeg så fortsætte med at forklare om.\,.\,? (13:50) .\,.\,Nå jo, jeg skal jo først og fremmest lige pointere, at en objektdefinition så typisk ikke er beregnet til at blive brugt i den semantiske databases QL. I stedet forventes det, at brugerne selv tilføjer faktiske attributter (som i princippet altså er omfortolkede relationer) til objekterne, bl.a.\ så at attributterne fra objektdefinitionen også kommer med og bliver til ``faktiske attributter.'' Bemærk, at dette faktisk gør, at man endda kan rette i objekters definerende attributter, nemlig hvis der ikke er nogen tvivl om, hvad objektet repræsenterer, men at der alligevel er sneget sig en fejl ind i definitionen. Et godt eksempel på dette (som dog slet ikke er det eneste eksempel) kunne være, hvis nu man skraber sig til an masse objekter andre steder fra på webbet, men at der så er en lille fejl på en af de lokationer, %...(14:09):
typisk vil man så alligevel kunne regne ud, hvilket objekt der er tale om. Brugernetværket behøver derfor ikke nødvendigvis så at lave et nyt objekt og gentage deres ratings omkring det, men kan så i stedet bare rette i de ``faktiske attributter,'' der indeholder pågældende information. Et andet tilfælde kunne være, hvis nu en af de definerende attributter ændrer sig. Lad os sige at man har en objektklasse af vindere af en eller anden pris, men at én prisvinder pludselig får deres pris annulleret. For ikke at miste ratingdata kan brugernetværket så sagtens bare omformulere de faktiske attributter, så de igen bliver tidssvarende, uden at det altså gør noget, at den oprindelige, ikke-tidssvarende objektdefinition beholdes (for denne bruges altså alligevel bare til at sætte objektet i gang, så at sige). Objektdefinitioner skal altså bare være rimeligt entydige, når de formuleres, hvis man gerne vil have objektet til at bestå, men behøver altså ikke at være fuldt ud korrekt og til alle tider. Endvidere kan objektdefinitioner også godt indeholde en formel tvetydighed, uden at dette gør, at brugernetværket bliver nød til at kassere objektet, for så længe brugernetværket ikke er i tvivl om, hvad var refereret til originalt med objektdefinitionen, så gør det ikke noget, at man senere finder frem til, at der også kunne være en anden fortolkning. Endvidere kan det også være, at brugernetværket får lyst til at dele tidligere objektklasser op i flere versioner. .\,.\,Hm, jeg kan faktisk ikke lige komme på noget godt eksempel, men anyway, lad os sige at netværket gerne vil dele en tidligere klasse op i to udgaver.\,. Tja, never mind, jeg behøver ikke at gå så meget i dybden med det her --- pointen er alligevel bare, at det er ret smart at objekters definitioner ikke behøver at være korrekte (og til alle tider) og hamret ind i sten fra starten af, men at man godt i praksis kan rette på objektdefinitionerne løbende (også selvom den faktiske `objektdefinition'(sstreng) forbliver den samme) uden at miste værdifuld ratingdata omkring objektet. (14:24)

.\,.\,Hm, lad mig lige hurtigt præcisere, at den semantiske database altså kender forskel på et attributdefineret objekt defineret ud fra en given string og så et string object defineret ud fra samme givne string. Så selvom disse to objekter er defineret ud fra samme data, så har de alligevel en typeforskel i den semantiske database, der gør dem forskellige fra hinanden. 

.\,.\,Nu kommer vi vel så til `udsagnene' i den semantiske database (som altså er en abstraktion i et lag over den relationelle database, jeg tænker skal implementere den semantiske database). Et udsagn består, foruden et unikt id (som altså også er med i abstraktionen), af et subjekt-objekt, et relations-/prædikat-objekt, og muligvis et objekt-objekt, alt efter om relations-/prædikat-objektet skal tolkes som en relation eller et prædikat. Her synes jeg så, det er værd at bemærke, at jeg faktisk også forestiller mig, at dette skal implementeres som en enkelt relation i den underliggende relationelle database, som altså både indeholder relations- såvel som prædikat-udsagn, og hvor der så bare lige er et bool-flag, der siger, om det er det ene eller det andet (og hvis flaget er sat til IS\_PREDICATE så skal objekt-objektet selvfølgelig bare altid være null). Herved er det altså udsagnet selv, der ved, om det er et relations- eller et prædikat-udsagn, og hermed skal man altså slet ikke tænke på typer i abstraktionen (a.k.a.\ den semantiske database), når det kommer til at danne udsagn. Så rent typemæssigt består `udsagn'- (eller `statement'-)typen i den semantiske database altså bare af: RelationStatement of Object $\times$ Object $\times$ Object $|$ PredicateStatement of Object $\times$ Object, og hvor objekt så har de førnævnte undertyper af `attributdefinerede objekter,' `string-objekter,' `int-objekter,' osv. Og det er så op til brugerne af den semantiske database selv ikke f.eks.\ at sætte et talobjekt in på en relations plads, eller ikke at sætte f.eks.\ AttDefObj(`Type=predicate, Title=isFunny, Description=[...]') eller StrObj(`isFunny') ind som andet objekt i et prædikat-udsagn. .\,.\,Hm, nu bliver jeg lidt i tvivl om ikke, man bare skal lægge op til en standard om at bruge lowerCamelCase overalt i stedet for attribut- prædikat- og relationsnavne, det synes jeg faktisk.\,. .\,.\,Jo, lad mig sige det for nu. Så kunne jeg altså derfor have skrevet AttDefObj(`type=predicate, title=isFunny, description=[...]') og StrObj(`isFunny') i stedet. Bemærk forresten at man i modsætning til i Javascript og XML m.m.\ ikke behøver at putte gåseøjne omkring værdierne, for det er kun mennesker, der skal læse det alligevel, aldrig maskiner. Maskiner skal nemlig kun læse de ``faktiske attributter'' m.m., som forklaret ovenfor. .\,.\,Hm, men jeg tænker godt nok også at bruge en lidt tilsvarende syntaks for query-sproget, så det kan jo godt være, at man så for objektdefinitionerne vil vælge at holde sig til en tilsvarende syntaks også, nemlig for konsistensens skyld. Men ja, heldigvis er dette i sidste ende bare op til brugerne, for igen: Det er kun mennesker, der skal læse objektdefinitionerne, ikke maskiner (på nær hvis man vil udplukke det strengobjekt som er defineret ud fra samme streng for så at lave streng-operationer på det, hvilket jeg nemlig vil have, at man skal kunne, men det kommer jeg til på et tidspunkt). (Men i forhold til at fortolke værdierne i et attributdefineret objekt, så er det altså kun i reglen mennesker, der skal gøre dette.) 

Nå ja, og i tråd med, at jeg altså gerne vil implementere relations- og prædikatudsagnene i samme tabel i den underliggende (relationelle) database, så skal StatementID'erne også bare løbe fra ulong 0 til ulong $0 - 1$, hvor prædikat- og relationsudsagn altså bare er helt blandet sammen, og hvor ID-nummeret altså bare afhænger af, hvornår udsagnet blev oprettet (igen uagtet undertypen). 

Nu når vi jo så til Rating-typen i den semantiske database, hvilket så består af et udsagnsID, et brugerID, et flag der angiver rating-typen og så ellers to dataobjekter, hvoraf det første meget gerne skal være en double float, der beskriver gennemsnitspunktet for brugerens rating. En af de halt basale ratingtyper kan så være, hvor det andet dataobjekt også er en double float, der beskriver interval radiussen, således at sådanne ratings altså gives som en slags step-funktioner, der så altså er defineret ud fra et midtpunkt og en (halv) intervalbredde. Desuden må ratings også meget gerne indeholde et timestamp (date--time) for, hvornår de blev givet. I modsætning til de andre ting i den semantiske database, så skal brugere ikke generelt have adgang til andre brugeres ratings, altså medmindre at brugerne har givet tilladelse til andet (og ikke har trukket denne tilladelse tilbage, hvad brugere nemlig også skal kunne når som helst). .\,.\,Det kan dog godt være, at man i betaversionen af applikationen bare undlade at implementere denne del, og i stedet bare advare alle brugere i betaversionen om, at alle deres ratings vises offentligt indtil betaversionen slutter. .\,.\,Man ja, lad mig vende tilbage til emnet omkring, hvem kan se hvad, og om hvordan brugere har ret til altid at null'e deres egne bidrag og/eller data, der forbinder dem med objekter m.m.\ i databasen, for det er ikke et vildt centralt eller presserende emne. (15:33)

.\,.\,Sikke tiden er fløjet, men det er jo også en omfattende ny omgang idéer, jeg har skulle skrive om. Jeg mangler nemlig også endda at skrive mere om QL'et, og så skal jeg også skrive om nogle nye idéer ift.\ det med at brugere skal kunne oprette HTML-, CSS- og Javascript-biblioteker/-udvidelser i databasen, som andre brugere så skal kunne loade og bruge (hvis de kan se, at mange har gennemgået koden og godkendt den). Mine ben summer helt vildt efter at blive rørt lidt (som i: helt vildt meget, faktisk), men når jeg kommer tilbage, så kan jeg måske bare lige skrive noget kort om QL'et, skrive om brugerudvidelserne, og så sikkert vende tilbage til QL'et, for jeg regner med, at jeg skal bruge nogle af de kommende dage på at få det på plads.\,. Men ja, gåtur nu! (15:45)

(16:44) Okay, jeg er kommet i tanke om, at man jo selvfølgelig ikke bør bruge strengobjekter som prædikater eller relationer, for det strider jo klart imod den semantik, jeg har lagt op til (nemlig hvor alle dataobjekter kun skal fortolkes som den værdi, de repræsenterer). Og dermed giver det faktisk også mening at kræve, at prædikater og relationer i udsagnene faktisk skal være attributdefinerede objekter, hvilket jo så også sagtens kan lade sig gøre, for den semantiske database kender jo som nævnt allerede forskel på objekt-undertyperne. 

På denne måde kan man forresten også, indså jeg så i samme forbindelse, nok gøre så at.\,. ja, det kan man.\,. gøre så at attribut-(/relations-/prædikat-)nøgleordene kan ændre stil efter brugerens behov.\,. .\,.\,Jeg vender tilbage til dette emne, når jeg når til QL'et, men jeg forestiller mig nemlig så, at brugere kan hive passende nøgleord ud af objekterne (fra deres ``faktiske attributter'' --- hvilket vi forresten også kunne kalde `levende attributter'.\,.), og identificere dem med objektets objektID, nemlig som hvad der svarer til variabeldefinitioner i QL'et. .\,. 

(17:04) På den anden side! Hvad med at jeg bare siger, at der også skal være endnu en objekttype, nemlig en `attribute'-type, som så lige præcis er defineret af strenge, der repræsenterer prædikater eller relationer, og som så følger den standard jeg talte om her tidligere på dagen (altså hvor relationer er formuleret som attributter i stedet, og hvor predikater omvendt gerne starter med et verbum (og gerne med lowerCamelCase-konventionen))?\,!\,. .\,. .\,.\,Hm, tjo tja, lad mig lige tænke lidt over det.\,. .\,.\,Tjo, måske har jeg faktisk fat i noget ret godt her, for i bund og grund handler det så om, at man så kan bruge attributnavnene direkte i QL'et, hvilket jo så netop giver mening, fordi hver attributnavn kun kan referere til ét attributobjekt, og i øvrigt også fordi navnet har en programmeringsvenlig form.\,. nå ja, måske skulle man endda kræve, at attributnavne ligefrem er ASCII, således at alle kan være med på trods af forskellige keyboard layouts. Det virker som en meget god idé.\,. Hm, og skal man så fjerne UTF-8-typen, eller skal have to typer, eller skal man som tredje mulighed bare.\,. tja, ikke have en ASCII type alligvel, men bare tjekke for hvert nyt attributobjekt, at strengen ikke indeholder ikke-ASCII.\,.\,? .\,.\,Det sidste er nok faktisk det nemmeste og det bedste.\,. (17:27) 

.\,.\,Hm, måske kunne man faktisk godt gå tilbage til den standard jeg beskrev, med UpperCC for attributter og lowerCC for prædikater (og eventuelle relationer), hvor sidstnævnte så starter med et verbum. For der er nemlig ingen, der siger, at man behøver at bruge samme konvention for objektdefinitionerne.\,. Hov vent, nej, for navnene skal jo gerne være programmeringsvenlige til QL'et, så derfor skal det faktisk være lowerCC det hele. 

Hm, jeg skal nu lige tænke lidt mere over, om det giver mening.\,. eller rettere, hvor godt det giver mening at have det på den måde med de her ``attributobjekter''.\,. .\,.\,Hm, kan det ske, at det kunne være sådan, at disse ``attributobjekter'' så bare bør være beregnet til meget fundamentale attributter, som altså indtager meget centrale pladser i hele semantikken, og at man, ligeså snart man bevæger sig væk fra de helt fundamentale --- meta!\,.\,. --- ting, så i stedet bør begynde at bruge de ``attributdefinerede objekter'' i stedet.\,.\,? (17:39) 
.\,.\,Hm, alternativt kunne man forresten også bare ændre navnet og fortolkningen af `attributdefineret objekt'-typen, så den også inkluderer objekter, der alene er defineret ud fra lowerCC-navne (nemlig med den attribut-/prædikat-/relations-fortolkning, jeg har snakket meget om).\,. Hm.\,. .\,.\,Tja, og dog: hvorfor ikke dele det op i to typer?\,. .\,. 

Hm, nu tænker jeg så at gøre, så at man enten kan bruge strenge inkapslet i gåseøje eller bruge variable --- eller objektID'er, hvis man virkeligt vil det.\,. nå nej, vent. Lad mig sige det sådan her: Man kan enten bruge strenge inkapslet i gåseøje eller objektID'er, når man referere til en attribut i QL'et, og desuden kan man også altid bruge variable, som så enten kan indeholde en streng eller et objektID. Men ja, jeg vil jo snakke videre om QL'ets opbygning på et senere tidspunkt.\,. .\,.\,Men jo, jeg synes umiddelbart, at dette giver.\,. hm.\,. .\,.\,Jo, det giver mening, for så bør programmører bare i reglen starte med at omdanne alle attributstrengene af de mest almindelige attributter til variable i stedet, sådan at man herefter kan undlade alle gåseøjnene.\,. Hm, men vent, hvad er idéen så med attributobjekter, for så kan man da ligeså godt bare bruge attributdefinerede objekter i stedet?\,.\,. Hm.\,. .\,.\,Ja.\,. (18:04) .\,.\,Ja, jo, men er det så ikke bare det; tilbage til at dele ting op i dataobjekter (af diverse undertyper) og attributdefinerede objekter og til så at kræve at alle prædikater/relationer/attributter skal være af sidstnævnte type?\,.\,. .\,.\,Virker fornuftigt nok, og så er det forventet, at alle bruger et helt grundlæggende bibliotek i første omgang til at definere variable, der repræsenterer de helt grundlæggende attributter. (18:08) .\,.\,Jo, fedt! 

.\,.\,Hm, lad mig egentligt begynde at kalde det `semantiske objekter' i stedet for `attributdefinerede objekter'.\,.\,!\,:) (18:13)

.\,.\,Hm, det eneste er, at objekter nok rammer lidt ved siden af, når vi jo f.eks.\ snakker prædikater og relationer, så hvad med at kalde det `semantiske entiteter' i stedet! Og ligeledes kan jeg så kalde det `dataentiteter' i stedet, hvilket så videre inkluderer `string entities,' `int entities,' osv. Fedt! Især den ændring med at erstatte `attributdefineret' med `semantisk,' det er virkeligt bare en dejlig ændring.\,.\,!\,:) 

(10.02.23, 9:51) Bemærk, at det nu jo heldigvis i princippet bliver ligemeget, det med at opfordre til at bruge en særlig standard for prædikatnavne og relations-/attributnavne. Men fordi jeg har tænkt mig at designe QL'et, som jeg har tænkt mig, så vil det stadig blive ret naturligt, og en god idé(!), det med generelt at gå efter at formulere relationsvariabelnavne som attributnavne. 

I går aftes fik jeg tænkt en del mere over QL'et, og jeg fik også tænkt over, hvad der lidt er det store emne/problem ligenu for mig, hvilket er implementationen af ``brugergrupperne.'' Angående det sidste er jeg så faktisk kommet lidt frem til, at databasen nok kan komme rigtig langt med den helt åbne del af den, altså den del af den som alle har adgang til at se.(.\,!) For brugere får så bare et buger-ID, når de opretter sig, og har i udgangspunktet ikke noget brugernavn i den offentlige database. Det kan de så tilegne sig ved at gå ind på sit eget brugerobjekt, eller rettere sin brugerentitet ifølge min nye terminologi, og up-rate en given brugernavnsattribut. Hjemmesiden kan så have en bot, der parser for grimme ord, og hvis ikke sådanne findes, hvis brugernavnet ikke allerede er tilknyttet en anden bruger, og hvis botten kan se, at brugernavnet er up-ratet af brugeren selv (med det givne bruger-ID), så kan den give et up-rate på navnet også. Ved så at bruge en `brugergruppe' kun bestående af den bot, kan brugernetværket altså herved nemt get'e brugeres selvvalgte brugernavne, hvis de har nogen, og få det vist passende steder i applikationen. 

Lad os så forestille os, at hjemmesiden har en privat database over brugere. For hver bruger kan den private database så.\,. ja, ikke engang gemme hvilke bruger-ID'er er tilknyttet hvilke privat-database-brugere, men bare for hver bruger gemme antallet af oprettede offentlig-database-brugere (hver med et offentligt bruger-ID i den offentlige database). I den offentlige database kan hver bruger så også bare få en offentlig krypteringsnøgle, som kan bruges til at indsende uploads, inklusiv ratings, med. Men den private database gemmer altså ikke den offentlige nøgle selv, den videresender den bare til den offentlige database, når den får den, og sletter den så selv. Og når det er sket, så kan den private database bare forhøje en counter med én, nemlig som så repræsenterer den private brugers antal af offentlige brugere. Hermed kan den private database så begrænse, hvor mange offentlige brugere, den vil tillade hver private bruger at have, men dette antal må gerne være så stort som 10 eller tæt på. På denne måde er hver bruger nemlig sikret, at de har nok forskellige identiteter på den offentlige side, således at de kan undgå at hver enkel (på nær måske én) offentlige profil kan tilknyttes vedkomnes virkelige identitet --- eller tilknyttes en af vedkommendes online identiteter for den sags skyld --- hvis brugeren ikke ønsker dette. Ok! Og med dette, så kan man sikkert bare gøre brugerentiteterne samt deres ratings til noget, som alle i princippet har adgang til, og komme rigtig langt med dette! Det tror jeg på! .\,.\,Og det forsimpler jo virkeligt nogle ting, samtidigt med at det også åbner op for rigtig mange muligheder for brugernetværket med et enkelt slag! Virkeligt nice!\,:) 

Det gør også, at den offentlige database nu også potentielt set kan implementeres som en spredt, decentral database. For hver del af den spredte (distributed) database kan så bare have sin egen private database, som de stoler på i forhold til at være en kilde til offentlige brugernøgler (og som ikke tillader for mange per private bruger). Hm, dette leder jo så med det samme til at tænke på, om ikke man så skulle indføre et præfiks til alle bruger-ID'er i princippet, sådan at man åbner op for, at den offentlige database på et tidspunkt kan blive spredt, hvorved hver afdeling af den spredte database så kan sætte sig på sit eget præfiks og så eller nummerere alle sine brugere i rækkefølge fra 0 til ulong $0-1$.(?) .\,.\,Hm, men så skal man vel også have en præfiks på alle adresser i det hele taget.\,. det kan jeg mærke, at jeg ikke har nok forstand på (altså spredte databaser) til at kunne forudsige. .\,.\,Ja, så lad mig selv undlade at tænke på sådanne præfikser, og hvad har vi, for heldigvis kan man jo altid bare tilføje sådanne med tilbagevirkende.\,. Nej, vent.\,. Hm.\,. .\,.\,Ah: Jo, man kan tilføje præfikser på alle adresser med ``tilbagevirkende kraft,'' men man kan dog ikke tilføje det til de semantiske objekter med tilbagevirkende kraft, så hvis vi ser på det semantiske.\,. den semantiske entitet, der skal repræsentere en offentlig brugerprofil, så skal det altså gerne lige præciseres, at hvad bruger-ID'et refererer til (nemlig en bruger i den originale af de semantiske databaser/databaseafdelinger), det er klart. Hm, men jeg skal så lige tænke over, om brugerentiteter udelukkende skal være semantiske entiteter, eller om der også skal være en hvis bruger-ID-dataentitetstype.\,.\,? (10:41)

.\,.\,Forresten, angående det med præfikser, så vil den første database aldrig nå op på de høje nok long adresser, så efterfølgende databaser/databaseafdelinger kan i princippet bare starte fra en passende stor long-adresse, og så nummerere alle sine adresser derfra. For hvis de nederste bytes er 0 i den valgte startadresse, så vil dette jo bare svare til at vælge et præfiks. (Jeg ville bare lige nævne det, men jeg har ikke tænkt mig at tænke mere over det emne (omkring en spredt database)).

Angående brugerentiteterne, så skal dette jo nok bare være semantiske entiteter med en fast skema for entitetsdefinitionerne, hvor databasen, eller rettere databasecontrolleren, uploader en sådan entitet, hver gang en ny brugerprofil oprettes. Her skal brugerprofilens (unikke) offentlige krypteringsnøgle så indsættes. Og når en bruger uploader en rating til databasen, så krypteres dette upload bare med den tilhørende private krypteringsnøgle, hvorved databasen så kan tjekke, at ratingen stammer fra den givne bruger, og hvis dette stemmer, så opretter databasen en rating i databasen, hvor brugerreferencen så simpelthen bare bliver den semantiske brugerentitets adresse i databasen, og altså ikke selve bruger-ID'et. Det lyder altså ret fornuftigt.\,;)\,\textasciicircum\textasciicircum\ 

\ldots (11:18) Inden jeg går videre, skal jeg også lige nævne, at det så er meningen, at brugere kan bruge vilkårlige tredjeparter til at hjælpe brugeren med at fordele tillid fra én offentlig profil til en anden (mere anonym) profil. Dette kan ske ved at brugeren krypterer en meddelelse med flere af af sine offentlige krypteringsnøgler og sender den i hemmelighed til en tredjepartsinstans, som vekomne stoler på ikke vil røbe hemmeligheden og vil slette hemmeligheden fra hukommelsen, efter at proceduren er fuldført. Instansen kan så tjekke, at de givne profiler er tilknyttede, og kan så beregne en vis samlet tillid, samt muligvis en vektor der approksimativt beskriver brugerens samlede interesser og/eller holdninger (muligvis krydret med en lille tilfældig vektor som plusses på). Og herefter kan instansen så per brugerens forespørgsler så oprette ratings i den offentlige semantiske database omkring de profiler, som brugeren ønsker.\,. nå nej, omkring \emph{den} profil, for i reglen vil man kun gøre det for en profil ad gangen (som modtager tillids-, interesse- og/eller holdningsbekræftende data om sig) for ikke at afsløre tilknytningen offentligt. *(Tja, eller også kan man bare bede instansen om at holde på hemmeligheden i et forlænget tidsrum sådan at den kan rate flere profiler i det tidsrum, nemlig ved så at tilføje tilfældige delays mellem hver profils ratings (og hvor vektorernes tilfældige krydderi så også ændres for hver gang).) Instansen har således en offentlig profil (hvor omverdenen altså kender profilens tilknytningen til instansen) i den offentlige database, hvor den så efterfølgende kan oprette ratings, for den givne brugerprofil som ønsket. .\,. Og når den har oprettet de ønskede ratings, sletter den altså så alle hemmelighederne igen fra brugeren af. Efter hele denne procedure kan andre brugere i brugernetværket nu (eventuelt; hvis de altså har tillid til instansen) bruge instansens ratings til at hjælpe dem med at fordele tillid m.m.\ til diverse brugerprofiler. (11:36)

Ok! Inden jeg fortsætter omkring QL'et, så bør jeg lige snakke lidt om web applikationens interface, og om hvordan brugerne selv kan udvide det. Lad mig starte med at tegne et billede af, hvordan jeg forestiller mig at en tidlig implementation af interfacet kunne se ud. Jeg forestiller mig nu for det første, at der skal være en QL-kolonne/søjle / -fold-ud-menu til venstre, hvor avancerede brugere (hvilket jeg så lidt regner med at alle de helt tidlige brugere vil være (eller rettere blive; relativt til de efterfølgende brugere)) kan skrive og gemme QL-scripts. Når et QL-script (query language, btw) har kørt, så kan scriptet enten vælge, at outputtet skal være en ny kolonne helt til venstre, en kolonne helt til højre, en kolonne lige til højre (eller venstre) for QL-kolonnen (da denne også muligvis kan forekomme som en søjle inde imellem andre søjler/kolonner), eller om outputtet skal åbnes i en ny fane i browseren, eller bare i et nyt vindue i samme browserfane, hvor brugeren så kan skifte mellem disse applikationsvinduer. Det typiske output vil så være en liste af semantiske entiteter, som regel af en vis samme ``type.'' Her er det så vigtigt at pointere, at ``typen'' her bare er defineret ud fra en semantisk (``levende'') attribut ligesom alt andet omkring entiteten (lige på nær dens entitetsdefinition). Men ``type'' skal nu alligevel gerne være en ret fundamental attribut, som hjemmesiden selv i høj grad i starten hjælper med at sætte for hver entitet (og helst i forbindelsen med oprettelsen af entiteten). .\,.\,Hm, ja, man kunne faktisk benytte sig af en vis konvention starten om, at brugere (såvel som hjemmesidens selv ift.\ dennes bidrag/``uploads'') \emph{skal} definere typen i starten af enhver ny semantisk entitet. Hvis en bruger så definerer en typen mærkeligt/forkert ift.\ de efterfølgende definerende (semantiske) attributter i definitionen, jamen så kan brugernetværket bare let kassere entiteten (for det vil nemlig være så godt som umuligt at overse ``typen,'' ift.\ hvordan jeg tænker, at det kommer til at blive, nemlig fordi jeg tror, at ``typen'' vil blive et helt centralt element i alle (gængse, fornuftige) QL-scripts). Jeg forestiller mig så, at hver (tidlig, avanceret) bruger i princippet så vælger en HTML-skabelon for hver type, eller rettere to: En til brug når entiteten vises i en liste og én til brug, når entiteten vises på dens egen ``side,'' så at sige (som jeg også har været inde på før). %(12:00) 
%(12:08):
.\,.\,Noget der så måske er nyt, er at jeg nu forestiller mig, at mulighederne som brugerne har ift.\ at gå til entitetens egen ``side'' eller at folde flere søjler ud på baggrund af entiteten --- eller at tilføje nye variabeldefinitioner til ens QL-script til venstre! --- det skal alt sammen bare implementeres i omtalte HTML-skabeloner (som også godt må indeholde Javascript!) fra starten af. Så man åbner altså det hele op for brugerne fra starten af (og hvor de tidlige brugere altså herved er programmører). Og nu er jeg godt nok bevæget mig lidt væk fra, hvordan jeg forstiller mig et tidligt interface, men lad mig bare fortsætte med dette mere generelle omkring interfacet, og hvordan det kan udvides af brugerne, og så kan jeg senere fortsætte omkring, hvordan jeg forestiller mig en tidlig implementation. (Hm, lad mig lige skifte paragraf og fortsætte omkring det generelle.\,.)

Den gode pointe er så, at det rent sikkerhedsmæssigt ikke kommer til at være anderledes, end hvis brugeren selv programmerede noget i javascript og så åbnede det i en browser. Her kan man jo også søge online på kodeudsnit, hvilket man så delvist kan gennemgå selv og delvist kan vurdere tillid til ud fra, hvordan andre online brugere har ratet kodeudsnittet på siden, hvor man tager det fra. For det skal nemlig ikke være sådan, umiddelbart, at de tidlige brugere kommer til at loade deres HTML--JS-biblioteker/preamble-QL-scripts via queries, som så kan ændre sig i princippet fra gang til gang! I stedet er det meningen, at brugerne refererer til bibliotekerne/scriptsne direkte via deres entitets-ID'er(/-adresser) i databasen, hvilket nemlig ikke ændrer sig fra gang til gang. Så ja, og fordi alle biblioteks-/preamble-script-referencer er konstante, så er det altså helt ligesom at kode javascript i almindelighed. Når vi så ser på en specifik HTML--JS-skabelon, som brugeren vælger til en vis entitetstype (altså den semantiske type), så skal der selvfølgelig være en klar konvention om, at al Javascript i disse skabeloner kun ændrer på ting inde i skabelonen, og altså ikke ændrer på nogen globale variable osv. Og hvis en bruger uploader en skabelon, der gør dette, så vil den jo bare aldrig blive stemt op af nogen troværdig bruger. Hjemmesiden bør forresten også selv hjælpe til med at verificere at skabeloner ikke gør nogle uhensigtsmæssige og/eller skadelige ting, men i sidste ende er det vigtigt ikke at stole på noget, medmindre man kan se at nogle af de troværdige avancerede (måske tidlige) brugere har up-ratet det (og ingen af dem har down-ratet det). 

Hjemmeside-interfacet skal så fordre brugerne med indbyggede JS-funktioner, som de kan gøre brug af i deres HTML(--CSS)--JS-skabeloner, som så bl.a.\ kan bruges til de nævnte ting såsom at åbne nye kolonner ved siden af den relevante kolonne eller i en af enderne, og såsom at tilføje nye variable til brugerens script til højre. Disse funktioner skal også selv inkludere muligheden for at bygge diverse QL-scripts. Bl.a.\ kunne man have en funktion, der parser en string skrevet i pågældende QL-sprog og returnerer et slags query objekt, som brugeren/programmøren så videre kan bruge til at åbne nye kolonner. Jeg har ikke nævt det endnu, men selve listerne i hver kolonne skal også have en HTML--osv.-skabelon, som brugerne så i princippet selv kan vælge. .\,.\,Ah, man kunne eventuelt gøre det sådan, at man laver en funktion, der tager en reference til en liste-HTML--osv.-skabelon samt en QL-string, og så åbner en ny kolonne (ud fra noget ekstra input som også specificerer valget om, hvor og hvordan denne skal åbnes) og videregiver QL-strengen, eller en umiddelbar oversættelse af den i form af et query-objekt, til den nyåbnede kolonne, hvorefter den refererede liste-HTML--osv.-skabelon så kan gå i gang med at bruge query-strengen/-objektet til at ufylde listen med entiteter. Så kan det så videre være denne liste-blabla-skabelons ansvar at vælge den blabla-skabelon, som bestemmer, hvordan entiteterne skal vises i listerne. Bemærk, at der bliver en cirkularitet i dette (fordi den åbnede kolonne så igen kan indeholde entiteter, der skal tildeles den skabelon, som man åbnede listen med i første omgang), men dt kan man sikkert sagtens løse på en god måde. (12:49)

Og ja, når det så kommer til alle disse skabeloner, så er det altså alle sammen nogen som brugeren selv skal gå ind og vælge til (altså når vi er i hjemmesidens tidlige stadie, hvor brugerne er programmører). Så på den måde bliver der altså ikke fare for (og dette kan man let sikre), at interfacet åbner henter flere skabeloner under brugerens normale brug af interfacet. Alle skabeloner skal nemlig bare åbnes som en del af QL-sproget (som altså derved næsten kan siges at blive lidt mere end bare et QL) preamble/header. Og preamblen/headeren bliver altså en ting, der er helt adskilt fra brugerens, lad os kalde det et `arbejdsscript.' Headeren, lad mig kalde den det, kommer til at indeholde skabelon-includes, og den kommer faktisk også til at indeholde noget andet ret vigtigt, nemlig nogle indledende indstillinger for hver ``type,'' lad mig forresten kalde det den `semantiske type' fra nu af, som så nemlig automatisk bliver sat på som et indledende filter for alle efterfølgende queries af pågældende type, medmindre altså at man escaper disse indledende indstillinger igen. Dette gør at brugeren selv kan sætte et personligt filter for, hvilke entiteter brugeren generelt gerne vil se optrædende i diverse kolonne-lister, som brugeren åbner i interfacet. Dette filter kommer så nedenunder alt, hvad en bruger, der har valgt et bart header-filter, ville se, hvis denne bruger brugte de samme skabeloner i interfacet. Så for at opsummere, så kommer headeren altså til at bestå (indtil jeg kommer i tanke om andre ting også) af skabelonsvalg for hver semantiske type samt grundfilter-valg for hver semantiske type. 

Og nu kunne man så spørge: Jamen, vil det så sige, at brugerne kun kommer til at kunne se entiteter af de semantiske typer, de allerede har godkendt i deres header? Ja! Eller faktisk nej, ikke nødvendigvis, for brugeren kan nemlig også helt selv vælge et filter for, hvad der skal ske med resten. Så hvis vi forstiller os en række header definitioner, så kan man altså starte med en grundlæggende indstilling for alle semantiske typer, som så efterfølgende kan overskrives af alle brugeren følgende skabelons- og grundfilter-indstillinger. Men et oplagt valg ville faktisk være simpelthen at udelukke alle entiteter, der ikke har en type som brugeren kender (og så må man bare holde øje med på anden vis, om der skulle tilføjes nogle andre semantiske typer (med tilhørende skabeloner), som brugeren kunne være interesseret i at begynde at gøre brug af også). Men! Til gengæld så er det så i høj grad værd at opfordre til / lægge op til, at brugerne også i høj grad benytter sig af undertyper! .\,.\,Hm, sådanne undertyper kan forresten benævnes i den semantiske definition ved, at man skriver ``$<$super type$>$.$<$subtype$>$=.\,. Nej, vent.\,. Nej, man skriver selvfølgelig bare ``subtype=$<$super type name (including dots if super type is itself a subtype)$>$.$<$subtype name$>$''. Og så er det altså mening, at brugere så også kan bruge disse semantiske undertyper i deres header, nemlig til så at overskrive valg for supertypen.\,:) (13:19)

.\,.\,Måske skal en kolonne også kende sine egne børne kolonner (og altså holde en liste over deres ID'er), således at en kolonne også herved kan få mulighed for at opdatere/overskrive en eksisterende barnekolonne. Ja, det lyder ret fornuftigt.\,:) 

Det er faktisk lige før, at jeg ikke behøver at sige så meget mere om, hvad jeg forestiller mig for en tidlig (brugerdrevet) implementation af interfacet (hvad jeg ellers skrev for lidt siden, at jeg ville vende tilbage til), for jeg har jo faktisk gennemgået det ret meget før, så det ville lidt bare blive en gentagelse af de pointer. Ja, så jeg tror altså, at jeg bare går videre til at snakke om ``QL'et'' nu her.\,. (13:24)

(13:49) Hm, jeg tænkte lige på, at jeg nok kan finde et bedre navn end Exisd med tiden, når jeg også har navngivet ``QL'et,'' og så kom jeg så til at tænke på, at der jo bliver to sprog i det, jeg har refereret til nu her som ``QL'et'' i de seneste paragrafer. Der bliver det faktiske QL for det første, og så bliver der interface-indstillings-sproget, som jo så nok nærmest kan siges at blive et JS-framework til at bygge interfaces, der så snakker direkte (tilsyneladende) med en semantisk database.\,. .\,.\,Og ja, i forhold til navnet på hjemmesiden, så kan det så bare.\,. ja, ligesom have navn efter dette framework.\,. Nå nej, vent, for der er så både et interface-indstillings-sprog, og så et ``sprog'' til at skriveskabelonerne i, som så bare er et JS-bibliotek. .\,.\,Ja, så tre sprog i virkeligheden, hvoraf det sidste så bare er et JS-bibliotek, og altså ikke er et egentligt selvstændigt sprog. Interface-indstillings-sproget kommer så til at kunne include JS-filer skrevet med omtalte bibliotek, og det kommer også til at have QL-sproget som et indre sprog i sig (eller hvad man nu teknisk kalder sådanne ``indre sprog,'' hvis ikke man bare kalder det det, det kan jeg ikke huske.\,.). (14:00)

%Jeg, og mine ben ikke mindst, bliver lige nødt til at gå en tur. Og når jeg vender tilbage, så kan jeg så lige skrive om, hvad jeg tænker omkring den helt grundlæggende syntaks i QL'et (altså det faktiske QL).. (14:07)

(15:18) Okay, jeg er lige kommet hjem fra en gåtur og har lige her sidst på gåturen fået nogle nye vigtige tanker! Jeg har desværre ikke så meget mere tid i dag, før jeg skal noget, og jeg kunne virkeligt godt tænke mig at nå at færdiggøre denne omgang noter i dag, så jeg må bare prøve ad at skynde mig og hamre derudaf.

Først en lille hurtig indskydning: Angående HTML--osv.-skabelonerne, bl.a.\ til at vise selve entiteterne i listerne, så skal disse altså selvfølgelig også selv kunne query'e databasen med QL'et (med AJAX) for at få de ressourcer, som skabelonen gerne vil indsætte i sig selv (inklusiv også eventuelt til diverse knapper og andre interface-funktionaliteter). Ville bare lige sikre mig, at dette var på plads.

Og en meget lille indskudt ting: Alle gemte strenge i den semantiske databasen skal være html-escaped, i hvert fald når de serveres af databasen, således at at hvis en bruger skal hente noget kode fra databasen, så skal brugeren/programmøren selv encode det tilbage til gyldig HTML(--CSS--JS), hvis han/hun vil dette. Det omvendte tilfælde ville være, hvis brugere altid skulle sanitere strenge fra databasen som en aktiv handling. Nej, i stedet skal det være de-saniteringen, der skal være den aktive handling fra programmørens side af. 

Nå, og nu videre til brugergrupper og ratingtyper. Det er jo her faktisk rigtigt fedt, at jeg er tilbage til et system, hvor alle brugere har adgang til al data i databasen (altså den offentlige (semantiske) af de to som hjemmesiden bruger (ikke den førnævnte private database)). For det gør også, at brugergrupper nu kan implementeres via bots, og altså implementeres i applikationslaget (så at sige)! Jeg skal lige tænke lidt mere over, hvordan dette kommer til at bruges i QL'et, så lad mig lige gå videre og nævne noget kort om ratingtyper først.

Det helt korte af det, jeg vil nævne om ratingtyper er, at disse også nu kan implementeres i ``applikationslaget!'' Brugerne kan altså også her i princippet selv implementere bots (eller menneskebrugere for den sags skyld, hvis de har tiden --- og hvis brugergruppen måske ikke er så stor (ift.\ hvor mange menneskebrugere har ansvaret for den).\,. Hov, vent!\,x) Nu snakker jeg jo om ratingtyper, ikke om ``brugergrupper,'' så never mind: for ratingtyper vil det selvfølgelig altid bare være bots, der udfører opgaven), der læser ratingdata fra databasen og så beregner nogle parametre for hvert relevant udsagn ud fra disse grundlæggende rating, hvorefter botten så kan tilføje sin egen type rating, ikke af det samme udsagn, men af et udsagn, der relaterer sig til udsagnet (og altså som har udsagnet som objekt (.\,.\,hov, det har jeg forresten også glemt at snakke om!\,.\.)), og som så indeholder information i sin ratingværdi om, en given descriptor-parameter omkring brugernes rating-fordeling. 

Okay, lad mig så lige tænke lidt videre over disse to ting, samt hvordan det kommer til at indgå i QL'et --- og lad mig prøve at tænke lidt hurtigt.\,.\,!\,;) (15:40)

.\,.\,Nå ja, jeg skal jo lige i hvert fald nævne nogle nye ting omkring QL-headeren, nemlig for det første, at de semantiske typer og undertyper i første omgang i headeren lige selv skal defineres, nærmere bestemt ved at man altså definerer, hvordan man get'er dem fra databasen i første omgang. Selvfølgelig kan man så ikke bruge denne information i de query-statements (eller konstante definitioner), der skal definere dette, så jeg skal lige sikre mig, at det ikke kommer til at blive cyklisk, men at type-gettingen kan defineres på en fornuftig måde til at starte med (muligvis med sit eget indledende sprog, hvem ved.\,.). Ok, og så tror jeg så også, at det bliver vigtigt at brugeren i headeren også får defineret en særlig `brugerentitets'-type, som brugeren nok kommer til at gøre meget brug af (fordi den type så skal bruges i alle efterfølgende queries, når `brugergruppen' (man spørger i query'en) skal defineres for query'en). Hm, det kan da forresten være, hvis nu jeg ikke kan finde på en god måde at fjerne det cykliske i det, at man så i stedet bare bør sige, at typer og undertyper defineres på en helt fast måde i entitetsdefinitionerne (som altid kan parses eksakt af databasen/serveren), og at alle brugere så \emph{skal} holde sig til den konvention (medmindre de vil risikere at blive totalt ned-ratet af alle de gamle brugere). Ja, så den udvej er der også altid (og i så fald kan det så også være, at man vil vælge at separere type- og undertype-definitionerne for sig, og så ændre den gamle `semantiske entitetsdefinition' til ikke at behøve at inkludere typen og eventuelle undertyper af entiteten). Så det er altså altid en mulighed.\,. Ok. .\,.\,Nå ja, angående typen `brugerentitet' (og her snakker vi altså den ``semantiske type'').\,. Hm.\,. %(15:54)

.\,.\,Hm, jeg har desværre ikke så meget tid tilbage.\,. (15:55)

.\,.\,Hm, men det er ikke bare sådan, at man nu, fordi alt det med ``brugergrupper'' og rating-descriptorer bliver implementeret i applikationslaget, så bare kan nøjes med et helt simpelt QL, altså et hvor man for hver query statement (som så også kan bruges til at definere variable, som man kan bruge i efterfølgende query statements) bare basalt set behøver at pege på et prædikat-udsang.\,. som altså rettere er et udsagn på formen this.$<$Predicate$>$, this.$<$Relation$>$.$<$Entity$>$ eller $<$Entity$>$.$<$Relation$>$.this, hvor `this'-keyword'et altså står i stedet for de entiteter, man gerne vil query'e.\,? Hm.\,. .\,.\,Nå, jeg har ikke rigtigt mere tid, så det må jeg lige tænke videre over på vejen (og måske tage nogle noter på min telefon, eventuelt). Der var vist dog også lige en anden ting, som jeg eget gerne lige ville nå at nævne.\,.(?) (16:04)

.\,.\,Nå nej, det skal jo rettere være noget i retning af: ``this . $<$Predicate$>$ ?, this . $<$Relation$>$ . $<$Entity$>$ ?'', ``$<$Entity$>$ . $<$Relation$>$ . this ?'', this . $<$Attribute$>$ == $<$Entity$>$ ?'', ``$<$Entity$>$ . $<$Attribute$>$ == this ?'' (hvor whitespaces er ligemeget).\,. 

.\,.\,Nå jo, der var en ting om, hvordan man eventuelt kan benytte de der intervalbredder for de almindelige ratings, men det må jeg skrive om i morgen (og måske på telefonen på vejen).\,. (16:15) .\,.\,Men helt kort sagt handler det om, at man starter fra de mindste intervalbredder af og så ligger hver rating oven i alle dens eventuelle indre ratings, hvis der er nogen, fordelt ligeligt, hvis vi altså snakker stepfunktioner, og så forstætter man så bare denne proces for alle rating én ad gangen med større og større intervalbredder. Til sidst ligger man så det hele over i et histogram, og så har man en god, sigende rating-fordeling, som man så videre kan bruge til at udtrække descriptors fra. (16:18)

%Fra toget på vej til byen (teater): 
	%"(10.02.23, 17:15) Nu hvor hele den semantiske database er åben for læsning, så kunne et simpelt QL bare være en undermængde af SQL.. ...Tja, nej, lad mig lave et lille QL-sprog, det er beregnet til det. I øvrigt så skal man jo uanset hvad have mindst én bruger med i syntaksen. Så hvis vi tager det, jeg skrev hjemme for lidt tid siden, så kan der være en brugervægtning-expression lige efter spørgsmålstegnet. Og dette kan så bare være et lineært aritmetisk udtryk i form af en parentes med konstanter ganget med variable, som repræsenterer enkelte brugere hver især. Men "en enkelt bruger" kan så til gengæld nu repræsentere en hel brugergruppe (nemlig ved at brugeren så er en bot, der varetager brugergruppen --- ved at summe ratings sammen).
	%
	%Og i forhold til at sammensætte ratings til et samlet filter, så kan QL'et også bare bruge simple aritmetisk udtryk til at opnå dette."
%

(11.02.23, 9:03) Jeg tænkte i går i toget, at man jo nu i princippet også bare kunne gøre QL'et til en undermængde af SQL, nu hvor databasen alligevel er helt open, men nej. Implementationen af den semantiske database skal være skjult, så derfor skal den semantiske database også have sin egen form for QL. Nu her i sengen i tidligere i morges kom jeg i øvrigt til at tænke på navne, og kom på at SEARQL måske kunne være et muligt navn til QL'et. (Jeg tænkte så, at det bliver lidt problematisk, hvis man skal finde på et filefternavn til det, men nej, her kan man bare skrive .srql eller .srq.\,.) .\,.\,Nå ja, og SEAR står altså her for `semantic entity and ratings'.\,. 

Nu har jeg så også tænkt lidt videre over, hvad jeg skulle gøre med de der ``brugergrupper,'' og her er jeg så kommet frem til to ting. Det ene er, at man jo nemt kan gette, om en entitet er en brugerentitet, for man kan jo i databasen kan jo bare slå op om der findes ratings fra den bruger.\,. ja, og databasen skal selvfølgelig også selv kende alle brugere, så det kan forresten godt være, at alle brugere bare \emph{skal} have sådan et unikt (long) bruger-ID. Ja. Nå, og den anden, virkeligt gode ting (som jeg også havde lidt i tankerne i går, men jeg kom lidt fra det igen, fordi der var så meget), er, at man skal kunne lave brugergrupper ud fra prædikat-expression kald (med tilhørende indledende bruger / brugergruppe, som enten er ansvarlig, eller som man bare bruger til, at definere den nye brugergruppe). Lad mig forresten nævne, at jeg tænker at et brugergruppe-udtryk altid skal komme efter spørgsmålstegnet, i den cirka-syntaks, jeg skrev i går. Dette kan så være en brugerentitet --- og altså gerne entitets-ID'et frem for bruger-ID'et (sidstnævnte kan bare være synligt i den semantiske definition af entiteten), men det kan også være et andet prædikat--brugergruppe-expression (gerne holdt i en variabel, dog). Der skal i øvrigt også gerne være et all keyword, som repræsenterer brugergruppen af alle brugere, ikke at man kommer til at bruge det så meget, og ikke fordi man ikke kan implementere det på anden vis, men jeg synes på en eller anden måde (måske), at det er passende alligevel at have.\,. %(9:30)
Og ja, jeg skal også juske at sige, at brugergruppe-udtryk i øvrigt også kan være en linearkombination af tidligere definerede brugergrupper. Så sammenlagt kan man altså definere nye brugergrupper aritmetisk ud fra gamle, og man kan også danne nye brugergrupper ud fra gamle ved at udspørge de gamle om en vis semantisk rating (med et vilkårligt prædikat) af alle brugere (og hvor ikke-ratede brugere i den forbindelse så bare får vægten 0 i brugergruppen). .\,.\,Og lad mig nævne, at det forresten ikke gør noget, hvis brugerne bruger et prædikat--brugergruppe-udtryk, der også giver ratings til ikke brugere, for der skal altid bare være et (grund-)grundfilter, der gør at alle ikke-brugerentiteter altid bliver sorteret fra automatisk for prædikat--brugergruppe-udtryk, der indgår i et brugergruppe-udtryk. (9:37)


(13.01.23, 10:01) Okay, jeg har en hel masse flere tilføjelser nu. Jeg føler virkeligt, at jeg er ved at have godt overordnet styr på det/de grundlæggende lag i systemet nu. .\,.\,Hm, hvor skal jeg starte.\,. .\,.\,Hm, godt spørgsmål, lad mig lige prøve at gå igennem det hele i tankerne en gang og så vende tilbage.\,. 

\ldots Hm, nu kom jeg forresten lige på et nyt navn: UDA (udtales you-dee-ay) for User-Driven Application, og så kan jeg mere specifikt kalde min en SUDA (es-you-dee-ay or suda (perhaps with `oo' sound)), nemlig en Semantic User-Driven Application. (10:48) %..Hm, jeg næsten mærke, at det bliver det..:D..

.\,.\,Hm, hvis det ligger godt i munden, kunne man så næsten kalde min applikation for Asuda.\,. (A for a (som i: ``a semantic user-driven application).\,.) (10:55) .\,.\,Nej, for det navn clasher med, hvis man skal sige ``a SUDA'' i en sætning.\,. hm, \emph{hvis} man altså ikke siger es-you-dee-ay da i stedet.\,. *(hvad man nok vil.\,.) .\,.\,Hm, jeg har også for nylig tænkt på SemNet *(som i: Semantic Network) som et navn til min web applikation, og det kunne jo egentligt også være en mulighed. (Og SemNet kan så bare siges at være en SUDA.\,.)

.\,.\,Nå, men jeg skal videre i teksten.\,. 

Okay, lad mig starte med, at brugerrating-entiteter i databasen også skal være `udsagn,' nemlig hvor man så specifikt tager et andet `udsagn' og bruger som relations-entitet i tripletten. Eller rettere, for nu vil jeg så også have, at man også skal kunne fortolke entiteter som funktioner/constuctors, så faktisk kan man så i første omgang tage en funktion på udsagnet, der går det til en relation, og specifikt altså rate-bar relation, nemlig fordi serveren så kan gemme brugerratings ved at denne udsagn ud fra den relation (med bruger-ID'et som første input og ratingværdien som det andet input). 
Det at brugerratings nu også er udsagns-entiteter gør, at brugerne også kan bruge semantisk logik omkring dem. For eksempel kan de således query'e databasen for, hvilke brugere har ratet et vist udsagn over eller under en vis værdi, hvilket er meget brugbart --- og rart at denne mulighed kommer automatisk nu, fordi man jo alligevel skulle kunne lave tilsvarende queries for vilkårlige udsagn. .\,.\,Tja, eller der bliver faktisk en lille forskel, for ``brugergruppen'' man spørger for at udtrække en hvis brugerrating vil jo altid bare være underforstået (nemlig en underforstået grundlæggende bruger/bot i databasen.\,.).\,. .\,.\,Hm.\,. 
.\,.\,Ja, det kommer til at svare til, at der bare er en automatisk grundlæggende bruger, der forbindes med queries til entiteter af typen ratingentiteter (og mere om sådanne typer senere i dag), og at denne bruger bare automatisk siger 1 (fuld rating) til alle disse udsagn, således at gyldigheden af dem bare afhænger af, om de eksisterer i databasen.\,. Hm, hvilket vil sige, at man egentligt også bare kunne sørge for, at det er omtalte bruger/bot, der bare bestemmer typen `rating(entitet)'.\,. .\,.\,Ja, og dette kan så bare implementeres ved at man gør `rating' til en fundamental type, der bestemmes automatisk af database(servere)n selv. For jeg vil nemlig alligevel gerne have det sådan alligevel, og jeg vil i øvrigt også gerne have at typen `bruger' på tilsvarende vis også skal være sådan en fundamental type. (11:38) 

\ldots Hm, men skal alle uploads ikke have en bruger på sig, så kunne man ikke også bare gøre det.\,. ja, på en lidt anden måde, lad mig lige se.\,. 

\ldots (12:25) Hm, det er ligefør, at alle rating-aggregater bare skal implementeres via ``bot-brugere'' i princippet (og databaseserveren har så bare nogle grundlæggende ``bots'' og kan også oprette flere, hvis der er nok efterspørgsel efter det).\,.(?\,.\,.) 
.\,.\,Og `brugergrupper' kan nemlig så også i princippet implementeres via bots, nemlig som databaseserveren/serversystemet/hjemmesiden så også bare eventuelt kan påtage sig at administrere (ud fra en fast protokol, ligesom), hvis der er nok stemning (som i: nok stemmer) for det.\,. 

For ja, noget af det seneste, jeg kom frem til, før jeg satte mig for at skrive i dag, var nemlig, at brugergrupper altid skal oprettes som entiteter i databasen, og godkendes, før de kan tages i brug. Og derfor kan dette jo også sagtens implementeres ved, at databasen så godkender en brugergruppe ved at oprette en bot til at oprette og vedligeholde stemmer samlet fra denne brugergruppe.\,. (12:37) .\,.\,(Brugere skal dog stadig kunne lave linearkombinationer af brugergrupper i queries, men de kan så ikke nødvendigvis --- ikke i første version af QL'et --- indsætte nye, custom-made brugergrupper her i disse udtryk (ved at indsætte et prædikat-udtryk med this-typen `bruger').)

.\,.\,Jeg har også en masse andet, jeg skal have skrevet om, særligt også tilføjelser omkring ADA-delen af det (og om hvordan bruger loader scripts og XML ved applikationens opstart, og ikke mindst hvordan XML-datastrukturer kan opdateres for hver bruger --- og om at bruge difs/deltas!), men lad mig lige gå en tur (i det her ultrafine vejr, som det er for tiden) og lige prøve at summe det her grundlæggende omkring databasen mere på plads.\,. (12:55)


(16:20) Okay, efter gåtur og efter at have tænkt videre efterfølgende så tror jeg endelig, at jeg er ved at have typesystemet (et lidt nyt et) og implementationen nogenlunde på plads. Lad os se. Vi har for det førte nogle datatyper. Jeg mener nu, at der skal gælde, at alle datatyper af en fast længde mindre eller lig long (8 byte) skal forekomme direkte i tabellen. Så når man vil referere til en dataentitet, så gør man det altså ved simpelthen at angive typen først, og hvis det er an datatype mindre eller lig en long, så skriver man den så direkte i den efterfølgende kolonne (hvis man skulle opstille det i en fornuftig tabel). Og for datatyper/typer såsom `string,' `blob' og `semantic term,' så giver man så en reference i stedet, som så også (indtil man på et eventuelt tidspunkt i fremtiden bliver nødt til at tilføje flere bytes) er en long. Men ift.\ implementationen i den relationelle database, så kan man altså stadig se på datatypen, i hvilken relation/tabel, man skal slå referencen/ID'et op i. Så entiteter med forskellige typer kan altså godt have kolliderende ID'er (men self.\ ikke, når de har samme type). Nå, udover datatyperne er der også.\,. Hm, nu kaldte jeg det lige `semantic term,' men i virkeligheden bør det hellere hede `atomic (semantic) term' og `compound (semantic) term'.\,. .\,.\,Hm nej, lad mig kalde det et `semantic atomic term' og bare et `compound term,' hvilket så er en overtype, hvorimod `semantic atomic term' er en helt specifik type. .\,.\,Hm, nej, det dur faktisk ikke; lad mig finde et andet navn for `semantic atomic term'.\,. .\,.\,Hm, hvad med `interpreted term,' hvor man måske så kan sige `iterm'.\,. nej, det ser mærkeligt ud.\,. .\,.\,Hm, lad mig egentligt også hellere kalde det et `described term' i stedet.\,. .\,.\,Og jeg behøver ikke forkortelse som sådan (DescribedTerm virker fint). Ok. Og så har vi CompoundTerm's, som har undertyperne MonadicTerm of Term $\times$ DescribedTerm og DyadicTerm of  Term $\times$ DescribedTerm $\times$ Term. Disse compound terms vil typisk repræsentere udsagn, men de kan dog også repræsentere funktionelle udtryk. Nå ja, og vi har i øvrigt også typen User, hvilket bare holdet et bruger-ID (long) --- ligesom at DescribedTerm i øvrigt holder en string (hvilket ses direkte som en string i den semantiske database, men som så dog er implementeret som en string-reference, nemlig til et string-ID i en String-relation/-tabel i den undeliggende relationelle database, hvor anden kolonne i den tabel så rent faktisk er en VARCHAR(n) (og grunden til at det skal være sådan, er i øvrigt altså for, at jeg kan have 8-byte data stående overalt i de overordnede relationer/tabeller)). Og sidst (mener jeg nok) men ikke mindst har vi så typen, SemanticInput, hvilket altså er rating-typen (bare hvor jeg har generaliseret navnet an anelse), som har formen, User $\times$ CompoundTerm $\times$ Term. Her er det sidste Term selvfølgelig rating-værdien/-dataen, og bemærk, at fordi alle typer, hvor det kan lade sig gøre at putte dataet direkte ind (nemlig hvis de er mindre eller lig en long), får gjort dette, så vil talværdierne altså stå direkte i den relationel-database-tabel/-relation, der implementerer `semantic inputs.' Fordi vi sammenlagt har så få typer, som vi har (nemlig en håndfuld datatyper og så en fåtal af ikke-datatyper), så kan flaget i databasetabellerne (som både forekommer i den semantiske og i implementationen i den relationelle database), der bestemmer datatypen (når dette er nødvendigt, hvad det eksempelvis ikke er for User i SemanticInput; her kan man bare have en long uden at have noget flag foran), bare være en char. (17:16)

Det næste man så egentligt bør snakke om, efter at man har snakket om den semantiske database, det er jo oplagt QL'et. Men fordi jeg har så mange andre små ting, jeg også skal sige, så lad mig lige prøve at få klaret en del af de ting først.\,. 

.\,.\,Nå ja, jeg kan jo passende starte med at snakke om difs/deltas, for dette skal jo også indgå som en type i den semantiske database!\,.\,. (17:19) .\,.\,Hm, dette bliver så en rekursiv type.\,. Hvad skal vi kalde den.\,.\,? .\,.\,Hm, man skal jo næsten også gerne kunne tage udsnit fra flere eksisterende strenge/tekster på én gang, så hvad skal vi sige der.\,.\,? .\,.\,Oh well, det kan jeg vende tilbage til. Pointen er bare, at der også skal være en DeltaString- eller CompoundString- eller EditedString-, eller hvad-vi-nu-skal-kalde-den-type, som altså danner en ny string ud fra nogle gamle strenge samt eventuelt nogle nye tilføjelser/indsættelser. 

(17:27) Lad mig så snakke om nogle ting mere oppe lige under applikationslaget.\,. For det første skal jeg have nævnt, at der gerne skal være en privat/lukket database også, som gør mere end bare det der med at tælle brugerprofiler pr.\ brugerkonto. Men det skal dog være valgfrit, om man vil gøre brug af denne private database, eller om man vil gøre noget andet, hvilket eventuelt godt kan være bare at sende alle inputs direkte til den offentlige database, hvis man vil det. I øvrigt skal man stadig bare ``logge på'' den offentlige database ved.\,. ja, eller rettere, man logger ikke på, men man sender bare sine inputs krypteret med den offentlige nøgle. Det jeg vist på et tidspunkt sagde med, at den offentlige krypteringsnøgle bare \emph{bliver} bruger-ID'et gælder dog ikke; databasen giver hver brugerprofil en long, der udgør bruger-ID'et, hvilket så bliver omdrejningspunkt for semantikken. 

Men brugerne skal altså også have mulighed for at uploade inputs til en privat database. Her kan de så bl.a.\ uploade og redigere data omkring indstillinger i deres eget personlige interface (altså den jeg kalder en UDA, fordi brugerne i princippet kan ændre den stort set vilkårligt). Om brugeren gemmer til denne private database, til den offentlige eller en blanding, så vil brugeren uanset hvad skulle fortælle databasen (selvom de jo godt kan starte et populært sted og så bare blive der), hvilke nogle scripts og HTML m.m., de gerne vil have loadet i deres applikation under opstarten. Alle scripts kan så få eventuelt inputdata via XML, der også hentes fra databasen under opstart. Her skal brugerne så sørge for, at alle scripts, der skal/kan bruge ekstern XML-data, sørger for at erklære og tjekke et navn på en fast måde (som jeg skal have fundet på), hvor det sikres, at der altså vil returneres en fejl, hvis to loadede scripts bruger det samme navn. Hvis scriptsne så loader uden fejl, så ``ved'' hvert script så, at det er frit til at tilgå og muligvis ændre i (hvis brugeren, der loader scriptet har tilladt dette) et specifikt XML-element (som så har en attribut a la dataIdentifier=$<$unique name$>$). Og scriptet, hvis det har fået lov, må så gerne ``flushe'' sit XML-objekt tilbage til databasen (som regel den private, men det bestemmer brugeren, der loader scriptet), således at brugeren kan loade sine ændringer siden sidst på ny, når denne starter webapplikationen op en anden gang i fremtiden. (17:45)

Angående det jeg på et tidspunkt skrev ovenfor omkring at undgå de cykliske i at loade lister, som så loader samme type elementer, så er svaret her selvfølgelig bare, at man først definerer sit listekolonne-script/-HTML, og kan så bagefter tilføje ressourcetyper til dette script, samt tilhørende visnings-script/-HTML til hver af disse typer. 

(17:48) I øvrigt, nu hvor vi er næsten helt ude i applikationslaget, så tænker jeg at UDA'en kan starte med at have disse ting: listevisningskolonner, som altså bruges til at vise en liste over ressourcer (med tilhørende filtre), som brugeren har spurgt efter (på den ene eller den anden måde); ratingkolonner til når man har selekteret en ressource og bare lige vil have en hurtig oversigt over gode mulige ratings/``rating-tags,'' man så kan benytte; ressourcevisningskolonner, som er ligesom ressourcesider, bare i miniformat; ressourcesider, hvilket så er, når man virkeligt har klikket helt ind på en ressource (vist i stort format for sig selv; ikke klemt inde mellem andre kolonner); terminalkolonner, hvor den avancerede bruger kan lege med at udforme queries i QL'et (og med variable, som er defineret i et tilhørende XML-dataobjekt); samt også; sidst men ikke mindst, kontrolkolonner, som er ligesom terminal kolonner men bare sat op så brugeren skal trykke på knapper og gøre ved som de fleste brugere er vant til for at opnå de samme ting, som de avancerede brugere kan opnå i terminalen (men måske altså bare lidt mindre end dette (men på sigt dog ikke foruden noget, som den almindelige bruger vil savne)). (18:00) .\,.\,Nå ja, og så skal der også være en kolonne eller side, der handler om at indstille start-op-indstillingerne for applikationen (og loade scripts/HTML/CSS og XML-data, samt definere typer og grundfiltre, hvilket jeg kommer til senere). 

(18:59) Jeg tror faktisk, jeg vil inkludere listetyper! Så skal der være 12 forskellige listetyper, nemlig med fra 0 til 11 elementer i sig (så den første er altså den tomme mængde/liste). Og så vil jeg kalde det MonadicStatement og DyadicStatement i stedet, og så ellers bare have et FunctionalTerm of DescribedTerm $\times$ Term, hvor.\,. Nå nej, og jeg vil så heller ikke kun have DescribedTerm som mulige prædikater, relationer eller funktioner, men disse kan også alle være af FunctionalTerm-typen i stedet. Så FunctionalTerm of (DescripedTerm  $|$ FunctionalTerm) $\times$ Term (så typen er altså rekursiv). Og Term her kan så særligt også være en listetype, hvilket netop gør at alle funktioner bare kan være umiddelbart unære. Hvis man gerne vil lave en funktion, som skal kunne gives lister længere end elleve elementer, så kan man sagtens det. Man kan selvfølgelig for det første give den lister af lister i stedet (eller af funktioner taget på lister), men man kan også bare give den.\,. Ja, eller det er jo så også at give den lister af lister i princippet, men hvis man altså meget gerne vil have, at alle elementer opfattes som ligestående, side om side med hinanden, så kan benytte en konvention om at implementere dette ved specifikt at lade det 11.\ input have typen af en ny liste, inklusiv den tomme liste, hvis det samlede antal elementer er rundt, og man er ved den sidste af alle tier-grupperne. .\,.\,Ok, og en sidegevindst er i øvrigt, at nu kan man lade databasen tjekke, at der enten er tale om et MonadicStatement eller et DyadicStatement, hvilket nok vil være godt til at guide brugere til aldrig at rate andet end termer, der semantisk set bør fortolkes som et udsagn (og altså til ikke at begynde at rate ikke-statement-termer direkte med et implicit prædikat; det vil vi helst undgå). (19:17)

.\,.\,Hm, på den anden side vil dette jo netop friste brugerne til at bruge direkte get-funktioner, i stedet for at bruge mere abstrakte get-funktioner, der kan tage højde (i princippet) for efterfølgende rettelser.\,. Så ja, lad mig lige tænke over, hvad der er bedst, for på den ene side gør man det jo nemmere for de første brugere, ved at indføre lister (med automatiske getElement()-funktioner), men på den anden side, så kan brugerne så også blive forvente med disse simple og automatiske funktioner.\,. (Så ja, det må jeg lige tænke over.\,.) 

.\,.\,Ah, men på den anden side, så vil det jo også være ret smart, hvis applikationen kan hente de uredigerede ting først og bare tjekke for de værste faresignaler, og når der så bliver tid, eller når brugeren klikker sig videre ind på ressourcen, så kan applikationen så lave mere grundige gets. Nice!\,:) Fedt nok, så indfører jeg nok de lister der. (19:27)


(14.02.23, 8:54) Hm, jeg kunne også indføre lister uden at indføre funktioner.\,. .\,.\,Hm, det ville måske ikke være en dum idé.\,. Og man kan endda så sige, at jamen funktioner kan jo også bare udelades i første version af systemet (men hvor man bare holder dem klar til at blive indført, så snart der bliver behov for dem, hvis der gør det).\,. .\,.\,Yes, og måske bliver der faktisk aldrig behov for at indføre funktioner. Og det giver mig så det bedste af begge verdner, for nu bliver de tidlige programmører (hvilket jo nok i høj grad bliver mig selv) tvunget til at get'e (gette) attributter omkring et objekt via semantiske queries, hvilket gør implementationen meget mere åben overfor efterfølgende rettelser og ikke mindst også for, at brugere kan få mulighed for at indstille metoderne til deres egne behov. Men samtidigt kan man så bare bede om en enkelt liste som programmør, hvilket.\,. ja, det gør tingene en anelse nemmere, men ikke mindst gør det det også nemmere, hvis brugeren skal indstille get-metoden/erne for et objekt, for så skal de kun ændre indstillingerne for én metode pr.\ objekt (altså fordi der så kun behøver at være én get-metode, som get'er hele listen af relevant data for et objekt). .\,.\,Ja, det er altså virkeligt nice på den måde. Så jeg indfører listetyperne som beskrevet i går aftes, men jeg fjerner så funktionstypen igen (og beholder typenavnene MonadicStatement og DyadicStatement), om ikke andet så bare indtil der alligevel viser sig (hypotetisk) at blive efterspørgsel efter den af en eller anden grund. :) (9:09)

(11:45) Okay, jeg tror, jeg ved, hvordan det grundlæggende QL skal implementeres. Jeg er faktisk gået over til nu, at det bare skal være en slags undermængde af SQL (når det kommer til selects men ikke til inserts), og at den implementation af den semantiske database, jeg har i tankerne (og har skrevet om her i går), faktisk bare skal blotlægges, sådan at denne implementation faktisk bare \emph{bliver} den semantiske database så at sige. Og det særegnede QL (\emph{hvis} man indfører et) skal så bare indføres/bygges i applikationslaget. 

Angående databasen, så skal jeg bare bruge CHAR til typeflaget, således at det bare kan sendes som en ASCII-char over http. Og det jeg så lige har tænkt over, er, hvordan rating-værdierne skal implementeres, og nu har jeg så fundet ud af, at de simpelthen bare skal implementeres som en long, eller rettere en BIGINT. Denne long/BIGINT bør så.\,. hov, eller rettere: Ratingværdien kan være af en arbitrær type (og kommer altså derfor med et typeflag forinden værdien), men den klare standard skal dog være at bruge BIGINT/long. Denne long skal så fortolkes som gennemsnittes for brugerens rating (hvor man selvfølgelig dividerer med  9223372036854775808). Og det smarte kommer så nu: Fordi de sidste cifre jo vil være semantisk fuldstændigt ubetydelige/underordnede (startende allerede fra den 4. --- måske endda fra og med den 4.\ eller endda den 3. --- byte), så kan brugerne altså frit tillade sig at bruge disse (sidste) cifre til at indkode anden semantik i ratingen. En god mulighed kunne jo således f.eks.\ være, at lade de første 4 bytes udgøre ratingmidtpunktet, og lade de efterfølgende 1--4 (hvad der lige giver bedst mening, for måske er selv én byte rigeligt til dette) bytes kode for intervalbredden af ratingen (i henhold til det, jeg har snakket om ovenfor). Bemærk, at selv hvis man nu kun bruger 2 bytes til at kode for midtpunktet, så kan gennemsnittet af alle (eller en gruppe af) sådanne ratings stadig godt fortolkes med en højere præcision. (12:08)

(12:25) Jeg tænkte lige lidt over, om man ikke også skulle have kilde-bruger og timestamp med for hver ikke-SemanticInput-entitet, men nu kom jeg så lige til at tænke på, at man som bruger alt andet end lige altid gerne vil up-rate et nyt upload i en eller anden forstand, typisk ift.\ et eller andet prædikat, som kunne have et passende navn a la `isUseful$<$Type$>$()' --- eller være en relation a la `isUsefulEntityOfType(type)'.\,. .\,.\,Og fordi man alligevel forventer at et hvert upload/insert kommer med sådan en rating lige bagefter, så kan man jo derfor også bare gøre dette til en konvention, nemlig at brugerne altid up-rater.\,. lad os sige `isUsefulEntityOfType(type)'.\,. eller rettere `$<$Type$>$.UsefulEntity==myNewEntity,' hvis man gerne vil formulere det på mere objekt-orienteret vis (hvad jeg jo synes, man bør), (at brugerne altid up-rater dette) med det samme (og altså i selvsamme insert-kommando), når de uploader en entitet. .\,.\,Ja, man kunne endda ligefrem kræve dette fra serverens side for alle nye uploads. 

.\,.\,Hm, eller skal man bare tilføje bruger og timestamps på hver entitet.\,.\, Tja, måske ikke, men lad mig lige lade spørgsmålet stå en anelse åbent.\,. .\,.\,Angående insert-kommandoer, så er det klart, at brugeren bare skal uplaode indholdet, og så er det serveren/databasen selv der sætter et passende ID for uploadet, samt en bruger og et timestamp, om ikke andet hvis vi altså snakker et SemanticInput (hvilket selvfølgelig skal have bruger og timestamp på sig).\,. (12:42)

(13:31) Okay, nu er jeg vist næsten klar til at skrive de sidste ting, jeg mangler at komme ind på, hvorefter jeg så nok så småt kan gå i gang med programmeringen.

Hvis vi ser på, hvad der så skal ske når brugeren logger på hjemmesiden.\,. Hm, eller lad mig starte med at nævne, at der på hjemmesiden, inden man er logget ind, gerne må være lidt about-information, og også gerne en liste af sponsorer, hvilket jeg tror bliver ret vigtigt for hjemmesidens økonomi. Ok. Når man så logger ind, så er noget af det første, der sker, efter at man/browseren/applikationen lige har fået bruger-ID'et i hånden (efter også at have fået oprettet og autoriseret forbindelsen) samt nogle helt grundlæggende andre variable, at applikationen efterspørger en række af start scripts/HTML/XML, der skal loades. Dette kan ske ved, at databasen spørges efter en liste over script (strings), som brugeren har gemt en positiv (skarpt) rating for. Alle disse script/HTML-strings hentes så og loades i rækkefølge efter pågældende rating-værdi, fra lavest (tættets på 0) til højest. Hvert script kan så potentielt set query'e databasen efter flere scripts / mere HTML/XML, eventuelt også på tilsvarende vis, og kan nemlig eventuelt vente lidt med at gøre dette, så den samlede side bedre kan loade sådan lidt ad gangen, hvis nu man har mange ting til at ske i sit samlede start-script. Og så er vi faktisk allerede stort set videre til applikationslaget, for jeg har allerede forklaret om, at hvert script så kan have sit eget XML-input (som kan loades separat fra databasen), og at der også skal være en funktion til at ``flushe'' og opdatere denne data. Nå ja, lad mig dog lige påpege, at databaseentiteterne der koder for brugerens egne indstillinger og præferencer, de kan eventuelt være uploadet til en privatdatabase, som man også har forbindelse til udover den offentlige. Brugeren behøver derfor ingenlunde at lade sine indstillinger være synlige for offentligheden, men kan dog godt vælge det (og på sigt vil dette faktisk nok blive ligeså sikkert i princippet i nogen tilfælde, fordi man så i stedet kan vælge at sørge for at gøre brugerprofilen umulig at sammenkoble med personen.\,.). 

Så ja, nu kommer vi så til applikationslaget, og her er mulighederne selvsagt åbne. Men jeg har dog nogle idéer til, hvor man kunne starte, både på mellemkort sigt og på helt kort sigt, for på den helt korte bane vil jeg nemlig bare implementere et rigtigt simpelt system, hvor brugerne primært bare opretter prædikater og kategorier, og så rater de ressourcer, som loades ind på siden med diverse ``rating-tags'' (og hvor de har nogle forskellige muligheder for at åbne nye kolonner ud fra en lille gruppe af forskellige typer entiteter (hvor vi her altså snakker brugerdefinerede typer)). Hov, og lad mig lige i den forbindelse nævne en kolonne-type, som jeg havde glemt ovenfor, og det er prædikat-kolloner, hvor man ser info om et givent prædikat.\,. hm, selvom dette dog også kunne hører ind under ressourcekolonnetypen.\,. Men ja, dette er jo det jeg skal i gang med at programmere, lige efter at jeg har fået back-end'en og applikationsopstarten på plads.

Angående idéerne til applikationsimplementationen på mellemkort sigt, så handler dette om.\,. .\,.\,ja, eller selvfølgelig vil der være nogle brugerflademæssige ting, som jeg ikke vil implementere i starten, men vil vente med til lidt længere sigt igen. Men hvis vi også ser bort fra dette emne, så er en virkeligt vigtig ting på den halvkorte bane at få implementeret en lille standard for, hvordan brugere kan indstille nogle af de grundlæggende ting omkring deres interface. I første omgang skal brugeren have godkendt nogle grundlæggende scripts, der skal køre i applikationens opstart. Men herefter skal brugeren også indstille nogle flere ting. Så selvom dette foregår i selve applikationslaget (hvor brugerne har total frihed i princippet), så kan vi altså stadig med fordel snakke om, at der skal ligge et nedre lag i dette applikationslag, hvor brugerne kan indstille helt grundlæggende ting omkring applikationen, og endnu mere specifikt omkring hvordan applikation må/skal interagere med databasen, og hvordan den skal tjekke og godkende det data, den får fra databasen.

Her forestiller jeg mig så, at man laver nogle små sprog/filformater til formålene, netop så så mange som muligt af brugerne kan sætte sig ind i disse formater og dermed også være i stand til at ændre dem direkte, uden at skulle lære at gennemskue XML først (hvilket jeg personligt heller ikke bryder mig om; man bliver helt skeløjet).\,. Så vi snakker faktisk en form for config-filer, hvis man kender det. Og i disse configscripts skal brugerne så kunne ændre indstillinger for forskellige ting, og her tænker jeg så særligt på følgende ting. Der skal være et config-script, der bestemmer, hvilke nogle domæner, som applikationen må hente URI'er fra. Eventuelt kunne det endda gøres sådan, at brugeren både bestemmer en RegEx for URL'en, samt bestemmer en salgs RegEx (muligvis binær) for dataen. Og måske kunne brugeren endda eventuelt få lov at vælge en transducer, der oversætter det godkendte (af den ``binære RegEx'' (altså en automaton, men brugeren skal jo kunne skrive den på en eller anden måde.\,.)) format til et nyt format, inden det kan indsættes i applikationens HTML.\,. .\,.\,Men ja, i starten skal dette selvfølgelig dog bare være et spørgsmål om at godkende URL'er, og på lidt længere sigt kan det så også måske handle om at parse og godkende de tilhørende URI'er. Det var den ene ting. (14:15)

En anden ting er at definere nogle brugerdefinerede typer for databasen. Dette bliver nok en ret grundlæggende ting, og altså noget som gøres tidligt i den samlede række start-scripts. Og til hver brugerdefineret type, så skal brugeren for det første kunne præcisere, hvordan applikationen skal verificere, at en entitet har den pågældende type, hvilket typisk vil handle om at spørge en brugergruppe om et specifikt prædikat (der så er taget på den givne entitet). 

Og her kan jeg så passende lige indskyde en paragraf om brugergrupperne. Vi er som sagt i applikationslaget, så dette implementeres bare her, men jeg forestiller mig, at der skal være en bestemt fast syntaks for at benævne en brugergruppe, hvilket altså i adgangspunket skal være en linearkombination af bruger-ID'er, nemlig altså hver med konstanter foran. Disse konstanter skal så tolkes som en vægtning af hver bruger --- som jo også kan være en bot, der implementerer en brugergruppe i databaselaget, når gennemsnittet udregnes. Så hvis bruger nr.\ 123 har stemt 0.1 til en rating, og bruger 456 har stemt 0.5 til en rating, og man bruger linearkombinationen, (3 * 123 + 1 * 456), så bliver resultatet for den givne rating altså et (vægtet) gennemsnit på $(0.3 + 0.5)/(3+1)=0.2$. Men dette er ikke det hele, for man kan så også have lov til at bruge et `inf'-keyword i stedet for en konstant (kun en enkelt dog), som giver en uendeligt vægting af brugeren, hvis brugeren altså har givet en rating. Lad os sige at bruger 123 har givet en rating på 0.1, bruger 124 har givet en rating på 0.2 og bruger 135 har ikke givet nogen rating. Så vil resultatet af (inf * 123 + 2 * 124 + 3 * 135) give resultatet, $0.1$ --- og (5 * 123 + 2 * 124 + 3 * 135) ville i øvrigt give resultatet, $(0.5 + 0.4)/(5+2)=0.128571428\ldots$ (he, jeg har ikke divideret i hovedet i lang, lang tid.\,.).\,. Ok, og i øvrigt kunne der også lige være et `me'-keyword til at benævne sig selv, og så kan man jo med fordel bruge `inf * me' i mange sammenhænge. Og sidst men ikke mindst kan man også sætte en konstant ind på brugerens plads, hvilket jo set i bakspejlet gør det lidt forvirrende, at jeg allerede har sat tal ind på brugernes pladser nu her til at repræsentere bruger-ID'er, men i praksis skal der altså i sidste ende ikke være tvivl om, hvornår et bruger-ID indsættes, og hvornår en tal-konstant indsættes. Og hvor en konstant indsættes, så skal det altså tolkes tilsvarende som, at en fiktiv bruger har ratet pågældende prædikat med lige det tal.\,. nå ja, dette tal vil jo altid være et kommatal (mellem -1 og 1), så selvfølgelig vil man nemt kunne vælge en syntaks, hvor man kan kende forskel (for der er en triviel løsning på dette). 

Angående det med at bruge en konstant fiktiv rating i brugergrupperne, så er det faktisk det jeg selv regner med at gøre i den første implementation af applikationen. Jeg regner således bare med at vise et ratinggennemsnit overalt, som dog lige er ændret ved at det har en vis konstant * 0.0 plusset på, så at sige. Så jeg regner altså med at bruge en brugergruppe a la `(1 * meanbot + 5 * 0.0)' (hvilket jo også bare kan skrives som `(meanbot + 5 * 0.0)') . Her er `meanbot' så et kaldenavn der referere til en grundlæggende bot, der rater alle statement ud fra samlede mængde (ikke-karantænede/-udsmidte) brugeres ratings (og som opdatere disse ratings løbende). 
I øvrigt er det værd at nævne, at jeg så forestiller mig, at denne ``meanbot'' så bare kan bruge de sidste (2--3) bytes i dens ratings til at indkode, hvor mange brugere (ikke-karantænede/-udsmidte), der samlet set har ratet udsagnet på det givne tidspunkt. (14:50)

%..Tager lige en kort.. hold da op, egentligt vildt at den allerede er blevet tre. Nå, men jeg tager altså nok lige en kort pause, inden jeg skriver resten..

(15:33) Okay, for så at vende tilbage til de brugerdefinerede typer, så skal brugerne også (i ``config-scriptet'') ikke mindst kunne sætte et grundfilter for hver type. Alle entiteter som de højereliggende applikationsscripts henter skal så i reglen filtreres med disse grundfiltre (der passer til den relevante type). Bemærk at dette så også i reglen (med mindre brugeren har specificeret noget andet (og har mulighed for at gøre dette)) gælder for alt tilhørende data, som applikationen get'er til en hvis entitet, da dette data jo også vil have en type. Og i tilfælde, hvor noget data ikke kan bestemmes til en af de brugerdefinerede typer, så kan brugeren også sætte et other-filter, som filtrerer alle entiteter, der ikke kan tildeles en type ud fra brugerens konfigurationer. 

Men på lidt længere sigt skal brugerne nok endda også gå ind og kunne bestemme specifikke.\,. Nå nej, nu snakkede jeg om et grundfilter, men brugeren skal så \emph{også} (på sigt) kunne ind og bestemme brugergrupperne for specifikke get-metoder. Lad os f.eks.\ sige, at brugeren klikker sig en på en video-ressource. Nu skal applikationen så hente data til videoen. Lad os så sige, at brugeren gerne vil have både undertekster og annotationer til videoen, hvilket applikationen jo så efterfølgende kan hente/``get'e'' til brugeren. Og her kan man så forestille sig, at brugeren gerne vil bruge én brugergruppe til at vælge, hvilke undertekster er mest passende for brugeren, men bruge en anden brugergruppe til at vurdere, hvilke annotationer er passende for videoen. Derfor skal brugeren altså gerne (på sigt) kunne config'e brugergruppen for hver enkle get-metode tilhørende en type. Dette er også derfor (og man kan også nævne mange andre eksempler, som dog har med det samme princip at gøre), at jeg gerne vil have, at man på sigt går væk fra, at gette al data til en ressource i én samlet liste (hvad man nok gerne vil gøre for nemhedens skyld i starten), og i stedet går over til implementere individuelle get-metoder til hver.\,. ``attribut'' til et objekt / en ressource, så at sige. Og så kan hver bruger nemlig selv config'e disse get-metoder yderligere, og specifikt altså gerne ved simpelthen bare at gå ind og ændre brugergruppen (i form af et linearkombination-udtryk som dem, jeg skrev lige før her ovenfor) for hver enkle get-metode. (15:48)

.\,.\,Hm, og var det det omkring dette nederste lad i applikationslaget (når vi lige ser bort fra QL'et.\,.), eller var der mere, jeg skullle sige i denne omgang.\,.\,? .\,.\,Jeg det må næsten være det.\,. Nice.\,:) Okay, jeg har så stadig nogle flere ting, jeg lige skal tænke lidt over. Nogle er ret små (dem skal jeg bare gå at summe lidt over sammenlagt), og så er der nogle lidt større ting, såsom bl.a.\ QL'et (som jo ikke bare skal være SQL, det er klart, selvom det i høj grad nu bare skal minde om, når det kommer til selects), som jo lige er en af de største todos sammen med at få styr på databasen helt præcis --- og også i øvrigt de der delta-(/remix-)strings! --- som jeg jo skal programmere som noget af det første. Men angående applikationslaget, så mangler jeg også at tænke lidt mere over, ikke de \emph{grundlæggende} filtre, men de efterfølgende filtre, som brugeren skal kunne indstille for applikationen. Jeg har noteret til mig selv, at jeg lige skal huske at nævne her, at brugerne gerne må kunne plotte deres filter-fordelingsfunktioner, når de har konstrueret nogle, men jeg skal jo nok lige tænke lidt mere over hele den del.\,. 

.\,.\,Hm, jeg kan lige nævne, at der gerne også må være et `all'-keyword til at konfigurere type-brugergrupper og -filtre (og get-brugergrupper), på samme måde som at der skulle være et `other'-keyword. Og så gælder konfigurationerne for ``all-typen'' så selvfølgelig bare for alle typer, hvor de for ``other-typen'' bare gælder for de entiteter, der ikke matcher nogen brugerdefineret type. 

Og en anden lille hurtig ting er, at databasen selvfølgelig gerne må slette entiteter igen, hvis de ikke længere rigtigt er i brug (og/eller er nedvurderet tilstrækkeligt af repræsentative udsnit af brugerne). Særligt må databasen jo gerne slette gamle ratings fra bots, hvis ingen bruger dem.\,. .\,.\,Tja, og det vil jo bare vise sig, hvordan det bedst giver mening.\,. Men hvis man sletter entiteter, så kan man bare frigøre pladsen og entitets-ID'et (for den pågældende entitetstype), således at nye uploads kan tildeles disse i stedet. 

Okay, jeg skal altså lige have tænkt lidt mere over de ikke-grundlæggende filtre (dem som brugerne frit kan vælge til og fra og udskifte i deres normale brug af applikationen), men ellers kan jeg altså så småt begynde at gå i gang med at programmere den grundlæggende del (startende med den semantiske database og det grundlæggende QL).\,:) Virkeligt nogle fede tilføjelser i denne omgang! (.\,.\,Altså her over de seneste dage, tænker jeg på.) Fedt at være nået hertil! :) (16:15)

(18:07) Jeg har fundet på nogle flere tilføjelser. Angående string-encodingen, så skal man jo selvfølgelig bare bruge den der htmlspecialcharacters()-funktion. Og det er så både det format, man sender til serveren og det format, man får af af serveren. Når man så skal loade scripts/HTML, så skal man så bare specielt i det tilfælde konvertere tilbage. (Og serveren tjekker selvfølgelig altid, at inputtede strings er af det format.)

Angående QL'et, så tror jeg faktisk, at det bare netop skal være en undermængde af SQL, simpelthen. Og så kan man altså implementere eventuelle mere specialbyggede QL-sprog i applikationslaget (UDA-laget). Så man sender altså sine select statements til serveren, som så parser dem med en automaton (en kompileret RegEx) og sender det videre til serveren, hvis det godkendes.

Når det så kommer til insert statements, så er der lige en anelse mere at sige. For det første vil jeg nok gerne trække det tilbage med, at det kun er ratings/semantic inputs, der får timestamp og bruger-ID. For jeg er kommet i tanke om, at nu hvor man (også) kan tilgå databasen rent SQL-style, så behøver man jo ikke at vurdere alle entiteters ``usefulness''.\,. Så ja, nu tænker jeg faktisk lidt bare, at bruger-ID og timestamp kommer med over alt. 

Det gør så faktisk, at insert-statementsne også bare kan være en undermængde af SQL, hvor serveren så skal tjekke at bruger-ID og timestamp kan godkendes. Og her har jeg så nemlig tænkt noget andet, og det er, at man jo så med fordel kunne gøre det sådan, at hvis timestampet er sat til en fremtidig tid (mere end bare nogle sekunder), så kan serveren vælge at tjekke, om der er plads i en buffer og så putte den midlertidigt over i denne, indtil timestampet bliver aktuelt, hvorefter serveren så kan uploade entiteten til den offentlige database. Og denne buffer skal altså være i en privat database. Dette giver en simpel måde, hvor brugere kan obfuskere upload-tidspunktet, hvilket nemlig kan være godt for at forøge anonymitet. Og så tænkte jeg så også, at en måde at implementere, at entiteter kun bliver uploadet til en privat database så bare kunne være ved at sætte en nul-timestamp (eller bare en der ligger før nutiden). For ellers skal man jo vedlægge et flag, men når man alligevel har pladsen til det.\,. tje, medmindre at der kan være en fordel for brugeren i at holde styr på sine egne private timestamps (for sine privat-database-uploads).\,.(?) (18:24) .\,.\,Oh well, ellers må man jo bare sende et public/private-flag (måske i form af `u' og `r' henholdsvis.\,.) med til serveren sammen med insert-statementet, som så altså bare i sig selv kan være fra en undermængde af SQL.\,. Hm, og hvad med selects, skal kan man også have behov for.\,. ja.\,. Her kan man også have behov for at sige ``søg kun i den offentlige database'' --- og måske endda også selects, skal kan man også have behov for.\,. ja.\,. Her kan man også have behov for at sige ``søg kun i den private database'' (foruden ``søg kun i begge databaser''). Så lad der også bare skulle sendes et char-flag med i købet for select-statementsne også. .\,.\,Hov, jeg skal lige komme ind på, at serveren jo.\,. Nå nej, det kommer sig jo gratis af, at man ikke behøver at inkludere værdier i insert statements i SQL, når disse værdier bliver sat automatisk i databasen. Så serveren behøver slet ikke at vide, hvad bruger-ID'et skal være, og brugerne skal så heller ikke have det med, når kommandoerne sendes til serveren. Ja, jamen så er det da bare det! Og ja, det er så applikationens altså applikationens ansvar at sætte timestampet, bare lige for at understrege dette. Ok! Jamen så bliver QL-delen ligepludselig ikke nogen særlig stor opgave.\,:) (18:35)


(15.02.23, 10:10) Timestamps kan jo også sættes automatisk af databasen, så jeg skal nok hellere lave det sådan, at brugere kan undlader at sende timestamps med, og måske kan de så bare sende et eventuelt delay med til serveren (ved siden af SQL-udsagnet).

\ldots Hm, jeg tror faktisk, at jeg skal bruge TINYINTs til at indkode databasetyperne.\,. %.\,.\,Hov, det har MySQL ikke.\,. ..Åh, jo..

\ldots\ Hm, jeg skal lige tænke over, om man bare skal sige, at man kun har BIGINT/long values til SemanticInputs.\,. men på den anden side, så er det kun en TINYINT/char til forskel.\,. .\,.\,Ja, lad mig bare sige, at det kan være det hele.\,. .\,.\,Men det bør dog være en fast konvention, at \emph{hvis} der indsættes en long/BIGINT, så \emph{skal} de første (én til fire) bytes i long-værdien repræsentere et midtpunkt/gennemsnit for ratingen. (13:54)

(14:21) Hm, det går først op for mig nu, at timestamps fylder mere end 8 bytes.\,. .\,.\,Ah nej, det kan godt være på 7 bytes. Ok, så kan man godt indsætte det som en BIGINT.\,. .\,.\,Og man \emph{kan} endda bruge 8 bytes (går jeg stærkt ud fra), hvis man vil have to decimaler på sekunderne.\,. 

(18:27) Hm, jeg behøver nok ikke den der ``meanbot'' i starten, for lad mig bare starte med at tillade AVG() i queries, og så tænker jeg forresten nu, at man skal tage gennemsnittet på ratingværdier ved at sige AVG(value $>>$ 32) (altså hvor $>>$ er right shift-operatoren (som MySQL vist rigtignok har)). .\,.\,Nå ja, og så skal der forresten lige være et indledende select, hvor man sorterer alle andre end den seneste rating fra for hver bruger--statement-kombination. 

.\,.\,Hm, på den anden side, hvorfor ikke bare bruge den der meanbot, for jeg kan jo altid bare sige, at gamle meanbot-inputs i høj grad skal slettes (altså hvor man ikke nødvendigvis sletter alle gamle.\,. Hov, vent, selvfølgelig kan man slette bottens gamle inputs ad libitum, for indtil man sletter gamle bruger-ratings, så vil de jo altid kunne gendannes!\,.).\,. Ja, så lad mig bare indføre den bot, så.\,. (18:38) 

.\,.\,Hm, jeg kan da egentligt også bare starte med at have en regel om, at nye bruger-ratings altid bare overskriver de gamle (når statementet er det samme), for mon ikke de fleste brugere alligevel helst vil have, at deres gamle ratings ikke bliver bevaret. Og så kan man spare lidt plads i den tidlige database. Så jo, lad mig bare sige det. .\,.\,Hm, men så vil det jo så sige, at der \emph{kan} være en fordel i at gemme gamle bot-inputs. Så lad mig sige.\,. Tja, og dog, for hvis det handler om at spare plads, så vil man sikkert spare mere ved at holde på bruger-ratings og så slette alle bot-ratings, for det vil nu nok være relativt sjældent, at brugere vil ændre deres ratings sammenlignet med, hvor tit der kommer nye ratings til et statement (udsagn) (og dermed hvor tit mean-botten --- eller avg-botten, kunne vi også kalde den --- skal opdatere sin rating til samme udsagn).\,. Ok, så vi siger hold på brugerdata og overskriv al bot-data (altså i starten/betaversionen.\,.). (18:46)


(16.02.23, 14:29) Jeg har nogle flere tilføjelser. For det første er der en tilføjelse om, at serveren/erne faktisk \emph{skal} kræve nogle udsagn-ratings omkring en entitet, som en bruger gerne vil uploade. Og derfor \emph{kan} vi lige netop godt droppe timestamp- og user-felterne for alle ikke-SemanticInput-entiteter. Og grunden til, at serverne bør kræve dette, er at brugerne alligevel altid gerne skal vurdere typerne og kategorierne for en ny entitet. 

Angående uploads, så tror jeg, jeg vil indføre et up- og down-rating knapper tidligt i betaversionen, og så vil jeg nok bare bruge en konvention om at disse ratings skal være på max-værdien af en (signed) long, og at intervalbredden (som så i dette tilfælde vil kunne læses alene på 5.\ byte) så sættes til det samme (som jo er halvdelen af max-værdien for en ulong). Og når brugere så vælger prædikater til et nyt upload, så skal applikationen bare automatisk sende sådanne up-rates med. 

Nå, en endnu mere interessant tilføjelse er, at jeg vil indføre compound predicates, som nærmest kan ses som en slags virtuel entitetstype. Man erklærer dem nemlig ved at vælge en relationsentitet samt en objektentitet (til at sætte ind på nr.\ 2 plads i relationen). Herved oprettes så en ny entitet (med eget entitets-ID --- og med en ny databasetype-tinyint-kode), og det, der så gør den ``virtuel,'' er for det første, at databasen så automatisk kopiere alle ratings fra eksisterende DyadicStatements med lige netop den kombination af relation og objekt og giver den til nyoprettede MonadicStatement, som tilsvarer DyadicStatements'ne, men hvor relation og objekt nu altså er udskiftet med den tilsvarende, nyoprettede CompoundPredicate. Og for det andet skal hver ny rating af enten et CompoundPredicate-MonadicStatement eller en relation med en relation--objekt-kombination, der svarer til et CompoundPredicate, have sin rating kopieret automatisk til den anden udgave af sig selv. 

.\,.\,Hm, lad mig dog lige spørge mig selv igen, om ikke det så var nemmere at fjerne Dyadic-Statement-typen, og så bare nøjes med compound predicates.\,.\,? Hm, jeg føler/husker, at der var en god grund, men lad mig nu lige se.\,. .\,.\,Ja, grunden er jo, at man tit kommer til at holde en entitet i hånden, hvor man så gerne vil søge efter en passende objekt-entitet (``objekt'' som i `sætningsobjekt') ud fra en given relationsentitet, som man også holder i hånden. Men hvis man så kun har compound predicates, så.\,. .\,.\,Ja, så kræver det vel et ret vildt/tungt (tabel-)join, gør det ikke.\,.\,? (14:55) .\,.\,Hm nej, for det kræver jo kun ét join for at opnå et view, hvor man har subjekt, prædikat, eventuel relation (hvis prædikatet er compound) og objekt (hvis prædikatet er compound). Og så kan brugerne bare søge i dette view. Jamen skal jeg så virkeligt fjerne DyadicStatements?? Det virker lidt mærkeligt, for konventionelt er DyadicStatements jo det \emph{eneste} man har, hvis vi tænker på det de konventionelle `semantiske web'-teknologier.\,.(!).\,. (15:01) .\,.\,Oh well, vi har dem jo stadigvæk i form af omtalte view, og ja, det giver jo klart god mening at undgå, at dataen skal kopieres i databasen (af flere grunde), så mon ikke bare, det er det, jeg gør.\,. (15:04)

.\,.\,Det koster os lige en gemt long/BIGINT mere i databasen for hver DyadicStatement, hvor man ikke regner med, at nogen vil være interesseret i at bruge prædikatet (men hvor brugerne regnes med altid bare at skulle bruge objektet, efter at de har subjektet (og relationen) i hånden), men sådan må det jo bare være.\,. .\,.\,Og til gengæld sparer vi en TINYINT for hver SemanticInput, og det må jo faktisk næsten gøre, at det kan svare sig rent datastørrelse-mæssigt. (For mon ikke der bliver flere end 8 ratings pr.\ DyadicStatement i snit.\,. det må der gøre.\,.\,:)) 

Ok, og den næste store ting er så, at jeg i betaversionen vil tillade (og implementere), at brugerne kan loade pre-parsede CSS- og HTML-includes til deres applikation ad libitum, hvor man så særligt sørger for, at HTML'et bare ikke må indeholde andre hrefs eller script-srcs, end hvad brugeren på forhånd har godkendt i en indledende config-``fil,'' som hentes til at starte med (fra brugerens private database-afdeling). Og i øvrigt må der så (selvfølgelig) heller ikke være nogen scripts, der har noget kode-indhold, og som altså ikke henter al deres kode fra en src. Brugerne skal så kunne ændre på deres egne href-/script-src-configs, men hvis brugeren klikker sig ind på denne config side, skal der så bare med det samme komme en skarp advarsel om ikke at ændre eller tilføje nogen kilde med mindre man har dobbelt- eller tripeltjekket, at kilden er sikker, gerne ved at søge på webbet eller ved at søge i den semantiske database / det semantiske netværk omkring kilden, og rigtig gerne ved at gøre begge ting! (hvorfor jeg skrev ``tripeltjekke''). 

Så her får vi altså allerede en ret nem måde, hvorpå brugerne allerede kan gå i gang på sikker vis med at indstille alle mulige præferencer for applikationen. 

Ok, men lad mig nu skifte til en ny sektion, hvor jeg så vil tilføje noter omkring, hvad betaversionen skal sigte imod at indeholde (så altså hvad jeg så småt vil sigte mod at bygge). Jeg har nogle få ting, jeg vil skrive nu, og ellers vil jeg sikkert tilføje ting løbende, imens jeg arbejder. Men hvis jeg kommer på nogle flere tilføjelser til noget af det mere grundlæggende, så tror jeg nok, at jeg bare vil tilføje dem i forlængelse af denne sektion (hvor jeg så bare giver nye dataangivelser, men ikke angiver * for ``tilføjet senere end den oprindelige tekst,'' som jeg ellers lidt fjollet har valgt at gøre for nogen af de ovenstående sektioner). (15:33, 16.02.23)


(17.02.23, 12:39) Jeg kom lige på en rigtig nyttig tanke her i formiddags, som helt klart at værd at skrive i denne sektion. Den er, at man i applikationslaget --- og i min betaversion --- skal indføre standard for at skrive attributter ned for en ny DescribedTerm/DescribedEntity med en tilhørende automatisk oprettelse af relations-statements (som nu altså oprettes via et compound predicate (sammensat prædikat)). Jeg tænker så, at indføre en parser til at parse entity descriptions på formen, ``$<$attribut$>$=$<$værdi$>$,$<$attribut$>$=$<$værdi$>$,\ldots'' Og for hver attribut søges der så på, om der findes prædikat-funktion (altså en relation omdannet til en anden-ordens funktion), der svarer til `$\lambda$value : hasAttribute $<$attribut$>$ = value,' og hvis ikke denne allerede findes oprettes en ny. Herefter uploades så et sammensat prædikat, dannet ved at tage den relevante $<$værdi$>$ og give til input i denne prædikat-funktion. Og når dette prædikat så er dannet --- eller dvs.\ først så søges der jo på, om det allerede er dannet, kan man sige (om end dette tjek så er serverens eller databasens ansvar (men det skal jo nok være databasens)) --- så sørger man så for, at en automatisk up-rating af dette prædikat (som altså fortæller at ``$<$attribut$>$=$<$værdi$>$'' for et af attribut--værdi-parrene i beskrivelse-strengen) bliver givet omkring den nye entitet som uploades med denne beskrivelse. 

Dette giver så et rigtigt godt udgangspunkt for applikationen, som gør at brugere allerede fra start lærer, at attributter skal/bør get'es på semantisk vis via relationer/prædikater. Og samtidigt klarer det også den opgave, jeg alligevel skulle finde ud af, hvor at jeg jo skulle gøre det til en standard, at brugerene får uploadet nogle kategori-beskrivende (m.m.) prædikater om den nye entitet, de er ved at uploade. For nu kan dette nemlig altsammen bare ske på en rigtig nem måde (ikke mindst for brugerne), hvor disse prædikater bare automatisk bliver parset fra beskrivelsesstrengen og uploadet.\,! Så med denne tilføjelse er man altså lynhurtigt bare godt i gang så at sige.(!) 

Lige for at komme mere ind på syntaksen, så vil jeg så nok altså gøre standarden til, at brugerne skal (hvis den parser jeg implementerer skal fungere) udforme beskrivelses strengen ud fra den nævnte syntaks, men hvor whitespace dog er tilladt mellem alle tegnene. Og jeg vil også tillade, at man laver attributter og værdier med længere strenge, hvor der også gerne må være kommaer og whitespaces i, og hertil skal jeg så lige finde ud af, hvordan de skal indkapsle disse strenge. Her kunne man jo bare sige, at det skal bruge gåseøjne yderst, og at alle indre gåseøje så skal escapes med `\textbackslash' (Og hvor `\textbackslash' så i stedet skrives som `\textbackslash\textbackslash'), men jeg har også tænkt på at bruge `\{\}' i stedet, og det tiltaler mig nu helt klart også.\,. .\,.\,Ja, for så bliver det mere naturligt at lave newlines.\,. Plus jeg kunne også bruge `\{\}' som en måde, hvorpå brugerne har lov at bryde syntaksen (uden at forvirre parseren).\,. Hm.\,. (13:09) .\,.\,Hm, jeg kan altså mærke, at jeg hælder til at bruge `\{\}'.\,. .\,.\,Parseren ignorerer så en `\{\}'-indkapslet delstreng, der står imellem to kommaer (eller efter sidste komma), og.\,. Hm vent, hvad med at tillade brug af begge indkapslingsmetoder.\,. Ah, og attributter behøves jo faktisk ikke at indkapsles! Okay, det giver mening. Og værdi-strenge kan så indkapsles enten med `""' eller med `\{\}', og så vil jeg også sige, at alle indledende eller afsluttende newlines *(al whitespace i det hele taget, rettere) bliver ignoreret/slettet for `\{\}'-syntaksen, medmindre altså der står et `\textbackslash' foran. Der må desuden godt være indvendige `\{\}'-parenteser, men hvis man vil skrive et `\}' uden at der først har været et (ikke-afsluttet) `\{', så kan brugeren så gøre dette ved at skrive `\textbackslash\}' i stedet, hvilket tolkes som `\}' --- dog medmindre at der også et `\textbackslash' (eller et ulige antal af dem) lige foran dette `\textbackslash\}'. Og brugen kan også skrive `\textbackslash\{' frit, uden at det tæller med som begyndelsen på en nestet `\{\}'-parentes. Så ja, det er altså min umiddelbare forestilling (nu) for denne syntaks.\,. (13:22)

(14:06, 17.02.23) Ah, i stedet for at indføre den der mean-/avg-bot, så kunne jeg også bare indføre et view med en automatisk beregnet AVG() (eller rettere AVG($x$ $>>$ 32) $<<$ 32), som brugerne så får adgang til. Og ja, i samme view kan jeg i øvrigt også give COUNT()'en for antallet af brugerratings. 

(17:25) Hm, i stedet for at have Compound-/FunctionalTerms, så tror jeg måske, jeg i stedet bare laver DescribedTerms om, så de får et description\_schema (i stedet for bare en description) samt et (nullable) schema\_input. For dette gør så tilmed også, at det bliver mere oplagt for brugerne at definere faste skemaer for nye uploads af entiteter af forskellige (semantiske) typer. .\,. \ldots Ja, og så laver jeg også bare en standard (i applikationslaget) til parseren for, hvordan man skal erklære inputvariablene, og hvordan disse så automatisk skal sættes ind i beskrivelsen/beskrivelsesskemaet, inden at attribut--værdi-parsingen udføres. (17:39)

(18.02.23, 11:22) Okay, jeg tror faktisk, at DescribedTerm (eller hvad jeg ender med at kalde det) i stedet simpelthen skal defineres med en liste af prædikater først til at bestemme type/kategori, og som nr.\ 2 liste skal der så komme en liste af attributter, hvor applikationen så faktisk automatisk selv finder de mest populære attributlister fra hver type/kategori-prædikat og så sætter dem sammen i én samlet liste. Disse attributter skal så tilsammen udgøre alle de ``definerende felter'' for entiteten, man er ved at uploade. Derefter kommer en ny attributliste, som applikationen også prøver at hente automatisk, hvor entitetsforfatteren så kan tilføje data omkring entiteten (altså data, der ikke behøves for at referere til / definere entiteten, men som relaterer sig til entiteten alligevel (og som brugere typisk gerne vil have serveret, når de iagttager entiteten)). Og ja, så skal der altså også være en femte liste, der så holder forfatterens (eventuelle!) inputs i disse datafelter. 

.\,.\,Hm, nu hvor dette så bliver meget specielt rettet mod at oprette ressource-entiteter, ville de så ikke give mening at genindføre en relations-/prædikat-funktion-/funktion-type.\,.\,.? \ldots Ja, for man regner jo ikke med, at nogen vil søge på det specifikke prædikat, der bliver dannet i en tilføj-data-, tilføj-links- eller tilføj-beskrivende/definerende-data/tekst-relation.\,. 
.\,.\,Hm, på den anden side, så kunne dette godt ske lige netop med tilføj-links-relationer.\,. Hm.\,. .\,.\,Hov, men den tilføjede data skal da heller ikke være en del af selve DescribedTerm; det kan bare være en del af, hvad applikationen spørger til, når man vil oprette et nyt term. Og derfor kunne man jo bare lade være med at tilføje data for sådanne relationer --- som så typisk bare vil have en fast liste af prædikater, der definerer relationen, og så en.\,. hov, men hvad man attribut-navnelisten?\,.\,. \ldots Hm, attributterne er jo også i sig selv relationer, så.\,. Okay, lad mig lige tænke over, hvad der bedst giver mening.\,. (12:09) .\,.\,Ah, man kunne vel i princippet bare oprette et ret grundlæggende Apply()/Insert(Sentence)Object()-term-skema.\,. (/-relation).\,. (12:13)

.\,.\,(12:22) Ah, nu ved jeg, hvad jeg gør. Jeg beholder de seneste ændringer i DescribedTerm, men jeg indfører så faktisk også relationer igen, men ikke via DyadicStatements. I stedet indfører jeg dem via CompoundPredicates.\,. /CompoundTerms.\,.  

\ldots (12:41) Hm, nu tror jeg faktisk hellere, jeg vil.\,. Nå ja, det bliver jo egentligt lidt det samme, hvad den databasen angår. Så jeg tror altså, jeg vil indføre CompoundPredicate (ikke CompoundTerm! for det er ikke meningen, at man skal danne andre funktioner end prædikat-funktioner!), og at jeg vil ændre DescribedTerm som beskrevet, og nu forestiller jeg mig så at lægge op til, at CompoundPredicates så altid (måske på nær i få tilfælde) skal ses og bruges som attributbestemmelser. Så når brugeren indfører en relation, så skal de meget hellere se det som, at de indfører en attribut til en vis type/kategori. Og ja, det er jo lige netop disse attributter (i form af hvad man også sagtens kan se/tolke som relationer), som applikationen skal hente i nævnte liste nr.\ 2, når en ny entitet / et nyt term oprettes. .\,.\,Og den store forskel på at se det som relationer og på at se det som attributter er altså, at relationer ofte ses som noget generelt, der giver mening at snakke om for alle entiteter, og som altså derfor ikke har et egentligt domæne så at sige, men hvor attributter i langt højere grad tolkes som havende et domæne, specifikt nemlig et domæne for subjektet i relationen (og også tit for objektet, men det er faktisk ikke så vigtigt for os). Og det vigtige ligger nemlig i, at brugerne gerne må udforme relationerne til meget specifikke typer/kategorier. Og hvis så en relation/attribut kan gå igen på tværs af kategorier, jamen så må man bare tilføje den en passende overkategori, og hvis nu den kan bruges på tværs af overkategorier, jamen så må man bare tilføje den hver især til alle de relevante overkategorier!\,. Simpelthen!\,. (12:54) .\,.\,Hm, og en anden tilsvarende fortolkning er jo så bare, at i stedet for at se relationerne som attributter, så ser man dem bare som domæne-specifikke relationer, hvilket nemlig giver det samme.\,.\,:) (12:57)

.\,.\,Hm, men spørgsmålet er dog, om ikke jeg vil kalde det for attributter frem for.\,. ja, ``domæne-specifikke relationer,'' for det sidste er alligevel en mundfuld, hvis man gerne vil præcisere det.\,. .\,.\,Oh well, men hvis jeg holder mig til at kalde det CompoundPredicates i databasen, så ligger dette jo bare åbent for fortolkning i applikationslaget.\,. 

.\,.\,Ah, men et godt spørgsmål er, om man skal lade CompoundPredicates være førsteklas-sestermer, eller om man i stedet bare skal sige, at de må fremhæves til førsteklassestermer via (Described)Terms (på en eller anden måde.\,.), før de bliver dette.\,.\,? .\,.\,Hov, men de skal jo være første.\,. nå nej, for man kunne jo måske også ændre Statements, lad mig lige se en gang.\,. .\,.\,(Hm, hvis ikke de behøver at være førsteklasseborgere, så kunne man eventuelt ændre Statements til altid at have tre inputs, som jeg også havde det en gang.\,.) .\,.\,Hm, det virker faktisk på en måde rigtigt (i.e.\ at det er det, jeg bør gøre), men lad mig lige overveje, hvordan brugerne så kan tilgå relation/attribut--input/værdi-kombinationen som et prædikat.\,. (13:16) .\,.\,Hm, hvad med at brugere kan oprette ``virtuelle prædikater'' på samme måde, som jeg har været inde på før.\,.\,? .\,.\,Hm, men så skal dette jo også indgå i den fundamentale del af databasen (for det skal jo gemmes, hvilke virtulle prædikater er blevet oprettet.\,. tja, medmindre.\,. hm.\,.).\,. .\,.\,Men lad mig da lige nævne den mulighed, jeg skulle netop til at nævne, hvilket er at oprette en VirtualPredicate-type, .\,.\,som så kommer til at svare helt til CompoundPredicate, bortset fra at når man rater et sådant, så er det i virkeligheden den relaterede relation--input-/attribut--værdi-kombination, der bliver ratet i stedet. .\,.\,Hm, men lad mig lige tænke lidt over det.\,. (13:30) 

.\,.\,Hov, nu fik jeg også lige en alternativ tanke: Hvad med at droppe prædikater, og så bare formulere alt med relationer i stedet?\,.\,. For selv hvs vi tænker på prædikater såsom ``isFunny,'' jamen her kan man jo også bare definere `is(adjective)'-relationen, og så definere alverdens adjektiver. Og hvad angår kategorier, så er det jo også netop ret naturligt, at sige `hørerUnder(Kategori)' og så indputte kategorien `fysik' f.eks.\ i stedet for at tænke på et helt prædikat om at `tilhøre(r)KategorienFysik.' .\,.\,Og ja, selv hvad angår tillægsordprædikater, så er det nok endda også mere naturligt for os (ift.\ hvordan vi typisk tænker og gør med vores naturlige sprog) at tænke i at definere adjektiver frem for hele prædikater. Så ja, det er da lige før, at jeg skal fjerne prædikaterne og så bare kun bruge relationer (ligesom konventionelle sem-web-teknologier). Så skal jeg bare lige overveje det her med, at brugerne gerne også skal.\,. ja, definere domæner til hver relation, men det spørgsmål svarer jo næsten sig selv: Man skal så bare lægge op til i systemet, at hver ny relation bliver tildelt domæner (og især faktisk hvad subjektet angår, selvom objektdomænet også nok bør angives, da det også kan hjælpe med forståelsen af relationen).\,. (13:45) .\,.\,Det skal dog ikke forstås som, at databasen (eller control-serveren) skal verificere disse domæner. I stedet hjælper domænerne bare med forståelsen/fortolkningen af relationen, og ikke mindst gør det nemmere at søge i relationer --- og at finde relationer, der passer på lige det man søger. .\,.,Hm, men vent, for prædikater er stadig nice at søge på, så.\,. Ah, men så søger man jo bare på unikke relation--objekt-kombinationer, når man vil søge på prædikater.\,. .\,.\,Hm, nu tænkte jeg lige på: skal alle termer så ikke bare bestå.\,. nej, det giver ikke mening, lad mig lige tænke videre.\,. .\,.\,Ah, nu ved jeg det måske. Hvis alle prædikater nu bare formuleres som relation--objektinput-kombinationer, kan vi så ikke bare lave typen Predicate, som har disse to felter, og så.\,. ja, er det så ikke nærmest bare det.\,.\,? .\,.\,Ja, som så bare svarer helt til CompoundPredicate fra før, bare hvor Statements så \emph{kun} tager prædikat-input af denne type, og hvor vi så heller ikke behøver at kalde dem ``Compound,'' hvis det jo alligevel ikke er andre muligheder. Hm, jeg tror, det bliver svaret (og så beholder jeg altså umiddelbart, som jeg bliver ved med at sige, de seneste ændringer der for DescribedTerm).\,. (13:59) .\,.\,Hm, jeg skal nok faktisk gå væk fra at kalde det ``termer'' og over til at kalde det Entities i stedet, for i matematik og i formel logik er ``termer'' jo aldrig prædikat- eller relationssymboler, mener jeg (og jeg har også lige søgt kort på det), så ja, lad mig sige `entiteter' i stedet.\,. .\,.\,Ja, og så kan `termer' passende være, hvad man kalder overtypen/-kategorien af entiteter, der ikke er hverken prædikater eller relationer --- eller lister. (14:07)

(14:45) Hm, jeg tror muligvis, jeg vil føje endnu en type til i form af `kendte ord' (eller hvad jeg skal kalde det), som altså er entiteter, der bare er beskrevet med et enkelt ord eller navn, eller en enkelt titel eller sammensat navneord.\,. .\,.\,Hm, eller også kunne man bare implementere dette via mine DescripedEntities, hvor første input så beskriver, at der er tale om ting, der i høj grad bør kunne forstås uden yderligere kontekst end bare ordet/navnet/titlen, og hvor inputtet så er dette ord/navne/denne titel, men det kan jeg lige tænke over.\,. .\,.\,Hm, nej, måske var min første tanke bedre, nemlig med bare at sige, at relationer --- især de meget grundlæggende af slagsen, men også i det hele taget --- meget gerne må indeholde i sig en beskrivelse af, hvordan det resulterende prædikat skal fortolkes (eller rettere forsøges at fortolkes), hvis inputtet er en tekst. Så ja, lad os bare sige det i stedet. (15:00)

(15:57, 18.02.23) Ah, jeg kan godt nøjes DescribedEntities, som jeg har tænkt mig, og så kan de helt basale typebestemmende prædikater, eller rettere relationer, som det jo bliver nu, bare få en tom liste som første input-liste.\,. .\,.\,Og, så kan det nemlig bare være standard, at applikationen klargør en `description'-attribut som den eneste attribut, når første liste er den tomme liste, og nr.\ tre (og sidste (for de ikkebeskrivende attributter bliver jo ikke et faktisk felt i DecribedEntity-typen, men bliver i stedet bare sat via eksterne relationer)) input-liste bliver så altså denne tekstbeskrivelse. .\,.\,Hm, så er det så bare lige at finde ud af, hvilken type description'-attributten (som i: attributnavnet) skal have.\,.\,? .\,.\,Hm, det må jo bare være (small-)tekst-typen, og så skal den grundlæggende fortolkning (som er antaget allerede fra start af, inden man fylder entiteter ind i databasen) altså bare være, at tekster i denne liste opfattes som attributnavne (på samme måde som fortolkningen af felter og feltnavne i OOP). .\,.\,Hm, og så kan man jo sådan set beholde denne fortolkning ret langt.\,. hvilket måske giver meget god mening.\,. .\,.\,Tja, og dog, for man må jo også gerne gøre det nemt for parseren at.\,. Ah vent, men man kunne jo også definere en god `x.hasAttribute(y)'-relation fra starten af, hvor y kan være et tekstinput.\,. For måske er det alligevel dobbeltkonfekt at specificere domænet, når dette allerede bliver specificeret i første inputliste (i DescribedEntity-typen).\,. .\,.\,Ah, men man kunne jo bare se attributter som noget, der i princippet, selvom de jo kan omfortolkes som relationer, er noget helt andet en relationer, i.e.\ har en anden type.\,. .\,.\,Hm, men for at alt dette skal gå op, så skal der jo næsten være en relationsfunktion (altså en prædikat-funktion-funktion), der kunne hedde noget i retning af `hasAttribute,' og som så tager attributnavn først og bagefter værdi, inden det så returnere et prædikat som endeligt kan tages på et subjekt.\,. .\,.\,Hm, og en måde at indføre dette på, kunne jo så eventuelt bare være, at.\,. hm nej, men måske at give Predicate to undertyper, hvilket så kan være.\,. Hm, okay der er to muligheder angående denne tanke: Man kan enten sige, at relations-inputtet i Predicate-typen godt kan være en tekst, hvor.\,. ah ja (tja), hvor teksten skrives som en relation, og hvor `has' derfor altid sættes foran automatisk, når applikationen skal parse en attribut og lave den om til en relation/et prædikat. Men jeg vil faktisk langt hellere gøre det sådan, at tekst-relationer altid bare skrives som attributter, og at `has' altså aldrig gemmes i databasen, men at `has' i stedet bare kan sættes foran i applikationen, når denne skal vise en liste over relationer, hvor disse tekstdefinerede relationer også kommer med.\,. eller endnu bedre: man kunne sørge for at alle tekstdefinerede relationer printes efter fulgt af et ` ='! For umiddelbart, som jeg lige kan se det, så vil dette virke overalt (både når hele prædikatet (eller hele udsagnet for den sags skyld!) printes, og også bare når relationen vises for sig)! .\,.\,Fedt!! (16:34)

(17:23) Ah, og vi kan forresten passende blande nr.\ 1 og nr.\ 2 liste sammen til én, nemlig således at hvert tekstinput i denne liste så bare opfattes som et attributnavn. Og den anden type input i denne liste, som man så typisk gerne vil se (medmindre vi skal gøre det til en klar restriktion), er så input af prædikat-typen. .\,.\,Hm, jeg tror nu nok, jeg vil undlade at gøre det til en fast ting, at resten af inputtet skal være prædikater, og måske skal dette ingen gang være en standard. Man kunne således også forestille sig, at applikationen (samt brugernetværket) også fint kan tolke kategori- og adjektiv-inputtyper, hvorved applikationen så selv automatisk finder hhv.\ relationerne `belongsTo(category)' og `is(adjective)' frem og uploader de relevante prædikater om den nye entitet dannet ud fra disse. Så ja, lad os sige, at input-databasetyperne også sagtens kan være DescribedEntities af vilkårlige custom-typer og -kategorier. (17:32, 18.02.23) .\,.\,Hm, og skal liste nr.\ 2 så bare give input kun til attributterne, eller hvad gør vi her.\,.\,? .\,.\,Hm, i princippet kunne listen indeholde input til både attributterne og DescribedEntity-relationerne.\,. .\,.\,Ja, lad mig bare sige det sådan, og så er det altså op til brugernetværket samt applikationslaget at sørge for, at nr.\ 2 liste fortolkes på denne måde, og at parseren i applikation altid kan genkende relationsentiteter (beskrevne), således at den automatisk kan danne de rette prædikater hørende til et nyt upload. Ok! 

(19:05) Hm, det kan også være, at man bare skal lægge kraftigt op til, at brugerne kan gøre brug af at definere simple relationer ved at skrive et attributnavn efterfulgt af `='.\,. .\,.\,Hm, jo lad mig faktisk gøre sådan, for det er jo ikke alle, der er vant til attributter, og man kan jo også sige, at der vel må være en god grund til, at vi har det med at bruge verber i daglig tale. Fint. Så jeg TTexts skal altså i reglen fortolkes som verber, medmindre de slutter med `=,' hvorved de så skal tolkes som første del af en attribut-erklæring --- og hvor man altså altid udelader det indledende `has,' der ellers ville gøre det til verbum i stedet (hvis man nemlig så også erstattede `=' med `er lig')!\,. Ok. (19:13, 18.02.23) \ldots (Hov, det er vist bedre at bruge VARCHAR i stedet til små tekster.\,.)

(22:24) Predicates kan selvfølgelig også bestå af et (beskrevet) prædikat samt den tomme liste som input. 

(19.02.23, 8:53) Det hele (i overført betydning) forsimples en del, hvis vi i stedet kræver, at attributter også skrives med punktum foran, og ikke bare med `=' bagefter, for så kan alle tekst-(/varchar-)relationer nemlig tolkes som verber, bare hvor man så har en konvention om at `$.<$attribute$>$=' tolkes som `has $<$attribute$>$ equal to.' 

.\,.\,Nå ja, og lad mig også pointere, at man nok bare vil bruge `.type=' for prædikater og relationer, og så lade `.type=term' være default (underforstået hvis ikke andet er sagt). Og så vil de typiske term-entiteter, som uploades, have en nr.\ 1-liste (en ``categorizing list'') bestående først af en række prædikater (med databasetypen Predicate! (ikke DescribedTerm)) efterfulgt af en række ``beskriv hvad kendetegner dette (denne) term specifikt (inden for den kategori du lige har valgt via prædikaterne)''-relationer. 

.\,.\,Hm, skulle man egentligt dele denne ``categorizing list'' op i to lister?.\,. Hm, hvorfor ikke.\,.\,? (9:07) .\,.\,En ``categorizing list'' og en ``specifying list''.\,. .\,.\,Hm, og man kan jo egentligt ligeså godt sørger for at dele det op, så DescribedTerm dermed får tre hovedfelter (eller ``hovedkolonner'' hvis man tænker på det som en tabel i stedet for en type). Ja, det må være bedst sådan. (9:13) .\,.\,Ah, og dog, for man vil jo tit gerne bruge dem sammen. Hm, jamen så må nr.\ 1 input jo bare være en L2List (længde-to-liste).\,:) .\,.\,Hm, men hvordan finder jeg et fornuftigt tabel-kolonnenavn til ``categorizing predicates and relations for specification''??.\,. .\,.\,He, ``catepreds and specirels''.\,.\,x) .\,.\,Hm, det er faktisk lige før det virker, hvis jeg putter underscores mellem hver forkortelse også, for fonetikken (og rytmen i det) gør vist faktisk, at det er til at huske.\,. he.\,. Men lad mig lige tænke.\,.\,:) (9:24) .\,.\,Hm, hvad med bare ``CPs and SRs?''.\,. (9:26) .\,.\,Ah, jeg tror, jeg vil kalde kolonnen `cpred\_and\_srel\_lists.' (9:28) .\,.\,Ah, eller jeg kunne endda bare kalde det `cpreds\_and\_srels.'

(11:04) Hm, eneste problem med min attribut-sytaks er, at den ikke er så naturlig, når der kan være mere end én værdi (hvad der oftest kan).\,. .\,.\,Ah, vent! Lad mig da bare bruge kolon til sidst (i stedet for `='), når man regner med, at der typisk/ofte vil være flere end én værdi! (11:09)

(18:01) Ah, jeg kom lige på en idé, som jeg nu hurtigt derefter kan se, faktisk løser et Russell-paradoks, som jeg faktisk ikke rigtigt har tænkt på indtil nu, nemlig at man ikke kan have et isPredicate(), der siger, at det selv er et prædikat. Og løsningen (som jeg altså kom på inden jeg tænkte på, at det løser det paradoks, og at idéen --- eller en anden idé der løser samme paradoks --- derfor er nødvendig) er at se Predicates, hvor første input, nemlig relations-/prædikat-inputtet (som jeg i øvrigt bare kalder relation i databasen), er en tekststreng (varchar), som.\,. ja, eller rettere at se første input i disse tilfælde som ``anonyme prædikater/relationer'' i den forstand, at man ikke direkte kan tage nogen prædikater eller relationer på dem. Dette er nærmere bestemt fordi, de aldrig får et ID i databasen --- eller dvs.\ de har et ID til den pågældende tekststreng (varchar), men denne tekststreng bør ikke i sig selv tolkes som et prædikat eller en relation; kun som en datastreng af chars. Ja, denne datastreng vil kunne forbindes med en semantisk fortolkning, men ID'et referere altså til de \emph{u}-fortolkede dataobjekt. Det er kun, når man putter dette data ind som første input i Predicate-typen, at strengen bliver fortolket (som et prædikat eller en relation i stedet for en karakterstreng). Og dermed kan disse fortolkede prædikater eller relationer altså siges at være anonyme, fordi de ikke får noget ID i databasen, der referere semantisk til dem. 

Men hertil skal det så siges, at man jo godt kan indføre en relation, der siger `has (useful) relation (or predicate) interpretation' --- eller tilsvarende: `is (useful) relation (or predicate) lexical item.' Og på den måde kan brugerne godt finde en måde alligevel at lave.\,. tja, jeg skulle lige til at sige ``lave en fællesmængde af alle prædikater inklusiv de anonyme, der bruges i databasen,'' men mon ikke vi så vi alligevel bliver nødt til at glemme håbet om så at få selve `is (useful) relation (or predicate) lexical item' med i denne mængde, det går jeg ud fra (for ellers får man sikkert Russells paradoks). Men ja, dette var nu alligevel også bare en sidenote, for det er også helt fint, at der er et lille antal anonyme prædikater/relationer, især hvis man bare bruger dem til at komme i gang med semantikken, og altså ikke bruger dem aktivt i den efterfølgende brug af systemet. 

Så lad mig defor sigte efter, at finde på nogle gode grundlæggende anonyme prædikater og relationer, der kun er beregnet til at skabe at udgangspunkt for resten. Bemærk i øvrigt, at dette nok gør standarden omkring `$.<$attribute$>$='-syntaksen overflødig i det helt grundlæggende plan, men derfor kan man godt stadig gøre brug af denne standard i et nedre lag i applikationslaget alligevel, nemlig fordi jeg regner med, at forfatter til (beskrevne) prædikater og relationer gerne skal føje en `.identifierSuggestion' til sådanne, i hvert fald til den de regner med skal bruges meget i det nederste lag af applikationen (altså nederste lag af, hvad jeg kalder ``applikationslaget'' (se ovenfor for mere om, hvad jeg mener med dette)).\,. Dette er dog bare, hvad jeg lige umiddelbart tænker; det kan også være, at `.identifierSuggestion' slet ikke bliver nødvendigt, hvem ved?\,. (18:27, 19.02.23)

\ldots (18:49) Hm, hvad med at have `isPredicate,' `isRelation,' `isCategory,' og `isTerm'-/`isObject'/`isInstance'/\ldots som de (eneste) grundlæggende (``anonyme'') prædikater, og så have `.lexicalItem=' som den (eneste) grundlæggende relation? .\,.\,Hm, eller i stedet for `.lexicalItem=' kunne vi også bare have `.definedBy=', hvilket både er mere elegant og lettere forståeligt/genkendeligt, og samtidigt er det også mere generelt, fordi man så oplagt kan sige, at værdien/sætningsobjektet både enten kan være en ``lexical item'' eller en ``description'' (altså en lidt længere tekstbeskrivelse).\,. Hm.\,. .\,.\,Ja, og lad mig så bare opfordre til at.\,. hm, vent, hvad med at have begge relationer alligevel (altså have både `.lexicalItem=' og `.description=')?.\,. .\,.\,Hm nej, måske ikke, for måske vil man jo gerne netop have, at `.description=' \emph{ikke} bliver en ``anonym'' relation. .\,.\,Hm, nu hælder jeg faktisk næsten lidt tilbage til bare at have `.definedBy=', som så kan gøre double duty så at sige, for måske gør det alligevel ikke noget, hvis `.definedBy=' er en anonym relation.\,. .\,.\,Ah, jeg er i tvivl, så lad mig lige tænke over det, og lad mig også tænke over, om der er en måde helt at undgå anonyme relationer på.\,. Hov, forresten, prædikater bliver jo aldrig helt anonyme, for man kan bare bruge den resulterende Predicate-entitet som nyt prædikat-input (lige præcis i disse fire tilfælde og ikke andre.\,.).\,. 
\ldots Ah, jeg tror det kan løses, hvis bare jeg lige indfører en ny databasetype Relation (ligesom Predicate).\,. Hm, men spørgsmålet er bare, om det er det værd.\,. (19:24) .\,.\,For jeg kunne vel også bare indføre `.definingLexicalItem=' som den (eneste måske) grundlæggende relation.\,. .\,.\,Ah, der \emph{er} en stor fordel ved at indføre en Relation-type, for det kan nemlig lige præcis bruges til at tillade relationer defineret alene ud fra lexical items, men uden at dette så kommer til at koste brugerne, fordi de så ikke får noget relations-ID af sådanne definitioner. .\,.\,Hm, og så kunne man måske godt endda sige, at Relations-typen specifikt er for at danne relationer ud fra lexical items.\,. Hm.\,. .\,.\,Hm, hvad med at jeg i stedet bare opretter en ny type, der hedder LexicalItems, og så løser alle problemerne (måske) herved?!\,.\,. (19:35) .\,.\,Nej, for jeg vil nok gerne kunne skelne typen med det samme, men hvad i stedet med at jeg indførte typen VerbalClause, som altså netop er Relation-typen, hvor input kun kan være en ``lexical item''-streng? .\,.\,Og så skal min `$.<$attribute$>$(= $|$ :)'-syntaks bare være underforstået, når det kommer til, hvad man betegner med en `verbal clause.' (19:42) .\,.\,Hm, eller hvad med at lave en Relation-type, hvor input kan være to forskellige ting (ligesom for min nuværende Predicate-type), og hvor Predicate så også.\,. Nej vent, den kan jeg beholde, som den er, men til gengæld kunne jeg måske prøve at lægge op til en konvention om aldrig at bruge beskrevne til andet end med det samme at fylde ind i.\,. Hm nej, det duer måske ikke.\,. .\,.\,Ah, jo! Man kunne måske sige, at nyoprettede.\,. Hm, måske ikke.\,. .\,.\,Hm, nu har jeg muligvis lyst til at gå tilbage til `isPredicate,' `isRelation,' `isCategory,' `isObject,' og `.definingLexicalItem=' som de eneste grundlæggende/``anonyme'' prædikater og den eneste grundlæggende/``anonyme'' relation hhv. (19:55) .\,.\,Ja, og så må brugerne altså bare indføre attributter via DecribedTerms, hvilket jo egentligt også er meget fornuftigt, for så kan attributterne forklares, plus man kan tilføje andre varianter og former (bl.a.\ ved at omformulerer dem til normale lexical items) af dem, enten med det samme eller efterfølgende. (19:59, 19.02.23)

(20:20) Ah nej, nu tror jeg, jeg ved (nogenlunde), hvad jeg gør i stedet. Jeg indfører følgende grundlæggende typer: Relation, Category og Object (udover Predicate, som jeg allerede har). Og ligesom at Predicate ``selv ved'' om dens ``undertype'' er en `relation med input,' `e omfortolket tekst' eller en `prædikat defineret af sig selv givet som et DecribedTerm' (hvor sidstnævnte nu bare skal erstattes med undertypen `beskrevet prædikat' og så skal DescribedTerm-typen slettes), uden at det ændrer databasetypen af Predicate-instansen, så skal tilsvarende altså bare gælde for de andre tre grund-(ikke-data-)typer (Relation, Category, Object). Jeg tænker så faktisk at indføre endnu en type kaldet LexicalItem, som altså er en.\,. nå nej, vent, dette behøves nok ikke. Nej, never mind; man inputter bare textstrenge i disse tilfælde, og så er omfortolkningen (altså ``derefereringen'') jo bare indforstået her. .\,.\,Ah, og hvis man nu alligevel skal bruge mindst to input-BIGINTs til alle disse fire typer, nemlig fordi beskrevne entiteter (som der nu er fire typer af, nemlig i form af én undertype til hver af de fire ikke-data-typer) jo alligevel skal have to lister, jamen så kunne man måske bare sige, at hvis nr.\ 2 input ikke er den tomme liste for en beskrevet undertype, så er nr.\ 2 input simpelthen bare en (valgfri) ``description''.\,.\,!\,:) .\,.\,Fedt!!\,:) .\,.\,Hov nej, ikke for en beskrevet undertype! (for der vil de to inputs jo være de to lister (med ``cpreds'' og ``srels'')) men i stedet altså for den.\,. ja, hvad skal vi egentligt kalde den?\,.\,. Hm, lad mig for nu kalde den for den `leksikalt beskrevne undertype,' men jeg ændrer nok disse kaldenavne. .\,.\,Hm, ja, nej, lad mig egentligt hellere bare kalde denne undertype (som der er fire udgaver af for hver ikke-data-type) for den `simple undertype,' simpelthen. (20:38)

(20.02.23, 14:17) Jeg har overvejet nogle ting i dag omkring kategorier (også i går aftes) og ``koncepter'' (bare i dag), og nu er jeg så lige kommet på en idé, der muligvis kan løse de ting, jeg gerne vil løse, på en elegant måde (set fra det grundlæggende (database-)lag). Idéen er at starte med at give de ikke-simple entiteter en liste af relationer efterfulgt af en liste af lister, hvor hver liste så i denne liste liste så skal inputtes som relationsobjekt i den relevante relation fra den første liste (og hvor den nyoprettede entitet så bliver relationssubjektet). Hm, og så kunne man i øvrigt stadigvæk dele det op vertikalt i kategoriserende felt-input og specificerende felt-input.\,. .\,.\,Nå nej, det behøver jeg nok ikke, når nu jeg forestiller mig, at brugerne skal gøre stort brug af kategorier (hvad de skal!\,.\,.), for så bør en specifik (under-)kategori gerne indeholde al den relevante information, som jeg før nu her ellers har tænkt mig, at ``cpreds''-listen skulle indeholde. .\,.\,Ah, men vil det så sige, at en kategori (Category) også gerne selv skal indeholde information om den ønskede srel-liste?\,.\,. 
\ldots Hm, det ville være ret smart, hvis man kunne gøre det sådan, men problemet bliver jo muligvis, at det så bliver svært at ændre dette.\,. .\,.\,Hm, medmindre man måske tilføjede en grundlæggende relation til kategorier a la ``.OptionalField:''.\,. .\,.\,Hm, men alternativt kunne man så i stedet have ``.Field:'' (hvor felt-relationerne så rates ud fra vigtighed), altså i stedet for at gøre dette til en indre, fast ting i hver kategori.\,. Hm, ja, mon ikke det bliver svaret, og så er det bare spørgsmålet, om man ikke så kan gøre de ikke-simple entiteters input mere simpelt.\,.\,? 

.\,.\,Lad os se, måske kan man så erstatte cpred-listen med kategorier.\,. tja, lad mig bare tage én ting ad gangen. Så de ikke-simple entiteter kan have en liste af kategorier for det første, hvor relationen ``belongs to''/``.Category:'' er underforstået.\,. Herefter skal der så komme en liste med.\,. hm, ja, det bliver jo så en liste af prædikater, men disse prædikater dannes så automatisk af applikationen (i ``applikationslaget'') ud fra brugerens input til diverse felter, hvilket så kan deles op i beskrivende felter og datafelter (og som hver især tildeles en `vigtighed' via brugerratings (som brugere giver i forbindelse med den relevante kategori)). Hvis entiteten tildeles mere end én kategori, så loades alle relevante felter bare fra alle kategorierne (og ordnes stadig efter `vigtighed,' men måske opdelt i grupper af kategori alligevel). Bemærk i øvrigt, at et ``felt'' defineres af en relation, så felter kan nemt gå igen på tværs af kategorier. Nå ja, og en stor pointe ved at bruge `kategorier' er, at de hver især kender deres egne overkategorier, og dermed kan (og skal!) de også loade alle felter fra overkategorien.\,. Hm, men måske skal man så også kunne fjerne felter fra en overkategori igen ved at nedstemme dem.\. nej.\,.\,! For ressourcer vist i en inderkategori skal også kunne vises i en overkategori, så alle felter fra en over-/forælder-kategori skal arves til underkategorierne!\,. Ok, så langt, så godt. Nu kommer jeg så til, hvad der ellers kunne være af nyttige relationer udover ``.Category:'', som man kunne benytte, når man opretter nye ikke-simple entiteter.\,. (15:18)

.\,.\,(15:22) Hm, jeg tror faktisk allerede, jeg har en rigtig god idé om, hvad den muligvis sidste relation kunne være udover ``.Category:''/``belongs to''. Og nu har jeg i øvrigt også lige fundet ud af, at jeg hellere bør kalde relationen for ``belongs to'' end ``.Category:,'' og det kommer jeg nemlig til om to sekunder. Jeg tror den muligvis sidste relation skal være ``is related to''. Og fælles for ``belongs to'' of ``is related to'' er, at de er beregnet til (i hvert fald som en af deres primære opgaver) at udgøre en barne-liste-/barne-kolonne-mulighed i relationsobjektet, som i disse to tilfælde enten er en kategori (som entiteten tilhører) eller en term --- og jeg tror nemlig, at jeg vil gå tilbage til at kalde det Terms --- (som entiteten er relateret til). Dette er så i modsætning til alle ``felterne,'' hvilke er beregnet til at angive ting, der er interessante at få vist, når man er på (har selekteret) entiteten. (Og for lige at præcisere, så er altså i modsætning til det andet, for ``belongs to'' og ``is related to'' handler nemlig om at føje ting til henholdsvis kategorien eller termen (nemlig under særlige underfaner, som henholdsvis kunne hedde `Subcategories' og .\,.\,`$<$type$>$s related to this $<$type of this entity$>$''), når brugeren har selekteret denne kategori/dette term.\,.).\,. Hov, der kunne nu måske godt være en ``is related to'' for hver ikke-data-type. Og måske skulle jeg så droppe den konvention, som jeg skulle til at foreslå, hvad angår ``.Category:'' vs.\ ``belongs to'', hvilket nemlig var at bruge førstnævnte syntaks udelukkende for felter, for det virker til at den også kan blive gavnlig for relationer. (For så kan man nemlig passende opdele ``is related to'' i: ``.RelatedTerm:'', ``.RelatedPredicate:''.\,. hm.\,.) 

Okay, virker umiddelbart som nogle gode standarder, men jeg tror lige, jeg må tænke lidt over de her ``Concepts'' (altså muligvis en `koncept'-type), som jeg har tænkt over i dag\ldots\ (15:45) .\,.\,Ah, men er hele pointen ikke bare, at man så nu bare kan indføre Koncepter, som en overkategori af Termer, og for nyoprettede prædikater og relationer, kan man så føje sådanne koncepter til som relationsobjekter i relationen, ``.RelatedTerm:'' (som så i virkeligheden næsten ligeså godt kunne hedde ``.RelatedConcept:'' i stedet.\,.).\,.(?) (15:51) .\,.\,Tjo, tja.\,. Hm, måske skulle man bare sige, at de to første inputs er en liste af kategorier og en liste med (specificerende) prædikater, men at vi derefter så har et tredje (optionelt) input .\,.(som muligvis kan deles op i to), som består af en liste af først en relationsliste (hvor man så kan have ``.Related[...]:''-agtige relationer), og derefter en liste af inputlister, hvor hver af disse input lister så gives til den relevante relation. Hm, det lyder umiddelbart lidt kompliceret, men det kunne måske give god mening sådan.\,. (15:56, 20.02.23) 

.\,.\,Og lige for at præcisere: Jo, så `koncepter' bliver bare en underkategori af Terms. Ah, jeg tror faktisk, at jeg virkeligt er nået til et fornuftigt punkt her.\,:) 

\ldots Ah, på når lige, at det måske ville give mere mening bare at implementere dette tredje input (inkl.\ funktionaliteten omkring det) udelukkende i applikationslaget.\,. (16:15) .\,.\,Hm, man kunne også gøre det, at man rigtig nok implementerer det i applikationslaget, men at man så bare også giver mulighed for, at brugeren kan vælge at inkludere de resulterende prædikater dannet ud fra dette ``tredje input'' i specifikationsprædikat-listen. Ja, ok, så fordi der jo er den mulighed, så er der ingen grund til ikke at gøre det sådan. Så lad os sige, at ``tredje input'' bliver implementeret i applikationslaget, og at ikke-simple entiteter så i databaselaget derfor bare kommer til at have to input-felter, nemlig en kategori-liste og en specifikationsprædikat-liste.\,:) (16:21, 20.02.23)

\ldots (17:01) Ah, nu ved jeg det. Endnu bedre: Der skal bare være ét felt for de undertyper, jeg førhen har kaldt DescribedEntities (og som jeg til tider i dag har kaldt de ikke-simple undertyper), nemlig et felt bestående rent af prædikater. Og så implementeres resten simpelthen i applikationslages. De første prædikater i denne liste vil så typisk bare være (et fåtal af (ofte bare én!)) ``belongs to $<$category$>$''-/``.Category:$<$category$>$''-prædikater. Og nu hvor jeg nemlig tænker, at underkategorier skal kende deres egne overkategorier, så er det nemlig ofte tilstrækkeligt kun at give én kategori. Og dermed behøver jeg altså ikke længere at tænke i at separere den ``kategoriserende del'' fra den ``specificerende del'' af inputtet, for den ``kategoriserende del'' vil nu typisk kunne klares med meget få (ofte bare ét) prædikater. Nice, nice.\,:) (17:09, 20.02.23)

(18:34) Okay, jeg tror så faktisk også, at jeg går tilbage til at gøre det utypet i det grundlæggende lag igen, når det kommer til ikke-data-entiteter --- og selv endda for prædi-kater (så jeg fjerne Predicates-typen igen) --- således at ikke-data-entiteter udelukkende gives typer på semantisk vis. Jeg tror så, jeg vil lave tre ikke-data-entitets-undertyper: Den ``simple'' som beskrives ud fra et `lexical item' og en `description' (valgfri), ``prædikat-beskrevne entiteter'' som altså beskrives ud fra én prædikatliste, og som den tredje undertype ``sammensatte entiteter,'' som jeg så egentligt kun har i sinde at bruge lige netop til at danne prædikater ud fra en eksisterende relation og et relationsobjekt. .\,.\,Hm, skal jeg så prøve at sørge for, at der kan blive en tabel eller et view med alle entiteter på én gang.\,. eller måske bare alle ikke-liste-entiteter.\,.\,? .\,.\,Hm, hvis vi tager alle ikke-liste-entiteterne, nemlig Users, Statements (ikke SemanticInputs), de tre typer ``ikke-data-(ikke-bruger-eller-udsagn)-entiteterne,'' samt resten af ikke-liste-datatyperne, så har de jo alle samme højst to inputs.\,. Og hvis tekst- og binær-typerne bare får en reference til en underlæggende datatype --- som man måske så ikke kan se i interfacet med applikationslaget (når man ser fra applikationslaget) --- så kunne dette altså nok godt lade sig gøre.\,. Hm.\,. (18:54) .\,.\,Hm, og hvis alle bare er i samme tabel, nemlig en tabel (altså en databaserelation) over alle entiteter (og lister og semantiske inputs regnes altså så ikke for entiteter .\,.\,hm, jeg kunne også kalde dem units i øvrigt.\,.), så sparer vi vist også noget plads, for så behøver hver entitet(/``enhed'') nemlig kun at holde én TINYINT, nemlig den der beskriver entitetens egen (entitets)type, i stedet for at skulle holde TINYINTs for alle input-tabelkolonner. Ja, jeg tror faktisk, at det bliver det her, jeg gør.\,. men jeg kan så lige se på, om SQL har en måde at forene søjler på (det har det sikkert), og i så fald kan jeg jo godt lave.\,. Hm, nej, jeg tænkte at lave alle tabellerne for sig også, og så oprette den samlede entitetstabel som et view, men så går det ikke, hvis hver underliggende tabel kan have kollisioner imellem ID'erne.\,. .\,.\,Lad mig lige skifte paragraf i øvrigt.\,. (19:04)

.\,.\,Hm, en mulig løsning, hvis man gerne vil have de separate tabeller kunne dog være, at sørger for, at alle entiteter af samme type har den samme unikke række af mest betydende bits.\,. Hm.\,. .\,.\,Hm, det ville måske gøre det bedre i forhold til, hvis man kommer i risiko for at mangle long-adresser, også muligvis hvad angår muligheden for opskalering, hvis man sørger for at dele typerne op, så lad mig lige tænke lidt over det.\,. .\,.\,Hm, atten milliarder milliarder.\,. .\,.\,Ja, så vi kommer ikke til at mangle long-addresser.\,. .\,.\,Okay, jamen så lad mig sige.\,. Tja, hm.\,. .\,.\,Jo. Jeg har lige tjekket, at man godt kan forene (med alias og union) tabeller i SQL, og ja, jeg har lyst til at beholde en tabel for hver entitetstype og så bare forene dem til én samlet i et (såkaldt) view. Og jeg vil så sørge for at deres ID'er ikke kolliderer simpelthen bare ved at starte fra hver deres mest betydende byte (gerne med så mange 0'er på de mindst betydende bits i denne byte som muligt, hvorfor ikke?\,.). Dermed kan ID'erne overføres direkte as is til det samlede view, og dermed for vi altså heller ikke behov for nogen TINYINT-typeflag overhovedet på denne måde. Fint (fedt)!\,.\,.\,:) (19:28)

(21:23) (Nu hvor det grundlæggende er utypet, så bliver Russull-paradokset vist heller ikke noget problem overhovedet, \emph{hvis} altså det overhovedet ville være det i den typede udgave, det er jeg ikke helt sikker på alligevel. Men det er også ligemeget.)

(21.02.23, 10:18) Ej, jeg fik så mange gode idéer i går aftes. Jeg vil lige skrive et par af dem her, og så vil jeg gå ned og skrive resten i forlængelse af den næste sektion. Lad mig nævne her, at lister, ligesom tekster, også bør være med som (førsteklassesborger-)entiteter. Dermed skal der også bare være én listetype, der så bare holder en længe og en underlæggende liste (som ikke er i en tabel som kan ses i interfacet med applikationen). Og så skal brugerne i stedet bare kunne bede serveren om at get'e alle eller en andel af alle elementerne i den liste (som så sendes til applikationen), og så kan applikationen videre finde d af, hvad dem vil med disse. 

Angående om brugere og semantiske inputs skal regnes for entiteter: Tja, måske ikke i starten (første version/betaversionen), men jeg bør alligevel sørge for at deres ID'er ikke kolliderer med resten, hvis nu de på et tidspunkt skal gøres til entiteter. Og hvis de på det tidspunkt holder for meget data i sig (i deres ``tabelrækker''), så kan man jo bare gøre noget tilsvarende, hvor man lader dem holde en reference til resten af deres data (enten i form af en reference til en privat tabel eller i form af en reference til en liste-entitet). 

Men en af de klart største idéer, jeg fik i går aftes, var omkring, at jeg skal væk fra det med at sende SQL-kommandoer fra applikation til serveren, for jeg skal i stedet implementere, hvad der svarer lidt til det QL, jeg havde tænkt på, men hvor queries'ne simpelthen specificeres i HTML-attributterne! Så skal applikationen læse disse attributter, som eksempelvis kunne være nogle a la: querySubjekt=$<$entity ID$>$, queryRelation=$<$entity ID$>$, queryObject="get", askUsers=$<$``user group'' arithmetric expression$>$, numberOfElementsToGet= $<$number$>$, maxDateForInput=$<$date$>$, filters=$<$list of references to filters to use --- possibly in the shape of HTML div IDs where filter data (predicate IDs and parameter values) --- can be found$>$, allowedHRefs=$<$list of references to allowed hrefs --- also possibly in the shape of div IDs$>$.\,. Applikation skal så læse alt dette, men \emph{ikke} sende det direkte til serveren! I stedet skal det bare sende mere simple quries, hvor de kun lige er de første tre attributes her i denne liste, der bliver brugt plus lige en enkelt bruger --- eller brugergruppe, hvis databasen er begyndt at indeholde sådanne (for det emne skal jeg nemlig også lige genoverveje). Applikationen sender så bare flere af disse queries af sted til serveren, hvor den måske bare spørger om 100 eller deromkring elementer ad gangen, hvad ved jeg (på stående fod)?\,. Og så sætter applikationen selv de lister, den får (med tilhørende ratings fra spurgte bruger/brugergruppe), sammen til den endelige liste, som så skal blive til den liste af entiteter, som brugeren skal se i sidste ende. Herefter kan den så begynde (via AJAX) at bede serveren om at få serveret indhold omkring disse entiteter, hvilket jo så typisk vil ske ved at query'e hver enkelt element i listen efter nogle relationsobjekter. Og dette kan så også forgå på sammen måde, for applikationen starter jo så bare med at generere den HTML, der skal være omkring indholdet i hvert element, og de forskellige divs i denne HTML kan jo så også bare få qeury-attributter sat, sådan at applikation efterfølgende kan læse disse (i rækkefølge én ad gangen i listen) og så (via AJAX) query'e serveren for indhold til hver enkelt indholds-div i dem.\,!\,:) 

Bemærk derfor, at brugere der ombygger applikationen for sig selv og andre brugere, de behøver derfor kun at dele HTML og CSS med hinanden. Jeg kan altså derfor udelukke al JavaScript (mener jeg), når det kommer til applikationsudvidelser delt mellem brugere internt igennem sem-netværket. :) 

Nå, jeg har nogle flere ting fra i går aftes, jeg gerne vil sige, men da de relaterer sig meget til applikationslaget specifikt, så lad mig gå ned i næste sektion (som i skrivende stund kun har to paragrafer) og skrive idéerne der. Og hvis jeg finder på flere ændringer til det grundlæggende lag, så vil jeg nok bare oprette en ny sektion at skrive det i den (for jeg kan mærke, at der er for mange afhængigheder i det på tværs af lagene, så jeg ender nok bare med at skrive alle nye idéer i kronologisk rækkefølge alligevel.\,.). Ok.\,:) (11:09, 21.02.23)

(13:21) Nå, nu kommer jeg alligevel tilbage til denne sektion og tilføjer følgende: Ved at sørge for at compound predicates får deres eget most significant byte i deres ID'er, så ved man altid, når man har et prædikat i hånden, om det er et (sammensat) relationsprædikat eller ej. Og derfor kan man godt omdanne SemanticInputs, hvis man vil, så at der igen bliver en prædikat--relationskolonne (hvor prædikaterne så \emph{ikke} må være sammensatte prædikater), og endnu en kolonne med eventuelt (nullable) input, hvis prædikat--relationsentiteten i førstnævnte kolonne er en relation. Og når en bruger vil rate et sammensat prædikat, jamen så gives der bare én rating, nemlig til det samentiske input med relationen i. .\,.\,Hm, og måske bliver det faktisk nemmere alt i alt at implementere gennemsnittet via en bot-bruger. Og så kan control serveren bare løbende bede databasen om at opdatere ratingværdien (samt dato) for dette semantiske input. .\,.\,Hm, og selvom dette egentligt hører til i næste sektion, hvis vi snakker om betaversionen, så lad mig da bare sige, at hver bruger bare har én rating pr.\ prædikat, og at man altså ikke gemmer gamle ratings. For jeg regner jo alligevel ikke med at implementere den der ``maxDateForInput=$<$date$>$''-mulighed i betaversionen. Og det forøger også kun anonymiteten, at gamle rating (og tidspunkter) ikke bliver gemt. Og ja, lad mig så nemlig også bare undlade at gemme datoer (eller dato-tider for den sags skyld) for semantiske inputs i betaversionen. (13:34)

.\,.\,Ah, og så skal nøglen til semantiske inputs faktisk bare være bruger, predikat/realtion og (nullable) relationsobjekt (altså alle sammen ID'er), for hermed bliver dette jo unikt for hvert semantisk input! (13:42) (Og hvis man bruge automatisk voksende tal som ID'er, så vil disse ID'er jo kunne afsløre tidspunket alligevel, hvis de alle sammen er synlige oppe i applikationslaget.) .\,.\,Og fordi jeg er ret overbevist om, at SemanticInputs aldrig skal være med som førsteklassesborger-entiteter --- om så de bliver synlige for applikationen eller ej --- så er der altså ikke nogen risiko ved at bryde mønstret med at bruge BIGINTs som ID'er. *(Hov, jeg skulle selvfølgelig have sagt `relations\emph{subjekt}' i stedet for `objekt,' det er klart. I øvrigt tænker jeg, at denne sammensatte nøgle så nok skal ordnes efter subjekt først, så bruger, og så relation/prædikat-entitet. (18:13)) 





\section{Mål for betaversionen}

(15:35, 16.02.23) Okay, helt kort vil jeg bare lige skrive lidt om, at jeg nu forestiller mig, at der i en undermenu af fold-ud(men muligvis konstant ude, hvis vinduet er stort nok)-menuen skal være følgende ``over-kategorier,'' som man kan trykke på og få vist en liste til højre for. Der skal være ``populære (så altså ordnet efter popularitet) term-kategorier,'' ``populære prædikat-kategorier,'' ``populære relations-kategorier'' og ``populære prædikater'' (ikke prædikat-kategorier men simpelthen bare prædikater straight up).\,. Det kan være, jeg finder på / kommer i tanke om flere.\,. I listevisningerne for alle disse skal der selvfølgelig altså være en popularitetsrating, så man kan up- eller down-rate entiteten på den pågældende liste. For alle kategorier skal der så også være en knap til at få en ny kolonne med underkategorier og en knap til at få en kolonne med entiteter inden for den kategori (som også bare ordnes efter popularitet --- eller faktisk rettere efter usefulness --- som en standard ting).\,. .\,.\,Hm, jeg vil stadigvæk have det sådan, at man også skal kunne klikke på en vilkårlig entitet og tilføje den til sit ``arbejdsbord.'' Og nu tænker jeg nemlig så (og dette er en af de ting, jeg specielt gerne ville nævne), at dette arbejdsbord simpelthen bare skal være en XML/HTML struktur, som brugeren er fri til at redigere (og hvor en ny redigering så kan flushes til den private database). Og i al sin enkelthed skal denne XML/HTML-struktur så bare indeholde en række variabel-definitioner, hvor brugeren giver variabelnavne til alverdens entitets-ID'er --- og hvor strukturen altså i første omgang ikke er betydende for andet end brugeren selv, idet denne så skal kunne implodere og eksplodere tags i editoren. Men strukturen må gerne følge en standard, der så gør at applikationen automatisk kan indsætte nye variabeldefinitioner i denne struktur (og på en passede plads), når brugeren klikker `gem til arbejdsbord' på en entitet vist i en liste. 

.\,.\,Hm ja, og lad mig ikke sige så meget mere i denne omgang, for nu fik jeg nævnt alle de ting, jeg gerne ville skrive om. Når jeg så bygger på min prototype, så kan jeg tilføje flere ting omkring, hvad jeg gerne vil sigte mod i starten. (15:56)


(11:47, 21.02.23) Okay, det kan være, at jeg egentligt fik sagt det meste (og det vigtigste) ovenfor i forrige sektion. Jeg har også en lille idé om, at ``fold-ud-menuen fra venstre'' måske bare skal være en kolonne-/søjletype i stedet i applikationen, som altså så kan foldes ud fra alle andre søjler. For så kan man også nemt folde denne søjle ud i overlayet, når man besøger andre hjemmesider (med visse genkendelige URL'er). Det overlay forestiller jeg mig jo skal kunne foldes ud fra højre i form af en søjle, og hvis brugeren så bare kan folde, hvad jeg før så som ``fold-ud-menuen,'' ud til venstre for denne søjle, så kunne overlayet jo komme rigtig godt og nemt fra start herved. Og så kan man jo også bare folde andre søjletyper ud ad libitum fra den indledende søjle i overlayet. .\,.\,Ah, og hvis man så designer enkeltressource-søjlen, så den selv kan genkende, om den.\,. Tja, ellers hvis bare man laver en speciel søjle til overlayet (som jo selvfølelig skal kende URL'en for hjemmesiden), .\,. Ja.\,. Jeg har faktisk ikke mere at sige om det, for ja, man laver bare overlayet som en speciel søjletype til formålet, som så kender til den nuværende URL, og som så bare er specielt beregnet til at fungere på andre hjemmesider.\,. Ah, men vent. En første udgave af overlayet kunne jo bare være at bruge den normale enkeltressource-søjle! Dette er ikke den optimale løsning, men så er man til gengæld nemt fra start, og brugerne vil så derfra have incitamentet til selv at lave/forbedre den indledende overlay-søjle. Fedt nok, og hermed vil jeg altså nok bare nøjes med at bruge den normale enkeltentitetssøjle, som benyttes på hjemmesiden, som den indledende søjle i overlayet. :) .\,.\,Hov nej, man vil jo sikkert også gerne have en ratingsøjle, der tager udgangspunkt i en specifik ressource (og hvor titlen for entiteten så kan vises i toppen af søjlen). Så det er selvfølgelig bare \emph{den} søjletype, som jeg skal gøre til den indledende søjle i overlayet. Og så kan brugerne så også trykke sig fra den søjle og hen til den normale enkeltentitetssøjle, hvis de vil det, så ja, dette vil være et rigtigt godt udgangspunkt i overlayet. (12:05)

.\,.\,Lad mig lige prøve at finde frem til noget omkring optimering af database-queries.\,. 
\ldots Hm, har lige læst en smule om clustered indexes, og vil også læse lidt mere, men jeg tror allerede jeg kan svare på noget, som jeg også tænkte på i går aftes, nemlig om ikke bare jeg skulle nøjes med altid at lave relationsobjektet være det, som skal get'es i en query i betaversionen (altså sådan at queryObject="get" altid er underforstået). Det vil nok sige, og så bliver det bare applikationens opgave at sørge for, at ``belongs to''- og ``is related to''-relationerne altid uploades både forlæns og.\,. Hm, men hvad så med ratings?\,. hvad med om man i stedet lavede en slags ``virtuel \ldots''.\,. tja, eller også kunne det også bare være applikations opgave også at sørge for, at ratings af prædikater med de relationer i sig altid bliver givet identisk to steder.\,. Hm, det må jeg jo lige tænke over. Men lad mig lige læse videre.\,. (12:39)

\ldots\ (17:56) Okay, jeg har læst lidt, og blevet en smule klogere. Det korte af det lange er bare, at jeg på et tidspunkt nok skal oprette en partitioning af SemanticInput-indexet --- med flere partitions særligt for DescribedEntities.\,. Hm, måske kan jeg endda faktisk lave det specifikt for Term-entities, selvom jeg nu ikke længere har denne type forskel, for selvom alle entiteter (altså hvad jeg lidt ser som ``semantiske entiteter'') har den samme type i princippet, så kan jeg jo godt give dem forskellige mest betydende bytes, således at man i praksis kommer til at kunne kende forskel på deres under typer (og f.eks.\ i forbindelse med en partitionisering).\,. .\,.\,Men ja, og det er rigtig godt (og fornuftigt), at jeg nu gør databaseimplementationen skjult i (interfacet med) applikationslaget (for jeg har nemlig også i sinde at insert også bare skal beskrives i et JSON-format, eller noget i den stil). For det gør, at jeg også har mulighed for selv at eksperimentere med andre databasetyper, hvis jeg nu f.eks.\ finder ud af, at hierarkiske databaser i virkeligheden er bedre egnet til formålet (hvad måske ikke lyder helt tosset.\,.). 

.\,.\,Nå, men noget andet jeg lige skal overveje, som jeg kom i tanke om, imens jeg skrev forrige paragraf her, er at jeg lige skal genoverveje.\,. Hov nej, never mind.\,. Ja, eller ikke never mind, for måske har jeg en tilføjelse, men det kan være, at det bliver en lille en.\,. (18:08) .\,.\,Okay, jeg har lige lavet en tilføjelse til forrige sektion. (18:13) Og tanken om at ordne den sammensatte nøgle efter subjekt først, så bruger, og så relation/prædikat-entitet, er en jeg fik tidligere i dag i øvrigt. Og som jeg forestiller mig applikationen nu, så tror jeg virkeligt langt de fleste queries kommer til at tage udgangspunkt i ét subjekt ad gangen, hvortil man så for et fåtal af forskellige brugere/bots spørger efter nogle prædikater og relationer (og muligvis nogle forskellige for hver bruger), og hvor man så for hver prædikat bare henter en rating og for hver relation henter én eller flere af de elementer, der er rates højest af pågældende bruger/bot. Så dermed tror jeg altså, at langt de fleste queries fra brugerne i almindelighed vil blive super effektive at eksekvere (hvis vi altså ordner indexet på lige præcis denne måde)!\,:) (18:21)

.\,.\,Nå ja, og så skal det endda siges, at jeg i første version af sem-netværket/hjemmesiden nok bare vil (starte med at) have det sådan, at brugerne altid bare ``spørger'' avg-botten om gennemsnittet og antallet af ratings. Og så kan jeg altid udvide derfra. (For eksempel kunne en af de første tilføjelser være at brugerne også kan query'e deres egne ratings, hvilket man så kan forestille sig typisk vil blive brugt således at brugerens egen rating trumfer avg-bot-ratingen.) :) (18:25) 

\ldots Hov, det kan forresten også godt være, at det kan give mening ikke at have rela-tion/prædikat-entiteten med i det her composite clustered index, hvis man altså overhovedet kan vælge det, når nu denne tabelkolonne \emph{er} med i den primære nøgle.\,. Hm.\,. 

\ldots Ah, nu kom jeg lige på, gad vide om ikke, der kunne være en måde at sørger for, at hver subjekt--bruger--prædilation( fik jeg lige lyst til at kalde det)-række får én reference.\,. Tja eller rettere: Hver subjekt--bruger--prædikat-række får en rating værdi som den eneste ikke-nøgle-kolonne, og hver subjekt--bruger--relations-række for så i stedet.\,. hm, hvad med at de får, et fåtal, måske bare én eller to, entitets-ID'er, som så skal være det/dem, brugeren har ratet højest for pågældende relation, og derefter kan i sidste ende så være en liste med andre entitet--rating-par (ja, for man skal forresten også have ratingen in-line/on-page i rækken for hver relationsobjekt man har in-line/on-page i rækken.\,.).\,. (19:02) .\,.\,Ah, eller man kunne også slutte med to ting: først en VARBINARY, som inkluderer et antal entitets-ID--ratingværdi-talpar mellem 0 og et lille tal såsom.\,. hov, nej, det skal måske ikke være et lille tal.\,. Men ja, dette kunne man gøre, og så eventuelt slutte af med en nullable BLOB eller større VARBINARY, som man så ved bliver gemt off-page fra tabellen.\,. Hm, nu kommer så et oplagt spørgsmål i forlængelse af dette, for det kunne jo så også være, at man i stedet skal dele SemanticInputs op i tre tabeller, nemlig én med prædikater og.\,. Tja.\,. .\,.\,Hm, tanken var at dele det op i.\,. Ah, jo! Og så kunne man lave et samlet indexed view over alle tre tabeller!! (19:13) .\,.\,Tja, og dog; det gør det vel egenligt ikke meget anderledes fra, end hvis man laver én indexed tabel.\,. Hm, men jeg hælder nu altså meget til det der med den der VARBINAY til, hvad vi kunne kalde `.:'-relationerne (altså de relation, hvor man forventer meget.\,.).\,. Ah, men vent, bliver det ikke nærmest kun den omvendte ``belongs to''-relation, og den/de omvendte ``is related to''-relationer, der kommer til at få mange relationsobjekter pr.\ bruger/bot, som jeg tænker mig at betaversionen skal fungere.\,.\,? .\,.\,Hm, jo, men det ændrer faktisk muligvis ikke på, at det med den der nullable VARBINARY som en kollone i SemanticInputs måske kunne være ret effektiv. .\,.\,Hm, tjo ja, men måske skal man bare have to tabeller (dog ikke tre). For hvis man putter `.:'-relationerne, altså nærmere bestemt dem med mange relationsobjekter pr.\ bruger/bot, i sin egen tabel, så kan man tilgå denne, når man skal navigere videre fra den entitet/ressource, man har selekteret, hvor prædikat-og-`.='-relationerne -- og også relationer, hvor man bare forventer et fåtal af relationsobjekter pr.\ bruger.\,. måske.\,. --- så meget mere bruges i sammenhæng med, at entiteten/ressourcen enten skal placeres et sted på en entitets/ressource-liste (i en applikationssøjle), og at datafelter specifikt relateret til ressourcen skal hentes/get'es lige efter, at entiteten er blevet placeret.\,. Hm, men hvis jeg regner med, at disse to ting skal foregå i to separeret i tid efter hinanden, jamen så \emph{er} det måske en helt fin idé at dele det op i tre databasetabeller.\,.\,:) (19:31) Hm, lad mig lige tænke over det hele.\,. 

.\,.\,Hm tja, men hvem siger, at man ikke kunne have lyst til at prefetche felt-dataen, det lyder da som en meget fornuftig idé egentligt. Eller rettere, man prefetcher jo nemlig bare long-/BIGINT-adresserne på dataen, så ja, hvorfor ikke prefetche dette som standard?\,. Og så kan det jo godt være, at nogen af entiteterne bliver filtreret fra alligevel i listen, men derfor kan det jo stadig være smart, at sende den data med dem alligevel (da det jo bare drejer sig om et fåtal longs for hver ressource). Hm, jamen så kunne man måske passende dele SemanticInputs op i maximum to tabeller, sådan at prædikater og datafelt-relationer er i den samme tabel. (19:38) .\,. .\,.\,Hm, og da jeg ikke kan tro, at det vil give meget mening at prefetche elementer fra f.eks.\ underkategorier, kategorielementer eller relaterede entiteter, så vil det nu alligevel give fin mening at separere mange-objekt-relationerne over i sin egen tabel. .\,.\,Hm, men man kan nu stadig godt lave et view over alle tre tabeller, og dette kan jo så være det view, som brugerne ser (oppe fra applikationslaget af). 

.\,.\,Og ja, lige for at gentage, så er det gode jo, at applikationslaget alligevel ikke kan se, hvordan.\,. Hm, tja, medmindre at applikationslaget dog lige skal vide.\,. ja.\,. Det kan være at queries så også lige skal holde information om, hvordan man gerne vil prefetche data til ressourcerne, når man spørger efter en liste af ressourcer. Men det kan jeg jo bare sige, at sådan skal det være; brugerne skal også definere, hvordan ``felt-dataen'' hentes for hver ressource, i samme qeury som også beder om listen. .\,.\,Hov vent, men prefetching giver jo kun virkelig mening, hvis man spørger den samme bruger/bot om datafelterne, som man også spørger om listen.\,. Hm, og det kan jeg jo ikke regne med, i hvert fald ikke, hvis jeg gerne vil have det sådan.\,. ja, nej, det giver ikke mening. Det giver kun rigtigt mening at adskille det hele i tre omgange af queries (hvor den indledende query også deles op efter bruger.\,.).\,. Hov, måske er jeg faktisk helt på vildspor her. Lad mig summe over det hele, og så vende tilbage, når jeg har noget fornuftigt at sige.\,. .\,.\,Hm, men jeg kan lige sige: Det giver rigtignok slet ikke mening at tænke i at prefetche lange lister. Og dermed kan vi altså roligt sætte mange-objekt-relationer over i deres egen tabel, hvis det er.\,. .\,.\,Hm, og prædikater og få-objekt-relationer må meget gerne være i samme tabel, så så langt, så godt.\,. (20:00) 

.\,.\,Hm, og man vil nok i øvrigt også gerne have opdelte bots, så de hver varetager noget meget specifikt, og derfor vil man sikkert typisk gerne bede om alle de relationer fra en bot, som den har ratet, når først man har valgt overhovedet at bruge den.\,. 

.\,.\,Ah, okay, man skal regne med at prefetche feltdata-entitets-ID'er for alle ressourcer i sammen omgang som, at man henter prædikat-ratingværdier om ressourcerne, og dette gælder så uanset om man henter feltdata og diverse prædikater fra samme bruger, eller om man måske både henter forskelligt feltdata fra forskellige brugere(/botter) og også henter forskellige prædikat-ratinger fra forskellige bruger. Man prefetcher altså bare så meget som muligt her (men dog ikke mange-objekt-relations-data), når først man henter prædikat-data fra entiteten alligevel. .\,.\,Tja, eller måske skal brugerne i applikationslaget også kunne vælge at slå denne feltdata-prefetching fra i visse tilfælde, hvis man regner med at skulle vægte virkeligt mange ressourcer og skære dem ned til ganske få (og derfor gerne vil have hver hentning af prædikat-data til den enkelte ressource til at gå så hurtigt som overhovedet muligt). Men ja, ellers er en god sigtesnor altså, at felt-data gerne skal prefethes sammen med prædikat-dataen, når applikationen er i færd med at danne en ny liste ud fra en brugerdefineret ordning--filtrering. Og det er meget rart for mig at kunne have dette i baghovedet.\,. (20:23)

\ldots (20:37) Ah, og forresten, nu hvor jeg regner med at fylde mange-objekt-relationerne over i sin egen tabel, så giver det ikke mening det med VARBINARY'en. Jeg kan i stedet bare have én tabelrække pr.\ objekt, hvilket er meget dejligt at tænke på. 

(22.02.23, 10:18) Som jeg også tænkte på i går aftes: jeg føler ikke, at der er nogen grund nu til at putte mange-objekt-relationerne over i sin egen tabel, så. Måske især ikke.\,. ah, især ikke fordi man jo.\,. nej.\,. .\,.\,Nå, det kan man tænke over på sigt, hvordan man eventuelt kan dele det op igen for at styrke performancen, men i starten kan jeg bare have det hele i samme tabel. .\,.\,Ah, men jeg kunne måske i det mindste sørge for at give de grundlæggende mange-objekt-relationer, som jeg regner med, skal bruges meget, nogle særlige leading bytes, hvorfor ikke?\,.\,:) 

(11:51) Hm, måske giver det faktisk en smule bedre mening at have brugeren først i SemanticInput-nøglen.\,. Jeg er i øvrigt begyndt at kalde SemanticInputs for Statements bare, selvom det første er en smule mere korrekt. Men nu hvor jeg ikke har en Statement-tabel, så synes jeg alligevel det giver mening at omdøbe SemanticInputs til Statements. Og så er det bare underforstået.\,. Tja, måske ændrer jeg det faktisk tilbage, men lad mig nu se.\,.

.\,.\,Ah nej, det giver rigtignok mest mening at have subjektet først i nøglen, og især hvis man måske sørger for løbende at vedligeholde indexet, så ordnen af bruger--bots'ne forøges at holdes således, at bots der oftes bruges sidder tæt sammen i ordnen, og især hvis deres brug også er korreleret.\,:) (11:59)

.\,.\,Hm, jeg tror jeg vil kalde det for StatementInputs for nu.\,:) (12:01)

(15:46) Hm, jeg kan ikke have object=NULL i StatementInputs, hvis object skal være en del af nøglen. Nu har jeg så lavet det om så 0 er reserveret som ID til at signalerer, hvad der svarer lidt til NULL. Men problemet er så lige, at jeg ikke sparer den plads. Og nu kom jeg så til at tænke på, at det næsten er en smule synd at bruge SQL, når nu jeg bruger så mange konstante felter/kolonner så langt hen ad vejen.\,. Hm.\,. .\,.\,Hm, ja, jeg må jo lige læse om andre databaser også. .\,.\,Men lad mig nu bare lige færdiggøre SQL-implementationen først, og så også bare bruge den i starten, indtil jeg får taget mig sammen til at prøve at implementere den semantiske database i en anden type (underliggende) database.

(16:32) Ah, jeg er lige kommet i tanke om, at mit system med at bruge den mest betydende byte ikke duer, når vi skal til at inkludere data-termer såsom datetimes og longs. Så nu vil jeg i stedet lave et konstant type-flag i hver tabel, som teknisk set bliver en del af term-ID'et, hvis jeg kan.\,. \ldots\ Ja, for jeg venter bare med at indføre typeflagene til Term-viewet, hvor de så bliver virtuelle kolonner. (18:02) .\,.\,Og det bliver i øvrigt et ikke-indexed view, så formålet er med andre ord altså bare at bruge dette view i compiletime.\,. .\,.\,Hm, hvorfor jeg jo egentligt ikke behøver at lave unions på kolonnerne.\,.\,:) (18:06)

\ldots Hm, nu overvejer jeg at droppe at have dataentiteter som førsteklassesborger-termer, og så samtidigt bare sige, at det kun er object i StatementInputs, der behøver et type-flag (og dermed altså gøre, at dataentiteter kun kan indgå som relationsobjekter i udsagn).\,. (18:36)  .\,.\,Tja nej, jeg tror bare, jeg beholder det sådan her for nu (hvor alle termer bare har et typeflag, og hvor datatermer er ``førsteklassesborgere'').\,. (18:46)

\ldots\ Hm, det giver egentligt ikke rigtigt mening det jeg har gjort med at kræve at den første byte er 0x00 for ikke-data-typer, så det fjerner sikkert igen (må jeg lige gøre i morgen). Og så kan jeg lige se på, om ikke jeg også skal putte AUTO\_INCREMENT tilbage på ID'erne, det ville jo nok ikke være helt dumt.\,. (23:06)

(23.02.23) Hm, jeg kunne nok spare en masse listebygningsværk, hvis jeg i stedet for beskrivende/specificerende felter i Termerne bare har en forfatter-siger-bot.\,. Og jeg regner alligevel med at have den bot, så hvorfor ikke.\,.\,? (10:09)

\ldots Hm, så skal jeg så til at lave nogle relationer for hvert af de grundlæggende felter, men der kan jeg jo også bare bruge mine `has lex item'- og `has desciption'-relationer. Desuden vil jeg nok tilføje en relation a la  `has abbreviated lex item' som en af de grundlæggende, så der bliver tre.\,. (10:24) .\,.\,Og jeg regner forresten stadig med at beholde type-flaget, og så kan jeg dermed også overveje, om jeg gider at slæbe rundt med en masse `.Type='-relationer også, eller om det bare skal være underforstået.\,. Tja, jeg kunne vel godt have dem med, men.\,. Tja, og dog.\,. 

(10:42) Jeg overvejer at lave ratingværdien om til en var-binær i stedet.\,. \ldots (10:53) Ja, lad det være sådan. Men det skal så stadig være sådan, at hvis man højre-shifter den ned til en int (eller en long), så skal man stadig aflæse midtpunkts-/gennemsnitsværdien, i hvert fald hvad brugernes ratings angår. 

Hm, jeg tror faktisk jeg vil beholde en SimpleTerms-tabel, men så omdøbe den til FundamentalTerms.\,. *(Har nu døbt den tilbage.\,. *Hm, men jeg skifter nu nok tilbage igen.\,.) .\,.\,Hm, jeg overvejede lige kort at skifte tilbage til at kalde det Entities, men man skal faktisk i stedet bare se prædikater og relationer mere som henholdsvis mængde og mængdelære-relationer, hvis man ser på det matematisk *(/formel-logisk). Og så er det først i udsagn-inputsne.\,. som jeg næsten kunne omdøbe tilbage til `semantiske inputs'.\,. at prædikat-mængderne og relations-mængderne bliver vakt til live og bliver fortolket som faktisk prædikater og relationer. .\,.\,Nå, men pointen omkring at have en FundamentalTerms-tabel er så, at denne så kan være ret kort, og at brugerne fra applikationslaget så bare kan hente den hele på én gang, nemlig på en måde, der adskiller sig fra, hvordan brugerne henter ting på normal vis (nemlig via opslag i SemanticInputs-tabellen). 

.\,.\,(11:28) Ah, på den anden side, hvad angår Term vs.\ Entities, så får vi jo nu et ret begrænset antal typer, så.\,. Hov nej, never mind! For prædikater og relationer får jo ikke sin egen type alligevel, så nu handler typerne egentligt bare om at skelne.\,. Nå ja, vent, typerne bliver helt de samme, som de lige var; jeg ændrer ikke i dem.\,. Spørgsmålet er, om jeg skulle blande ``simple'' og ``standard'' terms sammen til én type, men det tror jeg faktisk heller ikke alligevel. 

\ldots\ Hm, angående ratingværdien: det kunne også være, at man skulle dele det op i først en int (eller long), der beskriver ratingen, og så have var-binær-strengen bagefter.\,. (16:05) .\,.\,Ja, lad mig helt klart sige det.\,. 

\ldots Hm, jeg bør måske også beholde RelationalPredicates-tabellen også.\,. (16:27)

%Jeg skrev på et tidspunkt "id <= 0x0011111111111111," men det skulle jo self. have været "id <= 0x00FFF..." i stedet.

(24.02.23, 10:09) Jeg vil faktisk gå tilbage til kun at have en BIGINT som id til hver entitet, og så vil jeg lade serveren og databasen om.\,. Hm vent, eller skulle jeg ikke bare sætte start-værdien til noget forskelligt for.\,. tja, men hvem siger, at jeg får tabeller for hver type.\,. .\,.\,Hm, lad mig egentligt også lige læse lidt mere om indexes.\,. \ldots Ah, ``B-træer,'' og hvor man giver plads imellem inserts på pages'ne, indtil der er fyldt op: Lyder ret nice.\,. (10:22)


(15:52, 25.02.23) Hm, jeg kan forsten lige nævne, at jeg nu tænker at inkludere en `created\_at'/`updated\_at'-dato i SemanticInputs, som så altså bare \emph{ikke} inkluderer tiden (men kun datoen). Og nu kom jeg så lige til at tænke på, at man eventuelt kunne gøre denne dato til den sidste del af nøglen (den primære), for på den måde kunne man jo netop åbne op for, at brugere og bots kan få deres gamle ratings gemt (hvilket nemlig kan være gavnligt, hvis nu man eksempelvis mister tillid til en brugergruppe, men gerne stadig vil benytte dens gamle vurderinger). 

(18:32) Jeg tror heller, jeg vil lægge op til en ``.$<$abbr lex item$>$:''-syntaks i stedet for ``.$<$abbr lex item$>$='', for jeg regner lidt med, at MySQL FULLTEXT search også ser `:' som et whitespace, ligesom den vist gør for `.'. 
.\,.\,Tja, på den anden side, så kan man jo også søge på starten af ord, så måske er det faktisk ligefrem at foretrække at bruge `=', hvis den ser `=' som en del af ordet. Ja, så lad mig da bare skifte tilbage til `=', for det kan vel i så fald ikke skade.\,. (18:39)

(19:22) Hvis en bruger føjer noget til en kategori (i.e.\ rater noget med den som subjekt), som brugeren ikke selv har ratet som en underkategori af nogen kategori, så bør applikationen helt klart spørge brugeren, om ikke de vil rate kategorien som en underkategori af noget (sådan at brugeren kan få gavn af kategori-systemet, bl.a.\ til at få vist forslag til prædikater fra forælderkategorierne og mere).\,:) 

(19:48) Tror i øvrigt bare, jeg bruger local storage til ``arbejdsbordet.'' 

(20:03) Man kan selv definere sine egne full-text stopwords, så det er jo dejligt. Jeg tror i øvrigt muligvis jeg vil gå væk fra at udskifte `has' med `.'.\,. Det overvejer jeg i hvert fald.\,. Og så tænker jeg nemlig også at indlede lex items med en parentes med subjekt-typen i (for relationer og for prædikater), og ende dem med en parentes med objekt-typen, hvis vi snakker en relation.

(20:15) Jeg tænker forresten bare, at der skal være en separat søge-søjle (i applikationen). Og når man så vælger en term fra søgningen, så tilføjer man jo bare denne til arbejdsbordet. .\,.,Alle lister, der figurerer rundtomkring i diverse søjler (.\,.\,ja, bortset fra f.eks.\ søge-søjlens søgeresultats-liste.\,.), skal så have en `tilføj term' knap, hvor man kan tilføje en term fra sit arbejdsbord.

Brugere skal også kunne gemme nye inserts/uploads, de arbejder på (bare i local storage). Dette kan bl.a.\ bruges til, hvis de lige vil bruge noget tid på at finde flere termer, som den nye term skal relateres til. (20:31) I øvrigt kan det også være, at de skal kunne åbne en allerede uploadet term igen for så at lave ændringer og nye tilføjelser i den (i hvert fald på sigt.\,.). 

(20:43) Hm, jeg tror så, jeg vil droppe ``full lexical item,'' og så bliver ``abbreviated lexical item'' jo bare konsekvent til ``lexical item.'' 

(21:58) Hm, det kan være, at jeg også skal lave en `prædikater relevant for denne kategori er også relevant for:'-relation, hvor det så er denne, der gør at kategorier kan arve prædikatforslag fra andre kategorier, og hvor denne relation så pr.\ standard opvurderes, når brugeren opvurderer `.subcategory='-relationen (men hvor denne anden relation så gerne skal vises under, så brugeren slev kan indstille vurderingen væk fra standard-opvurderings-værdien (som måske i øvrigt bare er at give relationen selvsamme rating, som `.subcategory='-relationen fik.\,.)).\,. 

(22:42) Der kan også være et (mere konstant) ``arbejdsbord'' vandret i toppen med brugere/bots/brugergrupper (som (sidstnævnte) altså også er implementeret via bots), hvor brugeren kan skifte mellem, hvilken bruger/bot bliver spurgt i alle efterfølgende queries (indtil brugeren skifter igen). I øvrigt tænkte jeg også lige på, at man måske kunne lave et særligt `(brugerkategori) bør få oprettet en brugergruppe-bot til sig''-prædikat, hvor serveren så kan sætte (og løbende justere) en tærskel for, hvor mange brugere skal have opratet dette prædikat, før at den så opretter en brugergruppe-bot til at kopiere alle brugerne i denne brugerkategori (vægtet over en vis tærskel --- og af en given brugergruppe, som skal specificeres i prædikatet.\,. (så prædikater bliver altså herved teknisk set en relation.\,.)), selvfølgelig ved at den først tager et vægtet gennemsnit for brugergruppen (vægtet ud fra en forudbestemt vægt, som altså også indgår i prædikatet) og så giver stemmer til alle relevante.\,. hm, og domænet for brugergruppen kan også være specificeret på forhånd, måske.\,. alle relevante termer indenfor et vist domæne (muligvis) i form af dette gennemsnit --- og hvor opt\_data så kan indeholde antal stemmer i alt.\,. Det var lidt rodet, og jeg må overveje emnet mere i morgen, men ja, rart lige at få det nævnt.\,. (22:53)

(26.02.23, 9:03) Jeg kom i tanke om, at man jo kunne implementere mine ``simple brugergrupper,'' som jeg vit før har kaldt dem, simpelthen ved at pege på en kategori, eller rettere et prædikat (som ikke nødvendigvis behøver at være dannet af ``tilhører kategori $x$''), og en tidligere brugergruppe til et bestemt tidspunkt. Dette giver så i første omgang en ny brugermængde med en særlig vægtning, altså en brugergruppe. Men man kan så videre gøre denne brugergruppe mere dynamisk ved faktisk at definere brugergruppen, som de brugere medlemmerne af denne proto-brugergruppe stemmer ind i den endelige brugergruppe. Alle medlemmer af protobrugergruppen kan så fra start automatisk have ratet som selv op (med et prædikat, der siger: ``tilhører den endelige brugergruppe''). Men hvis de så begynder at give andre brugere positive ratings derfra, jamen så gør en del af deres andel i den endelige brugergruppe nu til de brugere, de har opratet (selvfølgelig vægtet ift., hvor højt de har ratet sig selv (som typisk vil være den maksimale rating, 1), og hvor de samlede vægte som en bruger deler ud af altid automatisk normeres så de tilsammen summer op til, hvad den givne brugers egen vægtning var i protobrugergruppen). (9:14)

Nu overvejer jeg så også lige, om ikke jeg skal sætte ``rat\_val'' (rating value) ned til bare en char som standard.\,.\,? .\,.\,Og her er disse simple brugergrupper jo så et godt eksempel på, at man dog også har brug for muligheden til at kunne bruge flere bytes til at kommunikere sin data (altså f.eks.\ når man vil uddele af sin brugergruppevægt til andre brugere).\,. .\,.\,Jo, fint, det gør jeg, for jeg behøver jo forresten ikke at finde på standarder til, hvordan man kan bruge denne (``optional data''-)varbinary. For den kommer nok ikke rigtigt til at blive brugt før.\,. Nå nej, avg-botten skal jo også bruge den.\,. .\,.\,Ah, men så kan den indledende standard (hvor man nok gerne vil gøre efterfølgende standarder bagudkompatible med denne) bare være, at hvis der kun er én byte i var-binær-strengen, jamen så bør den simpelthen bare tolkes som nr.\ 2 byte i en SMALLINT-rating (sådan at der altså kommer flere (binære) decimaler imellem -1 og 1). (9:22) 

(26.02.23, 21:55) I ``start-indstillingerne''/filter-indstillingerne skal brugerne bare vælge både et prædikat og en (bruger/bot/)brugergruppe sammen som et par for hver vægtning/filter. 

(27.02.23, 15:31) Hm, det er godt nok ikke så effektivt (pladsmæssigt), hvis jeg saniterer tekstinputtet, før det kommer ind i databasen i stedet for efter.\,. .\,.\,Hm, og på en måde er det ene vel lige så sikkert, som det andet, for jeg skal alligevel bare implementere saniteringen i ét lag (medmindre jeg vil verificere i et andet, men det tror jeg ikke), og i så fald handler det jo bare om, at man sikrer sig, at man gør det (korrekt) over det hele i det lag.\,. .\,.\,Så ja, lad mig sanitere under udtrækningen fra databasen.\,. .\,.\,Hm, men er det så control-laget, eller.\,. Ja, det må vel være i control-laget, for dette skal jo gerne sende læseklare HTML-elementer til browseren.\,. 
.\,.\,Ja, ok. Jeg tror på, at dette er en fornuftig nok måde at gøre det på.\,.  

(18:18) Hm, mon der findes en god måde at undgå forfatter-/skaber-botten på, hvor det så i stedet kommer an på.\,. Hm, brugeren første upvote?\,.\,. Hm, måske skal jeg bruge forfatter-botten alligevel.\,. 

.\,.\,Hm, men så kræver det jo, at jeg også har en basal relation, der siger noget i retning af: ``denne bruger valgte dette prædikat om dette subjekt,'' og så skal man jo til at bruge lister med det system, jeg har (og det vil jo ikke være super fedt, vel?\,.).\,. (18:22) .\,.\,Hov, nå nej, det er jo bare at lade forfatter-botten kopiere de første bruger-inputs, hvad snakker jeg om.\,. (18:24) Ja, så det er vel stadig det jeg gør.\,. .\,.\,Lad mig forresten kalde den ``creation bot'' i stedet.\,. (18:28)

(28.02.23, 11:32) Ah, jeg skal jo selvfølgelig også have en tabel over RelationalPredicates (alligevel).\,.

(11:46) Ah, jeg kom lige til at tænke lidt igen på mine ``simple brugergrupper.'' For det første, skal brugergrupperne jo gerne være en slags bots --- måske gerne af sin egen undertype --- og så skal skaberbrugerne til brugergruppen (altså dem fra hvad jeg kaldte ``protobrugergruppen'' i forgårs) jo bare rate en relation som siger: `(bruger) hører til (brugergruppe)'.  Så nu tænker jeg altså, at hver ny brugergruppe, som måske altså har en særlig undertype af botsne, så skal defineres ud fra en tidligere brugergruppe samt en slutdato --- som dog kan sættes med tilbagevirkende kraft, så det altså ikke er en deadline, men en dato, der altid bliver sat på en dato, der ligger senere end den. Når slutdatoen så er sat, så dannes en ny --- konstant! (og måske skulle jeg dele det op i konstante og dynamiske brugergrupper, come to think of it.\,.) --- brugergruppe ud fra skaberbrugergruppen med dens eksisterende vægtning og altså ud fra, hvordan disse brugere har up-ratet `(bruger) hører til (brugergruppe)'+`ny brugergruppe'-prædikatet for andre brugere. (11:58) .\,.\,Så lad mig også lave en tabel til disse konstante brugergrupper.\,. Nå ja, og så er det altså meningen, at en tidligere brugergruppe ligesom ansøger om, at oprette en ny konstant brugergruppe, hvor de er forfatterne. Først allokeres den nye brugergruppe altså så af serveren, og hvis den nye brugergruppe så godkendes med en given slutdato (som ligger før godkendelsesdatoen), så kan brugergruppen få sat et signal flag i sin tilhørende tabel, som siger at brugergruppen er live, hvilket vil sige at serveren nu aktivt varetager, at kopiere ratinger (med passende vægte på) fra brugerne i brugergruppen (med live-flaget) og inputte dem med brugergruppe-botten som ophavsbruger (altså user\_id i SemanticInputs). 

Nå, inden jeg tænker over, hvordan ``dynamiske brugergrupper'' skal implementeres, så lad mig lige nævne, at jeg nu måske tænker, at begrænse Texts til en begrænset TEXT(n)-datatype. Og så kan jeg lade 0xC0 være længere tekster, som så kan indføres senere, når siden går fra at være mest af alt et web-indeks til at være en side, hvor selve indholdet er gemt (så man henter det direkte fra siden). *[Nej, jeg beholder bare TEXT-typen (altså den maksimale TEXT(n)) som den eneste i starten, for så kan alle eventuelle begrænsninger bare ske i et øvre lag, ved at hver bruger bare får en begrænsning på de bytes, vedkomne kan uploade (pr.\ tidsrum).] Og så kan det være, at jeg vil lade 0xD0 være diff-tekster/sammensatte tekster, men det kan jeg jo bare finde ud af på sigt. 

Nå: dynamiske brugergrupper.\,. (12:19) .\,.\,Hm, de kan få sin egen bot-undertype, selvfølgelig, og kunne måske bare være, hvor man ikke sætter nogen slutdato for den nye brugergruppe? .\,.\,Ja, og i øvrigt kan man jo godt have en dynamisk brugergruppe som skabere til en konstant brugergruppe, for så er det jo bare underforstået, at det er vægtene ved udgangen af slutdatoen i skaberbrugergruppen, der skal gælde, når det kommer til at beregne vægtene i den nye konstante. Ok! Nemt! (12:22)

Det er jo underforstået, at ``Bots'' refererer til Native Bots, så måske jeg skulle kalde dem det i stedet. Og så er det i øvrigt også oplagt at tænke på, om ikke tredjeparter også skal have mulighed for at lave bots, f.eks.\ til at varetage brugergrupper. Og hertil må man jo bare sige: jo, det kan sagtens blive en ting på sigt. Så tænker jeg, at dette så bliver en undertype til User-typen, og hvor tredjeparter så ansøger om at få en sådan speciel brugerprofil, hvor der så naturligvis kan høre særlige privilegier til, sådan at tredjepartsbots måske kan få lov at indsætte et større volumen af data pr.\ tid i form af de ratings, de indsender. 

Super. Så nu har jeg en god plan for brugergrupperne!\,:) (12:27)

(13:07) Ah, og ``avg-botten'' bliver jo bare den første brugergruppe (som så er dynamisk).\,.

(14:02) Hov, jeg tror nu, at jeg vil gå tilbage til, at det kun er standard-termerne, som bruger det system med den tabel, der pt.\ hedder NextIDPointers.\,. 
%
%%.\,.\,Hm, nu overvejer jeg egentligt, om man ikke bare skal sige.\,. .\,.\,Hm ja, måske skulle man faktisk.\,. .\,.\,Tja, nej.\,.
%
\ldots Nah, på den anden side, det fungerer også fint sådan (med NextIDPointers som den er). (14:25)

(18:47) Hm, jeg tror jeg vil gå tilbage til en ``.$<$noun describing the Object$>$:''-syntaks, men så vil jeg sige, at navneordet gerne må være i flertalsform, når vi snakker en én-til-mange-relation (i stedet for en én-til-én-relation, hvor navneordet så til gengæld bør være i entalsform). Og punktummet står så bare i stedet for `has as one of its' (hvor $<$pluralized noun$>$ så følger efter), når vi snakker én-til-mange-relationerne (i stedet for at stå i stedet for `has a/an'). \ldots\ Og jeg tror jeg vil sige, at navneord skal capitalize'es, både for faktiske navne ord og for relationer, der følger denne forkortede syntaks. (20:19)

(21:24) Ah, jeg kom lige på en god idé. Jeg vil også lave en ReversedSemanticInputs-tabel, hvor obj\_id og subj\_id så er byttet om i primærnøglen (og dermed i dets clustered index). Og så skal man kunne oprette reversions af existerende relationer, således at alle deres inputs bliver kopieret over i ReversedSemanticInputs, bare med ombyttes obj\_id og subj\_id, selvfølgelig. Jeg kan lige tænke lidt mere over, hvad man skal gøre for at (søge om at) oprette disse.\,. ``to-vejs''-relationer, men i første omgang kan jeg da lige sige, at jeg forestiller mig, at det skal benyttes for bl.a.\ ``.Lexical item:''-relationen, og for ``(Category).Elements:''-relationen. .\,.\,(Hm, lad mig forresten lige kopiere min nuværende beskrivelse af ``.Lexical item:'' ind som en (ikke-renderet) kommentar under denne paragraf.\,.) (21:32) 

%Kopieret:
	%This relation states about its subject and its object, the latter of which should be a text string that is part of an English sentence with a meaning attached to it (i.e. a lexical item), that the following is true: The object (a string) forms a lexical item that can be seen as defining the subject.
	%
	%For instance, if the subject can be referenced by a noun, then the object could be a string forming that noun. And if the subject is a relation that can be referenced by a verb, then the object could be a string forming that verb.
	%
	%A special example of the latter case is if the subject is this very relation descibed by this description. In that case, the object could be the string: 'can be referenced by the lexical item given by'.
	%
	%However, a shortened version of this lexical item might also do, and in fact even prefered in some cases, especially for relations such as this. This is why ".Lexical item:" has been chosen for the original lexical item of this relation. We thus propose the following standard for shortening lexical items of relations. 
	%
	%If possible, lexical items of realations should formulated according the syntax: ".<capitalized noun describing the object>:". If it is expected that users will generally be interested in querying for several fitting objects for a given subject, the noun should be pluralized. In that case, the '.' can be seen as standing in the place of 'has as one of its'. But if, on the other hand, it is expected that users will only be interested in querying for the best fitting object for a given subject (as is the case for this relation), the noun should simply be in its singular form. And in this case, the '.' can be seen as standing in the place of simply 'has' (or 'has a/an').
	%
	%Furthermore, parentheses might also be included at the beginning or at the end (or both) of the lexical item, such that the contents of these parentheses denote the intended type of respectively the subject and the object. For example, a relation that states that its subject is a subcategory of the object, both categories, the lexical item of that relation might be formulated as "(Category).Subcategories:(Category)". And as another example, a relation that states that its object is of the category referenced by the subject, the lexical item might be formulated as "(Category).Elements:". (Note that since the subject could be any category here, it does not make sense to include a type specification for the object). This latter example especially highlights the point of being able to specify the type this way, since it helps give meaning to what we mean by 'Elements' in this case. If the lexical item had been simply ".Elements:" on the other hand, it could reference a bunch of other relations as well, such as "(Molecule).Elements:" and so on.
%


(21:55) Hm, lad mig give to-vejs-relationerne en bestemt typekode, sådan at systemet bare automatisk gør sådan, at hvis et sem-input med en to-vejs-relation bliver indsat, så skal det omvendte input også indsættes i ReversedSemanticInputs. Og så skal jeg i øvrigt også have en fundamental relation der siger: ``.Lexical item of the inverse''. 


(01.03.23, 11:25) Jeg skrev om ude i kommentarerne under den næste sektion, at jeg ville bruge en speciel relation for at springe en kategori over, så at sige, nemlig for kategorier over relaterede prædikater til en kategori. Men hvis vi f.eks.\ tænker på kommentarer til videoer eller (SoME-)posts osv., så vil man jo nok også gerne kunne kategorisere disse i visse tilfælde (f.eks.\ hvis der er meget diskussion under et post). Så nu tror jeg faktisk, jeg vil tilbage til den løsning, jeg tænkte på lige inden den idé, hvilket var simpelthen at begynde at bruge funktioner igen, og så lave særlige funktioner til at bygge afledte kategorier. Disse afledte kategorier kan så fungere lidt ligesom `relevante relationer' for en kategori.\,. og ja, faktisk tror jeg så måske, at man vil erstatte rigtig mange én-til-mange-relationer med afledte kategorier i stedet.\,. .\,.\,Men ja, lad mig lige få sagt færdigt, at man så up-rater de afledte kategorier for kategorien over de termer, som de afledte kategorier afledes af. 

.\,.\,Hm, spørgsmålet er jo så nu: Skal alle én-til-mange-relationer så bare erstattes med kategorier.\,. ja, selvfølgelig på nær i hvert fald, ``.Elements'' og ``.Subcategories''.\,. .\,.\,Hm, måske er dette faktisk ikke en helt dum idé. .\,.\,Så når man up-rater relationer for (termerne i) en kategori, så vil det (i den tidlige version) handle om én-til-én-relationer (hvor objektet dog jo kan være en liste), og ellers up-rater man afledte (afledede?) kategorier i stedet, når altså vi snakker om at tilføje de knapper til termerne (som de vises, når man har klikket på dem i en liste), der folder nye lister ud (og som altså ikke bare folder data om termen ud i samme applikationssøjle). .\,.\,Lyder altså ret fornuftigt, hvis jeg selv skal sige det.\,.\,! (11:43)

\ldots I øvrigt kunne man jo også gøre det på sigt sådan, at når elementer vises til en kategori, så vises også en række af de mest populære underkategorier i toppen, sådan at man har muligheden for at trykke på disse og så ændre kategorien til en underkategori for samme søjle (medmindre man mellem- eller ctrl-klikker, eller klikker på en højre- eller venstre-pil inden for feltet). Bare lige en lille bemærkning, der var værd at nævne.\,. (11:58)


\ldots\ (14:07) Nu har jeg tænkt lidt over, hvordan det så kommer til at spille sammen med det her med ``ReversedSemanticInputs,'' og jeg er kommet frem til, at det jo faktisk går rigtig godt sammen, for så er det jo nok bare de to fundamentale kategori-relationer, der skal være to-vejs, samt relationer der refererer til (fulltext-searchable) strings, såsom `.Lexical item:' og `.Keyword string:'. For alle andre to-vejs-relationer skal jo nu som regel implementeres via `(Category).Elements:'. 

Så har jeg så også lige tænkt over, at man jo nok næsten burde samle NativeBots og UserGroups til bare UserGroups. Og så har jeg endda gjort mig nogle tanker om, at man kunne erstatte rat\_val og opt\_data med først en TINYINT NOT NULL og så en nullable SMALLINT (i.e.\ en short). Og så kan de to ting nemlig fortolkes som noget forskelligt, alt efter om user\_id er ID'et på en faktisk bruger eller en brugergruppe. (14:15) .\,. Hvis det er en faktisk bruger, så kan TINYINT'en signalere den faktiske rating. .\,.\,Hm, og måske skulle shorten (SMALLINT'en) bare altid være NULL, hvorved man jo også kunne dele SemanticInputs op i to og så lave view til brug for alle queries.\,. (14:25) .\,.\,Hm, lyder ikke dumt. Og for brugergrupper tænker jeg så, at det er shorten (.\,.\,ja, og hvis man laver et view behøver man jo forresten heller ikke at blande kolonnenavnene sammen for de to, så shorten kan godt hedde rat\_val.\,. tja, nej, men måske rat\_avg.\,.), der repræsenterer ratingværdien. Og så skal TINYINT'en til gengæld repræsentere det mindste tal $-128\leq n \leq 127$, hvor der gælder, at $2^n > x$, hvor $x$ her betegner en vægtet sum over antallet, der har afgivet en stemme for udsagnet (vægtet med brugergruppens vægte; de samme vægte som rat\_avg udregnes med). (14:31)

(14:34) Nå ja, jeg skulle jo også skrive, at forkortelserne af lexical items for relationer nu ikke behøver flertalsendelser. For nu opretter man jo kategorier i stedet.\,. Nå ja, og det er forresten noget, som jeg skal tænke noget mere over, for det er jo så ikke sikkert, at man skal bruge, hvad jeg nu kalder ``DerivedTerms,'' alligevel (til andet end prædikater).\,. .\,.\,For nu tænker jeg jo faktisk, at man tilføjer én-til-mange-relationer til termerne i en kategori, kald den $c$, ved at up-rate den specifikke én-til-mange-kategori for $c$ ud fra en (fundamental) relation a la: ``.RelevantCategoriesForElements:''. (14:41) Og så skal navnet på denne kategori jo hellere skrives af brugerne selv (i stedet for at parses fra et lex-item). Så ja, jeg tror altså ikke alligevel, der bliver behov for DerivedTerms (i hvert fald ikke i tidlige versioner; hvem ved, hvad der bliver behov for i fremtiden?\,.).\,. (14:43) .\,.\,Så det kan altså være, at jeg går tilbage til at kalde den RelationalPredicates i stedet.\,. Tja, eller også beholder jeg det bare, som det er nu, for det skader måske ikke, at holde muligheden for funktioner åbne, også selvom jeg nu har svært ved at se, hvad de skulle gavne (andet alså end til at lave prædikater med), men jeg kan jo lige tænke lidt over det.\,. (14:45) .\,.\,Hm, måske kunne lige netop en funktion til at reversere en relation være brugbar.\,.\,? (14:49) .\,.\,Tja, og dog, for det er nok bedre bare at lave en speciel tabel over.\,. Hm.\,. .\,.\,Hm, alternativt kunne man lave en tabel med alle de fundamentale relationer (som så inkluderer alle én-til-mange- og alle to-vejs-relationer, som jeg tænker det nu), og så kan der måske bare være et flag i denne tabel, der afgør, om de er to-vejs eller ej.\,. (14:56) .\,.\,Og fordi de så bliver fundamentale, så kan jeg måske endda bare nøjes med at give deres lexical items og descriptions som kolonneværdier i stedet for, at de skal bootstrappe deres egen semantik, for det bliver jo nemlig nok virkeligt sjældent, at brugerne vil query'e efter disse fundamentale relationer. Og når de gør, hvorfor så ikke bare gøre det på en separat introduktionsside, i stedet for at query'e efter dem i selve det semantiske træ?\,. (14:59) .\,.\,Hm, og bare fordi de ikke bootstrapper sig selv, men at deres semantik er beskrevet i deres tabelkolonner, jamen så kan man jo stadig godt bruge dem på sig selv og hinanden. 

.\,.\,Okay, og måske putter jeg også de grundlæggende kategorier ind i samme tabel (og så skal omtalte flag bare kunne vælge imellem flere typer end bare relationstyperne), og så kan jeg jo kalde den tabel for FundamentalTerms. Cool.\,.\,:) (15:06)

\ldots\ (17:10) Skråt to-vejs-relationerne! For de bliver nok alligevel ikke særligt brugbare i praksis. I stedet må brugerne altså bare manuelt up-rate relationen i begge dens udgaver (som hver især er skabt separat), eller også må man implementere en sådan automatik i applikationslaget. Jeg kom også ret hurtigt i tanke om, efter at jeg skrev forrige paragraf (og gik mig endnu en tur i det her totalt lækre vejr, det har været), at jeg jo ville få brug for DerivedTerms netop til at lave derived categories af. Men nu har jeg sidenhen faktisk tænkt noget lidt andet. Men lad mig starte med at sige, at jeg faktisk også tror, at der ikke bliver nok behov for derived/relational predicates. For det vil alligevel nok næsten altid være ift.\ en `belongs to: (Category)'-relation, at man ville bruge det, og her kunne man jo eventuelt i stedet bare gøre sådan, at brugere kan lave intersections mellem flere kategorier.\,. tja, eller.\,.(?) Det vil jeg faktisk lige tænke noget mere over.\,. Men ellers kan jeg sige, at nu hvor så mange semantiske inputs kommer til at handle om at up-rate termer som elementer af kategorier, så vil jeg adskille disse semantiske inputs for sig i sine egne tabeller. I første omgang har vi så en sem-input-tabel, hvor udsagnsdelen så bare består af en kategori (i stedet for subj+rel) og et element-term (i stedet for obj). Og her kommer så hvorfor, at man måske alligevel ikke behøver DerivedTerms, for så kunne en anden tilsvarende tabel bestå af kategori-funktions-input (i stedet for subj), en kategori-funktion (i stedet for rel) og så et element-term (i stedet for obj). (17:22) .\,.\,Hm, men måske får man så alligevel brug for DerivedTerms, for mon ikke man alligevel også får brug for, at.\,. tja, og dog.\,. Ville have sagt: ``brug for at kunne tage fat i den afledte kategori som et selvstændigt term,'' men hvorfor egentligt?\,.\,. (17:24)

.\,.\,Jeg skal i øvrigt også nævne, at jeg nu forestiller mig `.Subcategories:' også som en afledt kategori. 

.\,.\,Hm, i bund og grund er det vel kun, hvis man har brug for `belongs to'-prædikater til afledte kategorier, at man har brug for at kunne behandle dem som selvstændige termer.\,. .\,.\,Hm ja, og i så fald, så skulle man vel hellere bare have en DerivedPredicate-type.\,. (17:31) .\,.\,Ja, jamen så er det vel bare det, jeg gør. Så prædikater kan altså nu afledes (monadisk) af kategorier, og særligt også af afledte kategorier, hvilket jo så nok bliver den eneste måde, at ``ophøje'' en afledt kategori til et *(en) term, så at sige.\,. (17:34)

.\,.\,Og fordi jeg ikke tror, at.\,. .\,.\,Hm, overvejer, om DerivedPredicates så skal deles op i to tabeller.\,. men hvorfor ikke, kan man sige.\,. Ok.\,. 

.\,.\,Hov, men ift.\ derived prediates.\,. ah, så skal man så bare sørge for, at disse bliver sat automatisk.\,. eller?\,.\,. (For det bliver jo alligevel en del arbejde.\,.) Hm.\,. (17:40) .\,.\,Hm, men er det mon så bedre, at man i stedet for derived predicates så bare gør sådan, at man kan tage intersections af kategorier (og så i stedet også sørger for, at derived categories bliver oprettet som termer.\,.).\,. (17:42) 

.\,.\,Hm, men er det så ikke lige før, at alle prædikater bare skal være kategorier også.\,.\,? .\,.\,Hm, det ville ændre det en del, for så skal man ikke længere hente en masse termer til en liste, hvor man så samtidigt skal hente prædikat-værdier for dem; man skal i stedet bare hente termer fra lister.\,. Hm, men dette er jo ikke nødvendigvis mere effektivt.\,. .\,.\,Hm, men man kunne jo eventuelt så gøre `belongs to:'/`.Elements:' til en to-vejs relation (nemlig ved at man med andre ord bare sørger for at have to eksemplarer af hver kategori-input-tabel).\,. (17:49) 

.\,.\,Hm, og hvis man gør det sådan, at prædikater bliver kategorier, så kan man bare sørger for, at hver kategori for et præfiks, der fortæller, om titlen er et navneord (i flertal) eller et prædikat --- eller et adjektiv (så man sparer `is' i titlen(/tagget)).\,. (17:55) .\,.\,Hm, rent praktisk kunne dette give god mening, men jeg kan på den anden side ikke særligt godt lide, at prædikater kaldes ``kategorier''.\,. Hm.\,. .\,.\,Nå, jeg må jo lige tænke lidt over det hele.\,. (17:57, 01.03.23)

.\,.\,Hm, ville det give mening, hvis prædikater ligesom altid bare hørte til en kategori.\,.\,?\,.\,. (18:02) .\,.\,Hm, måske overtænker jeg det en smule nu.\,. 

(02.02.23, 14:17) Fik tænkt lidte videre i går aftes og fik et par idéer, og så har jeg tænkt en del videre i dag og fået en del flere gode idéer. Nu tror jeg, jeg er ved at have den. Jeg har tænkt mig, at det grundlæggende lag, altså databasen, skal laves ret meget om. Nu skal semantiske inputs bestå af en tabel med (prædikat\_id, rat\_val (short), og obj\_id) som primærnøgle (og vist nok som det eneste i den tabel). Og så skal hvert prædikat bygges i en tabel med (user\_id, subj\_id, relations/kategori-funktions-ID) som primærnøgle, og hvor selve prædikat-ID'et så altså er den eneste ikke-nøgle.\,. Nå ja, eller det vil sige, man kunne jo godt have.\,. .\,.\,Hm, måske skulle man bare have to kopier af denne tabel: den ene hvor (user\_id, subj\_id, relations/kategori-funktions-ID) er primærnøgle, og den anden hvor pred\_id er primærnøgle.\,. Fint.\,. (14:26) .\,.\,Pointen med at kalde det kategori-funktioer er så, at en bruger kan til hver kategori vælge en liste over kategori-funktioner, som skal danne de knapper, der forekommer, når et element vises (i en liste eller for sig selv). Når man så trykker på disse knapper, så får man altså en liste af alle objekter --- eller rettere så mange man vil have, måske fra højest til lavest rating, der hører til det givne prædikat\_id, der jo så aflæses i (user\_id, subj\_id, kategori-funktions-ID), hvor subj\_id jo så er det givne element, og hvor kategori-funktionen selvfølgelig er den valgte. (14:32) .\,.\,En funktion kan dog godt vide, at den selv er ``en én-til-én-relation,'' hvorved brugeren som regel kun vil få serveret det højest ratede objekt på listen, medmindre brugeren specifikt beder om den anden mulighed. Disse funktioner er også altid typede, idet de som minimum i hvert fald altid angiver objekttypen. Og denne type angives i form af en kategori. Dette bliver særligt relevant, når jeg om lidt når til, at brugere også for hver kategori kan up-rate `skabeloner' til denne, som så kommer til indirekte at bestemme, hvordan element- og element-liste-visningerne skal være for den kategori. Og ja, hvis en kategori-funktion så erklærer, at objekterne er af kategori $x$, så vil denne afledte kategori altså også arve samme `skabelon,' som kategori $x$ har. 

En skabelon er så også en funktion (ja, jeg er pludselig gået all-in på funktioner, sjovt nok), der tager en kategori som input, og som så indeholder en masse information, som applikationen skal læse og bruge til at indstille visningspræferencerne (altså i form af HTML og CSS). Særligt skal denne information også definere en række kategori-funktioner, som skal bruges til diverse ``knapper'' i HTML-skabelonerne. Tillige bør der så være --- og ja, man kan sikkert gøre det på mange måder, men nu foreslår jeg lige det her --- være en højereordnesfunktion, der tager en af disse knap-kategori-funktioner og spytter en ny kategori ud, nemlig en kategori over ``generelt relevante funktioner'' for knap-kategori-funktionernes kategorier.\,. Okay, det bliver helt klart en smule indviklet nu, men vi skal nok finde hoved og hale i det.\,. Men ja, pointen er så, at når brugeren trykker på en knap, så allerede uden at nogen brugere har givet input, så vil der allerede fra start være nogle forslag i toppen af kategori-listen (når brugeren har trykket på den givne knap), som den afledte kategori altså arver helt oppe, ikke fra dens forælderterm, men fra forældertermens egen kategori (altså den kategori som forældertermet er et element af). Og herfra kan brugeren så selv vælge at tilføje nye termer til denne liste, som så kan vises side om side med disse start-foreslag --- og brugeren kan endda også nedvurdere termerne, som er en del af startforslagene, hvis de har lyst. (14:47)

Nå, den mest tunge øvelse for databasen er jo generelt, når den skal hente en masse termer og sortere dem. Så det er også dette arbejde, jeg har forsøgt at optimere med disse nye tanker. Nu handler det jo i høj grad om (efter man lige har fået det relevante prædikat-ID), bare at pege på et prædikat-ID, og så få et antal elementer fra toppen eller fra bunden af den tilhørende liste (som nu er ordnet efter rating!). Hvis man så skal lægge yderligere vægtninger/filtre over, så vl jeg \emph{ikke} længere have det sådan, at der så skal laves seeks *(search, rettere; vi snakker altså søgningen i logaritmisk tid) på hvet enkle term. Nu skal sådanne søge-/filterindstiller dannes ud fra sammensætninger af kategorier! Så man beder altså bare databasen om et antal lister af termer fra forskellige prædikat-ID'er til at starte med. (Man behøver ikke at bede om den fulde liste hver gang, og det er jo altid dejligt, hvis hver liste er så begrænset, at databasen kn skal læse listen fra en eller måske to pages.) Databasen serverer så også alle de tilhørende rating-værdier, nu kan applikationen så tage den mindste liste, og ordne termerne efter ID. Så kan den for alle de andre lister gøre det, at den går igennem listen og laver seeks på den mindste liste (den ordnet efter ID), og så smider termen væk, hvis den ikke er på listen. Hvis den dog er, jamen så gemmes ratingen fra den givne liste sammen med de andre gemte ratinger for det element. Til sidst får man så en fællesmængde af alle lister, hvor der for hver term på listen står alle de relevante rating ud for termen. Til sidst kan man så lave den aritmetik man synes for at kombinere disse ratings til en samlet rating, g så kan man ordne listen efter denne (måske uden at smide de grundlæggende ratings væk i hukommelsen (hvis nu brugeren vil ændre vægtningen f.eks.)). (15:01) 

Bum. Så det er altså sådan, jeg tror, det skal komme til at fungere nu.\,:) (15:01)

(03.03.23, 11:23) Mit forslag fra i går var ikke så godt. Det er rigtigt nok, at skabelonerne skal definere nogle kategori-funktioner/relationer, so bruges til at give startforslag til visse afledte kategorier af termerne, som skabelonen handler om at definere HTML og CSS for, men disse relationer skal bare være relationer a la: ``.Relevant predicate suggestions:' og ``.Relevant comment subcategories:''. 

Bemærk i øvrigt, at selvom det altså giver god mening at kalde dem kategori- --- eller prædikat- --- funktioner, nu hvor de tages på et subjekt og en bruger og spytter et pred\_id (som også kan tolkes som et ``kategori-ID'') ud, så kan vi altså også sagtens stadig se dem som relationer, ligesom vi har gjort hidtil. Men ja, fordi man så gerne skal formulere disse relationer hver især som et navne ord (i flertal eller i ental alt efter relationstypen (altså én-til-én eller én-til-mange)), så giver det jo også dermed fin mening også at omtale dem som prædikat-/kategori-funktioner. 

Nå, en vigtig pointe er så for det første, at pred\_id ikke bliver et selvstændigt term. Disse prædikater bruges kun til at definere selve listerne af elementer, som vises i applikationen. Hm, måske skulle jeg dermed faktisk begynde at kalde dem sets (mængder) i stedet.\,. .\,.\,Ja, lad mig det. For både `kategorier' og `prædikater' bliver jo nemlig også hver især en type, eller rettere en kategori (for typer er mere grundlæggende --- såsom `sets'/`mængder'), af termer. (11:36) En anden vigtig pointe er så, at prædikat-termer jo dermed nu ikke direkte fører til applikationslisterne, dem vi altså nu kalder `mængder,' men i stedet kræver dette så en `fulfills'-/`.Fulfilling terms:'-relation. Prædikat-mængden dannes jo nemlig ved at sammensætte (user\_id, subj\_id, set function ID), hvor subj\_id så er prædikat-termen, og hvor ``set function ID'' så er `.Fulfilling terms:'-relationen. (11:41)

En tredje, ny, pointe er også, at ingen af de to tabeller faktisk skal reverses! For nu vægter/filtrerer jeg jo lister via ``intersections,'' hvis vi kan kalde det det. Og når det kommer til keywords (og altså FULLTEXT-searchable Strings), så vil jeg hellere bare indføre en relation a la: `is a relevant keyword (/lexical item) for:'. Som en af hovedkategorierne i applikationen vil jeg så have en pseudo-kategori, som altså er et søgefelt på keywords i databasen. Når man så har fundet og valgt et ønsket keyword, så kan man så se, hvilke termer er relateret til dette via (some user, keyword/lexical item, `.Related (semantic) terms:' .\,. / `.Related Terms:')-mængden. (11:50) .\,. / `.Related s-terms:'.\,. .\,.\,Anyway, så derfor behøver vi altså ikke nogen tilbagegang i det semantiske træ --- i hvert fald ikke som en del af det helt grundlæggende. I applikationslaget kan man så stadigvæk gøre sådanne, at visse ratings automatisk bliver carbon-copied og givet til en tilsvarende reversed relation også, men ja, dette er i så fald altså udelukkende oppe i applikationslaget. (11:53)

(12:24) Nå ja, og jeg overvejer at gøre rat\_val til en TINYINT igen.\,. .\,.\,(12:31) Ah, måske kunne man godt få brug for en ekstra tinyint for de normale brugere også, for dette kunne jo bruges til at.\,. Nå ja, til enten at give usikkerhedsbredden på ratingen, som jeg har tænkt før, eller måske bare at give en selvdefineret vægt imellem 0 og 1, sådan at brugeren selv kan sætte sin effektive brugergruppevægt ned for den specifikke rating.\,. Hm, interessant at overveje, hvad der giver mest mening af de to muligheder, det må jeg lige gøre.\,. (12:34)

.\,.\,Men ja, for brugergrupper kan den første byte så være ratingen (fra -127/127 til 127/127), og så kan den næste byte udgøre en vægtet sum for, hvor mange har afgivet stemme til ratingen.\,. hm, og måske kunne man endda også bruge nogle bits fr nr.\ 2 byte og sætte dem over i forlængelse af den første (for yderligere præcision).\,. Tja, og dog.\,. (12:37) .\,.\,Nej, en skala med en opløsning på 127 til begge sider er rigeligt. Og så kan stemmeantals-vægten jo hermed blive den eneste kolonne, som ikke indgår i nøglen. 

Jeg er forresten ikke sikker på, at jeg får brug for.\,. hm datoer.\,. jo, måske.\,. (12:40)

Hm, stemmeantals-vægten kunne faktisk sagtens være en del af primærnøglen alligevel, og altså nemlig komme lige efter ratingværdien.\,. (12:45) .\,.\,Jeg kunne kalde det for en signifikans-indikator, hvilket jo så både kan bruges, om vi taler brugergruppe- eller bruger-versionen af, hvad værdien betegner. 

.\,.\,Hm, jeg tror, at det vil være meget nemmere generelt for folk at forholde sig til tanken om en stemmevægt frem for en afvigelsesbredde, så lad mig bare sige, at det er det, vi gør! (12:50) Og så spiller det egentligt også meget godt sammen, for så kan man bruge ``rating weight'' for både brugergruppe- og bruger-versionen, og så behøver jeg ikke at dele SemanticInputs op i to tabeller. .\,.\,Tja, selvom det burde man nu næsten alligevel, om ikke andet så bare for på en nem måde at sikre sig, at der bliver to filer.\,. nå nej, dette betyder jo ikke helt så meget alligevel. Nå, det er også lige meget nu; det kan jeg altid lave om på, hvis det er.\,. 

(Hvis jeg inkluderer datoen, så skal den selvfølgelig ikke være en kolonne i SemanticInputs, har jeg indset nu (pga.\ effektiviteten).\,.)

(04.03.23, 10:01) Jeg kom i tanke om i går aftes, at fordi alle semantiske inputs jo alligevel tager udgangspunkt i brugere og brugergrupper, så kan hver bruger(gruppe) jo egentligt godt selv have deres egen fortolkning af en relation, uden at det gør noget, så derfor kan man godt udelade beskrivelserne som en del af det grundlæggende lag. Jeg skal selvfølgelig beskrive de grundlæggende Termer i databasen, men brugerne behøver i princippet ikke at holde sig til de beskrivelser.

Så tænkte jeg også noget andet, og det var, at `skabelonerne' jo også kan indeholde et valg om en bruger for hver relation/kategori-funktion (der så sammen med det selekterede term selv giver en `mængde'). Så kan en brugergruppe naturligvis bare pege på sig selv ret ofte, hvilket også måske vil være lidt mere effektivt (for når man skifter bruger, så skifter man helt sikkert også til en anden disk page.\,.), men de kan altså også pege på andre, hvilket kan være rigtig praktisk, tror jeg på. (10:08) 

.\,.\,Nå ja, og nu overvejer jeg altså, at lave en tabel for alle relationer som en del af det grundlæggende, således at relationer defineres unikt ud fra objekt-navneord, subjektkategori, objektkategori og et er-én-til-én-flag. Og det er det; ingen beskrivelse. Og så kan hver gruppe dermed i princippet vælge deres egen fortolkninger, der hvor der kan være tvetydigheder --- eller bare gradbøjninger af (og altså rettere sagt mange versioner af), hvad relationen helt præcist indebærer. (10:13)

(10:22) Hm, og så kunne jeg også lave en tabel over kategorier, som så faktisk ikke skal have meget andet i deres grunddefinition en en titel, og så overvejer jeg endda lidt nu, om ikke man så skulle gøre obj\_noun og kategori-titlen til FULLTEXT-searchable kolonner for sig selv?\,:) Det tror jeg, jeg vil gøre.\,. 

\ldots Hm, men hvis det ofte er enkelte ord, så er full-text nok lidt overkill, når man bare kan bruge et non-clustered index.\,. \ldots (Ja, det er det jo selvfølgelig. Jeg ser lige, hvad jeg gør helt præcist.\,.)

\ldots (11:20) Hm, skulle man mon indføre en grundlæggende prædikat-type, som så kan blive en slags kategori-til-(under)kategori-funktioner.\,.\,? .\,.\,Ah, nå nej/ja, det svarer jo bare til `relevante prædikater,' så det får jeg allerede.\,. .\,.\,Tja, og dog, for det kan måske være meget smart at kunne.\,. Ah, men jeg skal jo finde en måde at brugerne skal kunne instantiere intersections (fællesmængder) på, så det skal jeg få inkluderet.\,. (11:26)

(12:23) Hm, måske er det i virkeligheden nemmere bare at bruge decimaltal, når det kommer til at dele de forskellige typer ind i forskellige auto\_increment-startpunkter.\,. 

(12:39) Jeg skal have fundet ud af, hvad jeg gør med ``standard terms, set ID pointers, creators and dates''\ldots 


(05.03.23, 9:43) Jeg fik en masse gode idéer i går. Lad mig starte med at nævne, at jeg nu forestiller mig, at alle standard-termerne (som man måske kunne kalde s- eller r-termer (for hhv.\ semantic, standard og resource, regular *(mest for ressource-term, dog))) samt kategorier skal defineres alene ud fra en kort titel samt en pointer (et ID, i.e.) til en henholdsvis en kategori eller en overkategori, om vi snakker et r-/s-term eller en kategori.\,. hm, r-term giver faktisk mest mening, for kategorier er jo også semantiske.\,. Ja, og så kunne s-termer i øvrigt betegne alle ikke-data- og ikke-konstruerede termer.\,:) 

Nå, det helt store nye er så, at med disse ændringer (og hvor hver bruger i princippet selv kan vælge deres egen beskrivelser (selvom hver titel--kategori-pointer-kombination udgør en unik r-term eller kategori (med ét ID))), så giver det pludselig vildt god mening, at fortolke hele det semantiske træ som et slags filsystem (med filsti-agtige stier).\,! Nu vil jeg derfor prøve at beskrive syntaksen for disse ``filstier''.\,. hm, lad mig bare kalde dem paths.\,. eller s-paths.\,. hm.\,. Nå, lad mig beskrive syntaksen for disse stier. (9:54)

Vi starter med at vælge en bruger (i starten af stien). Hm, det gør jo forresten også, at hver bruger skal have.\,. Nå nej, man kan jo godt bruge et bruger-ID, hvis ikke brugeren har et unikt alias i systemet.\,. Nå, og så kommer en række kategorier og underkategorier, separeret med `$>$.' .\,.\,Hov, måske skal der faktisk være to udgaver af `$>$,' lad mig lige tænke mig om en gang\ldots 

\ldots (10:47) Okay, jeg har tænkt mig lidt om, og nu har jeg en faktisk mere simpel syntaks, der også faktisk minder mere om gængse filstier. Lad mig faktisk bare starte med at prøve at definere syntaksen.

Vi har:

%$Path := UserIdent\ \texttt{/}\ Set\ |\ UserIdent\ \texttt{/}\ Term$
%
%$UserIdent := ident$
%
%$Set := Set'\ |\ Set'\ \texttt{\&\&}\ Set$ 
%
%$Set' := (Set)\ |\ Set''\ |\ Set''\ \texttt{||}\ Set'$ 
%
%$Set'' := Term\ \texttt{.}\ Relation\ \texttt{/}\ |\ ident\ \texttt{/}$ 
%
%$Relation := ident$
%
%$Term := Term'\ |\ Category\ \texttt{/}\ Term'$
%
%%$Term' := $ ..(11:18)...
%
%%(11:34) Hm, jeg skal lige overveje nogle ting... ...(11:44) Okay, lad mig prøve igen..

$Path := UserIdent\ \texttt{//}\ Set\ \texttt{/}\ |\ UserIdent\ \texttt{//}\ Term$

$UserIdent := ident$

$Set := Set'\ |\ Set'\ \texttt{\&\&}\ Set$ 

$Set' := Set''\ |\ Set''\ \texttt{||}\ Set'$ 

$Set'' := (Set)\ |\ Term\ \texttt{.}\ Relation\ |\ ident$ 

$Relation := ident$

$Term := Category\ |\ Set''\ \texttt{/}\ ident$

%Hm, kunne det være sådan her..? (12:03) ..Ja, det fungerer vist, for man kan altid se, om stien slutter med en skråstreg, og hvis det gør, så skal det parses som et Set, og hvis ikke, så skal det parses som en Term.. (12:06) ..Og så tænker jeg, at Category skal indebære følgende syntaks:

$Category := Category\ \texttt{/}\ ident\ |\ ident$

\ 

Når jeg skriver $ident$ og ikke $id$, så er det for at præcisere, at det ikke behøver at være et BIGINT-id, men at det også sagtens (og som regel vil være) en streng-identifyer.\,. (12:13)

.\,.\,(12:17) Hov nej, det kan godt være, at vi lige skal gøre sådan, at $Category$ ikke kan komme efter.\,. .\,.\,efter.\,. Hm nej, måske fungerer det; jeg tror ikke $Category$ kan være andet end i starten med denne syntaks, vel.\,.\,? (12:20) .\,.\,Nej, så måske fungerer det altså, som den er.\,. (12:22)

.\,.\,Lad os sige, at det virker for nu. Og lad mig så sige, at jeg så tænker, at `Elements'- og `Subcategories'-relationerne henholdsvis må (og bør) forkortes med bare `e' og `s' i stierne. Dermed kunne vi f.eks.\ navigere til underkategorien rockmusik med stien: `userid//Media/-Music.s/Rock music'. Men hvis `Rock music' så er defineret med `Music' som sin overkategori (husk at hver kategori er unikt defineret ud fra overkategori plus titel), så kan applikationen altså også omdanne denne sti til den ækvivalente sti: `userid//Media/Music/Rock music'. Så en skråstreg i kategori-præfikset betyder altså det samme som `.s/', men hvor underkategorien så bare allerede har den givne kategori før `.s''et som sin definerende overkategori. Hvis derimod `Rock music' er defineret ud fra en anden kategori, jamen så må man bare beholde `.s/' i stien. Hvis man så vil se en liste (eller nærmere bestemt en `mængde') over elementerne i `Rock music,' så kunne stien så se sådan her ud: `userid//Media/Music/Rock music.e/'. (12:36)

Jeg forestiller mig også at brugere selv skal kunne definere forkortelser for relationer og andre ting, men det vil jeg nu nok undlade at implementere i betaversionen. 

Nå ja, og hvis i øvrigt at et element af en kategori har denne kategori som dens definerende kategori, så kan $ident$ (altså i `$Set''\ \texttt{/}\ ident$'-underproduktionen) bare være termens titel. Og tilsvarende gælder for kategorier, hvorfor at et kategori-præfiks fra `$Category$'-produktionen altid vil bestå af navne/titler frem for BIGINT-ID'er. (12:41)

Hvis ikke et element eller en underkategori har kategorien som sin definerende kategori.\,. ja, så skulle man måske faktisk indføre, at brugere kan lade kategorier ``med-adoptere'' andre kategoriers børn.\,. For eksempel kunne vi, hvis `Rock music' er defineret ud fra en anden kategori end `Music,' gøre sådan at `Music' bare vælger at (eller rettere at brugeren vælger at lade `Music') adoptere `Rock music' som sit barn også. .\,.\,Ja, og så vil man endda igen kunne erstatte `.s/' med `/', for det kræves så selvfølgelig bare, at en kategori ikke må adoptere flere børn med samme titel --- i hvert fald ikke uden at lave aliasser for dem. Og der kan man bare se, så får vi jo hermed også allerede en mulighed for, at brugere kan lave forkortelser (nemlig via aliasser, hvis man altså bare gør, at en kategori også kan ``adoptere'' og omdøbe sine egne børn). Nice nok. (12:46) 

.\,.\,Nå ja, jeg mangler forresten også lige nogle afgrænsnings-suffikser på $Set$sne\ldots (12:51)

.\,.\,

$Path := UserIdent\ \texttt{//}\ Set\ \texttt{/}\ |\ UserIdent\ \texttt{//}\ Term$

$UserIdent := ident$

$Set := Set'\ |\ Set'\ \texttt{\&\&}\ Set$ 

$Set' := Set''\ |\ Set''\ \texttt{||}\ Set'$ 

$Set'' := Set'''\ (\texttt{[}\ num\ \texttt{]})? $ 

$Set''' := (Set)\ |\ 
	Term\ \texttt{.}\ Relation\ (\texttt{(}\ Range\ \texttt{)})?\ |\ 
	ident\ (\texttt{(}\ Range\ \texttt{)})?
$ 

$Relation := ident$


$Range := \mathrm{TBD,\ but\ perhaps\ something\ like\!:}\ \ 
	num\ \texttt{;}\ (num)?\ |\ 
	\texttt{;}\ num
$

$Term := Category\ |\ Set\ \texttt{/}\ ident$

$Category := Category\ \texttt{/}\ ident\ |\ ident$

\ 

\noindent(Nu fik jeg også lige rettet $Set''$ til $Set$ for $Term$-produktionen.\,.)

.\,.\,Så er det så meningen at \texttt{\&\&} betyder en intersection (fællesmængde) af mængderne, og \texttt{||} betyder en union (foreningsmængde). Begge disse ting handler altså så at sætte flere mængder sammen (ved at man først ordner efter term-ID'et), og for begge ting beholdes alle ratings fra hver indgående mængde, således at det altså først er i applikationslaget, at den endelige aritmetik og listesortering foregår (og hvor brugerne nemlig så kan ændre sorteringen for listen, uden at skulle query'e serveren igen). Forskellen er så bare lige, at for \texttt{\&\&} fjernes alle de termer, der ikke indgår i begge mængder på hver side af operatoren, inden at listen/mængden serveres til browseren, og altså til applikationen, der kører i den. (13:21) 

.\,.\,Åh, og jeg skal også lge forklare, at jeg med $(\texttt{[}\ num\ \texttt{]})?$ for $Set''$ altså tænker, at $num$ her skal repræsentere et maksimumantal på, hvor mange termer, man gerne vil have serveret fra mængden (hvis størrelsen overstiger dette $num$).\,. Hm, jeg skal forresten også have tænkt over, hvordan man vender rangen om (og altså negerer relationen), men det må jeg bare lige have mente at finde ud af på et tidspunkt.\,. (13:25)

.\,.\,Hm, man kunne måske bare angive dette via $Range$en. Lad mig i øvrigt lige erstatte $num$ i $Range$ med $float$ og $num$ i $\texttt{[}\ num\ \texttt{]}$ med $int$, sådan at det bliver:

\ 

$Path := UserIdent\ \texttt{//}\ Set\ \texttt{/}\ |\ UserIdent\ \texttt{//}\ Term$

$UserIdent := ident$

$Set := Set'\ |\ Set'\ \texttt{\&\&}\ Set$ 

$Set' := Set''\ |\ Set''\ \texttt{||}\ Set'$ 

$Set'' := Set'''\ (\texttt{[}\ int\ \texttt{]})? $ 

$Set''' := (Set)\ |\ 
	Term\ \texttt{.}\ Relation\ (\texttt{(}\ Range\ \texttt{)})?\ |\ 
	ident\ (\texttt{(}\ Range\ \texttt{)})?
$ 

$Relation := ident$


$Range := \mathrm{TBD,\ but\ perhaps\ something\ like\!:}\ \ 
	float\ \texttt{;}\ (float)?\ |\ 
	\texttt{;}\ float
$

$Term := Category\ |\ Set\ \texttt{/}\ ident$

$Category := Category\ \texttt{/}\ ident\ |\ ident$

\ 

Her betegner $float$sne i $Range$en jo så selvfølgelig min og maks på, hvilke nogle ratingværdier, man gerne vil query'e efter. (13:34)

.\,.\,Hov, det var forkert at rette $Set''$ til $Set$ i $Term$. Lad mig lige tænke over, om det skal rettes tilbage til $Set''$, eller hvad.\,. .\,.\,Hm, måske skulle rette det tilbage, og så også bare rette $Set$ til $Set''$ i $Path$.\,.\,? (13:40) .\,.\,Ja, lad mig bare gøre dette for nu. Så får vi:

\ 

$Path := UserIdent\ \texttt{//}\ Set''\ \texttt{/}\ |\ UserIdent\ \texttt{//}\ Term$

$UserIdent := ident$

$Set := Set'\ |\ Set'\ \texttt{\&\&}\ Set$ 

$Set' := Set''\ |\ Set''\ \texttt{||}\ Set'$ 

$Set'' := Set'''\ (\texttt{[}\ int\ \texttt{]})? $ 

$Set''' := (Set)\ |\ 
	Term\ \texttt{.}\ Relation\ (\texttt{(}\ Range\ \texttt{)})?\ |\ 
	ident\ (\texttt{(}\ Range\ \texttt{)})?
$ 

$Relation := ident$


$Range := float\ \texttt{;}\ (float)?\ |\ \texttt{;}\ float
$

$Term := Category\ |\ Set''\ \texttt{/}\ ident$

$Category := Category\ \texttt{/}\ ident\ |\ ident$

\ 

Nå, det kan være, at jeg lige vil gå mig en lille tur, men jeg har altså til gode at snakke om, hvordan brugere skal kunne up-rate relationer og skabeloner --- og endda andre brugere/brugergrupper, som man vil kopiere næsten fuldstændig, på nær lige at ens egne ratings kommer og overskriver deres --- og om hvordan prædikater nu bare skal implementeres via relationer, men det må jeg alt sammen skrive om, når jeg kommer tilbage (.\,.\,dog ikke sikkert, at jeg når at skrive det hele i dag, men det må jeg se.\,.)\ldots (13:47)

\ldots\ (15:30) Okay, lad mig lave det om, så at brugeren også bliver en del af $Set$-produktion-en.\,.

\ldots Hm, lad mig faktisk lige prøve først at lave syntaksen, uden `.s/' $\to$ `/' -forkortelserne, det er sikkert nemmest!\,.\,. (15:59)

\ 

$Path :=  Set\ \texttt{/}\ |\ Set\ \texttt{/}\ ident$

%$Term := Category\ |\ Set\ \texttt{/}\ ident$


$Set := Set'\ |\ Set'\ \texttt{\&\&}\ Set$ 

$Set' := Set''\ |\ Set''\ \texttt{||}\ Set'$ 

$Set'' := Set'''\ (\texttt{(}\ Range\ \texttt{)})?\ (\texttt{[}\ int\ \texttt{]})? $ 

%$Set''' := \texttt{(}\ Set\ \texttt{)}\ |\ 
%	UserIdent\ \texttt{/}\ Category\ ident\ \texttt{.}\ Relation\ |\ 
%	ident
%$ 

$Set''' := \texttt{(}\ Set\ \texttt{)}\ |\ 
	Set'''\ \texttt{/}\ ident\ \texttt{.}\ ident\ |\ 
	ident\ |\ 
	User
$ 

%(16:09) Ja, det blev noget nemmere, og nu skal jeg bare lige huske, hvordan jeg laver et tilde i monospace font.. ...(16:22) Okay, det er åbenbart: \texttt{$\mathtt{\sim}$}$.. ..Ah, eller bare \mathtt{\sim}..

$User := \mathtt{\sim}\ |\ \texttt{/}\ ident$ 

$Range := float\ \texttt{;}\ (float)?\ |\ \texttt{;}\ float$

\ 

(16:27) Ok, så fik jeg lavet ovenstående syntaks. Dette var rigtignok noget nemmere.\,. .\,.\,Hov, ændrer lige $Set'''$ i nummer to underproduktion (eller bare produktion? (kan ikke helt 100 huske terminologien.\,.)) af $Set'''$-produktionen til $Set''$ i stedet for.\,. .\,.\,Hm, tja, men så burde jeg også ændre $Path$.\,. Okay, lad mig lige ændre det tilbage, og så i stedet finde ud af, hvad jeg gør i stedet.\,.  .\,.\,(16:41) Hov, det lyder mærkeligt, men er det ikke ligefør, at det (altså $Set'''$ i nr.\ 2 underproduktion) skal ændres til 
$Set$ i stedet.\,.\,??\,.\,. .\,.\,Nej, så kommer det til at blive svært at gennemskue --- også selvom det måske kunne give mening rent syntaktisk (men så skal man altså spidse for meget øjne, havde jeg nær sagt.\,.).\,. (16:44) .\,.\,Okay, så lad mig prøve at lave det om, så der skal være en parentes, hvis der er unions og/eller intersections før et slash.\,.


\ 

\ 

\ 

\ 

$Path :=  Set\ \texttt{/}\ |\ Set\ \texttt{/}\ ident$

%$Term := Category\ |\ Set\ \texttt{/}\ ident$

$Set := Set'\ (\texttt{(}\ Range\ \texttt{)})?\ (\texttt{[}\ int\ \texttt{]})? 
$ 

$Set' := \texttt{(}\ Set''\ \texttt{)}\ |\ 
	Set\ \texttt{/}\ ident\ \texttt{.}\ ident\ |\ 
	ident\ |\ 
	User
$ 

$Set'' := Set'''\ |\ Set'''\ \texttt{\&\&}\ Set''$ 

$Set''' := Set\ |\ Set\ \texttt{||}\ Set'''$ 

%(16:09) Ja, det blev noget nemmere, og nu skal jeg bare lige huske, hvordan jeg laver et tilde i monospace font.. ...(16:22) Okay, det er åbenbart: \texttt{$\mathtt{\sim}$}$.. ..Ah, eller bare \mathtt{\sim}..

$User := \mathtt{\sim}\ |\ \texttt{/}\ ident$ 

$Range := float\ \texttt{;}\ (float)?\ |\ \texttt{;}\ float$

\ 

\ldots Sådan.\,. (17:03)

(19:20) Jeg har nogle flere tanker/idéer. Men det kan godt være, at jeg mest bare tænker i aften (som jeg jo tit gør), og så venter med at skrive i morgen. Men jeg vil dog lige nævne, at det jo nok ikke passer, det med at serveren smider termer væk fra en intersection, inden den serveres til browseren, for det er jo nok i selve applikationen, at union-/intersection-algoritmerne foregår. Og dermed kan alle queries til databasen nok egentligt koges ned til: ``(user\_id, subj\_id, rel\_id)($Range$)[int]''.\,.\,!\,.\,. .\,.\,Tja, på nær selvfølgelig lige, når man query'er efter data, og måske hvis man kan full-text-search'e på f.eks.\ keyword strings.\,. (19:26)

(19:31) Hov, og det giver nok slet ikke mening at strække Sets ud som sin egen tabel for SemanticInputs.\,. Lad mig lige genlæse om B-træerne, bare lige for at være helt sikker.\,. .\,.\,Hm, det ser faktisk umiddelbart ud til, at det \emph{giver} mening, men mon ikke man kan finde en løsning, hvor bladene i B-træet kun gemmer et offset *(nej, en værdi som skal tilføjes et nøgle-offset, rettere) af den samlede nøgle, det må jeg lige finde ud af på et tidspunkt.\,. (19:36)


(06.03.23, 9:43) Det kan være, at det på et tidspunkt kunne give mening at bruge en hierarkisk database i stedet, hvis ikke kompressionsalgoritmen er så effektiv til at komprimere nøgler, som den kunne være. Men det kan nu også sagtens være, at en rel.\ database såsom MySQL vil holde helt fint. Pointen er i hvert fald bare, at jeg skal sørge for at gøre interfacet åbent over for, hvordan det grundlæggende lag er implementeret. Og nu hvor interfacet fra applikationslaget og til control- og databaselaget kommer til at være på formen ``(user\_id, subj\_id, rel\_id)($RatingRange$)[$ListRange$]'' (ja, jeg skal også lige have rettet lidt i grammatikken (syntaksen) ovenfor), jamen så kommer det til at blive nemt at gøre dette interface åbent overfor ændringer i backend-lagene, for det kræver bare, at ``(user\_id, subj\_id, rel\_id)'' gives i et mere fleksibelt format (end at man giver dem som longs). Og hermed tror jeg faktisk, at jeg vil genindføre typeflag, som så netop bliver et præfiks på user\_id og subj\_id i dette format. Jeg tænker, at begge disse så kan gives som nogle (sikkert næsten altid to) hexadecimaler, efterfulgt af et komma, efterfulgt af en ny (variabel!) række hexadecimaler, der giver resten af ID'et (og hvor alle ledende 0'er altså kan skæres væk!). Og for rel\_id kan man jo nok bare nøjes med ID-hexadecimalerne, og alt andet end lige lade typeflaget være underforstået (hvis det mangler). (9:55)

Og det kommer jo så ret meget til at definere den semantiske database (hvis vi ser bort fra data-gets m.m.), som herved altså kan ses som en abstraktion over den egentlige databaseimplementation, hvilken jo så netop har friheden til at ændre sig. 

Bemærk, at det med stisyntaksen, som jeg begyndte at beskrive i går, det foregår alt sammen i applikationslaget, og dermed kommer brugerne altså også på et tidspunkt til at kunne ændre alt dette fuldstændigt. (9:59)

I går fik jeg også tænkt noget mere over, hvordan brugerfladen skal gøres åben overfor brugerændringer og alt det, og jeg synes, jeg virkeligt er kommet frem til noget godt nu. Applikationen skal have en liste over JavaScript-funktioner i hukommelsen. Eller rettere, den skal have en struktur af ID--funktionspar, hvor ID'et så gerne faktisk skal være en reference til en JavaScript-program-term i databasen, hvor scriptet kan læses. Brugerne har ikke selv adgang til at ændre i denne liste, medmindre de aktivt installere en browserudvidelse til at gøre netop dette. Ellers er alle funktionerne i listen kun nogle, der er godkendt af (og muligvis skrevet af) hjemmesiden selv. I hver brugeres Term-kategori, som jo er kategorien, der indeholder alle termer (inklusiv sig selv i princippet) kan de så up-rate JavaScript-program-termer ud fra en passende relation. Alt dette er dog \emph{kun} implementeret oppe i applikationslaget, så relationen for at up-rate JavaScript-program-termer behøver ikke at være speciel i det grundlæggende lag, og vigtigere: kategorien af JavaScript-program-termer er ikke noget fundamental kategori. Disse ting er bare noget, som hjemmesiden sørger for er der fra start, men de er altså ikke noget, som det grundlæggende lag behøver at kende til. Nå, men ved så at up-rate JavaScript-program-termer for denne relation, så kan brugeren signalere til applikationen, hvilke JavaScript-programmer på listen gerne må bruges af applikationen til visning af diverse mængder og termer. Og hvis brugeren har installeret en browserudvidelse, som tilføjer flere funktioner til listen, så skal brugeren altså stadig signalere til applikationen herved, at denne også vil gøre brug af disse funktioner (altså ved at up-rate dem for omtalte, ja, vi kunne kalde den ``ønskede JS-funktioner''-relationen). Hvis en bruger så ``logger sig på'' en anden brugers semantiske træ, og ikke har de samme funktioner på sin egen liste, jamen så kan brugeren få en lille advarsel om, at der måske vil gå kludder i visningen, og at visningen i hvert fald muligvis ikke vil være, som at pågældende bruger selv vil se tingene. Okay. Lad mig skifte paragraf og så forklare, hvordan applikationen så videre benytter disse JS-funktioner. (10:17)

For alle kategorier, inklusiv den ydre ``Term-kategori,'' kan brugere så up-rate data, som de godkendte JS-funktioner så kan læse. I princippet er alt frit for, men man kunne jo passende gøre bruge en vis standard fra start af, som siger at hver datapakke også i starten angiver ID'et på den JS-funktion, som er tiltænkt til at benytte den indeholdte data, bare for en god ordens skyld. (10:20) Dette data kan så beskrive, hvordan JS-funktionen skal opbygge forskellige visninger i applikationen --- f.eks.\ forskellige søjler, hvis man gerne vil have en applikation, der følger, hvad jeg forestillede mig, inden jeg kom på det her med ``filstierne'' (som jeg dog nu ikke er helt så sikker på). 

Selvom man i princippet kan gøre hvad man vil herfra, og dermed i princippet også godt kan bryde ud af denne standard, så tror jeg at følgende standard også vil være rigtig fornuftig til at starte med. Den går på, at man siger, at hver JS-funktion alt andet end lige skal tage ansvar for hele applikationens visning. Med andre ord altså at man alt andet end lige kun gør brug af én JS-funktion ad gangen. Så jeg forestiller mig altså nogle JS-funktioner, som sammen med den efterfølgende data, der gives til dem --- som jo godt kan være både HTML- og CSS-kode! --- hver især alene kan stå for hele applikationens indretning. Og hvorfor forestiller jeg mig så, at man alligevel skal have en hel liste af dem?\,. Jo, fordi her JS-funktion nemlig også skal være ansvarlig for, at den ikke kan gives noget data, som for den til at opføre sig malicious.(!) Og hermed vil de tidlige JS-funktioner altså nok være ret skrappe, hvad angår det data, som gives dem, nemlig ved at parse dem ret hårdt for f.eks.\ HTML-tags, der kan være farlige, osv. (10:30) Men når man så langsomt gør JS-funktionerne mere og mmere åbne (uden at komme til at gøre dem farlige (forhåbentligt)), så vil det jo være smart, hvis man også beholder de gamle. Så det vil sikkert blive sådan i starten, at hver ny JS-funktion tilføjet til listen vil udvide en gammel funktion, sikkert sådan at hvis den nye funktion fik det samme data, så ville den opføre sig på samme måde. Sammenlagt vil vi derfor nok få en række eller et træ af JS-funktioner, hvor børnene ofte er bagudkompatible med deres forældre. Denne liste er som sagt en som hjemmesiden selv udelukkende står for at bestemme, på nær hvis brugerne stoler på deres egen evne til at verificere JS-funktioner, og gør brug af browserudvidelser til at tilføje ting til denne liste (og dermed måske være på forkant af de andre brugere). Men derfor kan det jo sagtens være, at disse brugere også gerne vil holde sig til de samme principper, nemlig med at se alle JS-funktionerne som et træ, hvor børnene i reglen gerne skal være bagudkompatible med deres forældre. (10:36)

Og derfor, når en bruger ``logger sig på'' en anden bruger, der har tilladt flere (måske nyere) JS-funktioner, som den på-loggende bruger ikke selv har godkendt (enten via up-rates eller ved ikke at have installeret den fornødne browserudvidelse), så kan det jo dermed alligevel blive muligt, at brugeren stadig kan se meget af det samme, som ejeren af ``kontoen'' ser, netop pga.\ den mulige bagudkompatibilitet (fordi den på-loggende brugers JS-funktion stadig kan indsætte de fleste ting, som ``ejeren af kontoen'' ville få vist). (10:40)

Okay, hermed har vi så et system, hvor brugerne i princippet selv kan bygge oven på og ændre ad libitum. Lad mig så forklare lidt mere om, hvad jeg forestiller mig, at de tidlige JS-funktioner skal gøre. Jeg forestiller mig, at brugere så for hver kategori (ikke bare for Term-kategorien) kan brugere up-rate %..
HTML- og CSS-data for forskellige passende relationer til, hvordan forskellige felter og lister skal vises for elementerne i kategorien. Vi kan jo starte med at tænke på, hvordan elementerne skal vises i listen, man får serveret, når man bader om kategoriens elementer. Hertil kan der så være en relation der siger ``desired HTML for showing elements,'' som man så kan up-rate HTML-data til. Der kan også være en relation, hvor man kan ændre selve den omkringliggende HTML for elementvisningen. Her kunne man f.eks.\ forestille sig, at når brugeren klikker sig ind på elementmængden til kategorien, så vises underkategorier faktisk også i en liste til venstre for elementerne. Og dette kan man sagtens implementere via denne ``omkringliggende HTML til elementvisningen''-relation, for hver HTML-template har nemlig inden begrænsninger på --- udover de begrænsninger der er sat med vilje selvfølgelig --- hvad HTML'en kan query'e serveren efter; det behøver f.eks.\ ikke kun at være elementer til kategorien, bare fordi disse ligesom er temat / den centrale del af denne visning. (11:00) 

Lad mig lige allerede nævne her, at en vigtig pointe med det hele er så, at en underkategori så altid arver indstiller fra en overkategori, medmindre at brugeren har valgt (via up-rates), at underkategorien selv skal overskrive de gamle indstillinger og tilføje nye indstillinger til sine underkategorier. .\,.\,Ja, det er faktisk ikke så meget mere at sige om dette nu her, men lad mig bare lige understrege, hvor vigtigt denne pointe er for det hele ved at slutte denne paregraf af med et udråbstegn. ! (11:03) *(11:08) Nå jo, jeg havde mere at sige om det. En vigtig ting at forklare er, at kategorier ikke arver fra deres ``definerende overkategorier,'' som jeg har kaldt dem, nødvendigvis, men fra deres overkategorier i den nuværende $Path$.\,! Dermed kan visningen for en kategori altså ændre sig afhængigt af, hvordan brugeren navigerede hen til kategorien! (11:11) 

Nå, og hvad angår den indre element-HTML, så vil denne jo også indeholde knapper, som brugeren kan trykke på (og med specifikationer af, hvad der så skal ske ved tryk på disse knapper). Dette kan jo så f.eks.\ være at udfolde flere felter for elementet (og/eller at vise mere af diverse tekstfelter, som måske er afkortet i en umiddelbare listevisning). Knapperne kan så også specificeres (alt sammen via den samme HTML-data, nemlig den omkring ``den indre HTML for elementerne'') så de åbner ny sider. (11:08) .\,.\,(11:11) Jeg forestiller mig så nu, at nye sider generelt åbnes ved at angive en $Path$ til applikationen, og altså bede applikationen om at åbne denne sti. 

Lad mig i den forbindelse forresten indskyde, at selvom de her stier er implementeret ind i applikationen, og at applikationen godt kan køre på samme side, så forestiller jeg mig dog, at $Path$s også gerne skal kunne opdateres vi GET-metoden, nærmere bestemt således at brugeren kan toggle mellem GET og POST metoden, alt efter om brugeren vil have, at browseren gemmer navigeringshistorikken eller ej. (11:15)

Nå, men er det så kun de to relationer, nemlig den om ``omkringliggende'' og ``indre'' HTML for elementvisningen, som brugerne skal (kunne) up-rate HTML-data for? Nej, for en kategori har jo ikke kun `.Elements'/`.e' som sin eneste relevante relation. Det samme skal kunne gøres for `.Subcategories'/`.s'-visningen (hvor man altså også kan dele det op i indre og omkringliggende HTML, hvis man vil). Og der er også andre relationer, f.eks.\ er der jo selve alle disse relationer, som bruges til at vælge HTML-data (eller anden data; det behøver slet ikke at have form som HTML; det kommer alt sammen bare an på, hvordan JS-funktionen(/funktionsfamilien), man bruger, er defineret). For ikke at få et uendeligt antal relationer til dette, så kunne man så have én relation til at indstille præferencerne for visningen af disse lister, som nemlig så også kan bruges til visningen af selve den liste, hvor man indstiller visningen for sådanne lister (sådan at det valgte (mest up-ratede) visningsdata for denne relation altså også kommer til at definere visningen for relationen selv). (11:24) 

.\,.\,Bum. Og det forklarer sådan set allerede meget godt, hvordan jeg nu forestiller mig, at fundamentet i applikationslaget skal være opbygget på. Nu bør jeg så lige prøve at omdefinere $Path$en en gang (og give nogle forklaringer på ændringerne), og så vil jeg ellers begynde at ændre i min databaseimplementation.\,. (11:26)

.\,.\,Hov, lad mig lige sige, at jeg sikkert kommer til at lave mange ændringer i denne syntaks løbende. Men det gør ikke så meget, for det.\,. Nå ja, det skulle jeg måske også lige overveje: Er det samme JS-funktion, der skal parse $Path$en, når siden opdateres med en ny sådan $Path$ (enten via GET- eller via POST-metoden)?\,.\,. .\,.\,Hm, det synes jeg faktisk på en måde, men lad os så faktisk sige, at der i stedet for at være tale om en enkelt funktion for hver ID i JS-funktions-listen, så faktisk skal være tale om et helt namespace af funktioner, nemlig således at nr.\ to element i hver tupel i denne liste (nemlig hvor første element så er JS-program-term-ID'et) så er.\,. Hm, nå, det er faktisk også lige meget, hvad strukturen helt præcis kommer til at blive, men pointen er bare, at hvert samlet element (om så disse er tupler eller ej) i omtalte JS-liste altså definerer et helt namespace af funktioner, som applikationen kan benytte, hvis brugeren også tillader denne brug via up-rates for ``gyldig JS''-relationen for Term-kategorien. (11:35)

Nå, men lad mig så sige, at nu forestiller jeg mig en syntaks for $Path$ mere a la:



\ 

$Path :=  Set\ \texttt{/}\ |\ Set\ \texttt{/}\ ident$

%$Term := Category\ |\ Set\ \texttt{/}\ ident$

$Set := Set'\ (\texttt{(}\ RatingRange\ \texttt{)})?\ (\texttt{[}\ ListRange\ \texttt{]})? 
$ 

$Set' := \texttt{(}\ Set''\ \texttt{)}\ |\ 
	Set\ \texttt{/}\ ident\ \texttt{.}\ ident\ |\ 
	ident\ |\ 
	User
$ 

$Set'' := Set'''\ |\ Set'''\ \texttt{\&\&}\ Set''$ 

$Set''' := Set\ |\ Set\ \texttt{||}\ Set'''$ 

%(16:09) Ja, det blev noget nemmere, og nu skal jeg bare lige huske, hvordan jeg laver et tilde i monospace font.. ...(16:22) Okay, det er åbenbart: \texttt{$\mathtt{\sim}$}$.. ..Ah, eller bare \mathtt{\sim}..

$User := \mathtt{\sim}\ |\ \texttt{../}\ ident$ 

$RatingRange := float\ \texttt{;}\ (float)?\ |\ \texttt{;}\ float$

$ListRange := int\ |\ int\ \texttt{..}\ int$

\ 

Desuden forestiller jeg mig også (foruden det med at `.s/' skal kunne forkortes til `/' givet visse omstændigheder, jeg har snakket om), at man skal kunne definere macro-variable, sådan at gentagne første dele af $Path$s i stedet kan defineres som en macro, som så både brugeren kan bruge, hvis denne vil skrive en $Path$ selv, og som applikationen kan bruge, når denne vil vise/printe en $Path$ (måske den pågældende for ``siden'') for brugeren. (11:45, 06.03.23)

(11:52) Nå ja, jeg har faktisk også lige et par flere ting, jeg skal nævne omkring syntaksen. For det første så tænker jeg nu, at manglen på $\texttt{[}\ ListRange\ \texttt{]}$ skal signalere, ikke den fulde mængde, men at serveren bare skal servere den første og bedste term fra mængden (altså den med højest rating --- eller lavest, hvis $RatingRange$ startere med den laveste float af de to). Så manglen på $\texttt{[}\ ListRange\ \texttt{]}$ svarer altså med andre ord til $\texttt{[}\ 0\ \texttt{]}$ (eller $\texttt{[}\ 1\ \texttt{]}$ hvis man 1-indekserer). 

En anden ting er, at jeg også forestiller mig, at ting såsom domæne- og/eller syntaks-begrænsninger på hrefs også skal implementeres via JS-funktionen, eller rettere JS-biblio-teksprogrammet, bør vi hellere sige. Men her skal brugere jo også gerne kunne ændre disse indstillinger (i hvert fald avancerede brugere), og her forestiller jeg mig så, at dette kan ske ved at brugen uploader præference-data specifikt også til Term-kategorien (for man må regne med at disse begrænsninger skal være globale og ikke bare tilknyttet en bestemt underkategori). Og så kan jeg i øvrigt også lige nævne i denne forbindelse, at det med at bruge `$\texttt{../}\ ident$' for skift-bruger-syntaksen jo faktisk giver god mening rent intuitivt, for så starter alle $Path$s altid på brugerens egen Term-kategori. Og de globale begrænsninger (bl.a.\ på JS-programmer og hrefs) man sætter her kommer så også til at gælde, når man går over på andre brugeres Term-kategorier, nemlig fordi man starter på sin egen. Dette er selvfølgelig rent for bruger-intuitionens skyld; syntaksen er jo bare et symbol på, hvad der foregår. Men i forhold til at forstå, at når man kommer fra en given overkategori, så vil de følgende kategorier arve indstillinger fra denne, jamen så giver de jo god mening, at man altid starter på sin egen Term-kategori, for alle kategorier vil jo altid arve ting fra denne kategori (om ikke andet så i hvert fald begrænsninger på JS-programmer). Følte lige, det var værd at nævne også. (12:05, 06.03.23) 

(14:59) Jeg tror faktisk, at lader typeflaget være en (fixed) streng af tre chars i query-formatet i stedet (som altså ikke er begrænset til hexadecimal-karakterer, men kun til ASCII-karakterer). For så kan jeg altid indføre en ny standard oveni senere, der også kan bruge færre karakterer. 

Desuden mangler jeg faktisk at nævne noget ret vigtigt (som jeg jo faktisk lagde lidt op til i går), og det er, at brugere også simpelthen skal kunne vælge bruge andre brugeres (og selvfølgelig særligt bruger\emph{grupper}s) indstillinger til de kategorier, de har lyst til. Dette foregår så ved, at de simpelthen for kategorien vælger at benytte brugergruppens (lad os bare sige brugergruppe kun, selvom en bruger i princippet også kan bruge en anden bruger, men det vil være meget mere almindeligt og anvendeligt at benytte bruger\emph{grupper} her) stemme, når det kommer til at up-rate input-data til JS-programmet (som dog stadig skal godkendes specifikt af brugeren selv), i stedet for sig selv. Ja, og brugeren kan også bruge en blanding af sin egen stemme og en brugergruppes stemme. Så med andre ord: Brugeren kan stadig vælge at up-rate diverse input data til en given kategori, men brugeren skal også have mulighed for at ``få andre til at hjælpe sig med dette arbejde'' så at sige, nemlig ved for en speciel relation --- som kunne hedde noget a la: ``desired users to decide applikation preferences (for this category)'' --- at rate andre brugere eller brugergrupper op, enten alene eller sammen med sig selv. Og alt efter hvor meget ratingen overstiger en vis tærskel (f.eks.\ 0), kan den samlede stemmevægtfordeling så være herefter. Dette giver så signal til applikationen, at alle (HTML-)input-data-relationer (som jeg snakkede om her lidt tidligere i dag) skal have deres mængder vægtet med denne vægtfordeling. Hermed kan man altså få en applikation, der kan forbedre opdatere sit udseende også selvom man ikke selv gør noget aktivt, men nemlig fordi en af de valgte brugergrupper så up-rater nogle andre indstillings-(JS-)input-data til kategorien. Og husk at disse indstillinger kan have indflydelse på alle de termer og mængder, man kan navigere hen til fra denne kategori (og dermed alt hvad applikationen kan vise en, når man har navigeret fra denne kategori). Jeg har så nævnt at underkategorier dog godt kan overskrive indstillinger fra forældrene, og her kan man jo så passende gøre det sådan, at det også vil være den samme vægtfordeling, der skal gøre sig gældende i spørgsmålet om, om en underkategori til den givne kategori skal overskrive dennes indstillinger (og selvfølgelig også hvordan de i så fald skal overskrives). Med andre ord vil det være naturligt, at alle underkategoriernes data-input-relationer også arver stemmevægtfordelingen fra den givne overkategori, altså den vægtfordelingen der valgtes ved at rate brugere og brugere grupper ud fra en relation (med den givne overkategori som subjekt), der siger ``desired users to decide applikation preferences (for this category --- and its subcategories!)''.\,.\,:) (15:21)







\section{Hvad jeg gerne vil sigte efter med det basale applikationsdesign}

*[(25.02.23)] (11:50) Jeg har tænkt lidt over, hvordan udgangspunktet skal være for applikationsbrugerfladen, eller rettere hvad man gerne bør sigte imod (for betaversionen behøver ikke at inkludere det hele). Her i formiddages fik jeg også lige nogle gode nye idéer, som jeg vil fortælle om lige efter denne sektionsintro. Jeg regner så med at prøve at gå igennem mine seneste tanker omkring emnet, og så vil jeg nok lade sektionen (denne) stå åben efterfølgende, så jeg kan vende tilbage med flere idéer, til den grundlæggende opbygning af applikationen (eller i hvert fald udgangspunktet for den). Jeg vil så også.\,. Hm, enten vil jeg starte en ny betaversion-sektion, eller også vil jeg bare føje til den forrige, men uanset hvad, så vil jeg i hvert fald gå videre til, når jeg har skrevet om ting til denne sektion, gå over til så at skrive om, hvilke nogle hjørner jeg så kan skære i starten. For den første version af hjemmesiden behøver altså ikke at inkludere alle de detaljer, jeg beskriver her, og derfor vil jeg altså have en separat sektion, hvor jeg så skriver om, hvilke nogle ting i applikationsbrugerfladen, jeg \emph{ikke} vil implementere til at starte med. (11:58)

Og nu vil jeg altså lægge ud med at beskrive de idéer, jeg har fået her i formiddags, og så vil jeg i øvrigt også snakke om vigtigheden i at bruge ``kategorier.'' .\,.\,Hm, på den anden side så har jeg vist ikke snakket så meget om kategorier overhovedet, så lad mig egentligt tage det mere fra grunden i stedet, og så snakke om kategorier først.

Det kan virke lidt underligt, at introducere ``kategorier'' som noget rigtigt grundlæg-gende; næsten ligeså grundlæggende som `prædikater' og `relationer.' (Dog ikke helt længere, for `kategorier' bliver ikke længere en del af det grundlæggende (database-)lag, men det bliver i stedet bare noget virkeligt grundlæggende i applikationslaget.) Hvorfor ikke snakke om `mængder' eller `klasser;' begreber som er meget mere ``matematiske'' så at sige?\,. Jo, fordi begrebet om `kategorier' har en helt særligt fordel. Vi kan se kategorier som en slags lav-niveau repræsentation af mængder (hvis vi altså tager udgangspunkt i \emph{standardmodellen} for mængdelære), bare hvor kategorier dog godt kan indeholde sig selv. For vi kan nemlig se dem som ``kasser'' med skilte på, hvor vi så kan putte referencer til andre ting --- eller til den givne kasse selv --- ned i disse kasser. Nå, men at de kan indeholde sig selv, og at man derfor kan have en ``kategori af kategorier,'' som kan indeholde sig selv, er nu ikke forcen ved kategorier. Den store fordel kommer, når vi begynder at benytte begrebet om `underkategorier.' For hvis jeg siger, at $x$ er en god underkategori til $y$, så er det allerede en rimelig naturlig fortolkning af den sætning, at jeg mener at $x$ er gavnlig at se (tidligt) i en liste, hvis man som bruger beder om at se $y$'s underkategorier. Men hvis jeg derimod f.eks.\ sagde, at $x$ er en god \emph{undermængde} af $y$, jamen så ville folk i almindelighed rynke på panden og sige: ``jamen, alle korrekte undermængder af $y$ må da være lige gode, ikke?'' Selvfølgelig kan man prøve lave en specifik relation, der siger lige præcis siger: ``$x$ er gavnlig at se (tidligt) i en liste, hvis man som bruger beder om at se $y$'s underkategorier.'' Men pointen er så bare, at man får utroligt meget forærende (i forhold til hurtig intuitiv forståelse), hvis man så også bare kalder det `kategorier' og `underkategorier' i stedet; så er der langt kortere for brugerne til at forstå, hvad pointen med underkategorierne er! (12:13) %..Føler mig effektiv på tasterne i dag..

Så kategorier skal altså være en helt central ting i applikationen, forestiller jeg mig --- i hvert fald når folk vil lave semantiske søgninger, og ikke bare vil søge på keywords (hvor jeg i øvrigt nu tænker at benytte et full-text index på String-typen til gavn for dette (i stedet for at brugere manuelt skal bedømme relevante keywords --- det kan de også, men med et full-text index er vi meget hurtigere i gang fra start)). Og ja, man kan i princippet så starte med `kategorien over kategorier' som den mest grundlæggende ting (.\,.\,hvor man så må regne med, at den selvsamme kategori ikke bliver ratet højt som gavnlig underkategori, btw). Herfra kan man så søge efter underkategorier ved et følge træet ned herfra (imod hvad man søger efter) ved at følge kanter, der repræsenterer en grundlæggende `.subcategory='-relation. Nå man så er kommet til en underkategori, der virker lovende, så kan man så vælge at få vist termer, der hører til denne underkategori (i stedet for at bede om yderligere underkategorier). Disse termer kan så herfra ordnes og filtreres ud fra prædikater. Og så kan man altså forhåbentligt finde den eller de ressourcer, man er interesseret i.

Dette beskriver nok den mest almindelige semantiske (\emph{manuelle}) søgning, man kan forestille sig. Men der er også andre usecases, hvor brugerne har brug for nogle andre relations-kanter at følge. Lad mig lige tænke over, hvilken én jeg vil skrive om først, og så vende tilbage.\,. (12:26)

%I øvrigt, hvad angår navn, jeg kan egentligt ret godt lige SemNet, men det er nu også bare næsten lidt for.. 00'er-agtigt.. *Det er et okay navn. Jeg har også i tankerne: SNet, S-Net, s.net, sem.net, sema.net..

%..Hm, en af mine idéer (den vigtigste) her fra i formiddags er, at man nok kan implementere bruger-dataen simpelthen via kategorier og almindelige ratings omkring disse. Nu overvejer jeg, så om kan gøre noget tilsvarende for filtre/prædikatvægtninger, eller hvad man nu lige skal gøre der.. (12:55) ..Jeg er i øvrigt kommet frem til, at filtre--vægtninger nok skal åbnes som en barne-søjle, der så kommunikerer direkte til forældersøjlen (og nok kun denne), når brugeren laver opdateringer i filter--vægtnings-præferencerne, men det skal jeg nok komme ind på.. ..Hm, jeg tænker jo forresten, at filter--vægtnings-prædikaterne bare kan være den selvsamme liste, som vises for `relevante prædikater' til termen.. ..Hm ja, og så er det bare spørgsmålet, om, og i så fald hvordan, indstillingerne til disse prædikater skal gemmes.. ..Hm, og hvad med grundfiltrene..?.. 

%...Hm, brugere skal nok bare være entiteter fra starten, og så bør brugerne bare advares mod at lave offentlige profiler med en spcifikt identitet angivet, medmindre man er klar på at blive vurderet af andre. Jeg skal i øvrigt også lige finde ud af, hvordan brugere sletter.. ah, men hvis hver dataentitet bare gemmer uploadende bruger, så kan forfatter-botten jo bare slette felt-vurderinger om uploadet, hvis pågældende bruger ønsker det.. Ja, det må være fint nok.. 
%Lad mig i øvrigt lige sige, at min anden idé fra i formiddags handler om, at der både skal være en ".related predicate=" (er gået væk igen fra at bruge ':') -relation \emph{og} en ".subcategories of related predicates="-relation for hver term (og særligt hver kategori-term). På den måde kan en kategori altså i praksis bruges som sig selv og som den afledte kategori af relaterede prædikater (til den kategori) på én gang, uden at man først skal navigere fra førstnævnte til sidstnævnte kategori for at nå frem til "subcategories of related predicates," når det er det, man ønsker. Med andre ord bliver den afledte "related predicates"-kategori altså nu en slags "anonym kategori," der ikke behøver at oprettes, men som bare automatisk følger med hver kategori (og andre typer termer), nemlig ved at man konstruere omtalte relationer, som så at sige springer det led over, at man ellers først skulle navigere til denne afledte kategori (som så nu er "anonym"/underforstået i stedet). ..Men dette skal jeg altsammen også skrive om i den renderede tekst. Nu vil jeg dog først tage en gåtur (i det virkeligt fine vejr *(he, nå, det var faktisk ikke særligt meget sol at komme efter, og det var endda også lidt koldt og blæsende --- så endda nogle (meget) små snefnug til sidst på turen)) og tænke over filtrene/vægtningerne... (13:46)
%... (14:35) Okay, jeg tror jeg ved, hvordan det skal være med filtrene og grundfinltrene. Det bliver simpelthen også, at brugerne rater filtre op under en grundfilter-kategori. Og så kan applikationen bare vide, at hvis de rater over en vis (væsentligt større end 0) tærskel, så skal applikationen initiere dem fra start af. Og hvis de er lige under den tærskel, så skal de bare vises, når brugeren går ind i, hvad der så kommer til at svare til grundfilter-indstillingerne, men som altså nu også bare er nogle termer ordnet i kategorier og underkategorier via (i dette tilfælde specifikt brugerens egne) ratings. Og så skal der også bare være en anden form for kategori, som handler om både at sætte filtre og vægtninger, men hvor det så er meningen, at de samme prædikater som benyttes her også vises, når brugeren indstiller et filtrene--vægtningerne for en specifik kategori/term. Så brugeren kan altså bryde disse indstillinger midlertidigt --- og hvis brugeren vil, kan denne jo også bare gemme ændringerne som en ny start-indstilling.. Ja, lad mig kalde det "grundindstillinger" og "startindstillinger," disse to forskellige ting. Og så er forskellen altså, at "grundindstillinger" derimod ikke vises, når brugeren skal lave en specifik indstilling for en term/kategori, men i stedet skal brugeren altså helt ud og ændre i sine grundindstillinger, nemlig i en del af brugerens samlede semantiske træ, der ligger mere yderligt, hvis brugeren vil ændre disse. Nå, nu skrev jeg godt nok, at man skulle have specielle "kategorier" med hhv. gundinstillinger og startindstillinger, men idet jeg skrev det, kom jeg til at tænke på, at det jo nok hellere skal implementeres via specielle relationer, som tager kategorier som subjekt. For på den måde kan brugeren lave forskellige grund- og start-indstillinger for hver kategori (hvor underkategorier skal arve overkategoriens indstillinger), og det var nemlig en del af mine tanker omkring det. Så ja, det må være sådan i stedet. Og hermed bliver disse indstillinger altså også bare en del af det normale semantiske træ (som udspringer af 'kategorier af kategorier'), ligesom at 'relevante prædikater' også bliver det samme. Så når brugeren vil ændre på, hvilke grundindstillinger, hvilke startindstillinger og hvilke relevante prædikater skal vises for en given kategori og alle dens underkategorier, så gør de det altså bare ved at rate dertil egnede relationer (med pågældende ønsker som relationsobjekter i disse) med kategorien som subjekt. Og hvis brugeren så vil ændre i sine indstillinger, eller hvis denne eksemepelvis vil søge i prædikater, som brugeren har brugt før, jamen så kan de bare vælge den "brugergruppe," der kun består af dem selv, når de vil query'e det semantiske træ. I øvrigt kan brugeren nemlig også oprate de kategorier, der indgår i alt det her, sådan at brugeren for denne visning (der kun viser termer, som brugeren selv har ratet) kun ser alle de relevante kategorier, og altså ikke en lang liste som også inkluderer alle mulige kategorier, som brugeren ikke har gemt nogen indstillinger for. (14:57)
%*Nå ja, jeg glemte lige at nævne, hvad jeg kom til at tænke på sidst imens jeg skrev dette, nemlig at der dog stadig gerne må være et "arbejdsbord" i applikationen, som jeg har tænkt mig, men hvor dette arbejdsbord så bare er helt ustruktureret, således at det bare er de nyeste tilføjelser, der bliver vist først, og hvor appkilaktionen måske heller ikke nødvendigvis gemmer ting i denne liste imellem sessioner (ikke som standard i hvert fald), men at hvert arbejdsbord altså bare glemmes, når en session lukkes. (15:13)

(14:57) Jeg har nogle gode noter ude i kommentarerne over denne paragraf (og efter den forrige renderede paragraf). Jeg har dog også i sinde at gentage de ting her i den renderede tekst i denne sektion. Først tilføjer jeg dog lige nogle få flere kommentar-noter lige under denne paragraf (og før den næste renderede paragraf). 

%..(15:00) Hm, jeg vil bare i hvert fald for det første lige nævne, at dataentiteter selvfølgelig bare "slettes" ved at dataen i dem nulles. Nå ja, og så vil jeg også lige nævne, at hvis nu man gerne vil lave en forælderkontrolleret applikation over dette system, jamen så må man næsten lave en slags browser, der kun kan starte på denne sem-net-hjemmeside, og hvor man så \emph{ikke} kan ændre i grundfiltrene (i hvert fald ikke i visse af dem). Og samtidigt bør man så have nogle specifikke whitelistede href-domæner, som er de eneste brugeren må følge, også selv når brugeren har navigeret over til de hjemmesider (så børnene ikke kan navigere videre til forbudte domæner fra de domæner heller). (15:04)

\ldots (15:19) Hm, nu hvor jeg har skrevet så meget ude i kommentarerne (dog i et ikke særligt detaljeret sprog, så jeg skal helt klart skrive det igen her i den renderede tekst på et tidspunkt), så lad mig bare lige nævne, hvad der var min idé nummer to her fra i formiddags, og så kan jeg lige sige nogle flere ting om den også. Jeg forstiller mig, at alle termer, og særligt kategorier, skal have to relationer, som man kan gøre brug af (og som altså nok især er gavnlige for kategorier), nemlig `.relevant prædikat=' (er nemlig gået væk fra at bruge `:') og `.underkategori af relevante prædikater='. På en måde ville disse lister egentligt være mere naturlige at vise som børn af en kategori, som kunne hedde: `kategori over relevante prædikater til kategori $x$,' hvor $x$ så ville være den kategori, man startede på. Men ved brug af de to omtalte relationer kan vi altså springe dette mellemled over, således at brugerne altså ikke behøver at oprette denne afledte kategori for hver af de mere normale kategorier. Nå, men imens jeg skrev dette, kom jeg så til at tænke på, at dette faktisk ikke relaterer sig så direkte til den ting, som jeg gerne ville nævne, alligevel (og som jeg ikke har nævnt ude i kommentarerne heller). For den ting handler i stedet om en tredje relation, som bare kunne hedde `.relevant kategori='. Tanken er så, at hvis man eksempelvis vil se en liste over kendte hunde, jamen så kunne man eksempelvis starte med at navigere hen til en zoologisk kategori, og så finde termen `hund.' Men fordi man ikke er interesseret i de generelle koncept `hund' (og generelle informationer om hunde), så kunne man så måske være heldig her at finde en ``relevant kategori'' af `kendte hunde' (og hvis ikke, kunne man jo så selv tilføje denne). En alternativ rute ville jo være fra starten af at vælge en overkategori af specifikke dyr, men det vil jo være rigtigt godt, hvis der er mange gode veje i det semantiske træ til, hvad man leder efter. Så dermed kunne denne relation altså også blive gavnlig. Bemærk i øvrigt også, at denne relation også kan bruges på kategori-termer, nemlig som en måde at finde relaterede kategorier til en given kategori (f.eks.\ hvis nu man står på kategorien `hunde' i stedet i vores eksempel), som altså ikke hverken er under- eller overkategorier til kategorien, men som er relateret mere ``søskendeagtigt'' eller mere ``horisontalt'' så at sige, hvis vi forstiller os træet som groende nedad (selvom jeg nu tænker det meget som groende mod højre i hovedet.\,.). (15:38)

%..Lad mig også lige hurtigt nævne herude i kommentarerne, at jeg virkeligt synes godt om, at brugerne nu får meget større encitament til at benytte kategorierne (og putte ting i kategorier), fordi dette nu jo hjælper deres egen brugermuligheder direkte. Og dermed så bliver der altså en endnu mere forstærket sammenhæng mellem, hvad jeg ser som systemets ben i form af `semantisk strukturede ressourcer' og så til `tag-rating'-benet, som (i.e. sidstnævnte) jo nok er det ben, hvor meget af brugerinteressen ligesom skal/vil udspringe af i starten. Og det er nemlig rigtig dejligt, hvis brugerne så også hurtigt kommer i gang med at få glæde af det andet ben i idéen, nemlig benet omkring den semantiske træstrktur af ressourcerne. I øvrigt kan jeg nævne, at jeg også ser et trejde "ben" på næsten lige fod med de to første, og det er benet, der handler om "brugerdrevne algoritmer --- bygget på anonym og ikke mindst fuldstændig gennemsigtig (og frivilig) indsamling af data." Jeg ved godt, at der er flere elementer i det ben, men ja, når man samler disse dele af idéen (altså dem jeg nævnte her inden for gåseøjnene), så synes jeg altså, at dette også udgør et vigtigt ben i den samlede idé, som kan ses på ret meget lige fod med de to første. (15:47)


(07.03.23, 11:01) Nå, jeg har faktisk skrevet en del mere om applikationsdesignet, og også om (vigtige) ændringer i det grundlæggende system i den forrige sektion, som egentligt hedder ``mål for betaversionen'' bare. Den version er altså pt.\ hvor de nuværende planer for hjemmesiden og systemet er bedst forklaret. Og nu tænker jeg faktisk at gøre noget lidt fjollet, og det er at afslutte denne sektion med at forklare om ``mål for betaversionen.'' Nærmere bestemt har jeg tænkt mig lige at notere nogle få ting om, hvordan jeg tror, jeg vil starte med at indrette brugerfladen i første omgang.

En vigtig ting er, at jeg nok vil prøve at fokusere meget på den relation til Term-kategorien (se forrige sektion for at forstå, hvad jeg snakker om), hvor brugere skal godkende href-domæner og -syntakser(/grammatikker). Og ja, lad mig lægge vægt på syntakser/grammatikker, for jeg forestiller mig, at dette skal ske specifikt ved at brugerne kan up-rate RegEx-koder for, hvilke URLs er okay at hente ting fra. Måske kan man btw også indføre en modsvarende blacklist-relation til at blackliste URL-syntakser, det vil måske faktisk være en ret god idé. 

Angående JS-biblioteket, så vil jeg bare have ét i JS-listen (se forrige sektion), som gør at brugere kan definere HTML for elementerne, hvor de kan sætte attributter stort set ad libitum, og hvor de kan bruge næsten alle tags på nær script-tagget, men dog hvor alle hrefs kun bliver aktiveret, hvis brugeren har whitelisted (og ikke samtidigt blacklisted) en URL-syntaks som URL'en falder ind under. Der skal så også være særlige attributter, der fortæller applikationen, at den indre HTML i tagget skal AJAX-get'es vi en query til databasen, nemlig når det kommer til ``felt-data'' eller data-entitets-data. Så længe jeg bare kan få det sådan, at brugere kan definere HTML, der selv query'er for elementets felt-data eller data-entitets-data (eller anden ``definerende data''), og hvis applikationen også kan get'e ressource-instanser fra andre whitelistede hjemmesider (og det skal jeg lige finde ud af, hvordan jeg gør --- forhåbentligt bare via AJAX, hvor en URL med et vilkårligt domæne forhåbentligt kan gives), så er jeg tilfreds. 

Jeg forestiller mig så faktisk, at denne HTML i starten kun skal være for, når man har klikket sig ind på et element, enten så element-boksen udvides i en liste, eller så man kommer hen på elementets egen side --- så to HTML-definitioner skal der være plads til. Og når elementerne vises i en liste, så for man simpelthen bare vist deres definerende data, i.e.\ deres navn og deres forælderkategori. Og de samme ting skal gælde for andre relationer såsom `.s'/`.Subcategories'. Så ja, til hver relation skal brugere altså.\,. .\,.\,Hm nej, lad mig sige, at for Term-kategorien kan man up-rate HTML til diverse relationer udover `.e'/`.Elements', og så er det bare de to udgaver af element-visningerne, som man så er fri til at justere for hver kategori (og altså ikke bare for `Terms'). (11:27)

Jeg tænker så, at der bare skal være en liste over visningsindstillinger (hvilket altså hver især bliver et par.\,.).\,. Nej vent, der skal være tre HTML-indstillinger mindst fra start, for man skal også kunne indstille, hvordan elementer vises i lister allerede fra start af. Ok. Nå, men hver ``visningsindstilling'' kommer så til at bestå af mindst tre HTML-definitioner. Og jeg tænker så, at der bare skal være en lodret liste til venstre, hvor alle visningsindstillinger er indeholdt, men hvor den øverste på listen altid er den mest relevante, nemlig den som svarer til den sidste kategori med visningsindstillinger tilknyttes sig i ens $Path$. Og den næste kan så være den sidste kategori med visningsindstillinger, hvis man følger termets ``definerende path,'' hvis disse to muligheder altså giver noget forskelligt. (Og her er den ``definerende path'' altså den, man får ved hele tiden at følge forælderkategorierne tilbage til `Terms'). Men ja, alle de resterende visningsindtillinger kan også bare vises neden under disse én til to mest relevante i samme liste. 

Jeg vil også bare lave en liste med selekterede termer, som jeg har haft tænkt mig før. Så kan brugerne trykke på dem for at komme hen til deres term-sider (igen), eller de kan selektere dem, hvis de skal indsætte noget, eller hvis de skal konstruere en ny mængde at query'e for. Og når vi taler om mængder, så skal der også bare være endnu en liste (og jeg tænker at alle de tre nævnte skal være lodrette og til venstre (måske hvor man kan folde dem ud og ind --- eller måske som undermenuer i en fold-ud-menu)), som indeholder allerede query'ede mængder. Så når en bruger har bedt serveren/databasen om en mængde, så gemmer browseren den altså, og holder faktisk bare på den indtil at en vis hukommelsesbuffer bliver fyldt op, eller selvfølgelig hvis brugeren clearer listen eller logger ud/skifter bruger. (11:40) Hvis brugeren så beder om den samme mængde igen, så kan applikationen i princippet bare kigge i denne mængde (når jeg får det implementeret), og så hente den herfra, hvis den stadig er der. 

Så forestiller jeg mig også en vandret liste i toppen med selekterede brugere/bruger-grupper (som bevaret på tværs af log-in-sessioner). Og en vandret liste lige under denne med sammensatte brugere/brugergrupper, nemlig altså hvor hver sammensat bruger(gruppe) er et aritmetisk-agtigt udtryk, som jeg har snakket om før, som altså minder om en linearkombination af bruger(grupper). Brugeren kan så vælge imellem disse i forbindelse med, at brugeren skal til at bede om en ny mængde (i.e.\ ved at klikke på en knap, der fører til en mængde-query). (11:45)

Hm, og hvad mere skal jeg egentligt sige.\,.\,? Selvfølgelig er tanken så.\,. Nå jo, jeg skal sige, i forbindelse med ``overlayet,'' at.\,. .\,.\,Jo, at det skal være sådan, at når et Term bliver forbundet med en URL, så.\,. Hm, skal jeg så have en slags to-vejs-relation til dette?\,. lad mig lige tænke mig om en gang.\,. (11:48) .\,.\,Jo, i applikationslaget må jeg implementere en slags to-vejs-relation til at sige: ``kan findes (informationer m.m.\ om) på denne URL,'' hvor brugere både kan up-rate URL'er for givne termer via én udgave af denne relation, og hvor de også kan up-rate termer til en given URL via den omvendte udgave af relationen, og hvor applikationen så altså sørger for, at ratingen bliver givet begge steder. (11:52) .\,.\,Og det er jo selvfølgelig denne omvendte relation, som overlayet skal query'e for, når brugeren er på en hjemmeside, hor URL'en har et ``hit'' (som også er up-ratet over en vis tærskel) i den semantiske database. (11:53)

.\,.\,(11:56) Hm, jeg tror faktisk, at jeg vil undlade at implementere $Path$en til at starte med, og så bare lade visningsindstillings-listen ``huske,'' hvor brugeren kom fra.\,. 

(08.03.23, 8:52) Jeg er kommet i tanke om, at jeg for en del dobbeltkonfekt, hvis jeg ligger op til at gøre brug af `Categories'-kategorien, så jeg skal på en eller anden måde have gjort, så at det ikke bliver oplagt at bruge denne. Dette kunne jo mske løses ved at indføre en skarpere typeforskel mellem kategorier og andre termer, men lad mig lige tænke lidt mere over det nu her.\,. 
\ldots Hm, nu kan jeg faktisk godt se en brug for `Categories'-kategorien, så lad mig bare sørge for, at brugerne starter på.\,. Tja, det må jeg lige finde ud af, men jeg kan sikkert finde en måde at gøre, så det bliver meget mere oplagt at tænke i at ordne kategorier i et underkategori-af-underkategori-træ, udspringende fra `Terms,' frem for at lade det udspringe fra `Categories'.\,. (9:07) .\,.\,Ah, måske jeg bare kan gøre det ved at sætte en lille advarsel ved `Subcategories'-relationen for `Categories,' der siger: ``Overvej først, om.\,. hov, eller rettere ved tilføjelsen til denne relation, der lige forklarer, forskellen mellem at tilføje underkategorier til `Terms' og til `Categories'.\,. (nemlig at `Categories' kan bruges som en ekstra over-inddeling, hvis man er interesseret i kategorier, man ikke normalt er interesseret i at se først på `Terms'-underkategorilisten).\,. (9:15) .\,.\,(Hm, der \emph{er} forresten en typeforskel på kategorier og std-termer; det er forkert at sige andet (ville bare lige nævne/præcisere det.\,.)

\ldots Ah, man skal jo bare gerne starte på `Standard Terms'-kategorien i applikationen, og så skal man altså derfra klikke sig op til `Terms,' hvis man vil derhen.

(15:39) Jeg tror faktisk, at jeg vil gå væk fra at have data-kategorier overhovedet. Så hvis man vil finde en tekst eller lignende, så må man søge i std-termerne. Og `Keyword strings' skal så implementeres som en pseudo-kategori, hvor man \emph{kun} kan søge efter nøgleordene, og hvor der altså ikke er hverken en `Subcategories'- eller en `Elements'-relation.\,! .\,.\,Brugere \emph{kan} så i princippet godt selv oprette alle disse kategorier, hvis de virkeligt vil, men hjemmesiden skal altså ikke selv lægge op til, at det er sådan man skal finde data-termer (men at man altid skal finde dem som en underkategori af.\,.).\,. Ja, det skal jeg faktisk lige finde ud af, for det næste spørgsmål bliver jo så om, hvad man skal gøre med `Categories,' `Relations,' og `Users and bots'.\,. (15:44) .\,.\,Hm, det skal nok være det samme, nemlig at de skal være underkategorier til `Terms' (hvilket data-termerne også er; jeg opretter bare ikke selv disse underkategorier (i hvert fald ikke fra starten af)), men jeg bør næsten lige overveje, om jeg ikke kan finde på en bedre navngivning til ``Standard terms''.\,. .\,.\,Ah, hvad med `Entities'.\,.\,! Det bruger jeg jo ligesom ikke til andet nu, kan man sige.\,.\,! (15:52) .\,.\,Hm tja, måske. Jeg har nemlig forresten ikke lyst til at kalde det for resources, for det er altså bare lidt misvisende, ift.\ hvad det egentligt er.\,. .\,.\,Ah, måske vil jeg slet ikke kalde det noget (andet end `termer'), for måske skal jeg slet ikke bruge `Standard terms'-kategorien!\,.\,. (16:01) .\,.\,Ja, for jeg kan simpelthen også bare undlade at have `Relations' og `Categories' med fra starten også (da jeg jo alligevel ikke tror, de bliver vildt brugbare i starten). .\,.\,Og så har vi vel bare `Terms' som hoved-/``hjemme''-kategorien, og så `Users' og User groups' (og måske en overkategori til sidstnævnte to) til at starte med, eller hvad?\,.\,. (16:04) .\,.\,Ja, sådan må det næsten være.\,.\,:) (16:06)

(16:37) Ah, måske skulle jeg faktisk fjerne obj\_cat\_id fra Relations.\,. .\,.\,Ja, og hvis/når brugerne så på et tidspunkt vil få gavn af den, så kan de bare implementere den selv, så at sige, nemlig via en relation i stedet (og ikke en tabelkolonne).\,. (16:40)

(17:42) Ah, jeg tror bare, det skal være `Categories,' der skal være den yderste kategori (hvor man så starter med en mængde af dens elementer), og som så ikke indeholder sig selv. Og så skal jeg nok også putte obj\_cat\_id tilbage i Relations.\,. 

(18:22) Okay, obj\_cat\_id skal forblive ude. Og den store pointe bliver så, at visningspræ-ferencerne i høj grad skal hentes direkte fra træet af ``definerende kategorier,'' hvilket også vil sige, at brugerne ikke behøver at up-rate nye termer for de givne kategorier, som er definerende for disse termer. Og dermed kan vi altså sagtens have alle de her forskellige fundamentale kategorier, der deler termerne op efter deres \emph{type} (hvor ``typen'' altså er den, der afgør, hvilken databasetabel termen hører til), uden at de behøver at blive brugt i praksis i form af at brugere up-rater elementer og underkategorier til dem. De kan primært altså bare (i starten om ikke andet) bruges til, når ny termer skal \emph{defineres}, hvilket så netop også bliver meget betydende ift., når visningspræferencerne skal indstilles. Super.\,. (18:28)

%.\,.\,(18:34) Ah vent, eller måske skal `Categories' bare være en fundamental kategori, der ikke tilknyttes noget term.\,. 
%
%.\,.\,Hm, jeg tror faktisk næsten, jeg bliver nødt til at adskille typerne ad for alvor (nemlig sådan at `Terms' bliver den forhenværende `Standard terms').\,. Hm.\,. (18:38) 
%...Tja, jeg tror det går fint bare med at have de her fundamentale kategorier, som i starten kun bruges til at indstille visningspræferencer.. (18:49) ..Ah, men jeg behøver da egentligt slet ikke 'Categories' så.. ..Nå jo, til 'Subcategories'.. ..Hm ja, men kan det så ikke netop bare være en u-instantieret kategori, som jeg lagde op til der kl. 18:34 for tyve minutter siden..? ..Tja, men hvorfor ikke bare instantiere den. Ok, fint. Så jeg opretter altså 'Categories,' men det er ikke sikkert, at den vil have nogen praktisk funktion overhovedet (for det kan være at det så at sige bliver 'Terms,' der "sørger" for det hele..). (18:55)




(13.03.23, 16:43) Ah, jeg tror, at man i stedet for at up-rate HTML-skabeloner for individuelle relationer, så skal man bare up-rate én skabelon (i.e.\ den bedst ratede) for en given kategori, og så kan denne skabelon kalde sig selv rekursivt, så at sige, og kalde andre skabeloner, som så bestemmer, hvilken HTML skal komme frem, når man trykker på diverse knapper i skabelonen --- hvilke nu altså ikke nødvendigvis er bundet op på en relation hver især! I stedet beskriver skabelonen bare fuldstændig, hvilken HTML skal komme frem, når man trykker på en knap, og \emph{denne} (barne-)skabelon kan så \emph{eksempelvis} vælge at sende en AJAX-query til en relation (men jo også mske eventuelt til flere relationer). Nå, og den store pointe, jeg så lige har fundet på, er at man til en sådan skabelon (for en kategori (eller rettere for elementerne i denne)) også skal kunne up-rate kategori--skabelons-par, således at hvis den pågældende skabelon er i $Path$en, og man i denne path query'er termer, hvor visnings-skabelonen ikke er defineret som en del af den samlede skabelon, så kan applikationen i første omgang så gå over og søge i disse kategori--skabelons-par for at finde den nærmeste overkategori i termens ``definerende overkategorier,'' og hvis der findes et match her, så for termen så den skabelon fra det kategori--skabelons-par med den nærmeste overkategori. Og hvis ikke der er noget match her, så kan applikationen i sidste ende bare følge hele listen af ``definerende overkategorier,'' indtil den støder på en overkategori, som har en skabelon defineret for sig (hvilket muligvis kunne være `Terms'-kategorien som rosinen i pølseenden). Lad mig prøve at gentage det.\,. Så for en given kategori kan man i første omgang up-rate en skabelon, som definere en HTML-visning, der bl.a.\ kan indeholde knapper, som så kan kalde andre skabeloner, inklusiv skabelonen selv rekursivt. Men denne skabelon kan nu altså også bare query'e mængder og termer, uden at skabelonen definere, hvordan disse termer vises, i en listevisning eller en alene-visning (m.m.). Så skal applikationen så selv finde frem til, udenom den givne skabelon, hvordan disse termer skal vises. Og her kan den så altså i første omgang kigge i eventuelt up-ratede kategori--skabelons-par til samme kategori for at finde den nærmeste overkategori her, og så bruge den korresponderende skabelon i dette par, eller den kan i sidste ende (hvis dette ikke giver noget) simpelthen bare følge rækken af overkategorier (som slutter på `Terms'), indtil den støder på en kategori, som selv har en skabelon up-ratet for sig (over en vis tærskel skarpt større end 0!), og når den finder en skabelon herved (hvad den altid vil gøre, hvis bare `Terms' har en skabelon up-ratet for sig), så kan denne skabelon altså så vælges til term-visningen. (17:03)


.\,.\,For resten tænker jeg igen at starte med at implementere $Path$en allerede nu her (skal snart i gang med front-end'en). Jeg har dog nogle rettelser til dens grammatik, men det gider jeg ikke lige at skrive om; jeg vil hellere bare prøve at bygge det.\,. (17:04)

.\,.\,Nå ja, og lad mig lige sige, at nu hælder jeg faktisk mest over imod at kalde systemet/siden for openSDB i stedet.\,. (17:05)


(16.03.23, 9:38) Okay, jeg tror, jeg er nærmere nu på, hvordan skabelon-halløjet skal implementeres overordnet set. Jeg forestiller mig nu, at skabelondefinitioner i første omgang bør bestå af en liste af tre lister (hver især af variabel længde, som henholdsvis indeholder javascript programmer, HTML-tekster og CSS-tekster. Javascript programmerne skal så ikke loades af siden, men i stedet kan siden bare læse dem og se på, om de er indeholdt i en liste over JS-programmer, hvis funktionaliteter er inkluderet i det nuværende kørende JS-program i browseren/applikationen. Hvis ikke JS-programmet genkendes, så kan applikationen give en advarsel til brugerne om, at den valgte skabelon altså muligvis ikke får den ønskede rendering, når HTML-teksterne indsættes på siden. Og hvis applikationen genkender JS-programmet, men ikke har disse funktionaliteter med i det nuværende kørende JS-program, så kan applikationen måske eventuelt se på, om man kan gøre noget for at loade de funktionaliteter alligevel, muligvis ved at stoppe andre del-programmer sådan at der ikke bliver en kollision ved at loade de ønskede funktionaliteter. Bemærk dog, at dette dog er noget der \emph{eventuelt} kunne blive \emph{en smule} nyttigt, men jeg regner dog altså ikke med, at det bliver særligt nyttigt i praksis; man kan nok komme rigeligt langt med bare at sørge for, at applikationen bare har en liste af programmer, som den kan køre, hvis der er behov for det, hvor der bare allerede er sørget for i forvejen, at disse programmer ikke kolliderer i deres semantik. Ja, det vil klart være det mest praktiske, bare at sørge for dette.\,. Okay, nå, men både HTML- og CSS-teksterne må derimod godt loades af applikationen direkte fra listen, for her sørger man nemlig bare for, at de loades ved køre i gennem et tjek-og-verifikations-program, der for det første kører htmlspecialchars modsat, men også i samme omgang tjekker, at syntaksen overholder en vis grammatik, hvilket for HTML'ens vedkomne også indebærer, at script-tags er ulovlige i syntaksen. .\,.\,Hm, og hvad i øvrigt med links.\,.\,? .\,.\,Hm, applikationen kunne jo i forvejen have en umiddelbar whitelist over URL-syntakser, som kan bruges.\,. .\,.\,Hm, og så skal brugerne selv kunne tilføje whitelistede hjemmesider via et specialt prædikat (som implementeres som en relation med en kategori som subjekt, ligesom alle andre prædikater, og her skal kategorien selvfølgelig bare være overkategorien, `Terms'), og kan så også tilføje blacklistede hjemmesider til et andet prædikat, hvilket så bl.a.\ også giver mulighed for at overskrive og fjerne hjemmesider i applikationens start-whitelist. .\,.\,Men spørgsmålet er så, om dette virkeligt skal indgå i parsingen af HTML-teksterne, eller om ikke man kan gøre noget smartere.\,.(?)

\ldots Okay, svaret på det sidste spørgsmål her er meget simpelt, for man gør jo også bare links ulovlige i parsingen, men så kan man tilgengæld lave nogle klasser/attribut-specifikationer, som siger til et inkluderet JS-program, at elementet gerne skal udskiftes med et links, \emph{hvis} altså at linket også først kan godkendes fra listen over whitelistede syntakser (og hvis selvfølgelig den ikke matcher nogen blacklistede syntakser heller). Ok, men jeg har dog nogle andre ting, jeg også lige overvejer.\,. 

\ldots Nå, jeg har overvejet lidt omkring, JS-programmet, men er nu ikke kommet frem til noget vigtigt, der er værd at nævne. Okay, men lad mig færdiggøre denne tråd om skabelonerne. Vi er faktisk næsten i mål, for når man så har disse skabelondefinitioner (en liste af tre lister), så mangler man også lige én information mere for at have sig en skabelon, og der er et tal, der fortæller, hvilken HTML-tekst i listen, man starter på. Husk nemlig på, at HTML-teksterne/-delskabelonerne kan kalde sig selv og andre HTML-delskabeloner, og det er altså dem i denne liste. Men det kan være, at forskellige kategorier kan bruge samme skabelondefinition, men hvor man bare starter på forskellige start-HTML-delskabeloner, og herved kommer dette tal altså ind i billedet, for så kan alle disse kategorier referere til den samme skabelondefinition, men bare have hver deres tal. Så en `skabelon' bliver altså hermed en liste af to ting: et tal, og så en skabelondefinition (som altså igen er en liste af tre ting, nemlig JS-/HTML-/CSS-tekst-lister). (11:17)

Nå, det gode er så dog, at jeg bare kan starte med at have ét JS-program --- og én CSS-fil i øvrigt --- og så bare definere alle HTML-delskabelonerne, som jeg vil bruge til prototypen, ud fra dette/disse. Og herved vil jeg så også bare helt undgå at implementere noget som helst omkring de her skabelon-up-ratings (som brugeren kan gøre for hver kategori i princippet) lige i starten (indtil jeg lige kommer i gang og for testet at hele applikation-til/fra-database-kommunikationen er kommet op at køre og fungerer nogenlunde korrekt). Så lige i starten definerer jeg altså bare lige nogle få HTML-delskabeloner.\,.

Nå, men til gengæld er en anden vigtig ting omkring brugernes mulighed for at bruge forskellige brugergrupper automatisk til forskellige valgte kategorier, men det virker nu også som en ret stor opgave alt i alt, så den må jeg også lige gå at summe lidt over, imens jeg laver en grundlæggende applikationsprototype.\,. (11:26)


(18.03.23, 14:01) Jeg vil begynde at kalde dem `elementære termer' i stedet for `standard-termer.'

(19.03.23, 8:39) Jeg tror faktisk, at jeg gør mængder til (en type af) (afledte) termer.\,. .\,.\,Hm, og så udkommenterer jeg nok den `selectSetFromSecKey()'-procedure, som var min originale selectSet(), og omdøber selectSetFromSetID() til `selectSet()'.\,. .\,.\,Hm, og så sender jeg elementnummeret med i selectSetIDFromSecKey().\,. 


%(21.03.23, 18:01) Jeg tror lige jeg vil brainstorme lidt over front-end-delen. Lad mig lige nævne, at det faktisk er en rigtig god ændring, den med at mængder nu også er termer. Nå, front-end-delen.. Jo, for det første skal term.php (i min indtil videre private GitHub-mappe) starte med at parse en path, som bare er en række catID'er separeret med forward slashes. Og efter sidste skråstreg kommer så et vilkårligt termID. ..Nå ja, og stien må også gerne eventuelt starte med en bruger(gruppe). Serveren skal så faktisk ikke tjekke, om stien i sig selv "holder" (nemlig ved at tjekke ratingværdier for ".s" og muligvis ".e"-relationen (sidste i tilfælde af..)).. Hov, måske skal man alligevel lave forskel på sidste slash.. måske ved at klemme et "/e/" imellem.. for at kunne kende forskel. Nå, men forholdet er faaktisk ikke vigtigt, for stien skal egentligt bare bruges til at finde op til tre ting fra starten af: bruger(gruppe), præferencekategori, og det endelige term. Serveren kan så tjekke, at den valgte bruger(gruppe) er en del af brugerens whitelistede brugergrupper. Hvis ikke, så vælger serveren bare brugerens standard query-brugergruppe og printer en lille advarsel til brugeren. Denne bruger(gruppe) bruges så til, sammen med kategori-delen af stien, at finde frem til den rette præferencekategori, nemlig ved at følge stien op til 'Terms' og tage den første kategori, man støder på, som er en af query-brugergruppens erlærede præferencekategorier. Præferencerne (hvilket bl.a. vil sige HTML'en) loades så fra denne præferencebrugergruppe og termet sendes så til brugerens browser sammen med passende yderligere data. ..Nå nej, for inden da skal serveren også lige slå op i præferencekategoriens grundfilter-whitelist og -blacklist. Bemærk i øvrigt, at hvis ingen præference er up-ratet tilstrækkeligt for den givne præferencekategori, så er det fordi query-brugergruppen hermed har valgt, at præferencekategorein bare skal arve den pågældende del-præference af sin forælder (rekursivt, hvis denne forælder heller ikke har "overskrevet" sin egen forælders præference, hvad denne del-præference angår). Nå, hvis whitelist-ratingen eksisterer og er over en hvis grænse, og hvis ikke blacklist-ratingen eksisterer og er over en vis grænse for termet, så kan serveren så sende termet til brugeren, og ellers skal den sende dataen må en måde, så browseren ved at den ikke skal vise termet (\emph{hvis} termen sendes overhovedet). Bemærk at whitelisten og blacklisten nu kan være mængder hver især, da mængder jo som sagt nu er termer. Så brugeren kan altså up-rate to mængder til 'Term'-kategorien, som så kommer til at udgøre brugerens "grundfilter." (Og serveren slår så bare op via selectRating(), som jeg har defineret den nu.)
%Nå, og html'en bstemmes altså også (rekursivt) ud fra præferencekategorien. Og så skal jeg lave et system, sådan at f.eks.\ knapper og forms kan have attributter, som kan referere til termet selv, til den givne præferencekategori, til brugeren selv eller til den nuværende valgte query-brugegruppe. Så er der et JS-program, der læser disse attributter og tildeler knapperne m.m. den rette funktionalitet.
%En ny idé er så også, at knapperne og forms'ne også skal have adgang til at læse fra og skrive til datastrukturer, både en XML struktur, som ikke renderes, og også en struktur i local storage, hvis brugerens browser har sådant. Her skal man måske endda lave en slags script-divs, hvor brugeren kan definere små programmer, som divere knap-/form-attributter så kan referere til, for at fortælle hvilken datastruktur-ændrende handling, som knappen/formen skal have, når man trykker på knappen/på submit-knappen. Så skal jeg skrive et lille bibliotek af instruktioner, som så interpretes fra pågældende script-divs. Alternativt kunne jeg tillade en undermængde af JavaScript inden i script-tags, men jeg tror faktisk, at det her med at lave sit eget lave data-script-sprog som står i (ikke-renderede) div-tags er en bedre løsning her til at starte med. (18:32)
%(19:31) Det kan faktisk også bare være en JS-objekt-datastruktur i stedet for en XML-struktur (altså også for den struktur, det ikke gemmes i local storage (men som ophører hver gang brugeren lukker siden)). 
%(22.23.03, 11:34) Nej, XML er bedre, men det kan jeg vende tilbage til...
%(23.03.23, 10:26) Jeg var vildt træt i går, fordi jeg ikke fik sovet så meget, men i dag har jeg heldigvis sovet længe. Men selvom jeg ikke fik kodet så meget, så fik jeg alligevel tænkt en del, og kom frem til nogle rigtig gode ting. Lad mig prøve at opsummere i den renderede tekst..

(23.03.23, 10:27) Jeg har nogle få noter ude i kommentarerne over denne paragraf, som jeg lige vil prøve at inkorporere i den følgende opsummering også.\,. .\,.\,Nå, lad mig starte med at sige, at jeg i de noter skrev om, at jeg ville lave et lille scriptsprog, som kan transpiles til JS. Det vil jeg ikke længere; nu vil jeg (og er lidt i gang med at) definere en undermængde af JS i stedet som brugerne så kan bruge. Jeg har så lagt op til i går, at det skulle være en undermængde af jQuery-JS, men sådan tror jeg faktisk ikke helt det bliver alligevel. I stedet skal alle biblioteksfunktioner og -metoder importeres fra moduler i starten af scriptsne. Og her kan man så kun importere fra moduler, som er i en vis godkendt mappe (eller undermapper af denne). Og man kan så selv definere nye funktioner i scriptsne. Hm, og jeg kan egentligt godt lade brugerne loade flere scripts, hvilket så gør, at de slev kan definere biblioteker (men ikke moduler). Idéen er så, at preprocessoren automatisk omdøber alle identifiers, således at de får nogle præfikser, der er (meget) nemme at undgå at bruge af webudviklerne til omkringliggende JS-program. Og ikke nok med det, så kommer der faktisk typer indirekte i sproget, fordi preprocessoren sørger for at lade præfikset afhænge af konteksten for den første deklaration. Derfor kommer man så ikke til at kunne genbruge en variabel til en anden type (det knne man godt, men så ville semantikken ikke nødvendigvis være bevaret fuldstændigt før og efter præprocesseringen, og det vil jeg nok gerne have, at den er). Nå, og jeg har faktisk simpelthen tænkt mig bare at definere syntaksen ud fra et RegEx pattern, hvilket vil sige at syntaksen ikke må indeholde rekursion. Men det gør faktisk ikke så meget, så det tror jeg faktisk, jeg vil gøre! Fordi brugerne stadig kan definere rekursive funktioner, så kan de stadig i princippet få præcis det samme flow i programmerne (hvis man ser bort fra ekstra funktionskald). (10:47) %2 sek..
.\,.\,(10:56) Så lige for at gå tilbage til modulerne, så kan præprocessoren jo også bare sætte præfikset på de importerede funktioner (og eventuelt variable) i selve import-statementet. Dermed kan både modulerne og brugerscriptsne altså begge skrevet uden at tage hensyn til, hvad de endelige præfikser bliver. 

Lad mig så tale lidt om sikkerheden i scriptsne. For det første skal man sørge for, at scriptsne kun kan tilgå variable inden i den pågældende term-div/start-div (som serveren har startet med at servere, inklusiv det script, som brugeren har valgt for det). Ved at sørge for, at scriptet er (eneste) barn af denne div, så kan scriptsne bare starte med (eller præprocessoren kan sætte dette på automatisk) en variabeldefinition, der selekterer div-elementet. Og så skal alle funktioner godkendt i modulerne altså sørge for, at de kun tilgår DOM-elementer, der er efterkommere af denne div. Funktioner kan så tilføje og ændre børn til denne div (og ændre dens attributter), og de kan læse børn fra den. Hermed kan man altså også implementere en datastruktur, som funktionerne løbende kan læse og skrive til, som en XML/HTML-struktur indenfor start-diven, som man så bare sørger for ikke bliver renderet. Funktioner må også gerne læse fra og skrive til en hvis mængde variable i local storage, nemlig variable, der alle også automatisk har fået sat et præfiks på sig, så ingen af dem kolliderer med variable, som webdeveloperne bruger. Desuden skal html-specialkarakterer holdes uden for alle strenge, så dermed skal alle HTML-børn altså indsættes og ændres via tilegnede funktioner, ikke via direkte streng-printning. (11:11) .\,.\,Nå ja, og derudover skal man også holde alle links ude, både i form a link-tags, men også i form af expressions, f.eks.\ i CSS-attributter (og andre attributter). Så f.eks.\ ikke nogen `url()'-udtryk. I stedet indsættes links via specielle divs, som det omkringliggende program så kan, gerne efter.\,. Ja, eller rettere: scriptsne skal gerne kunne kalde et.\,. Ah, men så behøver vi ikke nødvendigvis sådan en link-div-repræsentation. Så kan vi bare lave en speciel modul-funktion, som kalder en yderliggende link parser, der så tjekker, om linket passer med en link-syntaks, der allerede er godkendt af brugeren, og som så enten giver grønt lys til at indsætte linket, eller fortæller at funktionen i stedet skal indsætte et dødt dummy-link. Og en sidste ting, jeg har tænkt på, er så, at funktionerne ikke direkte må tilgå input-handleren (men gerne query-handleren). I stedet må funktionerne sende input-requests'ne til en yderliggende queue i DOM'en. Hvorvidt disse requests så skal vente her på bruger-godkendelse, eller om de automatisk skal sendes videre til serveren med det samme, det er så op til brugeren. Men fordi brugerne jo også kan læse hinandens scripts, så vil folk nok ret hurtigt få tillid til, at de bare automatisk kan sendes videre, når det er --- i det mindste altså, når afsender scriptet er en af det scripts, som brugeren har meget tillid til. Hm ja, og derfor kunne man altså gøre sådan, at input-funktionerne, der sender til queue'en også kan vedhæfte et script-ID, der fortæller hvilket script er afsender på inputsne. (Bemærk desuden at rating inputs kan fortrydes af brugeren, fordi denne har adgang til sine seneste inputs (RecentInputs) og kan overskrive dem, hvis brugeren har lyst.) .\,.\,Jeg har forresten overvejet at tilføje en undo- eller en slet-recent-input-sql-procedure, men det er ikke sikkert, at det bliver nødvendigt. (11:28)

Nå, lad mig så gentage fra kommentarerne, at serveren så bare i starten får en bruger-(gruppe) (optional), en kategori-sti (optional), og et term-ID til sidst (required). Serveren slår så for det første op, om bruger(grupp)en er godkendt af den pågældende bruger (altså den der kommer med forespørgslen), og hvis ikke, så vælger serveren bare brugerens standard bruger(gruppe) i stedet (og tillægger også noget data med den returnerede HTML i sidste ende, der fortæller dette). Hvis brugeren ikke er logget ind, at matcher serveren bare i stedet med en standard liste af mulige brugergrupper, og hvis ikke der er et match her, så vælger den bare den mest standard brugergruppe. Kategori-stien bruges så efterfølgende til at bestemme visningspræferencerne.\,. Hm, jeg kan mærke, at jeg faktisk ikke har lyst til at gentage det, jeg skrev i går, så se ude i kommentarerne, hvis man er interesseret i at læse om, hvordan jeg mener, at dette bør gøres. Samtidigt (før eller efter) skal serveren også aflæse den valgte bruger(gruppe)s grundfilter præferencer. Her tænker jeg nu, at man i første omgang læser et prædikat, der afgør, om bruger(grupp)en har videredelegeret denne opgave til en anden brugergruppe (og altså bare kopiere indstillerne fra en anden bruger(gruppe)). Hvis ikke nogen brugergruppe overstiger en vis tærskel, så vælges bruger(grupp)en selv igen til dette formål. Hvad en en nu B(G) vælges til dette formål eller ej, så slår serveren nu op i to prædikater (som i øvrigt implementeres via relationer med `Terms' som subjekt (eller som objekt, om man vil (alt efter hvad vej man fortolker relationen))), nemlig i et prædikat for en whitelist og en blacklist. Begge disse bør defineres i form af et mængde-ID, hvor serveren så simpelthen slår op i disse mængder (og bemærk at mængder godt kan være konstante, hvis pågældende brugergruppe er ikke-dynamisk (inklusiv hvis brugergruppen er ``ended'')). Term-ID'et tjekkes så for, om det kan godkendes ud fra den valgte whitelist (som i øvrigt godt bare kan være en ``is a meaningful and concise term''-mængde, hvis man vil bruge en rigtig inklusiv whitelist) og blacklist. Hvis ikke den kan det, så sørger serveren igen for at fortælle dette i den endelige HTML, men hvis den kan, så sender serveren jo bare term-diven samt alt andet relevant data. Serveren sender så det valgte script *(nej, \emph{de} valgte scrits, for jeg kom jo lige frem til, at der godt må være flere) (som altså så vælges ud fra kategori-stien, hvad jeg ikke lige har beskrevet her) som det eneste *(nej, for der kan godt være flere scripts) barn i denne div (og indsætter den relevante data i form af attributter til denne div). .\,.\,Nå ja, der kan godt være flere scripts, som jeg lige har bemærket i nogle indsatte parenteser. Når brugeren så får HTML siden, så beskriver bruger-scriptsne så, hvordan denne term-div/``start-div'' skal renderes, nemlig ved i første omgang at beskrive, hvilke nogle funktioner skal kaldes, når dokumentet er klart. Og når det omkringliggende dokument er klart, så kan funktionerne så gå i gang, og opbygge hele det interface, som brugerne kommer til at bruge til den centrale del af applikationsinterfacet, nemlig til at navigere rundt i termer på siden og eventuelt uploade input selv. (11:55) %2 sek..

\ldots (12:17) Så med disse nye idéer, så tror jeg altså virkeligt, at jeg er tæt på at opnå, det jeg gerne vil, kan man sige. For jeg tror ikke, det kommer til at tage lang tid at lave et god undermængde af JS.\,. Hov, lad mig forresten lige slå fast, at siden så primært kommer til at blive en AJAX-side, nemlig hvor det er browseren selv, der sørger for eventuelt at skifte URL'en og eventuelt gemme dem som ``bogmærker'' (hvis jeg har forstået bookmark.create() ret.\,.). Så i brugerscript-biblioteksmodulerne bør jeg altså også på et tidspunkt tilføje nogle funktioner, der kan skifte URL-headeren, og som kan kalde ``bookmark.create(),'' eller hvad den nu hedder. Nå, men tilbage til at sige: Så jeg tror ikke, der kommer til at gå vildt lang tid med det. Og jeg har allerede database-queries nogenlunde på plads, og det kommer sikkert ikke til at tage vildt lang tid at få hul igennem til inserts/inputs. Så skal jeg jo lige skrive den server-procedure, jeg lige har beskrevet, og jeg skal lave den der queue og sådan. Og ikke mindst skal jeg lave/oprette et login-system osv., men altså alt i alt, går der nok ikke så lang tid, før jeg kan nå til et punkt, hvor jeg kommer til at programmere interfacet, ikke som webdeveloper, men i princippet som bruger! Selvfølgelig skal jeg jo løbende føje flere og flere biblioteksfunktioner til, og der kommer til at ligge en masse arbejde i at fejlteste, for ikke at tale om at oprette login-systemet og sådan. Så der er stadig en masse arbejde forrude, men jeg tror ikke, der går lang tid, før at jeg får et system, som jeg ønsker mig, hvor man tydeligt kan se, hvordan applikationen er ``user-driven.'' :) (12:28)

.\,.\,Selvfølgelig kommer der så også til at lægge en del arbejde i at få interfacet derhen, hvor jeg gerne vil have det, men det bliver altså rart at nå til et punkt, hvor jeg i princippet gør dette som ``bruger'' mere end (eller ligeså meget som, om man vil) som web-udvikler. .\,.\,Jeg kom også til at tænke på, at jeg lige skulle nævne, at ``overlayet,'' som jeg kalder det (som i øvrigt skal implementeres via iframes), så bare kan implementeres ved at vælge en speciel brugergruppe til at bestemme sine visningspræferencer, som specialiserer sig i netop ``overlayet.'' Man kan så i princippet også query'e en sådan special-brugergruppe på hjemmesiden selv, hvorved man så vil få overlayet serveret som HTML på selve hjemmesiden (ikke i en iframe). Men her er det så bare beregnet, at det er overlay-browserudvidelsen, der skal GETte denne HTML og putte det ind i en iframe på andre hjemmesider, som brugeren besøger (og som browserudvidelses kan genkende og/eller kan se, har relevant data omkring sig i den semantiske database). (12:36)

(24.03.23, 10:14) Hm, jeg tror faktisk, jeg vil kræve ungarnsk notation.\,. (Så brugerne sætter selv disse præfikser på, og så sætter preprocessoren bare et yderligere præfiks på også.)

(25.03.23, 12:01) Man kunne også kalde det en ``user-\emph{programmable} application'' (UPA) i stedet.

(18:31) Det jeg før har tænkt på med sammensatte tekster og delta-tekster, det må brugerne hellere bare selv implementere i applikationslaget, hvis der kan blive behov for det. 

Nå, noget andet er, at jeg jo er begyndt at programmere JS-undermængde-parseren i front-end-delen (altså i JS). Og så tænkte jeg faktisk, at brugerne så selv skulle verificere programmer, og så bare holde en liste i local storage over script-tekst-ID'er, som de allerede har verificeret (og som de kan huske, at de har verificeret), så de ikke behøver at køre parseren på ny, inden de loader scriptet i starten, når de lukker siden op. Hm, og måske kunne dette også give mening, egentligt, så lad mig lige tænke noget mere over det.\,. (18:36) 
.\,.\,Hm, måske giver det egentligt fin mening at gøre det sådan, og så skal applikationen bare kunne kalde en JS request handler, der så serverer.\,. Hm, serverer teksterne råt, men hvordan får man så lige præfikserne.\,. Ah, præfikserne ville så skulle sættes på lige inden upload til databasen.\,. (18:41) .\,.\,Ja, så applikationslaget sørger faktisk bare for lige at oversætte input-programmet til et verificerbart format (uden mærkelige whitespaces og med de rette præfikser), inden de uploades (bare som en tekst på normal vis) til databasen. Og så kører applikationslaget også selv verifikationsprogrammet, når scriptet hentes ned igen (i det oversatte format som det blev uploadet i). (18:45)


(26.03.23, 10:43) Jeg kom frem til i går i sengen, at det må skulle være sådan, at serveren kun.\,. Ah vent, men så skal serveren også selv parse programmerne.\,. Okay, så det er der muligvis ikke nogen vej udenom.\,. tja, medmindre at man kan bruge eval() til at køre scriptsne i applikationslaget efter at de er konverteret til ``usikkert'' format.\,. Hm.\,. (10:45) .\,.\,Ah, nå nej, nu kom jeg i tanke om, hvad jeg bør gøre. Jeg bør implementere parsingen begge steder, men i serverlaget bør parsingen dg kun lige række til at sikre, at scriptsne udelukkende består af funktionsdefinitioner.\,. Ja, og måske også klasse-definitioner sidenhen, men vent nu lidt.\,. (10:49) .\,.\,Ja, jo. Serverlaget skal også parse, men kun for at modulerne kun består at definitions-statements uden nogen sideeffekter, når scriptet køres, så længe funktionerne altså aldrig kaldes. Hermed bliver URL-linksne ikke farlige i sig selv (altså hvis man taster dem ind i browser-søgefeltet), så længe man bare ikke loader dem som script-sources på min hjemmeside uden at parse dem i applikationslaget først. Og så tænker jeg nemlig nu, at alle brugerscript skal være moduler --- altså ikke nogen main-procedure-scripts. Eller rettere, der skal være main-procedure-scripts, men disse skal --- ligesom i C --- i stedet definere en funktion kaldet `main' (plus præfikser!). Og når applikationen så har parset alle de relevante moduler til et samlet program, så kan applikationen så loade et script, der definerer en main-funktion, hvorefter applikationen så selv kan kalde denne main-funktion, og så skal inputtet til denne simpelthen bare være det HTML-element, som main-programmet skal holde sig inden for. (11:02) .\,.\,Yes, det var sådan, at jeg skulle gøre det (og skulle lige indse, at det så kræver, jeg også parser til en vis grad på serversiden).\,:) 

(13:17) Hm, hvis void- og ec-funktioner skal kunne tage arrays, object, og callb.\,. hm, vent.\,. Ja, nej, helt generelt skal jeg bare sørge for, at funktioner aldrig kan overskrives, og dermed heller ikke defineres --- om ikke andet så bare for en sikkerheds skyld --- inde i funktioner, hvad end disse er rene eller urene. Og i rene funktioner skal jeg så også bare sørge for, at arrays og objects ikke må ændres heller.

(13:59) Ah, jeg kan da egentligt ligeså godt bare droppe restriktionen om, at ikke-rene funktioner skal være ``void''- eller ``ec''- (exit code) typer. Lad mig da i stedet bare sige, at funktionerne kan få et `pure'-præfiks med sig, hvis og kun hvis de er rene.

(27.03.23, 11:02) Nå, jeg har tænkt en del over JS-undermængden, og regner med at skulle lave nogle ting om. Jeg tror måske, jeg lige skal tænke en anelse mere, så lad mig lige gøre det.\,. 

(13:23) Okay, det har taget lidt tid, men nu tror jeg endelig, jeg har styr på, hvad jeg gør.\,. .\,.\,Lad mig starte med at nævne, at jeg var gået over til at overveje at bruge en syntaks meget ligesom TypeScript, i stedet for at bruge type-præfikser. Nå, men nu vil jeg i stedet bare sørge for, at alle ikke-brugerdefinerede funktioner, som kan kaldes i bruger-scriptsne, bare selv skal tjekke, om inputtet er korrekt. Der kan så være nogle halv-globale variable, som sættes i starten af det script, der kalder den givne brugerdefinerede main-funktion. En af disse variable kan så være det div-element, som funktionerne ikke må bevæge sig uden for. I starten af hver biblioteks-html-element-funktion kan denne så tjekke, at input er efterkommer(e) af dette div-element. Nå, men det gode er så, at hvis man i fremtiden vil gå over til også at bruge en statisk typetjekker til at eliminere nogle af disse runtime-tjek, så kan man godt implementere dette så det er bagudkompatibelt med de ikke-typede brugerscripts!\,. For man kan nemlig i så fald lave en delvis typetjekker, der bare sørger for at typetjekke de variable/udtryk, der gives som input til de (fremtidige) biblioteksfunktioner --- og brugerdefinerede funktioner --- som så kræver inputtet tjekket på forhånd (statisk). Typetjekkeren skal så kunne type variable og funktioner automatisk, hvis det er muligt (ligesom alle moderne funktionelle sprog). Dermed kan brugerdefinerede funktioner, der er skrevet før typetjekkeren kom til, så også muligvis types, hvis det er. Og finten er så, at typeinformationen gemmes i en separat fil, som også uploades til databasen. På den måde kan typetjekkeren altid uploade typeinformationer om tidligere bibliotek (bruger- eller developer-defineret) på et vilkårligt tidspunkt. Og hvis et script, der skal bruge statisk typebestemmelser, importerer funktioner fra et JS-modul, så kan man så slå op i modulets separate type-informationsfil, for at få denne --- og hvis ikke denne information er kompileret, så kan man bede serveren om at kompilere den. Ok, lad mig så zoome lidt ud og forklare, at script-bibliotekerne består af to--tre filer uploadet i databasen. For det første er der den obligatoriske kildefil, som brugeren har udarbejdet, og som kan læses af både mennesker og af en transpiler. Formattet af kildefilen behøver ikke at være begrænset til JS, men i starten vil det bare være en undermængde af JS. Men på et tidspunkt i fremtiden kan dette udvides til et sprog a la TypeScript (TS), hvor man altså nu også har lov til at give typeinformationer med til bl.a.\ funktionsdeklarationer, hvis nu ikke typen kan bestemmes statisk (fordi den er tvetydig). Og fordi alle bruger-udarbejdede scripts alligevel skal transpiles uanset hvad, så gør det altså ikke noget, at man på et tidspunkt udvider det, så det ikke længere nødvendigvis er en undermængde af JS (f.eks.\ altså ved at tilføje TS-agtig syntaks). Transpileren oversætter så programmet til et nyt program, som \emph{er} en undermængde af JS, og som derfor kan loades direkte i browseren. Det er så også denne transpiler der tilføjer de præfikser til alle identifiers, således at programmet ikke kan tilgå andre funktioner og variable, end dem der er lovligt for det at tilgå. Serveren får så en bot til at up-rate det transpilede program som et ``kompileret/transpileret program'' til kildekode-filen. Brugere kan så selv verificere kildekoden i applikationslaget, og hvis verifikationen lykkes, kan de så loade scriptet (inklusiv de scripts det importerer (som selvfølgelig derfor også skal være med i verifikationen forinden)) og køre et nyt script, der kalder main-funktionen. Hm, og i øvrigt kan transpileren sørge for at præfikserne også afhænger af kildekode-ID'et, således at man kan loade flere uafhængige brugerscripts på én gang uden at skulle bekymre sig om kollisioner. Nå, og den tredje, eventuelle fil er til den omtalte typeinformationsfil, der altså ikke nødvendigvis behøves at findes for at scriptet kan verificeres. Det behøves den kun, netop hvis scriptet, eller et af de scripts der importerer fra det, skal bruge denne typeinformation for at kunne verificere visse typer statisk, nemlig hvis visse funktionsinputs typer afhænger af denne information, og hvis pågældende funktioner kræver at pågældende input får deres typer verificeret statisk. Og det er det. Hermed kan jeg altså glemme alt om.\,. Nå ja, vent, for jeg skal også lige hurtigt sige, at typeinformationen også up-rates på samme måde meget ligesom det kompilerede/transpilerede program, nemlig ud fra en passende relation og med kildekode-filen som subjekt. Cool, og hermed kan jeg altså glemme om typetjek for nu, men hvor jeg ved, at hvis nu man på et tidspunkt gerne vil indføre det for at eliminere nogle runtime-tjek, som i stedet kan tjekkes statisk, så kan man gøre dette på en bagudkompatibel måde.\,:) (14:13)

(16:14) Nu fik jeg lagt op til, at typetjekker-udvidelsen bliver implementeret i serverlaget, men man kan også implementere noget tilsvarende i applikationslaget med en browserudvidelse, hvis det endeligt er.\,.

(29.03.23, 20:40) Jeg går tilbage til at bruge type-præfikser (eller suffikser!), men kun for arrays og objekter, samt også funktioner der returnerer hhv.\ arrays eller objekter.

(30.03.23, 11:15) Ah, og det gode er, at bare fordi jeg kræver typerestriktioner på navnene nu, så betyder det ikke, at disse restriktioner ikke kan tages væk og erstattes med automatisk typebestemmelse i fremtiden.

(01.04.23, 11:35) I starten tror jeg faktisk bare jeg nøjes med at have det sådan, at scripts ikke bliver transpilet overhovedet, sådan at både brugere og serveren arbejder i det script-format hele tiden, der indeholder alle præfikserne osv. (og også alle kommentarerne og ekstra whitespace). Og så kan jeg på et tidspunkt bare lave et system, så de kan transpiles, og så brugerne kan arbejde uden alle de præfikser, når der begynder at blive behov for det. 

(01.04.23, 18:39) Jeg gider ikke debugge to steder på én gang (det ville være lidt dumt), så jeg tror jeg venter med at oversætte parseren til PHP også. Så jeg vil altså bare for nu lade serveren acceptere alle scripts (som jeg så sørger for at lade applikationen parse inden), og så kan jeg debugge parseren der i første omgang (imens jeg også tilføjer lovlige funktioner til sprog-undermængden), inden jeg på et tidspunkt også implementerer den samme parser på serveren. 

(04.04.23, 16:16) Hold da op. Jeg skrev lige en commit-kommentar, at jeg har svært ved at fokusere nu, men nu kom jeg lige til at tænke på noget, der måske lige har givet mig lidt second wind (måske, nu ser vi.\,.). Jeg kom til at tænke på, at jeg jo egentligt, nu hvor jeg ikke tillader nogen metoder overhovedet (inkl.\ (implicitte) getter- og setter-metoder).\,. hm, vent, eller ville jeg egentligt ikke hele tiden kunne gøre dette?\,.\,. Okay, jeg er stadig lidt sumpet i hjernen.\,. Lad mig se en gang.\,. .\,.\,Ah, jo, uanset hvad, så \emph{har} jeg førhen (for nyligt, self.) været bekymret, i hvert fald til tider, om at give brugere lov til at håndtere ubegrænsede objekter på et niveau, hvor disse får lov til at holdes af variablene i sproget. Ja, og jeg har nemlig også på et tidspunkt tænkt, at man \emph{måske} kunne lade brugerne lade variable holde objekter, som de selv har bygget på en begrænset måde, men ikke f.eks.\ HTML-element-objekter osv. Ja, men nu er jeg jo kommet frem til, at brugere faktisk godt må kunne bygge arbitrære objekter, for selv hvis det lykkes dem at bygge noget a la Function-objekter eller andre umiddelbart farlige ting, jamen så kan de alligevel ikke tilgå nogen af deres metoder (når nu jeg endda slet ikke har metoder med i sproget (hvilket muligvis på et tidspunkt kan erstattes af, at man bare begrænser metode-navnene, ligesom jeg gør med for funktionsnavne (og variabelnavne))). Og så har jeg jo tænkt, ``det var da dejligt,'' men åbenbart glemt, at dette så også medfører, at brugerne jo så kan blive frie til at gemme alle de objekter, de vil i variablene! (Lige for at gentage dette: Jeg tænkte altså først, at selv \emph{hvis} det skulle lykkes dem at lave ``farlige'' objekter a la Function-objekter, så ville det ingen gang gøre noget alligevel, men nu tænker jeg jo så: Jamen, hvis det ikke gør noget, så lad dem da bare håndtere alverdens objekter frit, \emph{også} selv Function-objekter osv.) Og dette slog mig altså lige nu her, og dermed slog det mig så også, at dette jo betyder, at jeg så nærmest bare kan kopiere jQuery til en vis grad her til at starte med --- altså hvor jeg så bare wrapper alle de ønskede jQuery-funktioner i ``upaFun\_''-wrappere (så bare med en mere funktionel/procedural virkemåde i stedet for OOP)! :) (16:35) .\,.\,Og hermed blev udsigten til at få lavet et omfattende nok UPA-bibliotek pludselig meget kortere!\,!\,:\texttt{D}\textasciicircum\textasciicircum\ 

.\,.\,Selvfølgelig skal jeg lige sikre mig, at de returnerede objekter (f.eks.\ jQuery-objekter) ikke kan indeholde noget følsomt/privat data fra brugeren, men det er vist også det.\,.\,:) 


(05.04.23, 14:39) Nå, jeg har indset, at det alligevel ikke er så lidt, for det er ikke sådan bare ligetil at tjekke, om et objekt er et legalt jQuery-objekt. Så jeg er gået i gang med så småt at implementere en API, hvor brugerne giver selectors som input til funktionerne, og altså ikke jQuery-objekter. Så ja, det bliver ikke helt ligeså nemt at lave den API, som jeg troede her i går, sidst jeg skrev, men det skal nu stadig nok være en rimelig overkommelig opgave.\,. (14:42)

(17:20) Jeg fjerner ikke-array-objekter fra JS-undermængden igen, sådan at ingen expressions må returnere ikke-primitive eller ikke-array-af-primitive typer. Ingen upaFun-funktioner må altså returnere andet end primitive typer, eller (evt.\ multidimensionale) arrays med primitive typer som deres blade!


(12.04.23, 09:02) %Haha, skrev selvfølgelig 12.04.93 først.xD
I går aftes kom jeg frem til noget omkring, at jeg nok, fordi man jo kan definere Content-Type headeren (og fortælle browseren, at den ikke skal tolke outputtet som HTML), kan undlade en masse tjek i serverlaget. Og måske kan jeg endda undlade at verificre JS-moduler helt, for det er jo kun web developers, der er i ``fare'' for at benytte et farligt script fra hjemmesidens domæne. Og hvis jeg bare omdøber UPA\_modules, så navnet indeholder ``DO\_NOT\_TRUST\_WITHOUT\_VERIFICATION,'' så burde der jo ikke være nogen ko på isen.\,. vent.\,. Hm, måske i forhold til.\,. Hm, i forhold til CSP, eller hvad det nu hedder, men der må man jo bare sige, at hvis developers godkender.\,. Tja, eller endnu bedre: Man må bare sørge for at signalere tydeligt til andre developers, at CSP-whitelisting af hjemmesidens domæne selvfølgelig bør ekskludere UPA\_modules\_DO\_NOT\_TRUST\ldots\ (9:12)

(18:30) Ah, den løsning går desværre nok ikke alligevel. Jeg skal nok sørge for at tjekke alt input i serverlaget, også inklusiv JS-moduler.\,. \ldots (19:12) Hm, eller måske.\,. Jeg skal i hvert fald lige tænke noget mere over det.\,. 

(13.04.23, 9:49) Okay, jeg kommer ikke til at lave parsing for diverse filtyper. I stedet vil jeg bare sørge for, at det altid tjekkes, at den hentede binære ressource altid tjekkes for at være højt nok ratet i en vis mængde. Og det samme skal også gælde for moduler, just in case.\,. .\,.\,Nå ja, og for URL-links, selvfølgelig. Nå ja, og her tænker jeg så, at man primært skal bruge en whitelist-mængde, og måske kan man så også bruge et eventuelt blacklist RegEx-pattern som et ekstra filter til en given mængde, for mere personlige præference-indstillinger. Det er klart at brugere, der ikke er logget ind, jo bare skal bruge nogle af de af hjemmesiden whitelistede brugergrupper, så for alle queries hvor brugeren ikke er logget ind, skal serveren altså tjekke ratingen ift.\ bestemte faste mængder (ikke ``faste'' som i `konstante;' mængderne er dynamiske). Og for brugere der er logget ind, der kan disse altså selv ændre disse indstillinger, og så tror jeg lidt bare, jeg må sige, at her er det brugernes eget ansvar ikke at godkende brugergrupper, som har fare for at loade farlige links og/eller loade korrumperede og/eller farlige (f.eks.\ potentielt farlige at downloade, hvis nu man kommer til at åbne dem som programmer). Der er ingen grund til sige, at det ikke må være brugernes eget ansvar (og man sørger jo lige for at give en advarsel, inden de får lov at godkende nye brugergrupper), for de har jo allerede dette ansvar, når de bruger søgefeltet i en browser, eller når de bruger søgemaskiner. Jeg bør dog selvfølgelig stadigvæk sørge for, at alle moduler verificeres som min undermængde her af JS. Men.\,. Tja, spørgsmålet er så lige, om jeg også skal lade serveren.\,. Hm.\,. Alternativt kunne jeg også give ansvaret helt til serveren, hvilket ville være mest effektivt (fordi man så kan gemme verificerede moduler i en bestemt mængde), men kommer dette ikke muligvis til at begrænse mulighederne for at udvide denne undermængde.\,.\,? (10:09) .\,.\,Ah, nej, for det kræver bare, at man på et tidspunkt åbner op for, at brugere også kan bruge andre mængder til at whiteliste JS-scripts!\,.\,. .\,.\,Hm, jamen skal det så bare være i serverlaget, at al JS-parsingen foregår?\,:) (10:12) .\,.\,Ja, simpelthen!\,:)\textasciicircum\textasciicircum\ 

(11:45) Hm, men nu kan jeg vel så ikke rigtigt cache ressource-godkendelser i local storage i browseren, hvis det bliver serverens ansvar?\,.\,. (Ikke at det nødvendigvis gør særligt meget, men det er jo godt lige at tænke over.\,.) .\,.\,Hov, det er jo faktisk egentligt bedre sådan her, ift.\ effektivitet altså, for serveren kan jo godt chache ressourcer, der bruges meget, og browseren skal jo alligevel loade ressourcerne via HTTP requests, så ja, man ville ingen gang spare noget ved lookups i local storage.\,:) (11:49)

(12:47) Jeg skal lige have overvejet nogle ting omkring full-text searches, og også omkring multiple title/objNoun queries, men det vil jeg bare gøre i løbet af i dag (og måske i morgen også).\,. 

%(16:03) Ret sjovt (og lidt irriterende) at jeg bare har gået og brugt en ikke-eksisterende String.prototype.test().!xD Men jeg kom lige i tanke om, at jeg jo alligevel skal oversætte metoden i alle parsing-programmerne, så det gør faktisk ike så meget, heldigvis. Og egentligt også rart nok at vide, at JS altid kompilere sine RegEx'er på en eller anden måde, inden de tages i brug, hvilket er fornuftigt. 

(16:06) Angående full-text searches så bliver svaret egentligt rigtig simpelt: Det skal simpelthen bare \emph{kun} være KeywordStrings, at man kan søge i med FULLTEXT searches. Og det man så typisk vil gøre her, er, at man så med det samme tilføjer en relation (og en bruger), sådan at man finder frem til en mængde af termer, der passer godt på de indeholdte nøgleord i den valgte streng (som så typisk vil indeholde en overmængde af de nøgleord, man selv har tastet ind). Og alle andre søgninger i databasen skal så simpelthen bare være semantiske søgninger, foruden primær- og sekundærindekssøgninger (hvilket også inkluderer sekundærindekssøgninger på term-titler m.m.). (16:12)


(24.04.23, 12:10) Kopieret fra commit message: ``Oh, I just realized something. I was leaning towards abandoning the syntax check (including even for JS programs), and now I just realized how doing so would actually just provide a perfect opportunity to showcase the potential for semantic technologies when it comes to program semantics verification..! So I'll keep the outer ideas about the UPA, including getting script from textID's in the SDB, but I will put a pin in all the ideas about automatic verification (and thus about my JS subset) for now.''

.\,.\,Well, I'm putting a pin in syntax-checking the JS subset, but I can still propose a standard of using various `upaf\_', `upav\_', `upai\_', `upak\_' prefixes, such that it is much easier to verify, that the user script does not cause any unwanted collisions. (12:16)

\ldots Let us actually just gather all that to one `upa\_' prefix, why not?\,. (12:55)

(25.04.23, 16:12) Okay, with my new insight that I ought to cancel all the automatic syntax analysis (for my ``JS subset''), that brings me directly to the front-end. And today has been about reconsidering and thinking about the design that I want to start implementing for the application. Luckily I have had a clear head today and have reached some good conclusions.

First of all, I will stick to my earlier idea about designing it in terms of ``columns.'' Each column has its own vertical scroll, and they can stand together next to each other, with a vertical scroll if there are more than the window can contain. If the screen/window is small enough, the columns size are resized such that the screen/window contain a whole number of columns, perhaps only one (e.g.\ for mobile devices). When scrolling horizontally, the scroll automatically moves such that no column is only half visible. When the mouse hovers near the sides of the window, a button to rotate the columns by one column width appears.

A column for a given term then contains a header, which can be collapsed (automatically upon scrolling down, if the window is small enough), a main part and a footer below, which can also be collapsed. The header can in principle contain a bottom to view different viewing settings, but I will only implement that later on. Otherwise the viewing settings is chosen by the script that opens the column, which is, however, supposed to look up the user's viewing preferences beforehand. Well, let me actually get to that later, and focus more on the possible contents of the columns for now.\,.

I imagine that the header contains different tabs, which each renders a different page for the main part of the column (with different data in it). The footer remains the same however, despite which header/main tab is chosen. For category terms, two central header tabs could then be `Subcategories' and `Elements,' just to give a good example. Choosing `Subcategories' should then load a main part which queries the set of subcategories for the category, and similarly for the `Elements' tab. Hm, and let me actually give some examples of tabs.\,. For elementary terms, the standard tab would be one which contains data about the term. Which data should appear then in the main part for this tab should then depend on viewing settings for the defining category of the term, but more on that in a bit. Two other very important tabs are then one that renders a main part containing ratings for the element, where the user can then of course give their own ratings, as well as a tab which shows.\,. Wait no, that is for categories: Category terms should have a tab with filter settings, such that the users can choose.\,. well, choose what appears to be settings for how the elements are sorted *(Well no, cause the lists won't be sorted exactly according to the chosen settings, but the settings might still look much the settings for adjusting a sort order.\,.) in the category --- and also for what terms are filtered away from the list, but underneath this appearance they will actually be determining some settings for an algorithm that queries the database efficiently and then also applies some additional processing for constructing the list that the user then sees. .\,.\,It's hard to describe here, but I think I have a good overall idea of how this can be implemented in a good way.

The footer also have its own tabs at the top, but these tabs, unlike for the header tabs, are meant to contain a more homogeneous list of data relevant to the term --- and which are not central or important enough to show in the main part. But in principle the header and footer can contain duplicated taps (with the same script for rendering the data!) .\,.\,Ah, but in general, the footer is intended for showing lists of terms that are related to the relevant term of the column by the same relation. The intension is thus to have only one relation per footer tab, and for each footer tab to then render a list of object terms to that relation (with the columns term as the subject). I hope this makes sense.\,. .\,.\,Whereas the header tabs are meant to give potentially inhomogeneous data fields for the main part of the column. So while the footer is going to works exactly like the header + main parts, the difference mainly lies in the fact that the main part is intended to include all the data that is most relevant to the term, possible with several relations making up the layout, and the footer tabs are meant to contain only one relation, and especially relations that didn't get included in the main part. Okay, I feel like I'm rambling a bit to much now; I hope it makes sense (somewhat) as I have put it now.\,. %(17:04)

The footer can actually also be expanded to fill out the whole column. And by a press of a button, it should be semi-collapsed down to the bottom of the column again. I imagine that the footer contains a few special tabs meant to adjust the viewing options themselves. .\,.\,Hm, I actually feel a bit tired now, and I also feel like I can do some more thinking before continuing, so let me just do that (and then return later in the evening or tomorrow morning).\,. (17:10)

(26.04.23, 10:00) I have thought more about how to implement the application. Let me start by mentioning some things about the columns, that I didn't get to yesterday. It should generally be the case, that when you middle- or control-click on a tab (from the header or the footer), the tab content should be rendered in a newly opened column to the right of the relevant one. .\,.\,Hm, and what else?\,.\,. .\,.\,Hm, I actually think that was it for the overall design. Let me therefore now continue by describing how to make all this highly modular.

When you open a new term in a new column, a script is then run which is responsible for constructing the main flow of that column, including the header with its tabs and the footer with its tabs, as well as deciding which header/main tab we start on, and whether the footer should be visible from the start or if it should be collapsed. Each term type will then typically have their own script, and whats more, any category can also overwrite its parents script, such that elementary terms will get the column-rendering options defined by its ancestor categories, where each subcategory can overwrite part of the rendering options of their ancestors. And I say rendering option\emph{s} in plural because there are a lot more scripts to run than just the outside one (hence why I say that the rendering options should be highly modular). Besides the main script for the column, which I will henceforth call the ``(outer) column script'' as to not confuse it with the main script of the application (the UPA) itself, (besides this column script,) there are first one script for each tab which this column script calls when it need to render a new (including the initial) tab. The column script will also potentially call some scripts that sets up listeners, which can then listen for data sent by a child tab (including if the tab is opened in a new column! (that still makes it a ``child tab'')) and then perhaps send that data on to other child tabs. I will personally need this when I need to implement text annotations (which has to be implemented rather soon after the first part, which is the system that deals with ratings and sorting via those ratings, since the annotations are now very important for verifying user scripts (in a more community-outsourced way)), since I would like for desktop users to be able to see how their new annotation will look on top of a text, while they are constructing it in a (child) column next to it. But in my first implementation.\,. Oh wait, I might also need it for sortings.\,. Yeah, I need it there as well, if a want users to be able to adjust the sorting settings in one column, and then be able to click and get the relevant category column to update with the new sorting settings. Okay, so I should actually make this signal handler for the script for rendering category columns.\,. %(10:26)
%...(10:42)
\ldots When the column script loads the tab scripts, it doesn't do it directly. The column script instead defines/uses some keywords, such as ``subcategory list'' or ``comment section,'' and then run a procedure from the application main script to get some functions that loads these things. Each of these functions are in principle defined from running their own script.\,. Okay, this is going to a bit complicated, so let me think about it, and about how to explain it\ldots (10:49)

\ldots\ (13:18) Okay, I've come up with several ideas about what to do. First of all, the UPA actually don't load just one main script in the beginning. It start by querying the ``preference user'' for a whole set of initial scripts, which are then executed in order from highest rated and down until a certain lower threshold (above zero). Among other things, these scripts define a class for loading content into an HTML element, given a key pointing to a function (e.g.\ for loading a ``comment section'' or a ``subcategory list'' etc.) as well as some inputs for that function. And the scripts then also load settings into this class, each of which defines or redefines a key by associating it with an actual function (e.g.\ a function to load the contents of comment section given certain input). For instance, we could have that the first script in the sequence defines this class, then the next script could define a bunch of ``content keys,'' as I will call them from now on, and associates a function with each of them. Then the next script might define some new content key--function pairs, and might even redefine some of the keys from the second script (since this might be easier to do rather than to write one script that does it all for each specific user preference; it will often be better to just specify preferences by overwriting previous keys). Now, the class in question then also implements signal handling for its children, meaning first of all that every time a child is loaded from a instance of said class, that instance then adds a child key--jQuery object pair to a list over active ``child processes,'' so to speak. Each ``child process then also holds.\,. Oh wait, the parent should not hold jQuery objects directly, but should hold a ``pointer'' (in the sense of how JS works regarding reference types) to the child object, which by the way are instances of the same class, and each child instance should hold a ``pointer''/reference to its parent. Each child can then send signals to its parent, who can then either react directly on these signals, and/or it can send these signals on to a collection of its other children.
So far so good, and in terms of loading the content that an instance of the class (children and parents alike), each instance.\,. Hm, let me actually think about just a few things before continuing.\,. (13:38) .\,.\,Hm, let me first of all call the class a `ContentLoader,' why not?\,.\,. (13:39) .\,.\,I by the way plan now to make the footer just another child of the column's content loader, but I need to think about, whether I should actually also do the same for the header, or if a should stick to wanting to treat that one specially.\,. (13:41) .\,.\,Yeah, cause the tabs of the header should each be associated with its own content loader.\,. .\,.\,Hm, let me think.\,. (13:44)

\ldots (14:47) Okay, I have finally landed on a solution, I think. The header will not be a special part of the class's API, but will just be another child, which then means that I have to implement serious signal handling from the get-go, since the header then has to communicate to its parent when new tabs are selected. Furthermore, having divs/containers that can render content associated with several content loaders, and not just one, is not a part of the basic API either, but rather the parents simply adds a new div/container / content loader child when a new tab is pressed, and then simply hides the previously active child div/container (and perhaps in some cases give the child a signal to empty itself, such that the garbage collector can free that memory again). So having a multi-purpose div that can render several different types of content, will actually just be implemented by having several divs which the parent can then switch between such that only one is ever visible at a time. 

.\,.\,And there we pretty much have it. I want to mention, that I then also want to even implement the box that contains all the mentioned columns as an instance of this ContentLoader class. New term columns will then have their own content loader attached to it, but if column opens up a child as a column next to it, instead of opening up the child within itself, what happens is (the way I intend to implement it) that the content loader implementing the column just widens to the width of two column, and then indeed opens up a column within itself, but just where it appears to be a new column that is outside of the parent. I will then just make the horizontal scroll snap to, not the actual border of the content children, but rather to multiples of a given column width. So when a column is now actually the width of two columns (and appears to \emph{be} two columns instead of one), the scroll will just snap to the middle of this expanded column, instead of only being able to snap to its edges. %(15:04) ..Hm, what else to say?.. I feel like that covers a lot of it, and I have already mentioned that the footer will now just be another child of the column's content loader (and so will even the header).. ..Oh:

.\,.\,Oh, and I should also mention that the script are now no longer modules: The set of scripts that the UPA loads (sequentially) in the beginning are now loaded as normal scripts (with the ``use strict;'' command). These script, however, should make sure to use `upa\_' prefixes for all globals, but they are free to overwrite each other's functions, and to change each others global variables, not least (which will be used to change content key--function pairs). .\,.\,Hm, let me think, however, if I need to also be able to use modules.\,. Oh, and I should mention, that the content loaders can also import scripts dynamically, when they need to render some content types, that are not frequently enough loaded to be included in the set of initial scripts. (This by the way means that I must include modules as well, but let me think about that in a moment.\,.) Such dynamic (more dynamic than the initial sequence of script (which are technically ``dynamically loaded'' as well, as with all scripts in JavaScript, but not ``as dynamic'' as those who uses the import() function)) modules can be loaded after looking up further preferences from the ``preference user,'' in particular for rendering terms of special subcategories, which the ``preference user'' has defined such that the rendering options overwrite option from the ascendant categories, but has nevertheless not included as part of those scripts that need to be fetched from the server immediately when starting the UPA (i.e.\ the scripts in the aforementioned set). Thus, the user can navigate to a new term, the UPA can discover that one of the ascendant categories of that term overwrites some rendering options, then the UPA can import() the required script for defining the new content types that terms of this category requires, and then the UPA can proceed to load the contents of the column of that new term (with content types that are not specified in the initial sequence of scripts). .\,.\,Okay, let me think about some of the details regarding this process, including the things about using both module and non-module scripts, and then I'll get back here\ldots (15:23) .\,.\,(15:28) Oh, I have also completely forgot to mention some further modularity when it comes to loading and then rendering contents of the leaf content loaders / content containers (i.e.\ the ones with no ContentLoader children themselves), but let me also just get to that after having thought for a bit\ldots (15:31)

\ldots\ (18:01) The UPA should use modules!\,. And then these modules should just add globals to the window or to local storage manually. And when they do, they should either prefix all these globals with a the same prefix, or the should add only one global object, whose properties are then the global variables. The prefixes aren't predetermined, and in principle the users can choose whatever, but in practice most user scripts will build on top of a previous API, and each of these UPA APIs will have their own special prefix/global object (initialized by the first script in the sequence of initial scripts). A user who want to add a preference-defining script to an existing API/library should then use the same prefix/object as that API/library, otherwise their script should not be accepted by the community and/or the developers. The rest of the webpage, the ``non-UPA part'' so to speak, should by the way also use it own modules, or should at least not add any global variables to the window. 

About the content loader ``leaves,'' let me first mention that I forgot that these will also need signal handling, so this signal handling will absolutely have to be something that I try to implement immediately (for this ContentLoader class). Now, in my thinking break here, I also had the idea that some content loader leaves might not be attached to any HTML element, namely since they can also simply be responsible for loading data(/content) into a JS object, instead of into a HTML container on the page. This could for instance be when loading a set of terms. The content loader should then signal to its parent, when the data is downloaded and ready, and then attach this data to the signal (or a subsequent one). Typically a loading of a set will also require subsequent queries to the definitions/data associated with a term ID. Either by signal from the parent or by itself, the JS object content loader can then query the database for this data, and then signal its parent once again, when all or some of the next batch of data is ready. The parent can then load this data (from both batches, one before the other) into another child (via signals of course) for which the boilerplate HTML has already been loaded in the meantime. This second child will then know where to put this data. Now, notice that if another user wants to change the HTML for the list that renders the given set in our example, they will then only have to make changes to the child that loads this boilerplate. .\,.\,In fact, since a parent can also make changes to its children directly, without signals involved, one could design list container child to consist of a parent which has a protocol for inserting the received data into an already-loaded boilerplate HTML child container. This boilerplate content loader will then \emph{only} have to define the boilerplate HTML, nothing else. And users who wants to overwrite previous preferences for the list (boilerplate) HTML with their own then only have to add a script that changes this exact content type (of the list (boilerplate) HTML), nothing else. And since this just require changing one HTML template to something slightly different, such a script will be easy to verify by the community / the developers. By the way, the mentioned HTML boilerplate will also include CSS classes, which means that users can make big changes just by changing some CSS classes to something else in the markup. And if the users needs to set some special CSS properties that aren't part of the existing CSS library (of the UPA ``script family'' that the user wants to build upon), the user can of course also very likely set these in a clear way that isn't hard to verify (by the community / the developers) either. So this is how the users will be able to change the HTML of various content types by adding/changing only small scripts without much logic contained in them. And of course (it almost goes without saying), users should also be able to add scripts that redefines existing CSS classes slightly, which means that a few lines of CSS (can will often not be very hard to verify as safe) can change the application's appearance pretty significantly. (I have already written some code a short while ago that exemplifies how UPA script can append CSS to the head of the document, and that is also the intention here; that users can add script that alters the application by doing exactly this.) (18:39)

(19:10) I should also mention that users who are logged in can always load and run the scripts that they have uploaded themselves. And let me also mention that the ``more dynamic'' scripts doesn't have to use import(): We can also implement dynamic script by inserting module scripts the same way as is done for the sequence of initial scripts (where these scripts can then change and/or add to the (prefixed) global variable(s) manually for the window).

\ldots (19:30) Ah, and the initial input that a content loader needs is also just given by signals. This also means that the parent can initialize the child instances without them activating before they receive their first signal to activate, which should then of course include a payload of the needed inputs. 

(27.04.23) jQuery already has custom events, so I actually don't really need to implement much.\,:) .\,.\,Hm, I maybe even don't have to actually make a class, cause another solution could just be have an function that initializes all content loader children within an element by running a function that reads the content loader class off each child as an attribute (how the content loader class should be defined anyway), and then simply branches to the initializing function.\,. no wait, actually, it should just set up an event listener for ``activation,'' and then define the content loader function to run upon that activation. Yeah, this seems to be the way. And a parent also does not need to keep a list of its children, cause we can always simply utilize jQuery for traversing (both up and down) at any point. (10:03) .\,.\,Yeah, so a ``content loader class'' will now not be a JS class, but simply a HTML class, which is initialized by its parent element, at first simply to listen to the first `activate' signal (for which it will then call the content loader function associated with the given content loader/container class).

(10:20) Let me use a new HTML attribute for the content loaders and call it `content-key'. I could also use the class attribute and add a `contains-' prefix to it to make the syntax better for selecting it for CSS styles, but I don't actually think the the content loader containers \emph{should} be selected for CSS.\,. Hm, not especially so, at least.\,. .\,.\,No, let me indeed use a spacial `content-key' attribute, which is then not intended to be used for defining CSS for the div; only CSS classes should be used for this. (10:26)

Before I get going, however, I should also briefly just mention, that I actually only intend to implement more complicated ``user groups'' later on, and in the beginning, I will thus only use one ``user group''/native bot, and that is just an average rating bot, biased a bit towards zero, such that the first couple of ratings cannot move the combined (zero-biased average) rating far away from zero. (10:30)

(28.04.23, 9:36) I think I know what to do about the user groups / native bots. I think I should have a private row (that no one can query) for RecentInputs denoting the time of the ratings. For each user group there can then be a bot with access to the database, which continuously scans the recent inputs and changes the ratings of the user group accordingly. When a scan begins, this bot then records the time (just before the scan starts). It then only looks at the changes between its last recorded time (for its last scan) and the one it just recorded. For each ``recent input,'' it then looks at the last input before the last scan's beginning time and also the last input before the current scan's beginning time (but not any inputs that a newer than that (i.e.\ uploaded during the scan)). As a defining feature, each user group then has to have an algorithm/formula/function that can update a rating when knowing a users former rating and a new one, and still maintain an invariant. For instance if the invariant is an unweighted arithmetic mean, the formula would be something like
$r_{comb, fin} = r_{comb, init} + (r_{ind, fin} - r_{ind, init})/N$, where `comb' stands for `combined,' `ind' is for `individual' (i.e.\ the user behind the given input), and `fin' and `init' stands for `final' and `initial.' When the scan is done, the bot overwrites its old `time of the last scan' with the new time and wait for the time of the next scan. A central program/bot can then also scan each user group to get the least `time of the last scan' and then apply a clean up procedure to RecentInputs for all inputs that lies before this least time. This cleanup procedure should then gather some of the inputs if the user has changed their opinion many times recently for a given set--object pair, and it should not least turn timestamps into dates, such that the `sensitive information' of the exact time of the input (which we will treat as such), is transformed into what we will see as `non-sensitive information,' namely the date of the input.

Now, such algorithms might have a little drift away from the exact right rating due to imprecision in the formulas. There might therefore also be a bot which very rarely will simply take a user group formula, not the relative one but the absolute one for obtaining the ratings given all user inputs (which should however still have a form where the combined rating can be constructed piece by piece, without having to keep all the user ratings in memory at once.\,.), and then goes through the entire RatingInputs table to calculate the combined ratings. Of course, this calculation will also not be 100\,\% precise, and precision isn't even \emph{that} important for the system anyway, so these procedures should just run very rarely (and perhaps be there more for backup purposes, really.\,.). But yeah, this is my new plan for the user groups, once it finally becomes time to implement them (more than just the standard zero-biased mean bot). Users should then be able to upload descriptions of the user group algorithms, that they will like to see (which will likely often be similar to previous algorithms but only which some changed parameters, probably often in the form of a set (so a set ID) of users where the ratings of the users in that set will then give the weights to the average). The user community can then up-vote various user group description that they would like to see, and the developers can then continuously initiate new user group bots that the community wants --- and discontinue old ones that are no longer used/wanted as much. The developers should also actively give suggestions for new user groups themselves. And these suggestions might often be due related to some machine learning-like statistics, namely where the developers measure yet another statistical tendency (in terms of correlation eigenvectors) and then tries to create a user group with weights according to how much each user fits (or more precisely is projected upon) said correlation eigenvector. (10:19)


(30.04.23, 10:07) Okay, I think I know what to do now.\,. If we look at contentSpecIndex in t2 now, I will make that into a class and absorb the functions in it (into methods). Then I will make at hold a contentSpecIndex object as a property, and actually then say that modules should be allowed to change this array as side-effects. But the modules should still export whatever ContentSpec instances the define/redefine. .\,.\,Hm, let me think for just a second before continuing, about whether this is really the optimal way to go.\,. (10:12) .\,.\,Sure, why not.\,.\,! Okay, and this means that later modules that imports content specs from an earlier module will have the side-effect of changing the --- oh, and I think I will call the contentSpecIndex class something like ContentLoader --- changing the ContentLoader instance that all the modules import. But because each module also export their newly (re-)defined content specs directly as variables, later modules will know that if they import these, then that is what they will get.\,. Hm, or maybe there is a better solution (which I just remembered my earlier idea for).\,. Let me think.\,. (10.19) %..Hm, thinking out loud *(or rather "on the keys") just for a bit: Maybe I should let ContentLoader hold a structure of other ContentLoaders.. (10:21) ..Or maybe I could keep a seperate struct over ContentLoader names / combined ContentSpec names.. (10:24) ..A "content spec(ification) index," btw.. ..Hm, no.. ..or yes.. ..(10:35) Hm, I don't think I need to gather them together; a "set" of ContentSpecs should just use clear and informative event strings, and define a clear specification for how the content is supposed to work over all.. I think.. maybe.. 
\ldots\ (11:54) I think I'm just about to have it, finally.\,. %..Let me think on the keys again, so ContentLoaders chould contain other ContentLoaders. And when a new script/module changes something quite fundamental about a ContentLoader, then there should definitely not be any side-effects, and the changes should be exported as a ContentLoader rooted in the "production" (to lend a term from Grammars), i.e. the content key, which is the outermost "production" that has either its HTML or its event semantics changed.. (11:59) ..Yeah, and should I even have side-effects to begin with after all..? (i.e. now that we can "package" subproductions and gather them in "child content loaders"..) ..Hm, or maybe the root content loader can just be assembled top-down in the end, by importing the content loaders/spec in order from the parents going to the.. (children..) well, maybe not.. Let me think a bit more (off keys).. (12:04) ..Hm, maybe a top-down constructor *(at the end..) is really just all that we need..!.. (12:05) ..(12:10) Oh, now I just had the idea (and think) that we should build all content loaders on top of a sort of tree of semantic definitions for the contents (mainly defining what their children can be and what the signals does).. Hm.. ..Hm, I could also see a way of implementing this tree via the ContentLoader class itself, together with some checkers that checks that modules do not build upon a part in the tree, that has already been built upon.. Hm, I think I'm close to the answer.. (12:15) ..Yeah, I think a good solution could be that we do indeed apply the changes as side-effects, but then just make sure that any module that changes either the HTML or the event semantics of a content loader in the tree increments some property denoting the "modification level" of whatever thing the make changes to (e.g. the HTML or a certain group of signals), and also makes sure that these levels has not been altered since the parent module that the.. main ContentLoader.. was imported from before making these changes (throwing an error if these levels have been increased in the meantime between running the parent module and the current one). (12:22) ..Let me copy this to the rendered text..

\ldots Copied from comments: ``.\,.Yeah, I think a good solution could be that we do indeed apply the changes as side-effects, but then just make sure that any module that changes either the HTML or the event semantics of a content loader in the tree increments some property denoting the ``modification level'' of whatever thing the make changes to (e.g.\ the HTML or a certain group of signals), and also makes sure that these levels has not been altered since the parent module that the.\,. main ContentLoader.\,. was imported from before making these changes (throwing an error if these levels have been increased in the meantime between running the parent module and the current one).''

.\,.\,Yeah, so I think my solution will be, that each module just imports the same main ContentLoader and then navigates to the given child content loader (cause now I think ContentLoaders should hold other ContentLoaders as children) that the module wants to alter. It then makes sure to check that the properties denoting modification levels of dependency groups within the ContentLoader are not above what the current module expects (or throws an error otherwise), and when the module finishes, it also makes sure to add and/or increase such dependency group modification level properties before exiting. Nice.\,.\,:) (12:30)

(14:02) Okay, and each ContentLoader should indeed hold other ContentLoaders (not ``ContentSpecs''), and should actually just hold their keys themselves. So when having to load a nested template, the ContentLoader simply searches all its children for a matching content key (which the children hold). And if no match is found, the task is actually handed over to the parent (a new idea of mine). So that way, child content loaders can use content ``productions'' that are defined above them at their parents. Now, if someone decides to add a ``content production'' from a parent in an environment, where it hasn't been before, that module should then probably also add a ``dependency group'' to the parent in question.\,. Hm, well that depends.\,. *(No, if a child starts using a production from an ancestor, that ancestor then has to ``konw,'' i.e.\ by setting or increasing a certain dep.\ group's mod.\ level for it.) But anyway, this is how it will be, and then this also brings me to commenting on these ``dependency groups'' again (which should contain a name/key, a description of the dependency, and then of course a ``modification level''). Cause the really good thing about these dependency groups is that the don't have to be set immediately when the dependency is.\,. well, caused, so to speak. Instead the users can actually just wait until they first start to make changes that reveal and/or create potential dependency issues. Then these users should first make a script that adds the relevant dependency groups to the given ContentLoader(s) that they want to modify, and then they can go on to make a new script/module after that, which uses these dependency groups to signal that the new changes might change how the content production functions in a way that is not necessarily compatible with the changes of other users. Then when other users want to make new changes, they can either build upon these changes, and perhaps increase the relevant dependency levels, or they might also start from the same script/module that initializes those dependency groups, but then go in another direction with them and thus follow said script/module up with a different script/module that starts incrementing the modification levels of these dependency groups. This was a bit complicated, so let me formulate it again. When a user wants to make some changes that might be incompatible with other user's changes, they should then (if this has not been done by others) add a module that initializes some new dependency groups, with modification levels all at zero, and then do nothing else. When they then go on to make a module that makes the given changes that are perhaps incompatible with others' changes, they should make it import the dep.\ group-defining module and then increase the relevant modification levels by one. Other users can then make new modules building upon this script, or they can let build upon the aforementioned (parent) script that initialized the dep.\ groups with mod.\ levels to zero (or they can of course also start from an even earlier ancestor module). So there we go. I hope this makes sense; now I will start implementing all this. (14:27) %(Well, perhaps after training break here at 15:00..)

(01.05.23, 11:53) I'll change the ``dependency groups'' to just be arbitrary ``modification signals'' that can be objects with an arbitrary set of properties. The can then also for instance contain ``reserved content keys.'' Now, a more important thing that I should also note, is that user modules are still okayed for use in groups, or more precisely in `sets.' This actually means that the same thing that we try to achieve with these ``modification signals'' can also in fact be achieved purely with the semantic database.\,. okay so I actually have to consider if we should even have the ``modification signals,'' I guess, but I strongly suspect that I will keep them, though.\,. Anyway, by grouping modules into sets --- and providing semantic documentations (in the SDB) to each module set --- we can actually achieve something similar to what the mod.\ signals tries to achieve. (These signals will just mean that these sets might be able to get much larger.) By grouping modules into sets, we can start with a small set of modules for the application/website where the modules don't care too much about the modSignals (and about repeated content keys and such), cause the limited size of the set makes it so that it isn't so hard to verify new modules against the old ones. And the initial modders --- and the developers --- will likely be able to maintain a good overview of the dos and don'ts of this initial UPA library. The moment that things start to get a little complicated, the modders and developers can then document the library so far an gather it into one module. And then new module sets can be made with this initial UPA library as the first okayed module in these new sets. Then modders and developers can add new modules to each set, which will then each be a branch of the UPA. .\,.\,So yeah, in total the point is that there is a lot of things we can do to partition the user (and developer) modules up into groups and to document the intended semantics (and the dos and don'ts), so we shouldn't worry too much about the modSignals (which I \emph{will} definitely keep, I'm pretty certain %*(13:05):
*(Oh yeah, absolutely.\,!)%
) in the beginning. In other words, let us not be too defensive as programmers in the beginning. Let us instead just assume that all the initial UPA content loaders will be refactored well-before the libraries start to get big enough that the modSignals start to become important.

(12:25) If we ever want to introduce what I was about to make in terms of ``ContentManagers,'' if say we at some point want/need to speed the programs a bit up, then these can just be implemented by adding a module that holds an array of such ContentManagers (which then does not have to be of a special class but can just be any object really) associated with a certain ContentLoader (which the given module modifies). And in an inward or outward callback, the.\,. %wait, and also this CM array can also be initialized dynamically by these inward/outward callback function, which is perhaps often better, actually. ..No wait, probably not (let me out-comment this)..
.\,.\,a CM key can then be added to the startMarker of the content production. Any subsequent callbacks, or other subsequent functions/scripts, can then at any point query the startMaker for the CM key, look it up in the aforementioned CM array (which should also be imported --- or should be initialized by the same module that uses it --- or could also in principle be initialized dynamically by an inward/outwardCallback by letting them attach it to window), and thus obtain access to the CM object, which can then be queried, modified and/or used to apply a method (one of its) on some element or piece of data. (12:38)

(15:48) There is one apparent weakness to my planned system so far, and that is that it seems very rigid in terms of being able to go in and solve/remove a bug/fault for a module already in use, due to the fact that texts are pretty much constant in the database. But there is an easy solution to this (which we have to use), and that is to say that the server is allowed to map requested scripts/modules to other ones, so that if a bug needs fixing in a script already in use, then the developers can just fix this bug in a new text and then map the old text ID to this new text (when it comes to scripts/modules and only that). Scripts that imports from the now remapped text ID does then not have to be remapped themselves in order to include the bug fixes in practice, but the developers might still want to remap these after a time, since this.\,. well actually, never mind, cause when querying scripts in the SDB interface UPA, the community can just make sure to implement that the remapped script are shown first thing (just with a visible note that the user is watching a later version, not the original). But yeah, the developers should however still also remap scripts that uses the old IDs that has since been remapped, at least after a time (so that each little debugging doesn't require an immediate remapping of a lot of text (also since this would cause a lot of remappings, leading to more dead texts in the database)). (16:00)

(06.05.23, 13:28) A remainder for myself to talk about my so-called ``wish ratings,'' that I remembered about this morning. But I'll do that later today; right now I need to go for a walk.\,. 
\ldots\ (16:18) Okay, let me write about it now. I'd almost forgotten about those ``wish ratings,'' but they will be very important as I see it. Now, I think they should actually be implemented by adding more bytes to the rating values. And maybe I will make it an early standard that a the first rating byte represents the mean, the next represents a spread of precision, and the third will represent a wish for modification and/or for future versions of the term (as a signal to the creator of RW (real-world) thing that the term represents). All these three bytes have signed values ranging from -1 to 1, going from -127 to 127. And then -128 means null/``not given,'' which can for instance be used (in our case with the three rating parameters) if wanting to upload a wish rating, but not a precision specification or/nor the standard (mean/midpoint) rating itself (and thus users can even express wishes (for modification, future versions or future RW things in the same category as the given one) without having to also express their opinion on the given term at hand). (16:27)


\section{Continued working notes while making the application}

(07.05.23, 12:32) I'm considering making changes to the SDB (i.e.\ the backend database system). I've already decided to convert to decimal IDs, to group all users and bots (native as well) in to one table (``Users''), and to make the sets homogeneous, but the last point requires some further thinking. I now think that I will make some fundamental sets, instead of categories, be the entry points for the database (as the ``roots'' of the ``graph,'' so to speak). Then I can perhaps.\,. Hm.\,. or maybe I should use ``meta-categories''.\,. %(12:37)
.\,.\,Okay no, I don't need meta-categories, 'cause we can can just use relations (with the Terms category as the subject) --- which I will by the way include a obj\_cat\_id on top of their subj\_cat\_id. .\,. %(12:43)
.\,.\,Hm, I btw think that I might stick to calling it Terms and Elementary.\,. Hm, no I should figure out something better.\,. .\,.\,Hm, what about Entities (as in: `database entities') and Terms?\,.\,. (12:48) .\,.\,Hm, that does seem like it could be a good idea.\,. 
.\,.\,Okay, so with this I have Entities of a small variety of `types,' including Relations, which can take any Entities as their subject/objects, and Categories, which always have Terms as their `elements.' .\,.\,Okay, this seems good, but let me think for a little bit.\,. (12:54) .\,.\,And if users really want to have categories of Relations and/or of Categories, they can just implement these meta-categories as as Terms, and then make a corresponding pair of `Subcategories' and `Elements' relations. (12:57) .\,. .\,.\,Oh, or they can just take the relevant fundamental set and make some relations to yield subsets.\,. well okay, that is the standard thing to do, but that does not give quite the same category--subcategories--elements system, so if they want a system exactly similar to that, they can do as I just wrote.\,.\,:) (13:01) .\,.\,Hm, but about the fundamental sets: All sets should probably have a subject and a relation, so maybe.\,. .\,.\,Hm, how about having a category.\,. Or a term.\,. hm, called `Enitities'.\,. Hm.\,. .\,.\,Ah, or how about a term called `This SDB'.\,.\,?\,:)\,.\,. (13:07) .\,.\,I like that (a lot.\,.).\,.

.\,.\,Hm, I probably don't need categories to have obj\_cat IDs, then.\,.(?) .\,.\,Oh wait, I can't even \emph{use} categories now for relations!\,.\,. Hm.\,. (13:19) .\,.\,But I guess I \emph{could} use sets instead.\,. (13:20) .\,.\,Hm, I guess I need to decide if I even need it, for while it is a good way to make a relation more clearly defined, couldn't we just instead simply allow relations to be ambiguous (their semantics depending on the subject).\,.\,? (13:24) .\,.\,Yeah, why not?\,.\,!\,:) (13:26)

\ldots Ah, but the Relations should, however, still know their subject and object \emph{types}, which then means that I can lose those for the sets.\,. .\,.\,Hm, this makes it quite nice, cause all of a sudden, types only appear in tables Relations, Lists and Creators; all other tables can make do with just the numerical part of the IDs.\,. (13:48)

(08.05.23, 10:09) Oh, now that users can trust the UPA more, I don't need RecentInputs for anything other than.\,. well, than for the native bots, but perhaps also external bots.\,. Hm.\,. .\,.\,(Let me call them `internal bots,' actually, btw.\,.) .\,.\,Well, external bots will be at a big disadvantage anyway, so maybe it doesn't make too much sense to.\,. well.\,.

.\,.\,Hm, I do want external bots to be able to participate, so it would actually be much easier if users could just have a standard random delay (with a standard spread, that is) to all their inputs, and then they should just be able to adjust this spread for any new rating if they want it to go through immediately.\,. Hm, so how do I implement this.\,.\,? (10:22)

.\,.\,Okay, so what I will do is to put a (now public) timestamp back on RecentInputs, and then for now just let that timestamp be the actual one. Then at a later point, I will make a private RecentInputs as well, which should include a user-specified spread as well. I should then also implement a ``bot,'' or a server procedure rather, that scans the private RecentInputs, rolls some dice, and then move the private inputs over to the public RecentInputs table depending on these dice rolls (or rather an RNG), the spread and the actual timestamp. And when they are moved over, the new timestamp will be they time of this movement, not the actual timestamp. (10:30)

(22:16) Ah, I just realized that we can't let users run their own scripts freely, 'cause then users might leak their password and thereby fool someone into logging on as their user, whereby malicious code can be run in the victim's browser. So yeah, I guess all code has to be reviewed first.\,. 


(23.05.23, 11:27) Okay, object nouns are the standard for the Relation strings, but this semantics can be escaped by prefixing the string with e.g.\ ``adj:'' or ``verb:'' or ``rel:''. I intend to use the first two examples here especially for Predicates (which will be a sort of ``virtual type'' in the SBD), where the subject should then be a category, the adjective or verb should be taken to relate to the object, and verbs should use singular.\,. well, verbs.\,. .\,.\,They should be inflected in the singular (present) case.\,. .\,.\,This is a much better solution than what I had in mind before about e.g.\ cutting out `elements' at the end of strings to yield the adjectives. (11:33)

\ldots (12:22) Hm, let me call the category--verb/adjective combination `Predicates,' and let me then call the more general subject--relation combination `Classes.' Then Classes and Predicates will actually be much more visible as types in the application than Sets, I think; Sets will be more the concern of developers/developer users.\,.

(24.05.23, 14:31) I've just come home from a little walk, where I had the realization that I can just make the Predicate Column into the Relation Column instead (and then just automatically insert the subjID held in the input data). This also means that I will actually (seemingly) only need the EntityColumns to begin with, which also makes things simpler.\,:) So let me start implementing these new ideas.\,.


(25.05.23, 10:22) I had many good (probably) ideas yesterday evening, I'm still not done thinking about them, 'cause its actually at a point now where I foresee that there might be big changes to the database and the semantical system itself. Let me think a bit on the keys in comments below this paragraph (in the source text) to keep me on toes about it for a bit.\,.

%Okay, der er flere ting, men jeg er nu bl.a. blevt ret sikker på, at kategorier og termer skal holde en (nullable) definerende entitet. Jeg havde også (i første omgang) nogle idéer i går om, at objNoun skal hedde relText (eller noget lignende) i stedet, og at der så altid skal være prefikser, der fortæller, hvilken type lexical item (ved ikke hvad ordet hedder på dansk) det er, og også samtidigt (implicit) hvordan dette skal tolkes som en relation. ..Jeg kan også lige nævne, at jeg også fik den idé, at kommentarer --- og andre "Appendices" (kom på at bruge det ord i sengen i går aftes/nat) --- skal være kategorier. Nå, men disse tanker gør så, at det bliver lidt redundency ift. prædikater (som i subjekt + relation) og kategorier, så jeg vidste at jeg skulle tænke videre i dag. I morges her i badet kom jeg så på: Mon ikke man kunne bruge min præfiks-idé for termer også, og så bare samle alle mine tre "semantiske entiteter" til én (nemlig til "Termer")?!.. Så det er altså de idéer, jeg skal tænke videre over nu.. ..Hagen er jo, at det lige bliver en tand mere kompliceret, når.. hm, når man skal arbejde med en masse præfikser, men det skulle vi jo alligevel, så nej, måske bliver det ikke \emph{mere} kompliceret.. Hm... (10:37) ...(10:49) Hm, jeg overvejer at lade termer og relationer smelte sammen, men stadig.. Hm, nej det giver ikke så meget mening, men jeg bliver nødt til at finde ud af, hvordan jeg stadig får fremhævet brugen af hhv. definerende superkategorier og kategorier... (10:51) ..(10:56) Ah, det gik lige op for mig, at jeg er fri til at gøre subject nullable i Sets, og dette ville jo være rigtig godt i et sådant nyt system, hvor relationer bare er termer igen (og dermed nemt kan blive prædikater). Og i og med at jeg jo nok gerne vil beholde det felt i, hvad der så ville blive den kommende Terms tabel, der definerer, hvor termen udspringer fra (altså de nuværende definerende kategori- og superkategori-felter), så kunne prædikater så bare også få lov at udsringe fra en kategori, sådan at man stadig har den nyttige ting med. Men nu bliver det så bare ikke "subjektet" af prædikatet, men det bliver, meget mere intuitivt, noget som prædikatet holder som en del af dets definition. Okay, det kommer altså til at blive nogle rigtig gode ændringer, de her, når jeg bare lige får dem udtænkt færdigt..!.. (11:01)
%...(11:24) Hm, I just had some good ideas to shift the nomenclature.. An important part is to use "Classes" a lot more, since it has the derived word 'subclasses,' which has a very similar semantics as 'subcategories.' My ideas are then to use 'Categories' as an outer division of terms, which is supposed to not just tell you what kind of term, we are talking about, but is also gonna be used by the application to "figure out" how to present that term (interface-wise). So we should be able to divide all terms into Categories and non-categories, where most non-categories should then be the "elementary terms" that springs from a category.. But these are not the only ones, cause then we also have predicates and relations, which "springs from" something different.. (I'm just think out loud now..) Then there are some virtual types that we should (probably) call 'Classes,' which consists either of a predicate term or a subject entity plus a relation term, and whose object types are always Terms (not other entities) to be considered Classes. A more general word that also includes predicates / subject + relations that take whatever types as their objects (not necessarily Terms) can be called Sets. This then means that I should change the name of what I currently consider Sets to something else, such as maybe 'RatingSets,' or something like that.. (11:37) ..Yeah.. But I actually don't know exactly yet how predicates and relations will even define their object types, so let me think some more.. (11:38) ..Hm, or should all object types simply just be terms, and then you always have to wrap the other types of entities up in Terms (having them as/in the.. "specifying field"..) before you can refer to them.. hm, and thus also before you can even define them.. So other types of entities should then always be created as either the part of creating a new Term, or the other way around: A new term should always (automatically) be created when e.g. a User or a RatingSet is created.. Hm, maybe..
%...(12:23) Ah, now I know! Categories.. Well, that means.. Okay, Categories \emph{should} be specail after all! And, since we now have out Classes, which are made up of relation [+ object] (I think I will finally exchange what I've been calling subject and object in the database), and which can have the nice relation to.. them.. ..'Subclasses'.. (Hm, but how to use a relation on a virtual time, something to think about there..) ..Since we have Classes, Categories are now free to be more specific in what they can hold in their "defining field." So the idea that I just had might very well work, and that is to let Categories always hold a description text, which then specifically define, not what's \emph{in} the category, but how the defining field and the specifying field (as well as the title/string/text, I guess) is to be interpreted! (12:31) ..Well, only problem is that the defining field will then always be a category, but I guess that's fair; then the categoriy descriptions will just define how the specifying field is to be interpreted.. That means that predicates has to hold the category.. that they "spring from" (is supposed to be understood in relation to, more precisely in this instance), which actaully makes the most sense anyway, come to think of it. (12:38) So is that almost it? Then categories always hold a category, but since Categories are not Terms, the semantics doesn't have to be the same, and when it comes to categories, the defining category can then be understood as always being a supercategory. (12:41)

(12:41) Okay, to summarize my new ideas from the comments above this paragraph: 

Relations will be swallowed by Terms, but Categories will remain as a separate entity type. Sets will be called RatingSets from now on, and will now have the `subject' last in the primary key such that the sets will contain rating and subject (not object) pairs (although this interpretation can still be escaped, but I will exchange the words (I try not to write `terms' too much when I'm not writing about Terms.\,.)). *(Oh, and I forgot to say here, that the object field will now be nullable for the RatingSets table, which means that relations (.\,.\,hm, which I guess `verbs' or `verbal clauses' is actually a more fitting term for, but anyway.\,.) can now be predicates as well.) Categories will have a super\_category field, a title and a description, which describes, not what is \emph{in} the category, but how the Terms using said category (or a subcategory thereof) as a defining category is to be interpreted, not least in terms of their specifying field. The Terms, as alluded to, has a category field, a title, and a specifying field (whose interpretation related to the Term is described in the defining category's description). .\,. .\,.\,Relations are now also Terms, as said.\,. Oh, and all other entities than cat.\,. Hm.\,. .\,.\,Hm, it would actually be nice if Categories was also a kind of term, so they could share the same ID range, thus eliminating the need for the use of the type characters when it comes to relations.\,. Yeah, so I could either have a tinyint (char) deciding if the defining field as a supercategory or a category, or I could perhaps just use negative integers for categories, how about that.\,.\,? Hm.\,. .\,.\,Hm, actually, even though it feels a bit hacky, I think I \emph{will} do that, i.e.\ save the byte, and then the application can just always turn that plus or minus sign into either a t or a c prefix.\,. Hm, I could do that, but let me actually think about some more.\,. (13:02) .\,.\,(13:04) Oh wait, do I even need for relations to be able to refer to Categories?\,.\,! 'Cause I have the Classes now.\,!\,.\,. well, except these are still not actual terms.\,. Hm, how about just not using relations at all but.\,. oh this is actually great.\,.\,!\,.\,. but only use predicate terms, which can just hold their objects as part of the defining field (and remember we still have the List entities).\,. .\,.\,So the RatingSets table will just consist of a userID and a predicate termID.\,. (13:09) .\,.\,Hm, maybe I will lose Patterns, but that is not so important right now.\,. .\,.\,Okay, to get back to the thoughts about the predicates and the Classes, now the Classes.\,. .\,.\,hm, couldn't we actually say that ``a predicate has these and these subclasses,'' and people would know what is meant?\,.\,. .\,.\,Sure; with an automatic reinterpretation of the subject (or object.\,.) predicate as a `class.' Nice! Fine, that simplifies the terminology: Now we are free to use `predicates' much more.\,:) .\,.\,Oh, and couldn't we even say that a term can have a ``subclass,'' and then it could quite easily be learned that that just means: Reinterpret the term as a class of things that fits the term definition (be it one or many), and then a subclass of/derived from that term is just a subset (but ordered after usability) of the things that fit that term?\,.\,:\texttt{D} .\,.\,Yeah, I think so; I think we can perhaps just spray the term ``Subclasses'' everywhere (i.e.\ having it as a potential tab of all Term Columns).\,.(!):\texttt{D} (13:25) .\,.\,And that can btw come with another ubiquitous tab: `Related terms.'\,:) 

.\,.\,Okay, so now my previous Category--Term navigation will just be a navigation of Terms, namely where the users can have the main navigation tabs: Elements, Subclasses and Related Terms. .\,.\,Hm, and then ``subclasses'' is either understood as terms with a narrower understanding.\,. oh wait, no, that is the general understanding; Terms that takes the current terms and narrows it down so that it fits more specific ``elements.'' For instance, a subclass of the term `Music' could be `Jazz music,' and a subclass of the predicate `(is) Funny' could be `(is) Witty.' (13:34)

%..(13:39) Nå, men jeg skal lige have fundet ud af, hvad jeg gør med kategori-ID'erne.. nå nej, pointen var jo, at de nu \emph{ikke} behøver at være Termer, og derfor godt sagtens kan få deres eget typepræfiks (men relationerne, eller rettere prædikaterne, behøver ikke typeangivelser, for de bruges kun om Termer).. Okay.. 

(13:58) Predicates' defining categories should define whether the predicates speak about the term is self (and its relation to the database and/or application (so on a meta-level)), or if the predicates should be instead be understood as speaking about the thing that the term refers to (so on a non-meta-level). .\,.\,And what else?\,.\,. .\,.\,Hm well, let me also mention that I'm thinking about whether I even need the Category entity type, or if a should just use texts instead (i.e.\ a ``defining description'' rather than a ``defining category''). Oh yeah, and the other thing that I'm also considering, is whether I should let the (Rating)Sets be re-absorbed into SemanticInputs (but perhaps not.\,.).\,. (14:03)

(17:43) I might actually keep the name Sets after all, and I might also let Patterns be.\,. .\,.\,(17:50) Hm, now I'm considering actually just.\,. keep a global id counter and then use that in all the ``Entity'' tables.\,. (And I'm also considering reintroducing types and make the predicates untyped.\,. but yeah, why not just use the same global id counter.\,.\,?).\,. .\,.\,No, 'cause then you'd have to look in all tables (of there's no types at all).\,. 

.\,.\,(17:57) Oh, I might actually keep Categories, and even keep the titles, cause they can still be used to specify in what contexts the Terms are to be interpreted.\,. .\,.\,Okay, so I'm actually probably just going to add a description field to Categories.\,. .\,.\,And apart from turning relations into predicates (and count them as Terms) and letting all terms hold a specifying field as well, I might then not actually change too much from how it has been until now --- well, at least not in terms of the database, for the change of not using Categories but just using Terms in their place in the application (and using a Subclasses tab instead of the Subcategories tab), that is of course huge.\,. Hm, but let me think some more.\,. (18:02) .\,.\,(18:05) Ah, or I could just make Terms hold a char denoting the type, and then.\,. then have all ``data Terms'' hold the data entity id in the spec.\ field, and this then makes it natural to also have a title for all data terms, but why not!?\,. .\,. (18:07) .\,.\,Hm, it perhaps seems a little redundant for KeywordStrings and Patterns, but.\,. .\,.\,But the title could just be nullable.\,.\,!\,.\,. .\,.\,Yeah.\,.(!) (18:09) .\,.\,Yeah, and this means that we can now give different titles to the same text, and then automatically treat them as two separate Terms.\,. (Not that you couldn't achieve such things before, but I don't know, I really like this idea.\,.\,!\,.\,.) .\,.\,No that \emph{is} actually something.\,! I thought about using it for attaching the user id to a comment before, and now I just realized that without these text titles, that would be much harder. 'Cause then one would either have to include it in the text, which is bothersome, or you would have to rely on a bot to look in the.\,. Creations table, but no, you couldn't even do that, attaching a user to a text would just be quite troublesome without being able to.\,. well okay, now that Terms can hold a spec.\ entity, you could do it.\,. oh wait! What am I saying! No, I still need to figure out how to make sure that users can sign their terms.\,. .\,.\,Oh, or maybe not! (18:20) Maybe a Category description can just state that part of the semantics attached to the term relies on knowing who created it, and thus that one needs to look in the Creators table to be able to get the full interpretation of those kinds of Terms. Nice!\,:) (18:22)

.\,.\,Oh wait, no, there's a few thinks left to consider in regards to all that, so let me think on.\,. (18:32) \ldots (19:09) Maybe I should let the Sets hold the object type, and then use the type chars everywhere where it is necessary (which probably won't be in a lot of places---for instance not in the spec.\ field, since here the types can be defined in the beginning of the description instead.\,.).\,.

.\,.\,Okay, I think this will work well. The system is not very much different from what I had before: Basically, the relations are now turned into predicates instead, of the Term type, and all Terms then have a spec.\ id (where the type is described in the Category description) --- and the Categories now have a description field as well. .\,.\,But that is pretty much it in terms of the database, isn't it.\,.\,? (And then in the application layer, we'll now let Terms do a lot of the duty that Categories did (though not all; not defining viewing preferences, e.g.).) (19:22)

(26.05.23, 9:10) I wanna call Categories Contexts instead in the database. And I think I might also remove the secondary index (and uniqueness constraint) for Terms and Contexts, especially after I realized that that wouldn't work very well at all for a distributed semantic database, which is what I hope we'll get in the future (at which point we can start calling the system the Semantic Web (the term taking the place of what will then probably start to be known as the `old *(.\,.\,or OWL.\,.) Semantic Web,' or something like that.\,.)). \ldots\ (10:37) Then again, maybe we should keep the secondary indexes.\,.

%(27.05.23, 9:24) Okay, jeg tror jeg vil sløjfe descriptions igen fra Contexts og lade denne i stedet være et "semantisk felt" (rating-baseret).. Men nu står jeg overfor at sjulle overveje, hvordan man måske kan specificere forskellige renderinger af prædikater i titel-teksten, så lad mig lige prøve at tænke lidt på tasterne (måske lidt on--off).. ..Nå, jeg tror måske allerede, jeg ved, hvad jeg kan gøre:

(27.05.23, 9:37) I think I will let the description be a ``semantic field'' instead, i.e.\ decided by ratings. And I think I might know what to do for the predicate titles in terms of rendering them. My idea is.\,. Well, first of all, one could have a placeholder for the relation object in the title text. This could be \$ or something like that. And my new idea is then to first of all use curly brackets around what part of the predicate sentence should actually be printed when the predicate title is rendered (above a ``semantic field'' or above a rating). Then I'm also considering using normal parentheses to further mark that part of the sentence can be left out of the title, if the application wants to render it more compactly. So an example could be a predicate title of: ``is \{funny (as a \$)\}''. The \$ and the parentheses should then be escapable using either \textbackslash\ or repeated characters.\,. (9:46) .\,.\,Hm, repeated characters is actually better when it comes to the parentheses, so let us say that.\,. (9:51) .\,.\,But anyway, I can now see that I also kinda need to know the word classes of the category Terms (which are now no longer a special type in the DB, which is why I don't capitalize it here).\,. .\,.\,Hm, how about just doing something similar.\,.\,! I could thus say that for category Terms, the contents inside the normal parentheses should be printed when wanting the singular word, and the curly brackets should be printed when wanting the plural word.(!)\,.\,. Hm, but how do we then tell the application to use.\,. Oh, but that is for predicates, but still, how do we.\,. oh, never mind: Users can already with this system define predicates like ``is one of the \{Actors (of \$)\}'', and also like ``is the \{Director (of \$)\}''. Nice.\,. (10:01) .\,.\,Hm, let me mention that I intend to use square brackets for when the spec.\ field is a List, and just like for arrays in programming (typically) say that one can write ``\$[1]'', ``\$[3]'', ``\$[1][3]'', etc. (I don't know if we want 0-indexig or 1-indexing.\,.) But another question is: Do we tell predicates like ``is \{funny (as a \$)\}'' to use the singular category noun explicitly somehow.\,. or do we just say that predicates should always use the singular version.\,.\,? .\,.\,Oh, and what about category terms like Music.\,!\,.\,? (10:09) .\,.\,Oh wait, users get to write a specific title for all spec.\ entities,x) so I guess there is no need for.\,. well, trying to account for different word classes. .\,.\,Great, well that should mean that me two ideas here for using parentheses and curly brackets for predicates and for category terms.\,. .\,.\,Oh wait, the last missing piece is then just, I guess, to introduce a syntax for fetching either the singular or the plural noun with \$.\,. .\,.\,Oh wait, there could be a problem with specific titles for specific spec.\ fields because it changes what title to search for.\,. Hm.\,. (10:24) .\,.\,(10:30) Hm, I have a new idea of using a syntax of ``(opt1/opt2/opt3)'' and then use e.g.\ ``\$(2)'' for choosing an option.\,. And then.\,. Ah, and then I could just always use curly brackets to cut away what's not inside and then use a syntax of e.g.\ ``\$\{2\}'' to denote that you should cut the outside part away two times in e.g.\ ``is \{\{funny\} as a \$\}''. Hm, and then I need to escape with \textbackslash\ instead; otherwise it'll get confusing. (10:35) And one could then also make titles like e.g.\ ``is \{\{funny\} as a \{\$\}\}'', in which case \$\{2\} will give us ``funny \$''. (10:37) And I \emph{should} actually use 1-indexing for all these bracket types, even for the square brackets (which fetches an entity from within a list). Nice, I'm quite happy with this, umiddelbart.\,:) (10:39)

.\,.\,(10:57) Hm, but even though I think I will stick to these ideas, I think I can actually avoid the parentheses, i.e.\ ``(opt1/opt2/opt3)'', if I just use ``is \{\{funny\} as an instance of \$\}'' instead.\,.\,:) .\,.\,Hm, although it would be nice with something a bit shorter in the middle.\,. .\,.\, ``is \{\{\{funny\} being\} an instance of \{\$\}\}''.\,.\,? (11:06) .\,.\,Oh wait, let us 0-index, and let (0) then be the option of not choosing any of opt1/opt2/opt3 etc.\,:) (11:07) .\,.\,Yeah, and I do think I will use predicate sentences on forms like ``is \{\{\{funny\} being\} an instance of \{\$\}\}'' a lot.\,:) (11:08)

.\,.\,Ah, and Context can just also just the same syntax of curly brackets to explain how the specifying entity should be interpreted (at the end of their title, preferably).\,!\,. (11:20)

(14:17) I just thought about: well what about the fact that we are no longer able to use a Relation as (partial) input in another Relation in order to e.g.\ say that a field is a relevant one for a certain category. But I just realized that we have our Patterns!\,. Så we can just upvote a Pattern instead, which the application can then use to make one or several searches for, not Relations, but rather predicate (Term) titles.\,!\,:) (14:20)

.\,.\,Oh, and I also realized something else important on the walk that I've just come home from: Actually we \emph{shouldn't} (generally) use objects (in the from of category Terms) for adjective-like predicates after all (I think)! I think it might be a lot better to just let that be a job for the (native) bots, namely to take a set of popular categories and then create subsets for each.\,. well, or perhaps sets of category--predicate pairs, and then make it so that each of these sets represent the predicate conjunction of simultaneously being part of the given category and being rated as fulfilling the given predicate.\,:) (14:27) .\,.\,(So to underline: This means that we will (probably) \emph{not} even need a lot of those ``is \{\{\{funny\} being\} an instance of \{\$\}\}''-like predicates (even though they are smart if we ever need such); instead we can just use: ``funny''.) (14:30)

(31.05.23, 9:42) I thought in the bed last night about how one would query a list of all the users and their ratings for a given predicate and subject. And that lead me to the conclusion that I should actually scrap sets as an entity type and absorb Sets into SemanticInputs. Since we can compress tables, this should still be fine (actually perhaps as good) in terms of getting the rating--subject sets stored compactly on the disk(s). I will then just use the secondary index that I already need ending in a rating, and then let it end in the userID column just before that. This will let me make another query procedure that selects pairs of users and ratings. Since any given subjects is normally unlikely to be rated by more users than in the ten thousands or so, it will be very rare anyway that anyone querying user--rating pairs wouldn't just want to query for them all at the same time. So there is certainly no need to add another secondary index that ends in the user column (not at this point, and it might even never be needed). (9:52) .\,.\,Oh, and for using what sets currently implement, one can just use a Term (with a context that defines it as a set) holding a list of a predicate and a user.

(01.06.23, 11:16) I'm wondering about how much we should use Contexts for disambiguating when we can probably rely a lot on.\,. Oh yeah, using the \{\} syntax will probably be better a lot of the times.\,. .\,.\,Yeah, so let's use that for the ``Terms'' Context as well (but not for the ``Relations'' Context).\,.

.\,.\,Hm, but should I actually use that () syntax as well.\,. (11:26) .\,.\,The () syntax, i.e.\ the one about ``(opt1/opt2/opt3),'' is too complicated, and we should rather just make sentences like ``is a useful instance of the \{Subcategories\} of the term \$'', i.e.\ where the \$ is preceded by noun of what the \$ title refers to, such the the \$ title functions just like that in the sentence: a title (similar to a proper noun). 

(02.06.23, 18:25) I've just had a few good ideas: Users should be able uprate Predicates and Relations for other Predicates and Relations such that the former is automatically suggested for sets (for further sorting/filtering) formed from the latter. And for the General Info page, and for ``semantic fields'' in general, users should be able to uprate Predicates and Relations for a given field, together with the main Relation of the field and the element ContentKey of (the SetList of) that field. But here, they are not just suggestions! They can also be some that are automatically applied to the set initially. And herby the application will already get a bit of the thing that I envisioned for my so-called (by me) ``wiki idea''.\,! This is a very good thing, 'cause I really want scientists and professionals to join in as soon as possible, and this thing about pages that can very in detail when you adjust some predicates, I think that make the application start more quickly to compete with other knowledge sharing sites, such as e.g.\ Wikipedia.\,. .\,.\,Anyway, I see it as a good idea to have implemented in the early versions of the app. There is also one more thing that I will mention here now (.\,.\,if I haven't already), and that is that Users should be able to uprate ``render-as-a''-Contexts for individual Terms, such that these terms, when rendered in the form of a Column, can get another effective Context than the one they are defined from (meaning that several users can agree on using the same Term for giving ratings to, but view it with different rendering preferences). (18:41)

(20:22) Ah, modSignals should just be implemented semantically instead. And at least to begin with when we make the UPA an actual UPA, we should not allow the users to just mix up an use modules at random from a set. Instead we should just confirm the proposed main scripts alone. One could then at some point make it more free, such that the users can import individual modules more dynamically, and not just use the same group of OK'ed main modules, but on the other hand, it might also turn out that users will be able to get the same freedom from just being able to uprate ContentKeys for certain things, like I want to do already from the beginning with the ``semantic page fields'' for the Term's `General info' page. (20:30)

(03.06.23, 10:26) Because I will make it so that only the main modules are OK'ed and adopted individually, I can just serve them from the server, and it also means that they can be minified (each main module and its sub-modules individually). I therefore also plan to move the application source code to the html/src folder, and start splitting and naming the files, following standard conventions. But I guess, should I still keep UPA\_scripts.php and potentially serve some sub-modules for user-made main modules from the database, or should I.\,. Oh wait, just because we keep UPA\_scripts.php, it doesn't mean they \emph{have} to be served from the database. But it's nice that user-made modules can refer to other modules by their entity ID, like how it is for the current UPA\_scripts.php, so let me keep it like that! .\,.\,I will not.\,. use it myself.\,. or wait, should I then still use it myself (as I've done so far).\,.\,? (10:37) .\,.\,Hm, maybe I could.\,. well, if I do both, so to speak, then I have to change UPA\_scripts every time I create a new module.\,. .\,.\,Hm, alternatively I should just introduce UPA\_scripts when we introduce the UPA part of the application.\,. .\,.\,Hm, maybe I \emph{should} just change UPA\_scripts for each new module.\,. (10:48) .\,.\,(10:55) Okay, I think I know what to do. We can actually scrap UPA\_scripts.php completely, and I can refactor the source code of the application in a more conventional way (in terms of files/filenames and directories). When introducing the UPA part of the application, we'll just use a syntax for importing modules via their entity ID, namely where programmers simply \texttt{import [\ldots] from "t$<$number$>$"}. Now, this syntax \emph{could} actually be made to function as is, if one makes some alias rules (i.e.\ via .htaccess for an Apache server), but maybe we won't need to make it function: Maybe we'll just stick to rewriting each import statement whenever a user-made main module, together with its submodules, is validated and adopted. (11:04)

(13:22) Oops, I have forgotten that the text type character is now `x.' So: \texttt{import} [\ldots] \texttt{from "x$<$number$>$"}. *(Or just \texttt{import "x$<$number$>$"}.)

(05.06.23, 17:37) Before I took a walk, here earlier, I happened to look at that elemIDHexStr type in query\_handler, and thought that I should probably rethink my Lists. And on that walk it then occurred to me, that I should of course also just simply implement Lists as Semantic Contexts, like I intend to do with searchable Strings (which I before that intended to implement via the former ``Patterns'' table). So there we go: Now I intend to remove Lists from the database, and implement them, when needed, via a ``Lists'' Semantic Context instead.

\ldots Oh yeah, and I might also then just implement comments by letting them hold the user ID in the comment Text itself (with a certain format).\,. Well, I might do that, or I might use Lists, I'll see when it comes to that.\,. (18:18)

(07.06.23, 13:42) Okay, I think that the whole list of predicates in the drop-down menu for the SetHeader should be set globally. I then think that.\,. .\,.\,Hm yeah, I think that users should then uprate Lists of.\,. well.\,. (13:47) .\,.\,(13:55) Okay, ideally we should actually have several sets of predicates that can each have different user weights attached to them.\,. .\,.\,And of course we should then just start by using only one of such sets.\,. .\,.\,How about just numbering these predicate sets.\,.\,?\,.\,. (14:02) .\,.\,Perhaps by simply changing a number in the Predicate title, and then just halt the predicate set search once the first predicate title number is reached where there is no predicate in the set above a certain rating.\,. (14:06) .\,.\,(14:10) Ah, or perhaps one could just (potentially) divide the rating range up into different intervals, with different global user weight variables associated with them (and each with some default user weight values that the variables start with).\,.\,!\,.\,. .\,.\,Oh, but what about reoccurring predicates, then?\,.\,. Do we just not allow that?\,.\,. (14:12) .\,.\,Sounds reasonable enough to not allow repeated predicates (with different userWeight arrays).\,. .\,.\,I think this might be the solution, and then we should of course just start by having only one userWeights array for the whole rating range (above a certain threshold, that is). (14:14)

(08.06.23, 9:57) I'm going to make SetField more versatile, and make it so that one can also give it input sets to mix with whatever it is going to query for. And I'm actually removing the SetHeaderDropdownMenu completely, making the sorting predicates something to be stored globally instead. Then the SetHeader should just have a checkbox for if you want to include the extra sorting predicates or not (whose default value can depend on data input). And at some point, there could also be a number to choose, which then chooses which of the user's stored sorting preferences to use. \ldots\ (And then I should make a sorting options Column where users get a fixed number of maximal sorting options to store, and when they can then define and change/adjust each one (where the number is then what you chose in a given SetHeader).)

(12:58) I just had the thought: Wouldn't it be much better if we simply had a FULLTEXT index on the Term titles instead of having the KeywordStrings table? (Especially because it would allow us to treat the search result as a set, which can then be combined with other sets, without having to make a (relatively) expensive join first.) I should think more about this *(later, that is).\,. 
\ldots Hm, or we could let KeywordStrings hold its best match (decided from a.\,. predicate + user.\,.).\,. and then continuously update it.\,. Hm, or not.\,. (13:14)

(16:12) Okay, I'm thinking about several things now, but let me just mention a thought: I probably won't even need the Lists (Context) for Comments, since the user can just be looked up in the Creators table instead.\,!\,.\,. (16:13)

(17:46) I've thought and have had some good new ideas. %..It's very hot right now, though, but let me see if I can write them down now.. ..Hm, two seconds.. ..Hm, let me just think about a certain detail concerning the database first instead.. ...(18:03) Ah, maybe I can just use spec_type = /^[0-9]$/ for rankings.. *(spec_entity_t) ..I've btw also just figured out how to perhaps use the parent contexts.. (18:07) ..No, I shouldn't use spec_entity_t; I need to make a new table, call it Strings.. ..(or KeywordStrings..) (18:17) ...(18:47) Ah, and I'll make the the spec. entity into the title (removing the title column) of Terms, and then its entity type will just often be Strings.!:) ..(And predicate titles will then be stored as part of the parent Context, but I'll get to that (and more), when I resume the rendered notes..) ..(I think that I will not include any FULLTEXT indices, by the way..) (18:51)
%(20:56) I'll keep the Term titles, such that the Strings table will only be used for searches --- well, no: they will also be used for uprating relations for e.g. the General Info page. And I probably won't use the {}-tyntax at all now; instead each layer should just be a Context (with the layers above being parent contexts).

%(09.06.23, 10:41) Ah, MySQL always uncrompress the pages before reading and.. well, not necessarily before writing, but still, I'm pretty sure that the compression does not depend on which bytes on the page belongs to which columns. And this means that I should probably not worry about splitting SemanticInputs up, or about adding e.g. a context_id to it.
%(13:02) Have realized, that instead of the RankedStrings table, that I've been about to make, I should just use VARBINARY(255)'s for ratVals once again, especially because of the fact that compressed pages of sets with a constant ratVal length can be compressed such that the length byte doesn't really matter. And the big idea for going back to VARBINARY(255) is that it allows us to make sets over Terms, where the ratVals can act as index keys for the Term titles/def_str's.!:D (13:07)

%(10.06.23, 11:09) I'm probably gonna change the database again by letting data entity IDs match the IDs of the (then required) Terms that wrap them, and now I'm perhaps also going to merge SemanticContexts and Terms.. But I just had an unrelated thought, which is the reason that I'm writing here: If the CPU always needs to uncompress the InnoDB pages in memory, then perhaps it \emph{does} makes sense to factor out the set key, still.. It seems that it would.. So yeah, let me actually do that.. (11:13)

\ldots

%(14.06.23, 8:25) Jeg fik ikke sovet så meget i nat (sikkert meget pga. varmen (ikke mindst pga. manglende vind)), så der går nok lidt tid, før min hjerne kommer op i nogenlunde ordenlige omdrejninger. Men jeg kan da prøve at starte dagen med så at skrive lidt noter her i stedet (bl.a. nogen som jeg fik tænkt her tidligt i morges i sengen)..:
(14.06.23, 8:29) I never got to write all my ideas from last time down, not even in the source comments. I have some more now, which I will try to write about now.\,. .\,.\,First of all, I recently planned on having a SortingOptionsColumn, but now that I've realized that the basic ``SetHeader'' should be hidden by default (unless the (combined) set is empty), I've also just come to the conclusion, that I can let the users mange the sorting options there. So now I imagine a drop-down menu of bars each containing a predicate, a ratTransFun option, queryParams options, and userWeight options. Firstly there should be the predicates which the SetList is based upon. And if the user clicks a button to extend this menu with sorting options, there can be first a new bar with a sorting option selection menu (where each sorting option has an identifying number or, more likely, an identifier string/name). Then there is a number of similar predicate + etc.\ bars, depending on which option identifier is chosen. Now, when the user then changes these sorting options (the bars below the initial ones), the data should  be stored in memory, and there should then be a button at the bottom to ``save changes'' to the given sorting option. %(8:42) ..to sek.. ..tilbage. (8:49)
.\,.\,And when the user saves, I then need to implement that the Sorting Options are then uploaded and stored in terms of ratings to certain predicates. Hm, which reminds me that I should make a Statement Context.\,.

.\,.\,Let me then also write about some ideas that I thought about last evening. I thought about the browser expansion, and the fact that I should really just advise that users use Google or other SEs to search for known things, and then use the browser extension to see the SDB entries on the URLs that the search hits (and for users to uprate associated terms for the URLs that might pop up on Google searches). At some point we can also implement a semantic word search on-site, by letting users type in search strings and then break these apart into several predicates of ``is related to this word.'' Then the users should basically see these the same as for the Sorting Options, where they can change parameters for each word, but where they can also correct/change the words themselves --- and add new ones to the list. But this is for the future: In the beginning we should rather just make use of existing web SEs and then the browser extension, like I was just saying. Now, some ideas about the browser extension is then that it should not only look at the current URL of the sites that you visit, but it should also take all anchors (links) on a webpage and make it so that hovering over them, perhaps wth the shift key held, should make the extension window, if open, take the given URL and make a search (via a special URL Column) and show a list of the relevant terms of that URL (with ratings meant to signify how much these terms are related to the URL --- and/or perhaps (also) a rating to signify how useful it currently is to look at the info of the SDB of the term.\,. well, maybe that's to complicated, but anyway.\,.).\,. The good thing is that this can then also be used for seeing if links are safe, and it can be used to, anonymously!, gain information about a link without having to query the WWW for it.\,! .\,.\,I really think this good be seen as quite useful for a lot of people, and be a good selling point of the app/browser extension. (9:07) .\,.\,It could even limit the nuisance of well-known phenomenon of clickbait a little further down the line.\,. .\,.\,Yeah, I really think that a lot of users might be quite happy about such an extension, and that it good be one of the SDB's good ``selling points.'' And since this kind of browser extension also then makes it easier to search for things, since you can just get as close as possible at first via a quick Google search, and then ``walk'' the rest of the way ``semantically'' (i.e.\ via the ``TermNavigation'' tabs).\,:) (9:13) 

\ldots By the way, the menu that I talked about should also at some point be able to contain potential filter predicates, which are not used by the SetList (and thus not part of the ``predSetDataArr''), but which can subsequently be looked of (or rather, the ratings can be looked up) for each list element, and these elements can then be collapsed if the rating for the/a filter predicate is above or below a certain limit. (9:35)

\ldots\ (11:47) When uploading and saving the Sorting Options, we should probably save them in the form of a JSON object (as a Text, which is then the subject of a Predicate holding the Sorting Option identifier as its def string). Luckily, such a JSON object can be verified non-recursively with a RegEx, which is nice, 'cause a will likely need to implement this before release.

%(13:10) Hm, I just got the thought that maybe I could remove the Instances Relation and make it the implicit Relation whenever you use a non-predicate as a predicate.. Because doesn't that also sorta fit with saying that subjects fulfilling a predicate are.. hm, a sort of instances.. maybe not.. ..Hm, but maybe I could do it still, and then just say that "Instances" in the context of a predicate refers to "Instances that fulfill the predicate to some degree" (a negative degree as well..).. ..Maybe.. (13:14) ..(13:17) Uh, if the standard interpretation of a predicate/non-predicate term + subject/instance term is: "aplies to"..!! ..:) That could work..!.. (13:18) ..(13:20) Or perhaps better, if I just change the tab name from "Instnces" to "Applies to"..:)..

(15.06.23, 10:11) I got a very overview yesterday of what I need to do for the main part of the app before the first release. (I didn't get much sleep, so that pretty much all I could do: think about things.) I also got the idea to make Lists a part of the initial semantics almost, namely by introducing a \$s[$n$] ($n\geq 0$) syntax for the def strings, where it is then understood that the format of such strings should be a comma-separated list of decimal integers denoting Term IDs. Then \$s[0], for instance, takes the first ID in the list and ``de-references'' it, both semantically and in terms of how the def string should be rendered.

About what I wrote in the source comments yesterday, the semantic inputs should indeed have their semantics slightly changed, or rather extended (in a backwards-compatible way), such that the statement that is rated is ``$x$ applies to $y$.'' Now if $x$ is a predicate, we can see that the semantics is unchanged. But if $x$ is another term such as a category term, we see that the statement can now be seen as being semantically equivalent as the ``Instances'' (i.e.\ ``is an instance of'') relation taken between $x$ and $y$. This means that we can then make an ``Applies to'' tab, which uses the same functionality for both predicates and non-predicates, implementing the predicate set page for predicates and the Instances page for non-predicates.\,.\,!\,:) So there we go, that is what I will do.\,:) (10:28)

.\,.\,(10:39) So for Terms Column, or rather non-User/-Text/-Binary Term Columns, we should just start with the following tabs: Info, Ratings, Subcategories, Applies to, Related terms, and Context. For the other Terms, lets call them the Data Terms, the ``Subcategories'' and ``Applies to'' tabs should just be removed, and the Info tab should just hold the basic data and nothing else, initially. (And let me underline that I'm only talking about the first release version here.) Now, about the non-Data Terms, the ``Subcategories'' tab should of course just be a set view of the ``Subcategories'' relation, and similarly for ``Related terms.'' The ``Applies to'' tab should be almost the same, but here, the Term of the Column \emph{is} the ``predicate'' of the set view, directly.  The ``Ratings'' tab should include one set view of two predicates combined, namely the ``\{Relevant ratings \}generally \{of \}Terms which are derived from the \{Context, \$t\}'' predicate (where \$t is then the Context of the Term of the Coulmn), and the ``\{Relevant ratings of \}the Term, \{\$t\}'' predicate (where \$t is the Term of the Coulmn).

The ``Context'' tab should include set views (with not too large initialNums) of the predicates that are relevant for Terms like that of the Column and all other Terms derived from the Context, in particular the ``\{Relevant ratings \}generally \{of \}Terms which are derived from the \{Context, \$t\}'' predicate as a good example. It should also include predicates used for the ``Info'' tab.

*(Oh, I need a ``Supercategories'' tab also (which can come after ``Related Terms''.\,.).)

The ``Info'' tab should contain a set view of all relevant ``semantic fields'' of the Term, which like the ``Ratings'' tab should %(10:59) to sek.. ...%(11:17):
\ldots (11:18) should consist of two predicates: one for the Context in general, and one for the specific Term at hand. I imagine that these predicates should all be ``noun predicates,'' such that what is uprated for these two predicates are in fact simply the ``Nouns for predicate definitions'' (see initial\_inserts.sql). The elements of this set view is then PageFields that contain a title, given by the noun in question and then a set view of the relevant predicate --- oh, and with initialNum also given via user ratings, so.\,. Hm, how do we actually do this (without getting repeated fields).\,.\,? (11:23)
\ldots (11:36) Oh, well, I'll perhaps figure this out later. I'm thinking about using a separate rating for each Term--noun or Context--noun pair.\,. wait, or maybe it is much better to just use the noun for that rating.\,.\,! Yes.\,.\,! Okay, so I'll do that.\,. .\,.\,And users can then go to the Term Column of a given noun to rate what initialNum it should have. But in the first version, I think I'm simply going to let all semantic fields start with one element (i.e.\ initialNum = 1), and then perhaps have a rather large incrementNum of e.g.\ 20. And then each field should just be collapsible, perhaps such that one only sees the title initially, and not even the first element. Yeah.\,. (11:42) .\,.\,Great. And let me note, by the way, that I will not implement any searching among these ``semantic fields'' for the Info page as part of the release version of the application. (11:44)

Okay, now on to the SetView and the topic of how the users should be able to sort the sets. I've decided, first of all, that the SetHeader is always hidden at the beginning (showing only a small bar with a drop-down button). When dropped down, this header should contain first a menu of the initial predicates of the set and then a menu where users can add additional sorting predicates. For a later version, I also intend to add a menu to add ``filter predicates,'' but let us just leave these ones out of the first version (not because they are hard to implement, but because they won't be too useful initially anyway). Now, until yesterday, I thought I needed to implement a way for users save various sorting options that they could then choose from later on. And while I want to implement this at some point, I finally realized at some point yesterday, that because the initial options for choosing the ratTransFun and the queryParams should be quite limited anyway, there is really no need for the users to be able to save their adjustments. So instead I can simply just use a set of ``Predicates that are useful for Sorting Options for sets''.\,. hm, what about Relations, by the way, will we need such for sorting?\,.\,. (11:55) .\,.\,Hm no, I actually don't think we do (at least not for the release version). If users want to e.g.\ sort using a predicate of ``is related to Term $x$,'' then they can just uprate that whole predicate.\,. well, that's the only thing to do here anyway.\,. .\,.\,Oh well, I think that Relations (that then automatically uses the Term of the Column as the object) won't be needed in the release version, so let us just say that for now. The users should then simply uprate predicates for.\,. well, okay, now the question is: Should they uprate it for a \emph{single} predicate of ``is a useful sorting predicate,'' or should it be ``is a useful sorting predicate for Terms derived of the Context, \$t'' instead? I think what I will do is keep the second option open by simply using ``is a useful sorting predicate for Terms derived of the Context, \$t'',\$t = the ``Terms'' Context, as the single predicate to uprate sorting predicates for. And a future version of the application can then extend this to also look for sorting predicates uprated for other relevant Contexts of the given Term. Yes, let us do that!\,:) (12:04)

Now, the reason why the setting won't be complicated enough that the users need to be able to save them, is that initially they should only include a couple (not all) of the possible queryParams, and they should only use ratTransFuns on the linear form $f(x) = a x$, where $x$ is the rating when interpreted as an \emph{unsigned} (i.e.\ non-negative) integer. *(Oh, or interpreted as a singed integer; that actually makes no difference.\,.) Then for cutting ratings out below a certain threshold, the user should just simply adjust ratingLo instead. (12:09) .\,.\,Oh, and importantly, I will only use one kind of userWeightArr for the release version --- and only use one ``user group'' bot as well --- and that is the userWeightArr consisting of the query user (times $\sim$$\infty$) and the ``zero-biased unweighted average bot'' (times 1). So there will even be a need to for any menus regarding choosing users and weights for the sorting options in the release version.\,:) (12:14) All, the predicate menu points of the SetHeader menus therefore need for the release version, is just: a factor for the ratTransFun (rating transformation function, btw), an isAscending check mark / radio button, a ratingHi and a ratingLo, and possibly also a queryNum.\,. yeah, I guess so. And most of the times, the user will only need to adjust the factor, and perhaps the ratingLo.\,. and that's really it.\,. :)

The SetHeader should btw also cotain the button to add another predicate bar to the sorting options menu, and each predicate bar should have a button to select/change the given predicate, selection from the list of predicates taken directly from the (single) \textit{``is a useful sorting predicate for Terms derived of the Context, \$t'', \$t = the ``Terms'' Context,} predicate. (12:23)

(12:49) I forgot to mention/think about the contentKeys of the semantic fields.\,. Now, I'm wondering about allowing repeated predicates/field titles versus not allowing different contentKeys for the same predicate/field title.\,. 

Oh, by the way, once we want to allow for more complicated ratTransFuns, I also thought yesterday about defining these via an alternating list of slope factors and x-axis values, such that we start from 0/-1 (depending on if it's unsigned or signed) and then draw a line with a slope slope given by the first number until the rating (x-axis) value of the second number, if the list does not just contain the single number, in which case it is just a linear curve. From there the list can continue with factors and rating values (alternating), and if the list ends in a factor, the curve is extended to the end of the axis with that slope, and if it ends in a rating value (i.e.\ an odd number of numbers in the list), then the last slope is just automatically assumed to be 0. There we go: an easy way to define a segmented curve of line pieces, which is also quite easy to compile into a (JS) function.\,:) (13:00) .\,.\,Hm, alternatively, one could of course also exchange to slope numbers for numbers representing the value at the end of the given line segment (with will be at the end of the whole interval if it is not followed by an x-axis value).\,. Perhaps that's even better.\,. (13:03) .\,.\,One could then also consider starting the list with an y value at the initial end of the interval, although this means that the list won't be backwards compatible with having just one number representing the slope.\,. although one could just superimpose that interpretation whenever the list consists only of one number.\,. (13:06)

\ldots (13:54) I have it: I don't think we'll need the contentKeys for the initial version, or rather the users won't need to think about them. I should instead just let the contentKey --- and the initialNum and queryNum --- be decided by the Context of the predicate instead.\,! And then I should first of all divide ``Nouns for predicate definitions'' up into two subcontexts: ``$>$\{s.\}'' and ``$>$\{pl.\}''. For predicates form both both these kind of nouns, I will then choose the standard TermElement contentKey, but let both the queryNum and the initialNum be smaller for singular nouns *(1 in the case of the initialNum) and greater for plural nouns. And for HTML semantic fields, which are supposed to act much like subsections of an article, as if the Info page is a composite article as a whole, I should make a completely different Context to make predicates of that kind. And queryNum should also be small for these, and initialNum should of course also be 1. There we go.\,!\,:) (14:01)

Some other points that I should mention are first of all that I just (just before the little walk that I took) had the idea to use \$l[$n$] instead of \$s[$n$] for the List syntax, saving \$s[$n$] potentially for another interpretation where the def string consist of a list of strings (which are not supposed to be de-referenced in their interpretation, as opposed to for the \$l[$n$] syntax). Let us do that.

Another point is that, even though I have made the Statements Context in initial\_inserts, I will not implement the part of the application for the release version, where users can click on ratings and see a list of users + ratings, although it should be one of the first things that I implement afterwards, preferably before I start implementing more ``user group'' bots, etc. (14:06)

I should also mention, importantly, that I will only implement unions for combSets, not any intersections at all, for the release version. So that removes that Sorting Options setting as well from the (initial) table.\,:) (14:08)

.\,.\,I haven't talked about this in my notes, but I will also make a HTML Context, which should then only be able to contain very simple HTML tags (otherwise the application will not print the text). (14:12)

.\,.\,And I think that these were all the points that I meant to mention for now.\,. no! There is another point, and that is that apart from the regular TermColumns and the Data Term Columns, there should.\,. well, no it \emph{shouldn't} actually be a Column, that was the point. It should instead be a quality of the Interface header, that there is a string search bar meant to take you directly to a String Term, if any one can be found that matches the string. I will then also use this procedure (automatically) for my browser extension, where a newly input URL will make the extension window update itself with a new Column, namely that of the String Term, if a match was found. .\,.\,I guess I should also think a bit more about how to handle non-exact matches, but I'll leave that for a later time. Let me instead just say now, that I imagine that the String context should work splitting the string up into strings of length 254, and then add/look for the series of Contexts where each subsequent Context in the series has the former Context as its Context, and where the def string is the given substring of length $\leq$ 254 prefixed with ``$>$'', and where the series then end in the last substring where ``$>$'' is then \emph{not} prefixed (signifying that the final Term is not understood as a sub-Context). The initial parent Context of this series should then simply be a ``Strings'' Context. .\,.\,Well no, it could also be a sub-Context of ``Strings,'' e.g.\ ``Strings''$\rightarrow$``$>$URLs''. (14:26)

(18:02) The ``Applies to'' tab should also have an expandable submission field at the top.

(19:14) I should also point out that the lookup of a String should be implemented with a special procedure that can look up a whole string at once and give the ID of the final Term. And on another subject, I think I'll actually go away from the $>$ prefix syntax and back to using a ``Context'' Context again. Oh, and another important point: The ``Context'' tab should not only hold predicate fields relating to the Term's Context, but should also hold a description (as a semantic field, i.e.\ changeable) of clarifications about how to interpret Terms derived from Term of the Column, i.e.\ Terms that has the given Term as their Context. But for Terms with the ``Context'' Context Term as their Context (.\,.), these should probably also just have this as their (only) field on their Info page.\,. .\,.\,If so, then ``Context'' Terms will also be special in terms of what tabs there are and what appears in them, just like the ``Users'' Terms, the ``Texts'' Terms, and the ``Binaries'' Terms.\,. (19:22)

%(21:37) And note that "Contexts", or rather "Sub-Contexts", is only meant for the Terms that cannot and/or is not supposed to be interpreted in the context of ther Super-Context themselves. So for Strings, e.g., I should not use the "Subcontexts" Context, since each Term in the series can also very well be interpreted as strings themselves (and in fact the should be).

(16.06.23, 9:58) One of the absolute first things that I should do after completing the first version of the app is to add a ``freshness'' and a ``trending'' bot, and add Comments (a Context and a tab) and not least Posts (a Context) to the site. And I should also either make the HTML be able to include images and small videos from other green-lit sites, and/or make a new type of Predicates for images and videos (similar to the HTML Predicates).

%..(10:02) And I'm also actually considering simply promising out money, via ratings of users signifying the amount, to users that help add content to the site and rate things, and for any open-source developer that helps extend the app on their own time.. ..(Of course those would be money conditioned on whether there will be a surplus to give from (which there very well should be), that goes without saying..)

(10:28) Oh wait, maybe the ``$>$'' syntax is just better\ldots\ \ldots Yeah, it is better.\,. (10:47)

%(12:23) Hm, thinking about how to reimplement computeAveragedSet() + computeCombinedSet().. Maybe an option could be to first transform the rating in place, and then average afterwards.. ..Hm, perhaps that's not such a good idea.. ..(12:31) Oh, maybe I should actually even rethink the bots and the userWeights system slightly.. ..Hm, I'm sorta considering introducing special "user x says" predicates, and I'm also considering using a priority list, then, rather than a userWeight array.. (12:34) .."user $s[0] says that $s[1] applies to $t".. (12:36) ..No, that's way redundant.. (12:38) ..But the app could just use predicate--user pairs and then automatically have the user either first or just after the queryUser in the priority list.. (12:39)

(12:41) I'm considering (see the source comments) using priority lists rather than user weight arrays.\,. .\,.\,(12:48) Yeah, I think I will use priority lists instead, 'cause then if users want to mix several large user groups, they can just add the same predicate several times.\,. or we could in the future also implement a way for users to choose more than one ``default user( group)s.'' (12:50) .\,.\,Hm, and at some point, we should add a switch to the predicate settings such that the specially chosen user (for the given predicate setting) can get higher priority than the (main) ``query user,'' but let me just start by always giving the ``(main) query user'' first priority.\,. (12:53) .\,.\,Hm, or the other way around.\,.\,? (13:02) .\,.\,Hm, or maybe just replace the special priority user \emph{with} the main query user.\,.\,? .\,.\,Yeah, maybe.\,. (13:05) .\,.\,Hm, \emph{if} a should even go away from using the userWeights at all.\,. (13:08) .\,.\,(13:17) Oh, we actually \emph{want} to ratTransFun to be applied \emph{before} the weighted averaging.\,!\,.\,.

%(14:48) Lige kommet hjem fra gåtur. Det går ikke så hurtigt med tænkeriet, som jeg gerne ville have haft det, men jeg har da fået tænkt nogle idéer. Der er dog stadig lige nogle ting, som skal overvejes. Kan være jeg vil prøve at tænke på tasterne her lige om lidt.. 
%(16:34) Ah. I think I almost got it..
(16:48) I have to use user weights.\,. (And then one can just combine that with a priority list by using some $\infty \times n$ type of ``numbers''/cardinals/ordinals at some point.\,.)
%..Hm, jeg har virkeligt ikke nået meget i dag..:\..

(17:44) Okay, I've got it, I think.\,. The SetLists should not care about priorities or anything like that. In fact, because I now want to take the ratTransFuns first thing before combining, doesn't matter how the pred + user pairs are grouped, anyway.\,:) So the SetList CL should actually just use an array of pred--user-ratTransFun--weight (plus some more stuff) tuples. Now, I need to implement a freshness and a trending bot pretty soon after release, but here I can first of all just say that all uprated predicates for sorting should instead be predicates \emph{or} predicate--user pairs.\,. Hm, something like that, yes.\,. And in terms of the Info page, one could just use a special context for semantic fields that should automatically include the trending and/or freshness predicates.\,. Something like that.\,. There well luckily be time to figure this out; even if we get a false start with some system, it will not be the end of the world to then have to change that system to something else.\,. But anyway, let me note that we should aim for a system in the future (for the application, that is) where each user can select a broad range of ``user groups'' an adjust each one's weight for querying. The each ``user group'' should further not only rate Terms in relation to predicates/relations, but should rate the predicates/relations themselves with a weight, which defines how much the user group's ratings of those predicates/relations should count. This will be important when we get to ``user-driven machine learning (ML)'' as I've often called it, where ``user groups'' can start representing correlation vector. But for any given correlation vector, there will also be a limited rnge of predicates/relations that that vector.\,. ``hits.'' Or in other words, the relationship between a correlation vector and each predicate/relation will vary in strengths, so to speak.\,. (My brain is not on its sharpest level rn, btw, which one might very well be able to tell.\,.) And an honest UD-ML user group should also limit its influence only to the predicates that matters for it, via weights. So therefore the user groups should also rate the predicates/relations themselves with how much they the user groups opinion ought to count in the combined sets. For small weights these should just be rounded down to zero, then, in which case the app can simply remove the user from the pred--user-ratTransFun--weight (plus more) tuple that is passed on to the SetList. (18:05) .\,.\,Yeah, and I think that was it, really; what I needed to say, except perhaps that this future system might then also use special sets of predicates, for which this system should be overwritten. This could for instance be for the ``trending'' predicate, where one might not want to bother about querying each ``ML user group'' for a weight that is expected to be 0 anyway, and where one would probably instead just want to simply query a special user/bot for their rating only. (18:08) %Okay, I give up for today. Hopefully I will be sharper tomorrow, and luckily reimplementing computeCombinedSet() will probably be quite easy now.. (7, 9, 13, of course..)..

%(18.06.23, 22:46) Åh, jeg har havde da nærmest helt glemt mine "simple brugergrupper" (altså dem hvor "tokens" er fordelt)..! Nu har jeg lige tænkt lidt på dem igen, og de kunne da muligvis være \emph{vildt} nyttige..!(!).. ..Tja, i hvert fald til visse ting såsom at fordele tillid, men lad mig da lige tænke over, bare her i løbet den kommende tid, om ikke man kan bruge det på en smart måde til alle mulige andre ting også (hvad jeg før har tænkt/troet, og hvad jeg da lidt stadig tror på umiddelbart).. ...Hm, tja tjo, måske gør det ikke så meget forskel..

%(19.06.23, 18:26) På overfladen ser det ikke ud til, at jeg har nået så meget i dag, men det synes nu på en eller anden måde, at jeg har lidt alligevel; ikke meget, men en ok mængde, især taget i betragtning af manglende søvn.. Nå, men jeg kom egentligt her for at brainstorme lidt over.. eller i hvert fald bare lige nævne, at jeg lige er kommet til at tænke på, at måske bliver "brugergrupper" egentligt ikke.. eller rettere, måske kommer der ikke til at være så mange af dem alt i alt, som altså også er gavnlige, for måske kan man opnå det meste med bare nogle få forskellige BG'er og så bare nogle virkeligt specifikke og varierede prædikater (og relationer).. Har ikke konkluderet noget omkring det, men er altså det, jeg lige har tænkt mig at tænke lidt over.. ..Hm, på den anden side: Det vil tiden jo vise (er der virkeligt grund til at tænke særligt meget over det nu? --- for de vil jo være gavnlige under alle omstændigheder i én eller anden grad..).. ..Ja, de bliver rigtig gavnlige (..sikkert \emph{meget})..

(20.06.23, 10:04) I'm considering going back to using shorts/smallints for ratings again. If I can then only find another way to implement a lexical index semantically\ldots\ \ldots Well, the question is of course, haven't I already kinda done that with my Strings (especially my intended Control server procedure (I think it should be implemented on the Control server rather than as a database proc., but it could also be database proc.\ instead.\,. in fact, that might be better.\,.) that can also give you the (concatenated) String Term immediately for strings of length larger than 255).\,.\,? (10:23) .\,.\,Hm, only problem is auto-completion, isn't it.\,.\,?\,.\,. \ldots (10:52) Oh, I think I might do something else. I think I might create a new basic Data type called Indexes/Indices.\,. .\,.\,Indexes, where one user controls a given Index. .\,.\,!\,.\,.\,:) .\,.\,Oh wait, or an Index could instead be created upon request (from the userbase) with respect to a certain Set (i.e.\ a Predicate Term + a User).\,! .\,.\,And then the database would have to schedule reoccurring events to update the index with all Term titles of Terms above a certain threshold in that Set.\,:) Okay, I'll do that! But it might be a bit advanced, so I have to figure out what I want to do in the meantime, if I want users to be able to search through predicate noun( lexeme)s for the semantic fields on the Info page.\,. (11:00) .\,.\,Hm, well the best option seems to be to just implement the Indexes table and make one (including the scheduled event) specifically for predicates noun lexemes.\,. So yeah, let me just make the app first with out any search bar.\,. well, except the string one.\,. Ah wait, but couldn't that also be an option for an early version of the app, to use the String searches.\,.\,? (Not that it will be a lot of work to make the Indexes, but let me think about it still.\,.) (11:07) .\,.\,Nah, I need to make the Indexes to make it work.\,. .\,.\,I'll then make an Index key query procedure that outputs a table just let a Set output, but where ratVal is then changed for the word string (and where the subject is the given Term that corresponds to the string, in particular the predicate noun lexeme Terms for the specific case that I just mentioned). (11:14)

(21.06.23, 9:46) Hm, maybe it's not actually worth it to have all this syntax stuff for the Terms; not worth the confusion/trouble to learn for new users.\,. Especially since all it does is to disambiguate some Terms that people won't really.\,. go to (i.e.\ not navigate to the Terms' Columns) much anyway. .\,.\,! Yeah, so maybe I should cancel all that.\,.\,? \ldots I'm talking about the prefixes of course; we still need \$s,\$l,\$t and \{\}.\,. .\,.\,Hm, I could remove `/', but it's more difficult to remove the others.\,. .\,.\,Hm, by the way, maybe I'll remove the ``Noun lexemes for predicate definitions'' Context.\,. (10:20) .\,.\,(10:27) Hm, maybe it will be a lot easier to learn if we just introduce a tinyint flag to the Terms table, 'cause then we can present the options easily as a selection menu in the Term submission fields.\,. \ldots Hm, or I could just do that anyway, keeping it like it is now.\,. (10:39) .\,.\,Oh! Can't I make concatenated Strings by using the def terms instead!\,.\,?\,. .\,.\,Yes, of course.\,.\,!\,x) (10:44) .\,.\,And I could also just go back to using that ``Subcontexts'' Context again, if I want to eliminate the other prefixes again, right?\,.\,.

\ldots (11:44) It's actually a kinda hard question. But I think I might remove the \{\} formalism except for Subcontexts in particular.\,. .\,.\,Let me by the way mention something else:

At some point we could make use a predicate to determine what (requests) should be cached in local storage. Each user can then OK this system and choose which user (group) to query about the predicate. And then the app could automatically store various (common) requests in local storage to increase its speed.

(11:49) Okay, I'll make the ``Subcontexts'' Context again, and remove the `/' and the \{\} syntax, except for the Subcontexts which keeps the \{\} syntax. .\,.\,And is that it.\,.\,? Oh, and I'll also add checks to insertOrFindTerm() to make sure that the context and the def.\ term each exists if not null. .\,.\,Oh wait, I should only use the ``Subcontexts'' for the `:'-type Terms (see my current code (initial\_inserts) and my last couple of commits), i.e.\ those that use the \$s,\$l,\$t (and \{\}) syntax, so maybe I should call it something else.\,.\,? (11:56)

.\,.\,Hm, why did I get rid of the Contexts/Categories table in the first place?\,.\,. Well, because it could be nice to just let Categories be regular Terms as well, but let me think about introducing Contexts again.\,. (12:07) .\,.\,Oh, and it gets rid of the type (char) prefix, so that was a good decision.\,. .\,.\,Uh, but I could make Contexts work like Texts and Users such that they share the ID range with all the other Terms.\,. (12:10) .\,.\,Yeah, how about that; then I'll make a ``Contexts'' Term, which users \emph{can} add Terms to, and in fact, they can \emph{only} add Terms that has a Context as its context. Then the only other type of contexts are the Users, Texts and Binaries, for which users can not add Terms directly (only indirectly via the appropriate procs).\,. Hm.\,. (12:15) .\,.\,Oh, and I'll use the \$s,\$l,\$t and \{\} syntax only for Contexts.\,. .\,.\,Hm, this does seem like the right choice.\,. the right way to go.\,. (12:17) .\,.\,Oh, and the users can add Terms with ``Contexts'' itself as the context.\,. well, but I might as well also introduce a special procedure for that.\,. well, perhaps.\,. (12:18)
.\,.\,(But I don't need to add a separate table for Contexts, as they can just be stored in Terms.\,.) (12:25)

\ldots\ I have just changed the database according to these plans, but now I'm wondering if I shouldn't turn ``Contexts'' into ``Subcontexts'' and then allow all Terms as contexts again.\,. (14:41) .\,.\,Hm, why on earth is this so hard to figure out.\,. .\,.\,(Well, I kinda know why, but still.\,.) .\,.\,(Subtle semantic ambiguities and redundancies can just often be quite hard to clear up when it comes to such systems.\,.) .\,.\,(14:48) Well, maybe I actually have cleared it up enough right now; maybe we \emph{should} just use two different Terms e.g.\ for the regular Term ``Music'' and the Context Term ``Music''.\,. .\,.\,Sure.\,. (14:51)

(16:17) Oh, maybe we can get the best of both worlds, if both Context and regular Term Columns have a tab to go to the other, namely where defTermID is toggled between 0/NULL and the parentCxtID/cxtID, and where the cxtID (table column, i.e.) is toggled between the parentCxtID/cxtID and 1.\,.\,!\,! .\,.\,Yes!\,:\texttt{D}\,.\,. (16:23)
.\,.\,Hm, yeah, one could do that, but I shouldn't implement that for the first version, I guess.\,. Hm.\,. (16:30)

.\,.\,Wait, what was wrong with allowing the subcontext--instance ambiguity (now that we no longer need the string syntax, and are only using special syntax when it comes to Contexts).\,.\,?(!).\,. (16:40)


%(23.06.23, 14:07) I've not done anything today, really, other than to think about the algorithms for combining sets and such.. But it turns out that this was important enough since I've just realized that I should avoid.. Well, I should not be sorting and combining large arrays in one algorthm (regardless of whether the app should query for the large sets or not.. well, probably not; the app should actually only even query for a small part of each set to begin with.!..).. So I'm hereby realizing that the efficiency of the combination methods actually won't be important after all..(!..) ..Yeah, this is very good news.. ..Oh, and I also shouldn't worry about letting the SetMangeger keep the various ratVals, again because this data will be quite limited in size now. (14:15) Hm, so I guess I should just make a more flexible implementation of the SetManager, perhaps by using recursion and letting SetMangegers also hold an array of SetManagers, instead of just a setData array (which the "leaf SetMangers" will hold..)..? (14:18) ...Oh, and I can't let ratTransForm deal with missing subjects in a set, like I thought a bit about, since we can't really tell when subjects are missing with this approach.. ..So the combined score/ratVal can only be accumulative, really.. ..Hm, which means I should use an algorithm much like my current one but without letting the missing subjects count, right..? ..Hm, and no need for recursion, so it could be a lot like my currunt alg., before the missingWeight stuff, except that I now also need to seriously consider how to extend combined sets.. (14:36) ..Oh no, maybe I should do something more complicated, perhaps with recursion.. (14:38) ..Hm, or maybe I'm wrong about not needing to query much of any set at the same time; maybe there will be some sets where we, in some situations, want to query a big part of the elements in that set..

(25.06.23, 12:11) Even though I use the ``Applies to'' tab for both Predicates and other Terms, I should actually rename it as ``Instances'' for non-Predicates at some point.\,. .\,.\,Oh, but that actually means that I should probably just cancel that extended interpretation where non-predicate Terms can be used as the pred(\_id) for semantic inputs.\,. .\,.\,probably.\,. (12:15) .\,.\,Hm, or maybe I should instead just start to call the ``Applies to'' tab ``Instances'' instead.\,. .\,.\,Let me do that for now.\,.

(13:14) I've btw also just renamed the ``Subcategories'' tab to just ``Categories'' (letting the sub- be implicit).


(29.06.23, 16:55) I've weirdly run into wall today for some reason. But let me just write about some updates here: I've decided to just take the app in its current version and let that be the first prototype that I publish, why not; I might as well work on it openly. So I'm going to write a readme text about the project, which I will continue tomorrow. I've just bought the opensdb domain name, btw. .\,.\,Uhm, I don't have to much else to say, but I feel like talk about the fact that gathering all the Data Types, e.g.\ Users, with the Term type, it opens up for having non-User Terms in the SemanticInputs as well. This means that we are no longer bound to record all bots in the Users table. And (.\,.\,Oh, I feel really out of it for some reason.\,.) .\,.\,and it means that users can create there own ``user group''/bot Terms and ask the Developers to implement and maintain these, and if the developers then decide to do so, the do not need to create a new User, or a new Term in general, as they can just use the Term at hand. Okay, I can feel that I'm not making much sense.\,. .\,.\,I should just stop for today.\,. .\,.\,Oh, but let me try to write about the following to things. I feel like I need to mention that my ``simple user groups'' probably \emph{will} be quite useful, especially for FoaF-like networks, and for distributing trust in general. \emph{And} it will probably be quite useful also for those ``discussion user groups'' that I've envisioned and talked about before (in relation to my ``Debate site ideas''.\,.).\,. And the second thing: Another useful type of bot/user group will probably be taking an existing user group/bot and deriving another one from it with the same ratings as the first one, but where the subjects (and/or perhaps the relations) that this new user group rates is restricted compared to the first one. This will be useful for cutting out subjects of sets and thus making smaller (proper) subsets of them (which is useful for app efficiency and.\,.).\,. Okay, my brain really doesn't work rn for some reason, but I think I got there. I think I managed to say what I felt like I needed to say (even if it wasn't very well said (or written, rather)).\,. (17:13)

(30.06.23, 10:08) I think I might remove the def\_term field and then use the \$l lists instead. And I also think that I will use a NULL context much more for contexts, making it a rarity that a term has more than one ancestor.\,. .\,.\,Ah, and let me then replace \$s with \$ and \$l[n] with \$[n].\,:)\,.\,. (10:14) .\,.\,Hm, I guess I will make this change, but let me think for a little more.\,. .\,.\,Hm, let me also remove the `:' syntax, by the way.\,. .\,.\,Oh wait, how about using variables instead, with identifiers that can then be used as labels in Term submission fields?!\,:) (10:22) .\,.\,Oh, that would almost be smart, but.\,. oh, but if I use a syntax like.\,. oh, perhaps with a trailing \$ as well.\,.\,?\,:) (10:25) .\,.\,Hm, then why not use $<>$ instead?\,.\,:) (10:26) .\,.\,(10:34) Okay, I need to use this $<>$ syntax for sure, first of all. And now I just thought, maybe I should also make a ``Template context'' Context for all the template Contexts.\,. .\,.\,Sure, and then this will function as the current `:', by the way. (10:36) .\,.\,Hm, I should make a queryUnsafe() method, then.\,. (10:45) \ldots (11:07) Hm, I'm not sure I can remove the def term, 'cause it is handy e.g.\ for the Ratings page.\,. .\,.\,Wait, no, maybe it isn't.\,. .\,.\,No, I can remove it.\,. (11:14)

.\,.\,Hm, let me say that all elements of a list (of the form $<n>$,$<n>$,\ldots) that exceeds the number of placeholders in a template will be appended inside a trailing parentheses in the title, each element (if more than one) separated by a comma. (11:20) .\,.\,Okay, let me start on making these changes (but it's okay if I don't finish it today).

(12:06) Oh, I guess I should also allow strings in the lists, because won't that be important to for searches, particularly in order to make the Indexes work.\,.\,? .\,.\,Hm, and I could actually use \# for IDs, then.\,. .\,.\,And let me use `;' instead of `,'.\,. .\,.\,(12:17) Yeah, let me do this; let me allow strings in the lists.

(13:36) Let me by the way mention that I now intend to go away from using non-Predicates for pred\_ids again (which means that I should probably go back to the ``Instances'' relation, and maybe split the ``Applies to''/``Instances'' tab up into two again.\,.).\,. (13:39)

(19:08) I've had the delightful idea to actually change context\_id to context\_str instead, simply.\,:)

(21:24) I had second thoughts but I actually think I will let context\_id $\to$ context\_str.
\ldots\ (22:56) Or maybe not.\,.

(01.07.23, 11:26) I think I will keep context\_id and just disallow inserts of Terms with Contexts who themselves has Contexts: no more than one ancestor Context, the parent. At some point we might change this, but I don't see why we would need to.

(03.07.23, 18:47) Oh, something else that  think I've forgotten to mention, is that I intend to try to implement a small version of my ``variable documents,'' or whatever I used to call them, which were central to my ``wiki idea,'' pretty soon after having finished the first prototype. I'm then thinking of implementing a kind of ``semantic field'' for text section, where users can upload and uprate small article section in some markup language (possible Markdown, if not HTML). And to this markup language I'm then gonna add a syntax to denote ``variable subsections,'' such that the user, instead of writing the actual subsection/paragraph, instead just states what predicate to query to get the subsection/paragraph. And in a further implementation of this system, it should also be possible for users to adjust extra predicates about the article section, for instance by saying that a ``detailed'' text is preferred by the user (in the given moment), or perhaps the opposite: that a ``brief and concise'' text is preferred. And then the point is that (at some point) the application will then not only query for the best section template matching this extra predicate (if not several extra predicates) but will also then use the same added predicates when choosing what ``variable'' subsections/paragraph to query in order to fill out the section template. If this idea can be implemented sooner rather than later, it will be very useful in showing the users the possibilities of my ``wiki idea'' (that I have often referred to it as before). (18:58)


(15.07.23, 12:36) I'm actually now playing with the idea of adding types back to the system, perhaps the types: Predicate, Category, Context, Object.\,. .\,.\,Hm, and I could perhaps also refer to Contexts more as Templates, if I want to use ASCII characters for type flags as I used to.\,. .\,.\,Hm, but should I then also have the types: Users, Texts and Binaries?\,.\,. .\,.\,Hm, maybe Contexts shouldn't actually be considered Terms.\,. .\,.\,The reason I'm thinking about this now is that I'm also considering making new tabs, something like: ``As an Object,'' ``As a category,'' ``As a predicate,'' ``As a context''.\,. .\,.\,Hm, but yeah, maybe I should add types instead, but then do it as a sort of prefix to the Context, and then indeed not have Contexts as being Terms.\,. (12:47) .\,.\,And let me indeed just use the characters `p,' `c,' `o,' `u,' `t,' `b,' if I do this.\,. .\,.\,But let me think a bit more on it before I commit to restructuring the SDB.\,. (12:51) \ldots (13:11) Okay, I actually think it is the right thing to do.\,. So let me get to it.\,.

(14:16) Hm, maybe Indexes should just be introduced as Objects.\,. I'm considering adding Statements to the list, though.\,. .\,.\,Hm, then again, perhaps no to both.\,. .\,.\,(14:25) Hm, now I'm instead considering just adding `Aggregation algorithms' to the previous list --- which also includes Indexes at this point in time.\,. .\,.\,Yes, so the types will probably be: ``Categories'',  ``Predicates'', ``Objects'',  ``Indexes'',  ``Users'',  ``Texts'',  ``Binaries'',  ``Aggregation algorithms (Bots).'' (14:32)

%Hm, I just pushed accidentally (the "I'm thinking of ..." commit) by clicking the wrong button in my editor (using Atom). I didn't know that it could do that, and now I'm wondering how on earth it does that; how does it have/know my/a token for pushing changes..? ..Oh, well.. (15:11)

(15:28) Hm, shouldn't I actually start calling Terms Entities instead now?\,.\,. .\,.\,I think so.\,. .\,.\,(15:32) Hm no, that doesn't work when Templates aren't Terms, but shouldn't I actually make Templates (Contexts) a part of Terms/Entities once again?\,.\,. .\,.\,Yeah, I should.\,. .\,.\,And I guess I will use `x' for the text type once again, then.\,. .\,.\,And `m' for Templates, I guess.\,. \ldots (15:45) Hm, let me stick to calling them Terms, right.\,.\,? .\,.\,Hm.\,. .\,.\,Yeah, I still like Terms.\,. (15:49) .\,.\,Or.\,. Hm no, maybe Entities \emph{is} actually better.\,. .\,.\,Yeah, it actually is.\,. (15:51)
%Puh, at omdøbe alle "terms" til "entities" er en del arbejde, så lad mig lige tygge på det engang.. Ville også have det fint med en pause.. (15:57)

(16.07.23, 16:32) I have thought a lot today, after having successfully changed the backend and frontend, and now I'm actually considering letting types also be given by an entity, just like the templates (i.e.\ a Type entity (with the type of Type)).\,. Hm.\,. .\,.\,Let me explain, by the way, that the idea is that Types can then be used more than Templates for defining e.g.\ the Column tabs and such, and they can also always be cached because they are not intended to be very many. .\,. .\,.\,Hm, but more than 128, though, probably.\,. .\,.\,(16:45) Hm, I actually really like this idea.\,. It also makes us able to have different types of properties that ``know'' themselves the rough amount of elements that are usually attached to them.\,. oh, and perhaps also even the ratingLo value.\,. .\,.\,Hm.\,. (16:49) \ldots (17:00) No, I'm really not sure; I  could also just do it all with Templates.\,. \ldots Yeah, maybe I should just let types be part of the syntax for the templates after all.\,. (17:02)

(18:10) Okay, I think I finally know what to do. I will gather the Term types into just one type: Term (which includes Predicates, Categories and Objects). The Term type will then be the only type using templates, which I will now rename as `classes.' Classes then consist of a class name and a template, which are separated by a `$|$' in their defining string. Some important class names are then `Predicate' and `Category.' Furthermore, I will also add caching back to DBRequestManager.query() and then just make sure that all class queries are cached in this prototype of the application (disregarding that this might be redundant due to the browser's cache).

(19:33) I have been thinking about the fact (I think) that I ought to actually use sets/categories more, including for properties. And now I just had the thought: Well, why not keep the types as they are now, with Predicates and Categories apart from Terms --- and actually \emph{not} refer to the former two as Terms --- and then make it so that SemanticInputs can take either Predicates \emph{or} Categories for their pred\_id field. Interesting idea.\,!\,.\,. .\,.\,Yes, this is it. This is what I'll do.\,.\,! (19:39)

.\,.\,(19:47) Hm, now I'm even playing with the thought of dropping predicates.\,. .\,.\,Oh, which is actually just similar to back when I wanted all relations and predicates to be formed from noun phrases.\,.

.\,.\,My current names for things makes it hard to use CHAR(1)s, so let me actually just use TIYINT type codes instead.\,.

(17.07.23) I'm actually indeed going to remove predicates and use only categories instead.\,.

(11:19) I'm actually gonna stick to calling it templates. And then I'm gonna remove that whole thing about ``Add field'' to the submissions, as well as appending extra properties at the end of templates. Instead the standard syntax for templates should be ``$<$class name$>$: $<$rest of the template introducing the fields/properties of the class$>$''. The point is to then only use the class name, when defining how Columns or Elements etc.\ should be set up/rendered. So when a user needs an additional field in order to fully define an entity, they can just use a different template that has the same class name, but adds another field (or more) to the rest of the template. And there we go.\,:) (11:27)

(11:53) Oh wait. It will actually be better then to have the class as a separate column.\,. although it will take an extra byte for non-Terms.\,. Hm, but it seems to be better still.\,.
\ldots Then again, if all other def items (or most, anyway) are gotten from IDs in the def sting\ldots 

\ldots (12:19) Ah, but then I should just let types be given by an entity like I mentioned yesterday, right?\,. .\,.\,Yeah, I guess.\,.

(18.07.23, 13:14) I think I will let template entities know their own intended type by letting them hold said type entity (ID) instead of their tmpl\_id (which was just null otherwise).

%(14:28) I need to figure something else out for the Titles, 'cause right now you need to click e.g. on "of" to go the a subcategory from its Title.. Hm, maybe I should just do something with holding down control to go to the def items.. uh, or I could (also) make.. only the.. full template titles link to the def items.. and not the entity itself..

(19:03) I should also allow Type entities to hold a supertype in their tmpl\_id column.

(19.07.23) I actually don't think supertypes will be too useful, at least not in the beginning. So for the time being, I'm not going to allow types to have anything but null in their second (tmpl/cxt) field. I'm nevertheless still going to rename the tmpl\_id column back to cxt\_id for the backend (to account for the fact that it is different at least for the Template entities).

%(21.07.23, 18:51) I dag er godt nok gået langsomt (med at arbejde). Det kan være fordi, jeg ikke har spist nok, så det må jeg hellere lige prøve at gøre, og så håber jeg, det bare er det. Håber ikke, det er fordi, jeg har behov for mere ferie (det føler jeg ikke umiddelbart, men jeg kan dog til gengæld altså mærke, at luften lige er gået lidt af mig igen..).. ..Jeg holder for resten muligvis en halv fridag i morgen, så hvis jeg gør, og hvis det altså ikke bare er det med, at jeg ikke har fået spist nok (til virkeligt at kunne klø på), så kan det være, at det kan give mig noget energi igen..
%(22.07.23, 15:24) I dag har jeg heldigvis bare vildt meget energi (så tager en fuld arbejdsdag).! Jeg har lavet alt hvad angår userDB i dag (der var meget lidt, jeg ikke har commited fra i går (med i mit første commit i dag) *(tror nærmest bare jeg fik erklæret PrioritySetGenerator, og mere nåede jeg ikke rigtigt den langsomme dag --- udover at tænke over, hvad jeg er gået i gang med i dag, dog..)); det går bare derudad.!

%(23.07.23, 16:14) ... :(:'(:( ...

%(25.07.23, 12:37) I woke up around 4--5 and started thinking about turning rat_val into a TINYINT and some more things. It naturally took me a long time to fall back asleep with those thoughts. But I did, and then slept til almost 12. So that's why I'm only starting now.

(25.07.23, 12:40) I've been thinking about turning the rat\_val into a TINYINT, and I've even thought about the fact that I could use larger integers, and then just only use the trailing byte a lot of the time, since the first 0 bytes of those integers should then be compressed together with cat\_id, taking up effectively no extra space.\,. But now I've got the feeling that I should change it from a SMALLINT.\,. .\,.\,No, I probably shouldn't change it, cause the important set queries to optimize are the large ones, and for most of those, it is nice to have more than 256 (128 non-negative) containers to put the rating values into.\,. (12:47) .\,.\,Perhaps.\,. .\,.\,Yeah, I think so.\,. .\,.\,And besides, it is also nice that users can determine the structure of certain sets more carefully than with a 256 resolution (e.g.\ when it comes to semantic properties, but also when it comes to some standard categories, I think.\,.).\,. .\,.\,So the conclusion must actually be that I keep it as a SMALLINT for now.\,. (12:52)

%(28.07.23, 13:37) Det går lidt langsomt i dag, men det hænger nu også meget sammen med, at jeg også går og tænker over, hvilke nogle ting jeg skal føje til min(e) README(s). Men nu har jeg fået arbejdet lidt på mean-botten, og:
(28.07.23, 13:39) Hm, I've run into a problem that seems to maybe cause some big trouble in terms of the efficiency of the aggregation bots (e.g.\ the mean bots), unless I can find something smart to do.\,. The trouble is: How do I record and get the previous rating for each user?\,.\,. Well, I \emph{could} perhaps look at the solution of just simply letting all bots and (what is now) events run immediately for each new user input. But.\,. Hm, well maybe that \emph{would} be a reasonable solution, actually.\,. (?) (13:43)
%..(13:43) Vejret er også så godt i dag, btw. Jeg har nu egentligt ikke savnet sommervejret, for juni var så varm, og det er helt klart bedst, hvis det er mere køligt, når jeg sidder og arbejder. Men nu får jeg da lidt lyst alligevel til at gå ud og nyde det.. Så spørgsmålet er, om jeg ikke skal tage en tænke-gåtur over det her spørgsmål..? (13:45)

%(15:01) Det er faktisk godt, at jeg lige har taget --- og tager --- noget tid for at tænke over det. For det første er der også den mulighed, at jeg bare tilføjer et index og changed_at igen til RecentInputs. Men i virkeligheden var de smartere bare at opdatere alle bots for hvert nyt input, hvis ikke det var fordi, at jeg jo også godt kunne tænke mig at lægge op til, at brugere (inkl. tredjeparter) skal kunne implementere bots også. Men dertil kan man så også sige, at hvis de skal det, så skal de nok alligevel have en speciel aftale med openSDB for at få lov til at uploade og opdatere så stort et volumen af ratings. Og så kan openSDB jo lige så got bare oprette botten selv i princippet, right?.. Så er der så lige den ting, at en +index og +changed_at til RecentInputs også kunne være gavnligt for at sikre imod at brugere troller ved midlertidigt at give en forkert rating for så at annullere denne inden at nogen vagthunde kan nå at se det. Men måske kunne man så sige, at \emph{hvis} dette bliver et problem, så kunne man så se på (eventuelt) at oprette et sådant RecentInputs igen --- nå ja, for spørgsmålet er så nemlig i øvrigt også, om ikke RecentInputs ligefrem skal udkommenteres i den første version af databasen.. Så ja, det er altså dertil, jeg er nået i mine tanker nu.. (15:09) ..Hm, men bliver det ikke bare det, der er svaret: Hvis RecentInputs på et tidspunkt bliver nødvendigt, hvad end det er for tredjeparters bots's skyld, eller hvis det er for at folk ikke kan drille med at skifte ratings kortvarigt, så kan man jo bare tilføje det da. Og så kan jeg altså starte uden, hvor alle bots så bare updaterer sig for hvert nyt rating input.

(15:14) I think I actually will use that solution, i.e.\ to just let all the bots and (what was) events run immediately for every new input, and I think that I might even out-comment RecentInputs from the database, then. The reasoning is that, if RecentInputs again becomes useful at some point, either to better allow for third-party bots, and/or to prevent users from being able to troll by changing ratings back and forth quickly (which might be important in some special cases, e.g.\ when it comes to ratings of code safety), then RecentInputs can just be added once again --- also with an index like the secondary one of SemanticInputs and with the changed\_at column again, which should then also be the last column of that index. I think this is what I'll do.\,. (15:19)

%(15:40) Hm, det er åbenbart i dag, hvis man skal nyde dette solskinsvejr, for i morgen bliver det vist (kraftig) regn igen...

%(29.07.23, 9:46) Hm, der er så også lige det, at man jo gerne vil sigte mod en distribueret database, og så kunne en RecentInputs/RecordedInputs tabel måske også være gavnlig...

%(30.07.23, 10:32) In the bath here in the morning (I slept until 10 for some reason), I...
(30.07.23, 10:33) I have just realized that because the inst\_id column of SemanticInputs will likely contain two or more leading bytes that are zero-filled, if the last byte of the SMALLINT rat\_val is zero-filled, it should just get compressed along with the zero-filled bytes of the inst\_id, at least most of the time, I think. I still want users to be able to make precise ratings in some cases, for example because I still want my idea about rating entities on a list by moving them around implemented at some point, but when they are just rating via a rating bar, I the app should just round off so that the last byte is zero.

%(12:10) Har af en eller anden mærkelig grund brug for en lille pause og en gåtur (må bare tage paraply med, for jeg synes allerede det trækker op til regn). ..(12:16) Nå, never mind, nu står det allerede ned i stænger. ... (13:51) Nu pause og gåtur! (Solen skinner ligenu.)

%(31.07.23, 11:57) Mit internet er nede, så det besværliggør arbejdet med front-ended (navnligt med at teste det). Derfor har jeg bare besluttet mig for i stedet.. hold da op, sikke det regner nu..!.. ..(12:01) i stedet at skrive på READMEs'ne i stedet. Efter en smule tænketid er jeg så gået i gang med at skrive et nyt draft, hvor jeg faktisk endda omskriver introduktionen. Så det er jeg i gang med, og nu søgte jeg så lge på, hvad man kalder en "instans af en semantisk ontologi," hvis man skal være præcis og altså ikke kalde det en ontologi. En "semantisk model?" Det søgte jeg på, og nu har jeg så lige fundet en wikipedia side om "Semantic data model (SDM)".. Så ja, nu er jeg da lidt spændt på at læse om, hvad det går ud på, og hvor meget det mon ligner mit... (12:05) ..Ah, det er vist bare en "model" ligesom UML er en "model".. I think.. (12:08) ..Ja, pyh ha.. (12:10) 

%(13:38) Jeg har skrevet det her indtil videre:
	%## Introduction to the project
	%
	%openSDB is an open source Semantic Database (SDB).
	%By 'semantic' we refer to the fact that entities in the database can be linked
	%via (user-provided) relations that can carry any meaning, just as in the case
	%of the [Semantic Web](https://www.wikipedia.org/wiki/Semantic_Web).
	%
	%In fact, this project seeks to revitalize the idea of the Semantic Web, but
	%with a different approach than the conventional one. Instead of trying to
	%extend the World Wide Web itself, this project instead aims to launch an open
	%source [Web 2.0](https://www.wikipedia.org/wiki/Web_2.0) site that utilizes
	%semantic data structures, not just as part of its data processing, but where
	%the users are actively engaged in building these structures.
	%
	%The point of this is to make it way easier for the average user of the web to
	%take part in building semantic data structures than it is in the conventional
	%Semantic Web, where contributing to the data structure requires users to
	%write special [RDF triples](https://www.wikipedia.org/wiki/Semantic_triple).
	%These are fairly complicated HTML entities that web developers then have to
	%add as metadata to their web pages. So not only does the conventional approach
	%require its users to have specialized knowledge in RDF triples, it also
	%requires them to have access to editing web pages!
	%It is thus not particularly hard to see why the Semantic Web never really took
	%off with this approach: It never managed to become very accessible for users
	%in terms of participating actively in it.
	%
	%openSDB first and foremost seeks to do exactly that: make the Semantic Web[^1]
	%much more accessible to all users of the web.
	%Its approach is to instead start out as a Web 2.0 site, running on top of a
	%Semantic Database (SDB), and try to make an interface for this database
	%(in the form of a web application) that is very easy and intuitive to use.
	%
	%[^1]: Although a more appropriate term in our case might be 'Semantic
	%Net(work),' since our approach do not directly extend the World Wide Web
	%itself.
	%
	%The danger of this approach, if not dealt with appropriately, is that, as one
	%might point out, it risks exchanging more accessibility for more centralization
	%as well, since a Web 2.0 site might have ownership over its source code, and it
	%might also be unwilling to share its data structures (the non-sensitive parts).
	%This centralization would very much be in contradiction with the original
	%visions of the Semantic Web.
	%
	%However, openSDB seeks to prevent such centralization first of all being
	%completely open source, second, by allowing any other parties to copy all its
	%non-sensitive data, and third, by committing itself to working towards a
	%distributed and decentralized database. This means that other parties will be
	%able to back up the application and the database, and to host their own version
	%of the system at any point. openSDB encourages other such hosts and wants to
	%work together with them forming a distributed database.[^2]
	%
	%[^2]: This will likely include implementing processes to remap entity IDs such
	%that database nodes (in the distributed database) can keep their respective
	%data structures in sync with other nodes.
	%
	%Thus if openSDB at any point does something that is against the interests of
	%its users, the unsatisfied part of the userbase can then immediately just
	%start up its own copy of the site from a backup.
	%
	%
	%## The massive advantage of making the Semantic Web more accessibly.
	%
	%If openSDB succeeds in making a sizable Web 2.0 site where the users can easily
	%participate in building the semantic structures, it will ...
%Jeg tror, jeg vil holde en lille pause nu (og tænke lidt over, hvad jeg videre skal skrive..)...

%(16:03) Har skrevet lidt mere:
	%## Not just facts, opinions as well!
	%
	%One of the prospects of the Semantic Web is to be able to easily be able to
	%search for specific facts on the internet, such as "who was the successor of
	%Julius Caesar?" or "what is the air-speed velocity of an unladen swallow?"
	%
	%If openSDB succeeds in making a sizable Web 2.0 site where the users can easily
	%participate in building a semantic structure, it will first of all mean that
	%more such facts can be recorded. There are in principle an infinite amount of
	%fact about our world, and we cannot record them all, but the more users a
	%semantic system has, the more facts can be submitted and validated by this
	%userbase.
	%
	%However, conventional search engines, such as Google's, are already quite good
	%for finding out facts, and it will take a while before a semantic network
	%could start to compete with those. And although AI is still quite unreliable
	%at this point in time, it is not unreasonable to think such technology will
	%make it even easier to search (reliably) for facts in the near future.
	%
	%But the vision of openSDB actually extends the vision of the Semantic Web to
	%include not only searching for facts, but to search for *opinions* as well,
	%and especially the averaged opinions of the userbase, or a particular parts of
	%it.
	%
	%This opens up a myriad of new possibilities and use cases. *...*
	%
	%*...*
	%
	%Extending the visions of the Semantic Web to include opinions instead of just
	%facts obviously does not make much sense if the userbase is limited to people
	%with special access to edit web pages, and with special knowledge of how to
	%write RDF triples, because then you would only get the opinions of those people.
	%
	%So in order to achieve this, we have to first do what openSDB seeks to do and
	%make a semantic system that is accessible to all and easy to use.
%

%(01.08.23, 15:48) Jeg har ikke fået lavet meget andet i dag end dispositionsarbejde: Jeg føler mig mærkeligt flad, når jeg prøver at skrive..:\ Nå, jeg vil prøve at give det endnu et skud nu, for nu synes jeg efterhånden (til gengæld), at jeg har fået godt styr på disp'en.. ..Og hvis det ikke går, så må jeg jo bare prøve at få kodet lidt (nok bare i blinde, hvorfor ikke..).. ...(16:20) Ej, jeg kan bare slet ikke tage mig sammen.. Det er mærkeligt, for jeg synes, jeg har sovet godt i nat, men det føles nu ikke sådan..

%(02.08.23, 16:51) Har holdt en slags pause fra kl. tolv i dag til nu. Vil prøve at se, om jeg kan få skrevet lidt mere i dag. (Internettet er btw ikke kommet tilbage endnu.)

%(04.08.23, 13:56) Jeg er lidt ked af det i dag, så det er ikke sikkert, at jeg når at få skrevet så meget...

%(00:07) Holy shit, jeg fik lige den vildeste idé (måske)..: Hvad hvis man begynder at bruge AI til at modtage tale inputs og.. Okay, ved ikke, om det er en vild idé, men jeg må jo tænke over den.. Men ja: modtage taleinputs og så bruge det til at konstruere semantisk input ud fra dette.. Jo! det er en vild ting: Det er en vild fremtidsudsigt. .. ..Hm, måske, ja.. ..Tja, joh, ja.. ..Det biver måske svært at gøre ordentligt anonymt, men hvis man kan det, så kunne man måse virekligt øge flux-(*)kvaliteten af bruger dataen... (00:13) ..
%(05.08.23, 11:51) Ja, hvis det kan gøres på en god anonym måde, så kunne dette blive ret revulotionerende på et tidspunkt, for det kunne betyde et meget højere indflux a semantisk data til det system, der nu end kommer til at stå for at bearbejde den data (forhåbentligt det nye semantiske web, som jeg jo prøver at sætte gang i).

%(06.08.23, 10:29) Jeg ved ikke hvorfor, men jeg er ikke særligt oplagt på at genoptage kodningen igen her til formiddag (har først lige sat mig, btw).. Jeg tror lige jeg tager det stille og roligt og prøver at summe lidt over, hvor jeg står først.. ... (11:26) Ah, jeg ved hvilken secton, jeg bør tilføje til README'en.:)...

%(16:22) Efter at jeg fik skrevet den sektion har jeg nu ikke lavet så meget: kun ændret contact_info.md og debugged SubmissionFields.js (outID -> entID).. Jeg tror bare, at i dag bliver en fridag i høj grad (på når den sektion, jeg fik skrevet der), og så går jeg bare ordentligt i gang med det sidste kodearbejde (som jeg har tilbage før jeg kan gøre beta-versionen/prototypen live (hvor lang tid det end vil tage mig i sig selv)) fra i morgen af.

%(08.08.23, 20:43) Non-related: I've just seen a video and read a bit about the Stein's paradox, but it seems quite weird. I think I might understand it, though: My question is: Is it true that if two people, "you" and "I", let's say, play a game where a point is sampled in 3D space from a 3-dimensional gaussian, either with a new random true mean value each round or with the same, that shouldn't matter (right?), and I continously pick the normal estimator as my guess and you continuously pick the Stein estimator. If the game is this: if I get closer to the true mean with my guess, then I win a bit of money from you, and you you get closer, you win a bit of money from me, then will I not with most likelihood have taken all your money at the end of the day, if we keep on playing? You will go home without money, but you will not be sad because you know that you had the "best" estimator.;) Is that not what would happen? And if I understand the thing correctly, to be fair, \emph{if} we instead played a game where the money owed in each round would be proportional to the differnce in the square of distance of our guesses to the true value, then you would win at the end of the day, right? Maybe I'm wrong, but that's how I'm currently understanding this "paradox" at the moment (and I don't care to investigate it further). (20:53) ..(20:59) It does still seem somewhat paradoxical/weird if "you" would win the second game, but I cannot beleive that "you" would win the first, absolutely not.

%(09.08.23, 10:35) Jeg er en lille smule syg i dag. Jeg havde tænkt mig at gå i gang med at få de rigtige SetCombiners i gang den individuelle bruger. Og nu tænker jeg så også lige lidt over, om jeg mon skal fjerne CSS-delen fra min ContentLoader klasse, om ikke andet måske midlertidigt..?

%(12:43) Jeg får en lidt mærkelig fejl nu, hvor setArr = [undefined, undefined, ...] i PrioritySetCombiner for min nye SimpleCategorySetGenerator.. Tror lige jeg holder en lille pause før, jeg går videre.. ...Okay, det er ikke en mærkelig fejl: Jeg har bare aldrig debugged mine SetCombiners, på nær at RatingMaxSetCombiner har virket fint tilsyneladende.. (13:30) ..Åh, jeg havde bare glemt et "return val;" i en map()-funtion..

%(10.08.23, 12:36) Jeg har lige skrevet første felt (forklaringen) til min AKA Startup Growth-ansøgning (lige akkurat 995 anslag). Jeg føler mig forresten slet ikke syg i dag, allerede. Snotter en lille smule og nyser lidt stadig, men den tunge følelse (af at være fuld af snot i hovedet) er der ikke. Elsker virkeligt (7, 9, 13), at min krop generelt (næsten altid) er så hurtig til at komme sig over forkølelser.:)

%(19:31) About Stein's paradox, "I" would also win the second game, obviously.. I'm wondering if this is perhaps something to do with these mathematicians integrating x before they integrate \theta, and that this particular order of integration means that they get the wrong answer.. But let me actually look more into, exactly what the theorem is saying (and why it doesn't conflict with my game examples, which it can't..)... ..Oh, and let me just mention: Integrating over x before \theta might create an artificial assumpton that \theta is always "small" compared to (the norm of) the x's in the space, which therefore might make bias that the Stein estimator adds give a better result: because there is an artificial bias introduced in the calculations as well. This is just a thought that I just had (and I've only just started thinking about this topic again (since last) a few minutes ago); I need to look more into the subject to see if it actually could make sense. (19:39) ..(19:44) Hm, it seems that the theorem actually exactly does state essentially that "you" would win the second game of my examples.. ..(19:46) Hm, and maybe I actually could believe that that might be true (and that the mathematicians are technically right (which would also be a crazy sensation if they weren't, of course)).. ..Yeah, 'cause although it still seems somewhat paradoxical to me that "you" win the second game, the idea that I just mentioned cannot change the outcome of the calculation: It would be okay to integrate over x first in that calculation (I'm pretty sure..).. (19:50) ..No, wait! No, it wouldn't! Not if \theta changes for each game, which is much more realistic if the game is supposed to mimic what empirical scientists (including data scientists) have to deal with. (19:52) ..And actually in any case, the first of my two games would also be what an empirical scientist would have to go for in terms of estimators: A scientist should go for an estimator, that is likely to beat other estimators in terms of which ones is likely to be closest. ..Well, unless there is a reason why you want to square the distance, but typically the "failure" a scientist makes would be measured linearily in terms of how far the guess is away from the true value, not quadratically. ..Although of course there might exist cases, where it is better to measure the failure quadratically, but it should by no means be considered a go-to standard for general cases.. (19:58) ..Oh, wow..! I actually kinda believe that I'm right about that thing about the mistake of integrating over x before \theta (and not the other way araound)..!! (20:02) Because if the calculation is true, it tells that "you" win the second game of my two examples for a any constant \theta. But it does not tell us (I'm pretty sure) that "you" would win for a randomly sampled \theta.. Hm, although that is of course not really possible, namely to sample a \theta uniformly from all of \mathbb{R}^3, and now that I think about this fact (since I was just about to mention it parenthetically only), I can see why it might actually make sense to integrate over x first after all... (20:06) ..Hm, or maybe not.. (20:07) ..Well, the fact that the order might matter \emph{would} be a very important critisism of the estimator, and might even show why frequentists (although I must admit that this reading has made me realize that I do not fully understand what the term means exactly) are just someone who tries to escape reality, but only ends up reaching wrong and confusing results in doing so.. ..Of course, since it seems that I do not fully kow what the term actually refers to, there might be okay interpretations of frequentism, but I do feel like I have a bone to pick with a lot of frequentist: You can't just close your eyes and ears to the fact that not knowing the universal prior for things matters in some instances, and that you therefore have to actively consider this fact and if it matters for your dataset (or if you have enough data that prior does not matter (which is exactly what a frequentist might miss! (unless I misunderstand the term completely)))! And the fact that I know have stumbled upon this result, where it seems that frequentists are actively using a biased estimator, when it gives them worse results in reality.. well, let me not go that far yet: I don't know any of this. I just know that I really can't see why "you" would win the second game, if we allow ourselves to look at the space of all \theta's (i.e. the "true mean") at once (and play the game at all points in \theta space at once.. well or in a grid, let's say, such that this set of points is countable.. sampling just one x for each of these \theta's).. (20:22)
%(21:08) Anyway, I think that (some) mathematicians are probably already aware of this, even if some data scientists are indeed creating an unecessary bais in their statistics (which it sounded like from one comment I read on the YouTube video that made me aware of the paradox..), so I don't think I will try to look more into it at this point..
%(21:37) Oh, wait! I couldn't help just thinking a bit more about it, and maybe I am wrong..!.. ..Hm, or maybe not, but it might be the case that the calculation if we integrate first over \theta by integrating over a ball and then letting R go to infinity actually gives "you" an advantage, since it might be the case that "you" win more than "you" lose on average by letting "your" guesses be biased, namely since "your" bias is strongest.. hm, when the sample is on the other side of the true mean.. hm.. ..(It's still all very fishy no matter what; the fact that the order of integration matters means that we are not telling ourselves the full story...) ..Hm, 'cause couldn't you, even if the just mentioned calculation would indeed still favor "you," couldn't you just make a third calculation, where you integrate over all of \theta space first, and then move on to integrate over x, and not (x - \theta)! but just x, integrated as a ball with increasing R (so an improper integral) from the origo of \theta-space, and then the calculation would favor "me," right..? To explain this further, we are thus looking at the contributions to all thetas in the infinite grid at once, and then we start aggregating the winnings and losses for all games where x lies within a certain R from origo, and when you then let this ball grow to complete the calculation, "I" would remain the "winner" of the game as R (integrating over x, not (x - \theata)) goes to infinity. Right?.. (21:56) ..(21:59) Oh, or maybe this would just get us the same result as when intgrating over (x - \theata) first?.. Anyway, I'm to tired to think more about it, and I probably won't return to the problem any time soon either ('casue it wouldn't really matter much to me, even if I \emph{am} on to something (I mean, it would matter to me in the sense that it could be quite cool to have found that kind of thing out by myself this quickly, but it wouldn't matter a bit to my current life and goals..)).. ..Oh wait, integrating over x, not (x - \theata), is actually a completely unreasonable thing to do, so it doesn't matter anyway. (22:07) ..Okay, I can feel that I am actually kind of curious to know more about the paradox, so I might actually think more about it "any time soon" after all, if/when I get some free time like now where I don't feel like it makes sense to think about my actual work more for the day.. (22:11)
%*(11.08.23, 16:58) As I said, I won't invest any energy into this subject for now, unless I'm sure that I'm done for the day anyway, but let me just mention now, that it could perhaps be very interesting to see what happens if we regularize ('cause I'm assuming that the integral in the calculation is improper one) the integral by introducing a gaussian probability function for \theta when we play the game, and then letting the variance of that gaussian tend to \infty. ..Hm, but maybe that would still give the conventional result.. Oh well, I will not think more about this now. (17:04) ..(17:18) Yeah, one will probably still get the conventional result, so maybe that result is just simply.. right...
%*(18:57) Okay, I can feel that I won't get anymore done today, so I've just thought a bit more about it, and it's actually quite clear and simple. The problem \emph{is} that these frequentists integrate over all x for a given contant \theta, instead of integrating over all \theta for a given x, which is much more natural given that the theory is supposed to answar, what should we guess that \theta is if we are a scientist who has just measured a given x. And when you integrate over theta for a fixed x, you see that "you" would lose more money on average in both games, also the second one. Of course integrating over an unnormalizable distribution might seem a bit weird, but that's the nature of the question; you cannot close your eyes and ears to the fact that the scientist, who has just measured x, \emph{has} to now consider how to guess at a \theta over the space of all \mathbb{R}^3. But if we decide that we are only interested to know what happens in the boundary where the \theta prior tends to a uniform distribution, then the only reasonable thing to do is to regularize the integral by taking a normalizable distribution for \theta and let it tend towards a uniform distrubution, either by having a ball where R \to \infty, or by having a Gaussian where the variance tends to infinity. And if you do that, "I" will win the second game (as well as the first) of my two examples above. The best estimator will be the more intuitive one, not Stein's. But I'm actually quite sure that mathematicians are aware of this, but I don't know why they would insist on spouting confusing and misleading theories, actually leading real data scientists (or so it seems from some comment I read *(where I person said that they used Stein's estimator very frequently when modeling, or something to that effect..)) to introduce a wrong bias into there analysis! In real life! Tsk tsk tsk.. ..:) (19:09)
%*..(19:13) (I didn't watch the YouTube video to the end, btw, only as far as the 2D example. But I did read a comment saying something like: Paradoxes like these is why I prefer a Bayesean approach, which makes me think that the video perhaps talked about the fact that the approaches are different (like the wikipedia article about the subject says)..)
%*...(19:35) Oh wow, its actually quite interesting, I don't think a Monte Carlo simulation would tell you the truth either, 'cause if you sampled from very large but bounded set of \theta's, the difference of the two estimators would just be very small on average. And in this sense, we can perhaps actually think of Stein's estimator "exploiting" the fact that the \theta-space and the x space will (typically) be set to the same (having the same dimensions).. Hm.. ..Yeah, and you can't really do anything else when you Monte Carlo, so that's probably it..!.. Stein's estimator probably "exploits" the fact that its difference from the intuitive one gets smaller the smaller the chance is that we land on the edge of the space, where the contributions that makes Stein's estimator win come from (as I see it..)..!..:D (19:42)
%*(19:50) Wait, but is there then actually a reason behind this appraoch (of looking at a fixed \theta and integrate over x) in real life after all..!..? ..I don't think I will think about this now, but it's definitely something that I should. For maybe there could actually be a reasoning behind choosing that approach after all...
%*(12.08.23, 21:00) Yes, there could be an idea behind it, and with a Bayesean approach, you can calculate exactly why, when and how much it makes sense to introduce a bias (and you can calculate the optimal bias, quite easily). This other approach, which I guess is the approach of (formal) frequentism, only hides the fact that it actually assumes a non-uniform prior for \theta when choosing a particular biased estimator. (21:04)

%*(9:06, 09.12.23) If I have a habit of subtracting 10^{100} from all quantities before I present them as a data set, and I give you three data points from three different and independent quantities that I have shifted by -10^100, all three of them. Then a frequentist naively following what their theory tells them *(even if they knew what I was doing!) would say that by adding a small amount to all these numbers, it would result in a lower squared error on average. But if I presented them the unshifted data instead, they would tell me that \emph{subtracting} a small (but significantly larger) value to all these would lower the squared error on average. These are contradictory statements (about the real world) and therefore this theory cannot be true, i.e. in the sense of providing only true statements about the real world.



%% Startup Growth ansøgning:
%
%%(Beskrivelse af din/jeres startup. F.eks. produkter/ydelser, kunder/marked, det unikke ved forretningsidéen – hvilke behov dækker det og/eller problem løser det.)
%
%Startuppen udvikler og vedligeholder en Web 2.0-hjemmeside bygget over en 'Semantisk Database,' som er en yderst fleksibel form for database, der bl.a. gør det muligt for brugerne at uploade meget alsidigt og nuanceret data om, hvad de mener om de forskellige entiteter/ressourcer på siden.
%
%Denne nuancerede data gør, at søge- og feed-algoritmer i princippet kan gøres langt bedre end, hvad der ellers normalt er muligt, hvilket gør at siden muligvis kan udkonkurrere andre Web 2.0-sider!
%
%Samtidigt har systemet ikke det samme behov for at prøve at trække så meget data ud af brugerne som muligt, men behøver kun det data, som aktivt og frivilligt gives fra brugerne. Systemet kan derved også anses for mere etisk.
%
%At brugerdataen gives aktivt og frivilligt af brugerne (som kan være fuldt anonyme), gør at denne data kan gøres offentligt tilgængelig.
%
%"Kunderne," foruden brugerne, vil derfor være sponsorer i form a virksomheder, der gerne vil have adgang til al denne nuancerede data.
%
%
%
%%(Hvordan er konkurrencesituationen?)
%
%Idéen er helt ny, og der er ikke nogen eksisterende konkurrenter, der prøver at gøre det samme.
%
%Dog skal hjemmesiden selvfølgelig konkurrere med eksisterende Web 2.0-sider i form af at kunne tiltrække brugere, det er klart.
%
%
%
%%(Hvad er planerne for vækst i løbet af det næste år?)
%
%Dette afhænger meget af, hvor mange sponsorer og hvor store sponsorater, vi kan tiltrække.
%
%Investeringsomkostningerne er dog ret små sammenlignet med, hvilken gevinst det vil være for sponsorerende virksomheder, hvis de via projektet kan opnå gratis adgang til en masse brugerdata om deres produkter og servicer, og at de dermed ikke længere skal betale i dyre domme for denne data.
%
%Dermed er det ikke utænkeligt at startuppen kan tiltrække store nok sponsorater til, at vi kan blive en hel håndfuld af ansatte, der arbejder på at videreudvikle, vedligeholde og gøre reklame for hjemmesiden.
%
%Og da startuppen har et internationalt potentiale, så kan vi muligvis udvikle os til en endnu større virksomhed på sigt.
%
%
%
%%(Hvad forventer du/I at få ud af at deltage i Startup Growth?)
%
%Nu har jeg arbejdet i lang tid selvstændigt på projektet, så på alle mulige måder vil det blive godt at få andre øjne på det. Og særligt vil det være godt at få fagkyndige øjne, der kan hjælpe mig med alle de ting, som jeg ikke selv har særligt meget forstand på endnu.
%
%Dette handler om praktiske ting såsom at få opstartet selve virksomheden, og også særligt ting såsom hvordan jeg kommer i kontakt med eventuelle sponsorer og får "solgt" min idé til dem.
%
%Jeg tror, der er rigtigt mange ting, I kan hjælpe mig med, og jeg tror virkeligt, at jeg har brug for denne hjælp.
%
%
%
%%(Beskriv kort dig selv eller dit team.)
%
%Indtil videre er jeg kun ene mand på projektet, men jeg håber på at kunne tiltrække flere, der kunne være interesserede i at være med i det!
%
%Jeg er en engageret ung mand med en stor passion for videnskab og teknologi, og en stor lyst til at prøve at gøre verden et bedre sted. 
%
%Jeg er uddannet først i fysik, med specialisering i teoretisk kvantefysik, og har sidenhen uddannet mig i datalogi også.
%
%I årene efter min datalogiuddannelse har jeg bl.a. arbejdet på et stort hængeparti fra fysik, samt en hel masse andre projekter, hvilke tilsammen så har ledt mig imod dette projekt og denne drøm om at vække den gamle vision om det 'Semantiske Web' til live igen, hvilket nemlig er, hvad jeg mener at kunne gøre med projektet (og mere til).
%
%
%
%%(Beskriv hvorfor du/I skal have en plads hos Startup Growth)
%
%Først og fremmest bør jeg få en plads på Startup Growth, fordi min startup muligvis har et kæmpe stort, internationalt potentiale, hvis bare jeg kan få det kørt godt i gang.
%
%Derudover er det også bare et kanonspændende projekt i det hele taget: Tænk hvis det kan gå hen og ligefrem udkonkurrere gængse Web 2.0-sider, og dermed ændre fundamentalt på, hvordan vi bruger webbet!
%
%Og jeg har i den grad brug for skub for at komme godt i gang med den næste fase om at lancere projektet, og det håber jeg meget, at I har lyst til at give mig!



%(17:39) I actually think that I will leave the "sorting menu" for later, perhaps after I have made the prototype live. And my plan is then just to make some room for IDs in Entities after the last insert of initial_inserts, which I can then use if I need more fundamental inserts, that should preferably be part of initial_inserts. ..And if all goes wrong, then I can just change initial inserts and not care at all about if I create any gabs.. ..Yeah, I should be able to do so..

%(20:40) I think I will jut let the interface use only one Column at a time in the initial version (before I add the possibility to use more than one Column at a time again (i.e. 1--3)).

%(12.08.23, 10:45) Hm, why don't I just make the interface scroll left and right for the initial version (instead of making a more neat system where you can cycle through the Columns)..? I think I might just do that.. ... Nå, nu endte jeg faktisk med at fosøge at lave det, og det har jeg så gjort. Og det gik faktisk ret hurtigt, endda på trods af at det skulle debugges en del. Så nu mangler jeg faktisk nærmest bare Tutorialen..!.. (14:40)


(14.08.23, 11:50) I'm close to being done with the prototype, it seems. I'll write the rest of the tutorial today (probably), and perhaps I will even start renting a server and try to install it (or I'll do that tomorrow if not). I have I point about the e-mails, that I want to write here, in a minute..

(12:00) I thought about adding an option to tell the site/database/back end to save your password. But I can always ask the users later, when there actually are a reason to submit your e-mail for being able to change forgotten passwords. So in the meantime, I will just keep the sign-up page as it is now. But the important point is this: If a user has not submitted their e-mail for being able to change forgotten passwords, then the database should actually save a \textbf{hash of the e-mail} rather than the plaintext e-mail. But I will only implement this later, because I still need to hold on to the e-mails until I(/we) implement the system that sends a confirmation request e-mail to the users to confirm the address. So for now, I will keep things the way they are.

(15.08.23, 10:49) I'm not currently informing the users that the time for rating something will be recorded, and instead of doing this, I've decided that I will simply erase the timestamps of RecordedInputs before sharing them, replacing them with $(1, 2, 3, 4, \ldots)$ instead.

I also just thought about the browser extension, and whether you could actually implement that on the website. But I don't think we are allowed to inject JS into iframes from other domains, so I guess that wouldn't work. But an idea that might work, if we want users to search among SDB entities using Google, is to include a Google search field on the page (like you see in a lot of places), and then look for related entities of the URL hits that you get there. I don't know if we could even proxy that search so that Google can't see which IP address the individual searches originate from.\,.(?) Something to think about, but of course I won't do this now.

%(16:30) Puh, holdt en pause og gik mig lige en god tur, "imens det var godt vejr," men nu er det godt nok blevet varmt..


(22.08.23, 19:23) Woah!! I just searched for React, which somebody mentioned to me. It seems almost exactly like a much-developed version of my ContentLoaders!! .\,.\,Interesting..!.. Hm, I don't know how to feel about this.\,. .\,.\,I guess it's bad in some way, 'cause it might mean that I ought/need refactor my code as React.\,. .\,.\,But nice in a lot of other ways, for sure.\,.\,:) (Including for me: It means that I might very well get to work with a similar flow as I do now in my career---without having to first try to spread the idea.\,.\,:))

.\,.\,Oh, and if it really is like that, it means that it will probably be a lot easier to do a lot of things, namely of one can just import a lot of solutions.\,:) (19:31)

.\,.\,Oh, this might also cement the fact that it is best for me to gain some experience from working now: I should probably try hard to get a job where I can use React (and apparently Next.js builds on top of React, even, so also that, I guess). .\,.Hm (ha).\,. .\,.\,I.\,. .\,.\,This is yet another time where something like this happens to me.\,. .\,.\,Something like this truly has happened to me more times than I can count.\,. (19:40) .\,.\,I\,.\,. .\,.\,It's a funny thing.\,. I need to think about all this in the coming days (let it simmer in my head).\,.

.\,.\,I feel so lost for words, but yeah.\,. It's just funny.\,. (19:45) (that this always happens.)

(23.08.23, 10:04) Okay, React does actually seem to be server-side (which explains the neat syntax), so it's not exactly the same.\,. \ldots (10:54) No, it can also render client-side.\,.


(24.08.23) As I've written in a todo in ContentLoader.js, I will refactor the front-end code as React instead. And this ought to just be the next thing that I'll do. Then afterwards I can resume the next tasks such as making the SetDisplays sortable and start laying the groundwork in order to build the browser extension (that can search the SDB for the URL that the user has visited), etc. (10:47)

(24.08.28) openSDB actually clashes somewhat with some other names, especially a Linux distribution (I think). So I should maybe change it, but on the other hand, this particular name isn't too important, since it is only one node.\,. Well, maybe I \emph{should} change it anyway.\,.

%(29.08.23, 14:05) I've read enough about React now, and I think I'm ready to refactor my front-end code now..

%(30.08.23, 13:28) Har brugt en del tid i dag på at overveje, hvordan jeg ungår re-renders, noget som jeg kan se bliver besvaret i https://react.dev/learn/preserving-and-resetting-state. Så hvis jeg lige havde læst den ordentligt noget før, eller havde fundet den noget før i dag, så ville jeg nok have sparet en del tid.. ..Har i øvrigt også fået VSC op at køre i dag, for Atom understøttede ikke rigtigt JSX. Og heldigvis har jeg fundet Atoms syntax-highlighting theme i VSC (der var nogle ret svage kontraster (og vattede farver) i standarden..!..), så det er jo dejligt..:)

%(01.09.23, 19:10) Har lige holdt en kodepause og sendt nogle ansøgninger i stedet, men nu kan jeg mærke, at jeg er fr træt. Men næste punkt er CSS og så at prøve at køre og debugge det! Men kan godt være, at det først bliver på søndag (skal afsted til Fyn kl. 11 i morgen).

(01.09.23, 19:11) I'm considering making thumbs up and down next to the rating bar/ stars and then make it give 0xFFFF for thumbs up and 0x0001 for thumbs down. The simple mean bots will then just include these values as is, but at some point we can make it a convention that these values means exactly that: thumbs up and thumbs down. And then more advanced bots can ``choose'' the actual values that respectively 0xFFFF and 0x0001 will be transformed into (such that 0xFFFE will actually typically be the highest rating you can give in practice i.e.\ because 0xFFFF will be automatically lowered by the bots).




\section{Some comments etc.}
(18:10.23, 11:37)
I have a few comments about my Sematic Database project.

First of all, let me mention that I intend to write a much shorter version of the READ-ME.md, where I pitch the idea starting from: Hey, why don't we make a browser extension with ratings and comments across the whole web, and where comments can be sorted after subject, and where ratings are arbitrarily nuanced (with predicates, much like tags) and rated on a scale. And then those ratings should also be able to be used for searching among things, and that could be done on a dedicated website (or several, but that point is for a longer pitch, not this short one) where users can then search for things via categories---and subcategories!---and predicates. (I will keep my current README, but that will then just become the longer version of the pitch.) (11:43)

.\,.\,I also want to mention that although I have put that idea in the background in my project for now, I still believe that it could be a good idea if users can adjust the precision of their ratings, namely by adjusting the amount of stars (down to just thumbs-up--thumbs-down if they want) that the ratings have. Ad since we are not using the last byte of the short (i.e.\ the two-byte short integer/word), this could easily just be encoded in this byte.\,:) *(Oh, and it might also be beneficial to have the user first click positive or negative rating, and then give them the stars only then, such that fewer stars will be needed, which \emph{might} create a better user experience---it is worth considering at least.)

.\,.\,I should also mention, that my current ``applies to \ldots'' statement titles should be changed. ``Belongs to \ldots'' would be a much better choice right off the bat. But I actually also want to have it, at some point, so that adjective predicates (i.e.\ from the standard adjective-predicate template) will be rendered (as statement titles) as just the adjective in question.



\subsection{Selling the idea to existing open source communities}

(02.02.24, 10:52) I am absolutely convinced that my Semantic Network Project will lead to the future of the web, and the idea \emph{must} also be ``sellable,'' in particular also to the various open source communities like the Linux community and Mozilla. The point is this: You have already shown, Linux and Mozilla, that open source project can very well compete with commercial solutions. Linux definitely competes with e.g.\ Windows---and I think that it is even quite a bit better! And Firefox definitely also competes with Chrome and Edge (it is my preferred browser)! So why couldn't open source also compete with web sites/platforms like YouTube and Facebook, etc.?? (And especially Reddit.\,.) `Well,' you might say, `a website requires servers and maintenance etc.' Yes, they do, but so what? If the users are happy to use the service, they will provide enough money to maintain the service. And if your service also helps other organizations/companies (like my Semantic Network Project!), then your absolutely golden: the service maintenance will be paid for! But hold on, should open source then try to compete with all existing websites at once, or which website should we choose to compete with first? Well! I know exactly what website we should begin with! Enter the concepts of a `Semantic Database' (SDB) and the concept of a `User-Programmable Application' (UPA)!

The great thing about an SDB is that you don't need to change the backend and add new relations to your database whenever you want to develop a new part of the web app. You have to do that with a relational database. If you for instance want to add a like button to your resources, you have to add new relations to contain the associated data. A relational database has its advantages. But one of the big advantage of an SDB is that you don't have to do this! The database is so flexible that you can just use the core of it for pretty much anything that you want.\footnote{The only time that you would need to write SQL (not counting the times that you upgrade the SDB solution itself, if you happen to be both a developer and a user of the SDB solution) is if you want to write the so-called `Aggregation Bots,' but these can also be implemented outside of the backend, and even by third parties.}
With an SDB, the users themselves can create whatever data structures they desire!\footnote{To fully make this point understandable, I should preferably have developed my prototype (openSDB) just a little bit further.}
%But of course, if you want to implement something like a like button, say, then you also need to change the frontend code, and this is also typically the job of the developers as well. %..Hm, lad mig lige tænke lidt inden jeg fortsætter, for open source kan jo allerede meget i sig selv, uden UPA.. (11:28) ...(11:50) Jo, lad mig bare fortsætte denne pointe. ..Ah, men lad mig lige starte på en anden måde..
This means that application can evolve quite a lot in how it can be used in different ways even without any actions required from the developers. This saves a lot of work from the developers, which means that the services are easier to pay for (by the users and donors).

But it gets even better! Introducing he concept of an `UPA,' which is that users can also upload scripts, (including React modules), HTML snippets, and CSS style sheets to the site! These will of course not be approved automatically,\footnote{Except if someone at some point were to complete my `safe JS subset' language, or a language like it.} but will be approved by the developers, requiring some work by them. However! Here the developers can safe a lot of work by out-sourcing the code validation to the users! This can be done by having a `safe script' rating, which the users can then rate. (This is almost trivial to implement.) And by utilizing something which I often refer to as `user groups' (implemented via a so-called `Aggregation bot' which then aggregates ratings of users), the developers can implement a user ranking based on how much they trust the users decisions of when a script is safe and not. This technology can even be furthered by implementing code annotations with attached ratings such that users can even rate specific snippets of a script for safety, which means that the next users to read through it can get an overview of which parts are the most tested/analyzed ones and which part are in greatest need for testing and/or analyzing. And with this, the developers then only need to read through the scripts that has risen to the surface in this process\footnote{Oh, by the way, users can also rate how interested they are in new script, meaning that the interesting scripts will get more attention by the user community.} a few times before accepting it for users to then be able to add to the site, i.e.\ as a kind of (so-called) `add-on' (similarly to how a `browser extension' works to change the contents of a site). Each user can then simply choose which extensions to use, a bit similarly to how open source projects can fork in general, but where the the process of forking back and forth a now just way more flexible, and can be decided by the individual users!

Not only does all this help reduce the cost of maintenance, which help justify the open source model, but it does something even more important as well. It answers the question posed in the beginning of this text of `what website to start with.' Well, if you start with this website, then it can branch and develop into all other websites! For instance, it could develop such that open-source `Facebook' is just under one tab in the site header, and YouTube, Reddit Wikipedia, etc., is under other tabs! So by starting with this website for this proposed open source website project, we get all possible websites for the price of one!

And what is more, because there are in fact more, the SDB website that I have in mind will also fill out a hole in the market that isn't discovered yet! I thus strongly believe that even without all the points above, and even without being an open source site, the `semantic website' that I have in mind would be a great commercial idea, had I wanted to make it a private enterprise. I really believe that the things that the site will afford will be greatly appreciated by the users, even without this whole deal of being open source, and about being a UPA. This is what truly makes this idea so golden: You don't even have to convince the users to join alone on being an `open source alternative to existing commercial sites,' and on being a flexible UPA, \emph{once} it has gathered enough interest. The website will also be able to attract users simply on the ground on filling out a hole in the market, providing the users with affordances/abilities that the didn't know they needed. If this sounds interesting, see my README.md introduction to the site on github.com/mjdamgaard/Semantic-Network-Project. (There is also a longer version (not well-edited, though) of that README which mentions more points about being open source etc.) %(12:42)

So there we go, this is an absolutely great idea---immensely great, I would say! Now, there is also another important point about how to ensure that the users can always trust the companies/organizations not to be corrupted over time. But I have described this in several other places (here in this `23-xx note collection.tex' document, possibly in my `main.text'/`2021 notes' document also, and in my READMEs in the GitHub repo of the project), so let me not repeat this here.

*\textit{The next paragraph is not so important; feel free to skip it:}

.\,.\,Oh, except that I should actually mention an important point.\,. .\,.\,Yeah okay so the outline of the idea is that the organizations/companies whose supplies the web services to run the website should all allow for any competitor/collaborator/third party to copy all their data and set up a copy of the site. This allows for a whole network of service providers where if one `node' in this network all of a sudden becomes corrupt (e.g.\ by not adhering to this rule), then the users can just immediately switch to and use some of the other `nodes.' Now, the new point that I wanted to mention is this: When this ``copying'' happens, it should be a public process that all users can see (perhaps getting notifications about it, even). The users have then agreed to this from the start, as part of using the services in the first place. And (now comes the point), if a user wants to have some data deleted about them,\footnote{Anonymity is greatly advised for the Semantic Network Project for the accounts/profiles that the users use in every-day matters, but a user might for instance accidentally reveal there identity even so.} then they just have to send this request to all nodes who has copied the data in question. (However, this is only in principle; in reality, the network will work together such that a user only needs to put in the request in one place, and then it will be sent to all other relevant nodes in the network (and confirmations of the deletion will also be sent back by each node).) Since all these nodes are required by law (as far as I know) to then delete the relevant data, this whole distributedness of the Semantic Network should thus not be a hindrance in practice for the users to get data deleted.

.\,.\,Okay, this was all I wanted to write here, I guess. If I think of something else that I've forgotten, then I will just append it here. (13:14)



(14:10, 16.02.24) There is actually another point which is quite important to mention in relation to the UPA idea. And that is that users will have to uprate the scripts, style sheets, and modules before they can use them.\footnote{
	By the way, if a user logs into another computer, they will be prompted again, in serious tones, to ensure that they trust the various scripts etc.\ that they have are using (going through each one of them). This will make it a lot harder for people to convince people online to log into their own account and accept their scripts, since they users will then have to click through several warnings in that process. By the way, all the scripts etc.\ will also by checked by a parser/language checker for a supposedly safe subset of the given language, and these extra precautions are thus only there to ensure that this parser/language checker does not need to immediately be absolute 100 \% devoid of exploits on day one.
}
And this means that it will be quite easy to figure out which script-etc.\ creators have contributed a lot to the community. So when we also get the donation system that I have in mind (see my other notes, e.g.\ my `READE\_longer\_version.md' document) into the picture. It means that people will with all likelihood start donating serious money to these creators. (And the site can also run adds as something which the users can opt in to, where the money will then go to the `user groups' that the user chooses.) Think of how much money is involved on sites such as YouTube for the perspective of the creators. With this open source UPA idea, we can get an open source site where the open source creators can be rewarded with similar sums of money! (14:27)




\section{New context templates}

(04.03.24) I think I might change the template syntax, such that the full title is written first, followed by the shortened title, where some numbered placeholders are used to refer back to the template placeholders, in order of appearance. And then I'll do something else about the capitalization problem. I will recommend uncapitalized titles as standard. And the application should then capitalize on its own. However, when the first letter should not be capitalized, I will introduce a syntax (I'm thinking something like \texttt{\_} %, \texttt{\_\{...\}},
and
\texttt{\textasciicircum}, %and \texttt{\textasciicircum\{...\}},
escaped by \texttt{\_\_} and \texttt{\textasciicircum\textasciicircum}) to change which letter are and which are not capitalized during capitalization.

By the way, if it at some point becomes obsolete with the shortened titles, opting to only use up-rated shortening schemes instead, then one can just start identifying the old templates with shorten-title instructions with the equivalent ones without those shorten-title instructions at the end of the template. (18:33)

(10:35, 05.03.24) No, I will instead just make it so that some codes can be added to the template, after yet another (probably `$|$'-)separator, after the short-title template. These codes can then for instance say `don't capitalize (if not capital by default) the first letter *(first \emph{word}, rather, as it could be a placeholder, in which case the capitalization is done recursively) of the full title,' `don't capitalize the first letter of the short title,' capitalize all letters of the first word of the full title,' etc. (And again, if this becomes obsolete, the contexts can then just be identified with their simpler versions without these codes, joining their IDs as one.)

(06.03.24, 9:55) No, never mind about the trailing codes for the templates. The most important thing is just that title/defining item can avoid capitalization, which can just be done with a leading `\_'. We can also use a leading `\texttt{\textasciicircum}' to say that all letters of the following word needs to be capitalized upon capitalization, but in practice, this is only really relevant for abbreviations, such as PDF (and XKCD), and in those cases, there is really no reason why they shouldn't just be capital always. But we can choose to include it. Now, I also think it might be useful to be able to use a leading `\_' for templates, but then again, in reality this should be more up to the app rather than the entities, i.e.\ whether the full or short titles should be capitalized, and when. So there we are. I still intend to remove my `\{\}' syntax in place of having the full and the shortened title separated by `$|$'. And in regards to this, one could even use several separators to write even shorter titles (in place of using nested \{\}'s)!\,. I think this could be useful e.g.\ for entities like the `Good Movies' entity, which by the way soon will become the `Good movies' entity, since the def-item `Movies' will no longer be capitalized, as it is not the first word of the title. The template could then be change to.\,. `$<$Adjective$>$ $<$Category$>$$|$\_$|$@1', hm, where the `\_' between the separators could then signify `repeat the last thing,' in this case the full title. And the ultrashort title at the end then uses a numbered placeholder, `@1' to pull over the `$<$Adjective$>$' def-item from the full title (such numbered placeholders will always refer back to the full title). One could also maybe write `$<$Adjective$>$ $<$Category$>$$|$\_$|$\_@1' if we want to use the leading-`\_' syntax to tell that the adjective should not be capitalized for the ultrashort title. Yeah, I think that's actually a good idea.\,. (10:14) .\,.(Oh, and the reason I sandwich in the `\_' is by the way because I want repeated special symbols, and in `$||$' or `\_\_', to escape the special meaning and become e.g.\ `$|$' or `\_' when rendered.)






\section{p-models/-ontologies}

I haven't written too much about my p-model idea, despite how important I think it is. But in very simple terms, it is about linking statements in terms of how correlated their truths are: `If one statement turns out to be true, what is the likelihood, that the other is then true?' So I should make sure to work on adding this when I (or hopefully `I' will be a `we') start to work on the discussion parts of the comment sections. (10:44, 05.03.24)



\section{New thoughts (21.03.24)}

(12:34) I feel inspired to continue working on my semantic network system (openSDB/the Semantic Network Project). I have my new templates, described above, that I want to implement. And I've also had some new ideas today about maybe actually including `predicates' again in the SDB, \emph{side by side} with the `categories.' But I should think some more on that. I've also re-convinced myself of how important it is for me to get to the point where I have a nice looking and easy-to-use browser extension, and with a website underneath that is also somewhat alright to look at, and where all the main things are implemented, especially searching/sorting categories. And yesterday I also thought about a great point that will attract a large amount of users, after the initial ones, and that is the following.

Users should be able to create a.\,. `subject'.\,. Hm, name to be declared.\,. .\,.\,a subject of interest, and then up-rate statements for the subject. Other users can then take the tour and go through each of the most relevant (as many as they want) statements, and rate how much they agree to it. The service providers can then agree to make statistics over this subject, deriving correlation vectors *(you know those from standard linear algebra ML) from this data subset (of ratings related to the `subject'), and then rate each user (once the site has taken off, we can make so that the users have to OK this first) that has made ratings related to the subject (and/or has taken the `tour,' as I just called it.\,.). And with just a relatively small amount of users participating, this can already lead to something very useful (due to the very high \emph{quality} of the semantic data), such that the users will be able to find a good `user group' for them regarding this `subject.' This can then easily lead to much, much improved recommendations, search results, comments, etc., compared to what the existing web has to offer otherwise. So this is how we go from a small community to a very large one: by attracting a large number of users based on the fact that they can get so good recommendations---and meet other users with similar tastes as themselves online---and with not very many users and very much data required in order for these recommendations to start being better than what the existing web offers. (12:55) *(!\,:))

(12:59) Hm, and as I said, I'm considering adding predicates (not replacing categories) to the fundamental system. The idea is that, 1, it makes it more intuitive if predicates can be formulated as predicates, I think, and 2, the predicates are not meant to be used as categories anyway! For I am dropping using the `$<$Adjective$>$ $<$Category$>$' template anyway, in favor of `entities that are $<$Adjective$>$.' But this category, as said, is not really useful as a category in it self. It is only useful when combined with another category. So I actually intend, now, to make a special type of category, formed from exactly one `category' and one `predicate,' and then make a bot that rates this category automatically. (And the service providers are then free to decide, which and how many of these compound categories its bot will rate (and bot ratings can always be deleted, e.g.\ if a compound category is to be discontinued).) (13:07) .\,.\,I always did intend for things to be like this at some point, but maybe I should do it from the start, especially if I also decide to make `predicates' special in the fundamental SDB, as I'm considering.\,. (13:09)

\ldots (13:42) Hm, maybe I'm slowly landing on indeed introducing `predicates,' but without altering the function of the `categories' in the fundamental system at all. And `predicates' could then just get a separate SemanticInputs table (maybe renamed to CategoryRatings and PredicateRatings), which works like the original one, but with the PK removed and the SK made into the PK.\,. .\,.\,(such that you e.g.\ cannot look up all `fun' entities, but you can only look up how `fun' a given entity is). .\,.\,I have also been considering adding an `is relevant' rating directly to PredicateRatings as well, but maybe not.\,. (13:48) .\,.\,Hm, it could definitely do some good (in terms of potential speedups in the future), but is it worth it---'cause it also creates a redundancy, since `Relevant predicates' should also be a properties category.\,. .\,.\,Hm, and I guess you could also make a bot that just attaches the (compressed) actual rating to the least significant byte in the short (integer) of the `Relevant predicates' ratings.\,. (13:52) .\,.\,Yeah, that's how we will make that speedup in the future. Okay, so this might indeed be the plan, namely to simply introduce a PredicateRatings, that only has a PK which can be used to look up individual ratings (not lists of ratings).\,. .\,.\,Hm, but how about just overloading the semantics such that a predicate entity in the `cat\_id' column just is automatically interpreted as the category of all `entities that are $<$predicate$>$'.\,. .\,.\,Right, 'cause I could actually imagine cases where a `entities that are $<$predicate$>$' category could be useful.\,. .\,.\,Okay, this seems like the right call.\,. .\,.\,Hm, but what about plural vs.\ singular predicates; should I make templates that define both?\,.\,. (14:05) .\,.\,Oh wait, couldn't we just.\,. hm, maybe not.\,. We \emph{could} just make a category template based on a predicate, and then just make the short title read like a predicate (i.e.\ a verb phrase), but this complicates all other titles a lot, so no. It's much better to introduce a `Predicate' type. And I think I will just let that type behave like almost like a category in its own column.\,. Hm, or maybe not, on second thought.\,. (14:13) .\,.\,Well, yes and no: I think I might actually still call it `subcategories' and `supercategories' even for predicates. Now back to the singular/plural matter, I think I might actually do this.\,. .\,.\,(14:22) Hm, I think I will just do this with an `is/are' (or `takes/take,' `has/have,' etc.) slash syntax, and then just for now disregard the potential for cases where several words could replace several words, as this will probably almost never happen. I've btw also thought of a RegExp-like approach, writing replacement codes for the plural version at the end of the template, but again: the slash syntax will probably be good enough for almost all cases.\,. (14:25)

\ldots\ (16:49) No! I will not introduce `predicates,' or make SemanticInputs take predicate entities. I have a much better solution. The point is: We decide what `categories' are. And `Funny,' for instance, can also be a category.\,! Note that since the F is capitalized, it can be interpreted as a proper noun: the category `Funny.' And that's it, we will recommend more than just normal nouns as category titles: Capitalized adjectives and verbs are also recommended! And this should \emph{not} even require a clarifying template context. For instance, `Category $\blacktriangleright$ Funny' is a perfect valid (recommended) entity. Furthermore, `Category $\blacktriangleright$ Has good acting' is also a valid (recommended) category. (And `Has' here always has to be rendered as capitalized, as it is always understood as a category.) But categories beginning with `Is' is not really recommended, unless the somewhat (unlikely) help disambiguate the whole thing. It is often better to then simply write the following adjective or noun alone (e.g.\ as in `Funny' and `Iconic movie' rather than `Is funny' and `Is an iconic movie'). (17:04)

This means that I will actually go back to using manual capitalization for each entity, since now the entities always have to be capitalized once again. And what's more, I will even go back to (or rather, not go away from) using the `\{\}' syntax, instead of the `$|$' syntax that I recently proposed above. The reason for this is simply that it is actually a good syntax: It's easier to write and is less verbose. (17:09)

.\,.\,Oh, and about the ``compound categories'' which I talked about earlier today, I will keep them, but calling them `aggregate categories' instead. And their syntax should simply just use code, not NL. More precisely, they should simply use plus signs between the categories.


(24.03.24, 18:10) I have a lot of things to write.\,:) I'm not sure I will get to them all this evening, 'cause I need to brainstorm over the most recent parts of it. First of all, I just want to quickly pat myself on the back again over realizing that adjectives and verbs can be categories as well. It really makes sense. And since last time, I have even realized that in a way, a category titled `Funny' is more semantically precise than a category named `Movies.' And it seems quite trivial in hindsight, 'cause a category are really just a title that you put on a metaphorical box of things/concepts, which makes it exactly equal to how we define `tags.' Categories \emph{are} just tags, and tags are categories. Anyway, so it doesn't even limit the semantical precision of the system, bootstrapping its own semantics, when we make this change, and the change is so handy as well. Okay, I got to move on.

I have some other really great news. And a big part of it basically boils down to the fact, that I thing the website (plus browser extension) can become really useful (enough to attract he first user community) even if it only really revolves around `external links' as the only property that is really used by the website / the users. This makes me think that I am \emph{much} closer to being able to complete a useful version of the website (on my own, even!) than I thought before! But let me get back to this more later, 'cause I really need to brainstorm about these latest ideas, which I just got on an afternoon(/evening) walk that I was just on.

I got the idea that I can probably actually merge the `Type' type and the `Category' type into simply `Category.' This will mean that instead of the first entity being `Type $\blacktriangleright$ Type,' it will be `Category $\blacktriangleright$ Category' instead. (I think that part of the reason why I didn't see and/or want this before is because I really wanted plural nouns for categories.\,. I think, maybe.\,.) Anyway, so instead of having a Type called `Movie' and a Category called `Movies,' we now remove this redundancy and simply use the Category `Movie.'

And then I've also gotten an idea on top of this: Maybe we should merge the type field and the context field of the Entities table into one.\,!\,.\,. My idea is then that when a Category is the.\,. Well, I think we should call the merged field `type,' by the way.\,. So when a Category is the Type, the interpretation is that the defining string should be.\,. Wait, no, let me think.\,. (18:33) .\,.\,(18:42) Hm, we can always make it the standard entity query to.\,. Hm, or maybe not, but we could still use a query type where all parent contexts are given at once (and maybe I should call it `context' rather than `type' if I merge them). But maybe this is not relevant, so let me think.\,. \ldots (19:07) Hm, one could make it so that the sequence of contexts are interpreted as supercategories, each one being a subcategory of the former, and then you could simply merge `Templates' with `Categories' as well, saying that a template is just a category with meta semantics, more precisely with semantic about the form and the intended interpretation of the defining strings of its members.\,. .\,.\,(Making the `$<>$' syntax part of the fundamental interpretation of the defining strings: If it is there, the entity in question should be interpreted as a Template, and otherwise the `$<$' and `$>$' need to be escaped.\,.) Hm, maybe I will run into a previous problem here again of determining whether the context is a supercategory or a parent category. Hm.\,. .\,.\,Hm, unless we just say that parent category contexts simply always need to be followed by a template defStr, and otherwise they are supercategories.\,. Doesn't sound too bad, actually, but I'll think some more on it.\,. (19:20) .\,.\,This actually sounds like a really good idea!\,.\,. 'Cause we hardly need non-category entities, or non-meta entities in general, with no template.\,! .\,.\,I think I might make this change.\,. (19:23)

(25.03.24, 11:29) Okay, I'm definitely on to something good, especially in regards to merging types and categories, and also removing the type field. But I should stick to the principle of using as little/few context(s) as possible. And in that regard, I might even go further than before and say that templates can contain the type as well.\,.

(12:00) It by the way makes sense, that I get these thought now, 'cause after having realized that the site can work well with only external links in the beginning, it doesn't feel as sensible to make all that effort to allow for.\,. entities that are not either `categories' or `subjects' (as in `topics'). In the beginning there will thus not really be much use for having types. (I am currently considering whether we should distinguish `subjects' from `categories' at least.\,.) .\,.\,Oh, of course we also have the `User' and the `Text' types, e.g.\,. (12:05) .\,.\,Hm, and what about Properties?\,.\,. .\,.\,Well yeah, of course we still need types, but they could then maybe still just be a part of the templates, or they could of course be the contexts of the templates.\,.

\ldots (12:39) Hm, maybe I should keep the type field.\,. .\,.\,Or should types just be `semantic'?\,.\,. (12:43) .\,.\,(Making use of a Type property.\,.) .\,.\,Yes, I think so. But now I also got another interesting idea: How about attaching a type to predicates, such that you could construct e.g.\ `Funny as a Movie'.\,. (12:51) .\,.\,WWII is a `Good' subject of history, but it is not necessarily a `Good' thing, as an example.\,. (12:53) .\,.\,This is actually so interesting, 'cause it can also be a help for the Aggregate/Aggregation bots/categories as well.\,.\,!\,.\,. .\,.\,So yeah, how about (often) ambiguous types, such that each entities might have several types, which are each `semantically' up-rated for the given entity (for a `Type' property), and where predicates/(ranked) categories have a common opportunity to be given an `as a' type? .\,. (13:00) .\,.\,Hm, but I guess the pragmatic thing is to then just make a template for these `as a' predicates.\,. Sure.\,. .\,.\,Wait, or shouldn't just be required?\,.\,. .\,.\,Oh, and maybe this could also separate `Predicates' from `Categories,' namely by saying that.\,. well, I guess categories might also benefit from the `as a' clarification.\,. (13:06) .\,.\,But yeah, this `as a' idea might actually be one of the most important ideas in this latest round, and that's saying quite a bit, I think---at least that's what I want: to `say quite a bit' about how this might be a very important idea.\,. (13:09) .\,.\,Because it really embraces ambiguity of entities in full, which I actually think is very important for the usability (while the semantics of the actual statements about the entities still needs to be somewhat precise (although not at all completely precise either)).\,. (13:12) %..I'm still typing quite slowly on this keyboard, despite having had it for over a year now---it has gotten a little bit better, but only a little..
.\,.\,(13:17) Oh wait, maybe Categories, including Properties, could include the type themselves, whereas Predicates.\,. does the same, never mind.\,. .\,.\,Ah, and yet, Categories/Properties do not need a template, necess.\,. oh, that might actually also be useful for them, never mind still.\,.

(13:26) Ooh, maybe the.\,. various types for an entity could be the first tabs in the app column.\,!\,.\,. .\,.\,Yes!\,.\,.

.\,.\,But we also don't want maximal ambiguity, which is why one can definitely still make good use of the templates to disambiguate entities (and making them get both a short and a full title). (13:34)

.\,.\,(A `Homonyms' tab could by the way also be useful.\,.) .\,.\,`\{Similar words\} and homo-nymns'.\,. .\,.\,`Homonyms and \{Similar words\}'.\,. .\,.\,Hm, `Homonyms and \{similar words\}', and then I should actually go back to automatic capitalization of the first letter when appropriate.\,. Hm, but what about e.g.\ iPad?\,.\,. .\,.\,Hm, maybe I'll just use an underscore to say `never capitalize,' I think I will.\,. (13:46)

.\,.\,So a pretty much typeless version of the system, where types are instead just specified in the categories/predicates.\,. .\,.\,And where Entities are thus not really specific semantic entities (after all), but are instead just `words'.\,.

\ldots (14:14) Okay, I think I might go as far as to add a type\_id field to SemanticInputs, and then just add it in the beginning *(just after the user\_id) of the.\,. primary index.\,. .\,.\,and of the secondary index, such that it will generally be compressed into almost nothing (I think). .\,.\,Oh, and a type\_id of 0 just means, as a `word'/`subject'/`category,' essentially.\,. .\,.\,I think.\,. (14:20) .\,.\,Hm, or maybe it isn't needed. But I'll perhaps keep this option open, why not?\,.\,. (14:25) I also just realized that this also makes it easier to decide the right tab to start on, when you have the variable types, cause then you just open the.\,. well, the relevant type tab.\,. .\,.\,Oh, back to the previous subject: You need the 0 type to rate the `Types' property, so yeah, it needs to be included.\,:) (14:29) .\,.\, So a type of 0 basically means `as all/any of its possible types together.'\,:) .\,.\,`as a general word (in the SDB)'.\,.\,:) .\,.\,Oh, and that can be the default tab if no specific type is relevant in the context (e.g.\ if users has just come from a word search, or has typed the ID into the URL field or some other entity search).

.\,.\,Okay, but do we really need whole templates, then, to disambiguate, or can we actually not just use the defStr alone, together with the `\{\}' syntax.\,?(!) (14:38) .\,.\,And the `\#$<id>$' syntax as well.\,.\,!\,.\,. .\,.\,Hm, then an Entity would simply consist of a defStr, which I might then rename to simple `title' for simplicity (and then make it implicit that this title can potentially be a coded one (i.e.\ with the mentioned syntax)).\,. I think this might be a good idea.\,.(!) (14:44)

\ldots (14:55) Wait, no, maybe the types are actually all we need in terms of disambiguation!\,.\,. .\,.\,Yeah.\,.\,!\,! .\,.\,Hm, or maybe not, but I could be close to something.\,. .\,.\,(15:00) Hm, I could consider using two levels of type specifications (and I am): One to select between homonyms and word functions (e.g.\ `as a name'), and one to select between `as a subject,' `as a person,' `as an actor,' etc.\,. hm, and now I'm also considering having an arbitrary amount of types, although that kinda brings me back to having to use the templates and/or contexts in general, right?\,.\,. Let me see.\,. .\,.\,Hm, I need to also have things like Properties in mind.\,. (15:08) .\,.\,Hm yeah, so maybe keeping the context field and the Templates is the right way.\,. .\,.\,Oh, absolutely: I also need the Templates for e.g.\ URLs and such. So no, the Entities will consist of a defStr and a context.\,. (15:13)
.\,.\,Wait, but wouldn't it be better, if we could just.\,. Hm, either make defStr able to be longer, or sort of hide the chopping up of the word somehow.\,. Hm.\,. .\,.\,Hm, I could do that. And how about going back to just letting `as a' be a (very fundamental) template.\,.\,? (15:19) .\,.\,Oh, but the point is: Maybe it's better that the type is connected to the predicate/category in SemanticInputs, rather than the instances.\,. .\,.\,Hm, but yeah, then we still need to be able to disambiguate the instances that need further disambiguation, hence why we need the contexts. .\,.\,And these contexts should preferable all be templates, actually (for disambiguating) (I think.\,.).\,.

.\,.\,Hm, now there's a pull between letting the type sit at the category/predicate, or simply using templates all the way to determine/disambiguate the type.\,. .\,.\,Oh wait, maybe there is a good way to do both! Maybe one could just wrap an entity in as many type specifications as one wants. And then the SDB just automatically orders them.\,. or maybe that's a bad idea, to order them, but still.\,. (15:29) Maybe I'm still on to something here. The idea is then that the category might get a type in SemanticInputs, but the instances might also be wrapped in an additional type to disambiguate them further. And when you then go to one such instance, you just first select the tab of the wrapper type, and then subsequently (but immediately and automatically) select the tab of the category/predicate type. .\,.\,!\,!\,.\,. (15:32)

.\,.\,Oh, the idea of using `as a' in general is actually really just so good.\,.\,! What a great and fortunate idea this was.\,.(!) (15:36)

.\,.\,And I think the idea with which I ended the last paragraph before the previous one is the right way to go.\,. (15:37) But let me take a walk and think some more\ldots

(15:46) Okay (before I go out), two quick things: No, I think that the initial disambiguation in terms of similar words/homonymns should be done by the entity itself as an initial thing. But (second thing) maybe we don't need templates for it, and maybe we don't even need templates for URLs, 'cause we could just make defStr/title longer (in terms of maximum), while the database can still just compress them.\,. at least if we then make the PK the actual string and not the (then subsequent) entID.\,. the regular (`table compression') way.\,.\,:)\,\ldots (15:51)

\ldots\ (18:40) Okay, I should actually move away from capitalization entirely.\,! For now that we ought to think of the Entities more as words, it makes sense to not capitalize them (for the EntityTitles).\,!\,:) And then my plan, I think, is the following. Entities are defined by a single defStr, and no longer have a context field, or a type field. But templates are still supported. Template instances are just made by starting the defStr with an entity id of the given template entity. I don't intend to use a template for normal disambiguation, such as in e.g.\ `rock (music).' Simply writing out `rock (music)' as the whole defStr is better. I will also increase the maximum defStr length, at least in the interface of the SDB. And then in the implementation, one might keep a table of common defStr prefixes, such that one might be able to.\,. chop it up.\,. I'll need to think more about this part. But another important thing is then to introduce Indexes, just like the table in create\_sdb.sql is now, but of course with only a defStr field. Both template entities and template instance entities can also be a part of such Indexes. And for these, you also simply search for them verbatim. So when searching for a template instance, one could then potentially first search verbatim for the template defStr, then take the obtained id and prepend it to a search for the defStr of the template instance (i.e.\ by searching in these `Indexes'). (18:51) .\,.\,(Or one could also write `rock (music genre),' btw.\,.)

(22:33) Hm, about capitalization, I could also use an idea of adding a trailing code to the Entities, either appended to the defStr or as a separate field, which determines the capitalization. This might nemlig %*(I wish English had this adverb.)
be needed anyway, I think, if searching is otherwise complicated by having capital letters in the mix, and not having only small letters.\,.

(26.03.24, 11:13) I need to think more about the Indexes and the defStr length. If I can plan a good implementation that hides the limit that (My)SQL has, then I will probably go forward with an initial max length, and then think about extending it in the future.\,. .\,.\,Hm, alternatively, one could make another defStr syntax for concatenating defining strings, but then the users.\,. hm, or could it be hidden in the interface?\,.\,. .\,.\,Well, this could be my future implementation plan, anyway, and then I should just reserve some syntax for it.\,. (11:19) .\,.\,Oh, and regardless of whether it should be hidden under the SDB interface or be visible to the users, I need to reserve this syntax all the same. .\,.\,And even if I find another solution, it doesn't hurt to reserve some syntax for concatenation templates, surely.\,:) Okay, so this is what I will do.\,. .\,.\,The obvious contender is of course just to use a string of `\#$<id>$'s, but then I should probably use a colon separator for the Templates, which does also kind of feel right to have: It does not feel completely right to just use a string of `\#$<id>$'s for the regular Templates, and let the.\,. function be treated on the same footing as the inputs.\,. well, it works for functional programming languages, so it could work. Hm, but I could also simply change the leading `\#' for the regular templates to another symbol, e.g.\ `@'.\,. (11:26) .\,.\,Yes, I will do this, since it also neatly keeps the room open for using the `\#$<id>$' syntax in the defining strings of the regular entities as well. (11:34) .\,.\,Hm, let me actually try to think of a better term to replace `defining string'.\,. .\,.\,`word'?\,.\,. .\,.\,`string'?\,.\,. .\,.\,Hm, I'll get back to this, and just use defStr for now.\,.

.\,.\,I'll also just keep the `\{\}' syntax for the regular, non-template-instance entities, why not?\,. (11:40) .\,.\,(But I don't plan on advertising it, really, or using it myself---only for templates.) .\,.\,So the special characters will simply be `\{', `\}', `\#', and `@'.\,. (11:44) .\,.\,And they should be escaped by `\textbackslash', which is then also a special character, escaped with itself as well. .\,.\,Oh, but, importantly, `@' should only be escaped if it appears as the (otherwise) first character of the defStr. .\,.\,Hm, but wouldn't it be cleaner, if we only used this code for the templates, and then only had the rule that leading `@'s need to be escaped for the regular, non-template-or-template-instance entities?\,.\,. (11:50) .\,.\,Hm, and leading `\#'s as well.\,. .\,.\,Wait, I also forgot about `$<$' and `$>$'.\,. .\,.\,which I am actually now considering changing somewhat, but let me see.\,. .\,.\,Having special characters is no problem, 'cause I will just make a radio button options to choose between `WYSIWYG' and `code' when submitting new entities. (12:03) *(And there should be the same options when searching for entities.) .\,.\,But back to thinking about the `$<$$>$' syntax.\,. .\,.\,Ah, I should replace that with a simple placeholder sign, such that the placeholder title (which was formerly written inside `$<>$') can instead just put before the given placeholder with a colon after the placeholder title (if one wants; all things are possible). The point is then that this allows the author to decide whether the placeholder title / property title is included in the short EntityTitle, or if it only appears in the full title, i.e.\ by using the `\{\}' syntax. (12:09) .\,.\,Good, so this might.\,. oh, but I still need to determine the syntax for the template placeholders.\,. \ldots (12:24) Ah, I can just use either `@' or `\%' (I think the former 'cause it's more visible on its own), and then this only need to be a special character in the context of a template alone. Regular entities still don't have to escape any other than the leading `@' (I think I will go with that.\,.). .\,.\,And WYSIWYG does by the way not need to be an option when writing templates; here you need to code. (12:29)

(12:42) Hm, I think I'm almost ready with this new version of the system. I hope it won't take forever to implement up through all the layers of the stack.\,. But I guess I should try to think about whether I am missing something, or if I'm good to go.\,. .\,.\,Hm, you could say that there are mostly somewhat superficial changes (that nevertheless makes searching much easier, and makes a huge difference all in all), and that the only major change is that the type of the entities are now kept more ambiguous until one makes a statement about them.\,. .\,.\,Yeah, and then there's top-level type tabs, which will cause significant changes to the app as well, but I'm ready for it.\,. (It is probably gonna be a lot of work, but I have the time, now that nobody seems to care about my theorem, ``fortunately'' enough.\,;)\,:))

\ldots\ (15:15) I will indeed just let the capitalization code be appended as part of the defStr. And I think I'll then keep `$|$' as a special character as well, used to separate the capitalization code from the initial string. And potentially, in the future, one could also imagine adding further codes to e.g.\ determine how to bend the words in other cases, and specifically (for English) the plural case. Of course, an introduction of this possibility will then make us want to edit old entities to add new codes to them, but at that point, we will likely have the (combined) resources to do so, no sweat.\,.\,:) (15:20) .\,.\,And for the capitalization codes, I think we should just use something like a comma-separated list of integers, denoting the words whose first letter should always be capitalized, and then we can also in rare cases have ``floating point numbers,'' where the integer before the dot is the word number, and the integer after dot is the letter number (used e.g.\ to capitalize P in `iPad'). And if the first word has no code to it.\,. Oh, and 1.0 could also be used to say, don't capitalize the first word, essentially. 'Cause as I was about to say, if the first word has no cap.\ code to it, then one can assume that one should capitalize the first letter of the first word if appropriate for the context. (15:29) .\,.\,But I will still try to work with entities that are not capitalized except for proper nouns, I think. *(Hm, and a dot with nothing after it could mean `capitalize all letters.')

.\,.\,Okay, let me try to get going with the restructuring of the system.\,:)\,\ldots

(16:08) Oh, I could perhaps simply call the defStr `definition' instead (and then use simply `def' as the abbreviation rather than `defStr'). Great!

(17:04) Oh, apparently the standard maximum length for VARCHARs in indexes are very low in MySQL anyway. So I'll just stick to VARCHAR(255). And then I'll implement a query proc that takes a potentially large string and outputs the IDs of all the entities of the chopped up (in lengths of up to 255 bytes, rounded down if the last character otherwise makes it exceed 255 bytes) string that can be found. And then it is the app's job to turn this into a new query for the entity of the concatenated string. (And note that it is possible to nest concatenations, 'cause when a string containing `\#$<$id$>$'s is inserted, these will in turn also be treated as placeholders, and will be further replaced (and so on) before the final render.)

(17:33) Hm, maybe it's a bad idea to have the trailing capitalization code. Maybe we should just add lower-case versions of all entities, use these in the Indexes, at least for the common natural-language-based entities, and then just make it so that whenever you go to an entity column, there is an automatic search for the `Duplicates' property (for some bot, which the user can change to another if they want), and if one exceeds some high enough rating value, the app will automatically redirect to that entity instead.\,. (17:38)

(18:06) Oh, I haven't really thought about how to implement User entities and Text entities, etc.\,!\,.\,. .\,.\,(18:13) Hm, why not actually just use separate IDs for users, texts, and binaries? And then I could just introduce other syntax similar to the `\#$<$id$>$' syntax to refer to these in Entities.\,.\,! .\,.\,Hm, I could perhaps then simply use e.g.\ `u\#123' to refer to User 123 and `t\#123' to refer to Text 123, while still omitting the `e' for Entity references, referring e.g.\ to Entity 123 by `\#123', still. (18:23) .\,.\,Oh, or maybe I'll then replace `\#' with `@' and move the `u' and `t,' e.g., to the other side of the `@,' thus writing e.g.\ `@u123' instead.\,. (18:25) .\,.\,(And write e.g.\ `@123' for Entity 123.\,.)

.\,.\,(18:30) Hm, maybe we don't even really need to be able to search for strings longer then 255 bytes. For when searching for a longer string, the given Index will probably always turn up just a few of the closest matches, where these index keys then ends on either an entity or a text reference. The app can then expand these, and the user can then choose between them---or the app can recognize which one the user is after. Great.\,!\,:) (18:34)

.\,.\,Ah, this is really great.\,. So nice, this new thing of just using (internal) references of different types.\,.\,!\,:)\,.\,. .\,.\,(And when you want to speak about these users and/or texts, etc., you then just do exactly the same thing as you do for any external references: You make a template (typically) for this kind of thing, and then insert the identification, in this case the internal references (/ placeholders), in that template.)

.\,.\,(18:41) Uh, and then I could maybe use `\%e', `\%u', `\%t', etc., for the template placeholders.\,.\,!\,.\,.
.\,.\,(18:44) Hm, and maybe I could use something like `@t123(\ldots, \ldots, \ldots)' for the templates, although I am not sure that templates should be have their own internal type, like Users and Texts do.\,. .\,.\,Hm, and I would rather just stick to the string of references, without any delimiters, which then just means that you have to always insert input by reference for the templates.\,. .\,.\,Hm, maybe Templates could be their own thing, though, i.e.\ be apart from the Entities and get their own type.\,. Oh, I cannot use `t' two times, by the way.\,. .\,.\,Hm, but I could use P instead, for `temPlate' or `Pattern'.\,. (18:51) .\,.\,Hm, it kinda does make good sense to also give templates their own `type,' then, since they are so meta.\,. .\,.\,On the other hand, we would like to be able to search for them via Indexes as well.\,. .\,.\,You could of course introduce Template Indexes, but.\,. .\,.\,But then again, why go to all this trouble, when there is no real benefit to it?\,. .\,. (18:57) .\,.\,I'll probably still use the `@t123@\ldots @\ldots @\ldots\ \ldots' syntax, but the `@t123' reference is then just simply looked for in the Entities table, i.e.\ at the same entity that is also pointed to by `@123'. .\,.\,Oops, I mean `@p123'.\,. .\,.\,And why not just call it a `pattern,' then. It works just as well as `template'.\,. (19:04) .\,.\,Hm, or I could call them `formats,' alternatively.\,. .\,.\,Yes, I actually like that better, 'cause I like `@f123' better than `@p123'.\,. .\,.\,I like that it looks a bit like a function (`f' for `function'). .\,.\,`Format' it is. (19:13) .\,.\,Or `format string,' if one likes. .\,.\,Oh, it's called `formatted string' in C in full.\,. .\,.\,Oh well, it still becomes just `format' when being brief. %..(And they are also called 'format strings' as well.)

(27.03.24, 10:51) I shouldn't actually call it a Type.\,! Instead it should be a `context category,' which can then be shortened to simply `context' or `cxt,' now that I don't use that otherwise. And for the EntityTitles, which should now also get the cxt (context category) as a property, one could print them as e.g.\ `Movies / The Lord of the Rings' or `Persons / Viggo Mortensen' (or e.g.\ `Actors / Viggo Mortensen,' if you chose that context). And at the top of the AppColumn, you can then choose a tab or a dropdown menu to see all the most relevant context categories for the given Entity (e.g.\ `Viggo Mortensen' or `The Lord of the Rings'). (And when showing the ratings in a given column, you then don't have to repeat the context category. That can just be implicitly implied.)

.\,.\,Hm, should I call it `cxtCat' and `predCat,' then.\,.\,? (11:00) \ldots (11:12) Oh, maybe I should actually move over to using Predicates instead of Categories, 'cause maybe it will now be fine to just use a predicate template (or `format') to say that an instance `belongs to $<$category$>$'.\,. .\,.\,Hm no, categories still just makes everything much easier grammatically.\,. \ldots (12:01) I might start calling it `tags' instead! .\,. .\,.\,Hm, I have also just been wondering about using a variant of the `$\in$' symbol, and I really liked U+22F2, since it makes sense associating that with a rating as well, I think. But I'm annoyed that LaTeX doesn't support it. But now that I'm thinking about calling it `tags,' one could also very well use.\,. $\Lleftarrow$.\,. Yes, it works.\,. Hm, the idea is that it also has the association that the tag to the right is stuck on the instance to the left, while also looking a little bit like a `member of' sign.\,. Hm.\,. .\,.\,Well, there are plenty of options, anyway, and it's not for certain that we will need it.\,. Hm, but it does seem like a good idea. .\,.\,Hm, or simply $\Leftarrow$.\,. .\,.\,Hm, I might like $\Lleftarrow$ a little better than that.\,. .\,.\,Yeah.\,. .\,.\,Back to `tags' vs `categories,' it \emph{is} a bit more intuitive to use the word `tag' for my categories, so I think I'll indeed do that.\,:) But one can also still refer to them as `categories,' especially when also taking about the resulting entity lists.\,. (12:23) .\,.\,Yeah, I'll use both: `tags'/categories.' .\,.\,But I'll try to use `tag' as much as possible, I think.\,. (12:29) .\,.\,Oh, but back to the possible abbreviation, it is actually also quite nice that `$\Leftarrow$' has a natural association with `$=$'.\,. .\,.\,But I'll think more about this some other time.\,. .\,.\,(This is also something which others should preferable look at, before we take the decision (and even then it should preferable be something that can be changed.\,. Oh, I could assign a special character encoding to this sign, and then you can just always change the appearance afterwards.\,.\,!).) That was actually a good idea, i.e.\ making it a special character encoding.\,. (12:37) .\,.\,Well, or one could also just use a standard template, and then one could get the app to render the template instances differently in certain circumstances.\,. (12:38) .\,.\,Anyway, let me move on now.\,.

.\,.\,Hm, tags and categories, and each category is also itself a tag, where the tagged entities are then rated according to how important/useful they are as a member of that category. Couldn't this be a fundamental statement about the semantics of the system?\,.\,. (12:50) .\,.\,Or should I make a `is a useful/important member of $<$category$>$' template? Hm, this reminds me, I think I should go back to still using this `$<>$' syntax as the template/format placeholders, instead of `\%u' etc., oh, and then I could maybe just reserve `$<$u$>$' etc.\ as special placeholders for users etc.\,.\,! (12:54) .\,.\,Good idea. But we should then still not let the name/title inside these placeholders be important to the semantics, like I did before. The full semantics should be understandable just from reading.\,. Oh wait, maybe the placeholder name/title should just be the context category!\,.\,. (I have actually had this thought before, very recently, I think.\,.) That way, when you click on it, you are transported to the right column! Oh, so this is actually a must.\,\texttt{:D} (12:59) And then it could actually still be permissible to make templates where the context category matters---at least somewhat---to the semantics.\,. (13:00) .\,.\,Great things.\,.\,:)

\ldots (13:17) Oh, and note that it now works well to just write the word `category,' instead of e.g.\ `@2,' since `category' will now be the full SK of the Entity.\,:) .\,.\,Oh, but shouldn't it be `categories' instead.\,. Hm, it's really hard to avoid this tug between using singular and plural nouns, but maybe using `tags' rather than `categories' could actually help here.\,.\,!\,.\,. Let me see, we could also have subtags and supertags, couldn't we.\,.\,? .\,.\,Or I could still call it subcategories and supercategories, i.e.\ `of the given tag,' hm.\,. .\,.\,Hm yeah, that actually seems to work.\,!\,.\,. (13:24) .\,.\,Oh, but what about Properties, can I.\,. no, it doesn't work so well for those, does it?\,.\,. (13:26) .\,.\,(13:27) Hm, maybe it's better to just write e.g.\ `$<$categories$>$' instead.\,. .\,.\,Ah, wait! Isn't `category/movie' almost as good as `categories/movies'?\,.\,. Hm.\,. (13:30) .\,.\,But again, what about Properties.\,.\,? (13:33) .\,.\,Hm, think property, but formulated as a tag.\,. (13:35)
.\,.\,Well, that works, doesn't it. You can define a property tag by: `$<$property$>$ of $<$e$>$'.\,.\,!\,.\,. .\,.\,So maybe by being consistent about calling it `tags,' we can finally get to just use singular nouns pretty much all over.\,.\,!\,.\,. (13:40)

.\,.\,Hm, and I'll probably write `category: movie,' then, instead of `category/movie'.\,. (13:42)

.\,.\,(13:46) Hm, now that Entities are no longer the only entities in the SDB (if we didn't include the semantic inputs), I probably ought to change their name as well. And I could maybe call them `words' now instead.\,.
%Hm, I just thought a bit about my timestamp notation for some reason. The rule can be said to be: If there are dots between a timestamp and a new sentence, then the timestamp belongs to the (end of) the previous sentence.. *More precisely, it binds to the left, unless the is a sentence to the right of it without leading dots between it and the timestamp.
.\,.\,Oh, but that's not only a bit confusing to a lot of people, it is also downright incorrect, I believe. Hm, I could go back the lexItems, but I'd rather not, no.\,. .\,.\,`Strings' is another contender.\,. .\,.\,Hm, and maybe `entities' is actually also still a decent contender, although it could be better.\,. .\,.\,Maybe I should actually stick to entities, if nothing better comes up (but I have thought of what the options are before.\,.). But I think I'll take a walk now, and think about all this\ldots (13:56)

\ldots\ (15:58) I think I will call the entity string `code' instead. This \emph{code} is then translated to another (HTML) string (which has a little bit of freedom to vary across implementations and applications, as long as the semantics of that rendered string does not change) before being read and interpreted by the (human) users. And, more importantly, now it makes perfect sense to go back to calling the `context (category)' `type' again. And I've also figured out what to call the Entities. I will call them objects, but only when their type is also decided (with the option of the 0-type as well). The objects therefore has two IDs (of what was formerly called Entities) as their key, and are actually not stored in a separate table: These objects only appear in the semantic inputs. Hm, so what to call the former `Entities' table, by the way, before I continue?\,.\,. (16:06) .\,.\,Well, `ObjectCodes' seems like the appropriate choice, doesn't it?\,. (16:07) .\,.\,Sure.\,. .\,.\,Or `Codes' might also be a good option.\,. Okay, let me continue. So an object consists of a type and a code. Both of these are then (typically) referred to by IDs (BIGINTs), where the type of the type object is then automatically taken to be the (0, `type') object. In other words, it is of the type `type.' And if we then dissect the fields of SemanticInputs, we first of all have the user\_id, where the type is interpreted to be the special, reserved `u' (user) type, which tells the app to query in the Users table in the database for further data (or go query for the object with a fundamental user template, if one wants to see semantic data about the given user). Next we have the obj\_type\_id, as I intend to call it, which is what I called inst\_type before (in the latest commit). This is of course interpreted as a `type,' meaning that if the user would go from the, not `EntList, but \emph{ObjectList}, and to the obj\_type of that list, the should be directed to the column of the (`type', obj\_type\_id) object. Next up we have the, not `cat\_id,' but `tag\_id,' which is an object that is then interpreted as of the type `tag.' So `type' and `tag' will be some of the most fundamental types, together with the 0-type. The 0-type, by the way, is when you interpret the rendered code as the label/lexical item/string of characters themselves (i.e.\ once the @-placeholders have been inserted, and/or when the the template has been assembled, if we're talking a template instance object). Then we reach the specifying data for the object itself on the list, which is of course the rat\_val, with the same interpretation as before (I've re-described this in the latest commit), and finally, the obj\_code\_id. (16:25, 27.03.24) .\,.\,And the object with this given rating value on the list is then of course the (obj\_type\_id, obj\_code\_id) object. (16:26)

.\,.\,By the way, I should also mention that e.g.\ `$<$@2$>$' is a valid template placeholder as well: you can reference the type within the `$<>$' by both its ID, like here, or with its object code, like in `$<$tag$>$.' And of course, when you write `$<$tag$>$,' this is the same as writing `$<$@$n>$,' where $n$ is the ID of the (`type', `tag') object (and you could replace `tag' here with any other type).

.\,.\,Hm, I can call it the 0-type, but when it comes to rendering it, it could maybe fittingly get the `\$' symbol.\,. (16:32) .\,.\,Oh yeah, and I also need to reserve a type symbol for it anyway, similar to the reserved `u,' so yes, let me reserve `\$' for it. (16:35)

.\,.\,Oh, but there is more to consider: How to interpret (and specify) the types of inserted object references.\,. well, I guess they should have two keys then, these placeholders.\,. Oh, and I should also be careful.\,. .\,.\,with object codes that are pure integers, but let me see.\,. .\,.\,Hm, but I definitely want the option as well to just insert/substitute the code directly.\,. Hm.\,. (16:40) .\,.\,Hm, let me go back to using `\#$n$' for direct code substitution, such as in e.g.\ `$<$@2$>$' *(which then becomes `$<$\#2$>$' instead) within the code of a template. Then I just need to figure out what to do for the.\,. substitution of object links.\,. .\,.\,Hm, I want something like `@u.$n$' (and `@tag.$n$' etc.).\,. (16:46) .\,.\,Hm, and that works without making `.' a special character in most contexts, since it is only when it appears in this form, right after a string of letters preceded by an `@'.\,. .\,.\,And since we always need the dot.\,. we can also include syntax like e.g.\ `@\#2.123' (instead of e.g. `@tag.123', if `tag' has the code ID 2).\,. But I just realized, I need to find a way to end this syntax; I need some sort of stop delimiter.\,. (16:51) .\,.\,Hm, why not use a syntax of writing e.g.\ `@tag[123]' and `@\#2[123]' instead.\,:) Nice. And then I should just remember to not allow `\#' at the beginning of types.\,. Hm, and how do I enforce this restriction on the codes of the types?\,.\,. (16:54) .\,.\,Should I maybe just put parenth.\,. Ah, or I could use single quotation marks!\,.\,. .\,.\,No, I could also simply say that using `$[$' and `\#' in type codes is to be avoided, since it will make it impossible to then use placeholders the use the SK, i.e.\ the code, instead of the type object's ID (i.e.\ with the `\#$n$' syntax). (17:00) .\,.\,(Well, using `\#' in the start of a type's code will certainly cause trouble (and using `$[$' will in general).)
.\,.\,And of course, using (`type', `u') or (`type', `\$') will also not work, unless you reference them by their code ID, since these will also be interpreted differently if you use the `@$code$[123]' syntax rather than the `@\#$n$[123]' syntax. (But of course, no one would want to do this either in practice; it's just a good thing to consider what the potential danger could be (and there is none, really, with this syntax, as far as I can see).) (17:07)
.\,.\,Oh, but why not then also simply say that using type codes consisting of pure integers makes no sense, and therefore, let us drop the `\#' for these template placeholders for compactness, why not?\,.\,:) (17:09) .\,.\,So the syntax that I might go with is using e.g.\ `@$code$[123]' and `@$n$[123],' where $code$ is then simply required not to be weird in order to work.\,:) (17:11)

.\,.\,For the `\#$n$' substitutions (and here we of course keep the `\#'), I also think that this should be rendered as a link, actually.\,. well, maybe.\,. .\,.\,Hm, ooh, or maybe it could just be an inserted bookmark, or what to call it, as a prefix to the substitute, where this prefix is then a link to the (`\$', $code$) object, where $code$ is what $n$ (which is a code ID) points to.\,.\,:)\,.\,. (17:16) .\,.\,Yeah, a little, unimposing, possibly raised (like a superscript, but in front of the thing.\,. a ``pre-superscript''.\,.) symbol with a link to the (`\$', $n$) object (or I could write `the (0, $n$) object' here, equivalently).\,:) (17:19)

(20:08) I think I will actually use that idea of adding a trailing code for capital letters. And this can also be used for things like URLs as well, that should be fine.\,:)

(28.03.24, 10:47) I should also let users type the.\,. Oh, let me start by saying, I think I will call it the `object string' rather than `object code.' And to continue the last thought, I should also let users type the object string into the placeholder syntax between the square brackets as well. But in order to not have two objects with different strings leading to the exact same HTML, I think I will automatically convert to the object string ID.\,. Hm, or let me call it the `string ID' for short, and maybe I'll then also simply call the table Strings.\,. .\,.\,Yeah. .\,.\,I will automatically convert to the string ID for any placeholder. So users can write e.g.\ `@movie[The Lord of the Rings],' but it will be converted to `@$n$[$m$],' where $n,m$ are the two string IDs. (10:55)

.\,.\,Hm, sorta unrelated note: Let me also prohibit some characters like newline and tabular (if I don't do this already.\,.). These can then always be included at a later time if need be.\,.

(12:05) Okay, I also have to consider the option of making an Objects table to allow for object keys of just a single ID rather than two.\,. .\,.\,No, that just makes things more complicated, I think.\,. (12:07)

.\,.\,(12:09) Oh, I just realized, the `\#123' syntax also needs an stop delimiter. So let me just use the syntax `@[123]' instead for direct string substitutions. .\,.\,Hm, and this could be different from `@0[123]', could it not?\,.\,. .\,.\,Hm, I'm not sure, 'cause users could also use.\,. single quote delimiters, hm, except that the string could start or end with such quotation marks.\,. .\,.\,Sure, let there be a difference: One is rendered with just that pre-superscript I wrote about above, and the other (`@0[123]') is rendered as a clickable link. (12:15)

.\,.\,(12:19) Hm, it might actually be quite nice if an object could have a whole sequence of types, each one being more specific than the last, i.e. One could probably define a fundamental template (format string) for this, as I've probably thought before, but let me see.\,. .\,.\,Yeah, that's it. So how should that go?\,.\,. (12:22) .\,.\,(12:22) Hm, how about a comma separated list, simply? .\,.\,Yeah, so the template would simply be `$<$type$>$, $<$type$>$'.\,. .\,.\,And then you ought to keep choosing the last of these placeholders, if you want to make long type sequences. In other words, you would always chose a non-template-instance type for the first input in the template (format string). .\,.\,That sounds good, and then I just have to ``tutor'' the users that this is how to do it.\,. Well, or giving/setting examples could actually also be good enough, but I \emph{should} also include it in the tutorial. (12:29) .\,.\,I'm happy with this idea---I've had so many happy ideas lately, and this is yet another one (even though it is a simple one, but it means a lot knowing/realizing that this can be done).\,:)

(12:49) Hm, should I or shouldn't I make an `object' type, maybe with the reserved letter 'o,' instead of using the 0-type/`\$'-type.\,.\,? .\,.\,Hm, but `\$' was the reserved letter.\,. .\,.\,Hm, guess I haven't thought enough about placeholders for general objects, where the type might still matter for the semantics, such as the template `$<$property$>$ of $<$object$>$'.\,. (13:03) .\,.\,Hm, but since the references specifies the types anyway, the type of the placeholders are actually kinda redundant, except of course that they help guide the users of how to use the template. And I guess I also should consider, whether type restrictions for templates could be beneficial in some way.\,. .\,.\,(13:11) Hm, I don't really think so. But let me think about whether to remove the placeholder name/title again---or to make it so that the inputs then doesn't have to specify type.\,. Hm, I think that might be a good option, and then for `$<$object$>$,' the \emph{do} have to specify the type.\,. .\,.\,Hm, since they are now always converted to integers anyway, I actually don't need the reserved letters, do I? It can just be e.g.\ `user' and `bot' etc., right?\,.\,. (13:17) .\,.\,Oh wait, no! The point is that the reserved letters \emph{shouldn't} be converted to an integer, since these are not custom types that.\,. wait.\,. (13:21) .\,.\,I was about to say something like `assign to custom objects,' but they can't do that now, anyway, since the users only create the strings now (and rate the object--tags, and upload texts and potentially binaries).\,. .\,.\,Hm, so I guess I should just let the first String be `object'.\,. .\,.\,Hm, and when it is `object'---or rather 1, since this is what it will be converted to---then the users are allowed to specify the type of the given format string input, and if it is anything else, I have to then choose whether they can overwrite this or not.\,. .\,.\,Hm, no they can't. (13:28) .\,.\,And they by the way can't have `@[123]'-like references as format string inputs; there needs to be something between the `@' and the square brackets. The `@[123]' syntax is only when there is a substitution that happens before anything else is processed/translated about the string. Hm, and this means that you could technically build a template out of a concatenated string, or a substituted string in general, but of course we only make one substitution per string, and this substitution never happens across the boundaries of any previous substitution (e.g.\ `\#[123]45' will not yield the same as `\#[12345],' if 123 just so happend to be the ID of the string `123'). (13:36) .\,.\,Oh oops, I mean `@[123]45,' not `\#[123]45'.\,. (I no longer use `\#' for anything, I believe.) .\,.\,Hm, but maybe I \emph{should} go back to using `\#' for these macros, in order to better tell them apart. Yeah, I should, actually. And that also means, by the way, that I can now shorten any general object reference to just.\,. Well, I can write `' everywhere instead of `0' when it comes to the placeholders and the references (if I want to.\,.).\,. (13:42) .\,.\,Oh, and that actually solves another problem I had, 'cause we would still need to use the `@[123]'-like references for inputs for typed placeholders, and therefore it's really nice that the general, untyped `object' references now also looks like `@[123]'.\,:) (13:45) .\,.\,Hm, oh, but I'm actually making the `object' string the one with ID 1, not 0. So all in all, you could say that the special `object' references always just have the `1' turned into `' before being uploaded. (13:48)

.\,.\,Oh wait, no, reserved letters for the fundamental types is actually a good idea for several reasons. (13:56) Hm, and since they don't have to be actual type objects in the database---except for `type' and `tag' which are also fundamental.\,. wait, no, what am I talking about, I need these types to be.\,. no!\,.\,. yes! I need them all, in fact, to be actual type objects. So what to do.\,. .\,.\,(14:01) Hm maybe `object' is actually not th best choice of word.\,. .\,.\,Hm, how about calling it `semantic object,' `semantic type,' and `semantic tag' instead.\,. (14:04) .\,.\,No, I just need to find a better word for `object,' or at least I think I do.\,. (14:05) .\,.\,Hm, `thing' might be appropriate (and intuitive.\,.).\,.
.\,.\,Hm, but I don't like `thing string' and `thing type'.\,. (14:07) .\,.\,Hm, but I could maybe call it `semantic object' up in the app, but still call types and tags just that.\,. (14:09) .\,.\,Hm, sure I could, but do I actually even need a type name for the untyped.\,. Hm, that kinda answers it, doesn't it: We think/speak of the untyped objects as exactly that, and I go back to using 0 as the null type ID for such objects.\,. (14:11) .\,.\,And users could maybe write `$<$any$>$'---if not simply `$<$$>$'---if they want an untyped placeholder. Right, this seems right.\,. (14:13) .\,.\,So 0 is the (null) ID that is replaced with `' before uploading strings with placeholders or references, not 1.\,.

(14:30) Oh, I should still reserve some letter and/or words for the internal entity types, such as for users, e.g. Let me chose the letter `u' for `users of this network' (which I now think I will call the type string). And I'll chose f for `format string'. .\,.\,Yeah, and so on.\,.\,:) (14:33)
\ldots (14:47) Maybe I'll actually just call it `templates' once again, but specify it as `string templates.' Then I could change the reserved letter `f' to `t,' and change the reserved letter `t' for texts to `x'.\,.

(14:57) Hm, an alternative option could be to let `thing,' or perhaps better `any,' with string ID 1, be the `untyped type,' so to speak (the `general type,' i.e.).\,. .\,.\,And I can still refer to it in more technical contexts as `(semantic) objects,' even if I choose `thing' as the general type in the application layer.\,. (15:01) .\,.\,And then I'll still replace `1' with `' for the template placeholders and the references, before upload.\,. .\,.\,Yeah, I like this.\,. I like it quite a lot.\,. (15:04)

(15:11) Hm, would it make sense to make two kinds of Strings tables; one for case-insensitive strings, containing only lower letters, and one with case-sensitive strings?\,.\,!\,.\,. %I'm actually dizzy rn, I need a break.. (15:13)

%..I'm still feeling dizzy, but:
(15:17) I should also reconsider if templates are actually really worth the effort, now.\,. \ldots (15:49) Hm, they probably are, since we'll need them for things like comments, etc.\,. \ldots But instead of using the `$<$type$>$, $<$type$>$' template, I might just work with strings directly of that syntax, and the same goes for the `$<$property$>$ of $<$thing$>$' template.\,. (16:04) .\,.\,Hm, come to think of it, I might not \emph{need} the templates for comments, etc., either, since I could just define and use these templates in the JS code.\,. And users could in principle also define their own templates, but then be responsible themselves for adhering to these.\,. .\,.\,(16:12) I could always just postpone introducing/implementing these templates.\,. Hm.\,. .\,.\,Hm, there is also the fact that having templates creates a redundancy, and I don't have any HTML in mind currently to distinguish a template instance from its equivalent counterpart. So yeah, I'll throw templates in the paper bin, at least for now.\,. (16:16) .\,.\,And I am making room for more fundamental types (similar to `user' and `aggregation bot' etc.) in initial\_inserts.sql, so I can always just introduce the templates again, at some point, if it turns out that they will be useful (and then I should make it so that template instances also get a ``pre-superscript'' link to the template object in their HTML when rendered). (16:19)

(16:21) But yeah, I should also find out about that idea of potentially having several Strings tables\ldots

\ldots\ (18:31) First of all, I wouldn't need to introduce another table if we ever (for some reason) where to implement strings that can be searched case-sensitively. (Note that we can also search case-sensitively with the trailing capitalization code, but then this is just the least significant information in the search.) For one could then just start allowing strings with capital letters in Strings. But no, it's a good idea as it is now, to just have this lower-case Strings table with trailing cap.\ code---and accent code, potentially, for languages with accents that matter. But I need to choose a better format for these codes than what I had, and on the walk that I've just come back from, I figured out the following format/syntax.

The capitalization--accent codes start with an integer, if the on should skip that many letters before coming to the first letter that needs changing. And note that we are only counting letters, not other symbols! Then comes a code for how the next letter(s) should be changed, e.g.\ `c' for `capitalization,' followed, potentially, by.\,. Hm, let me see, actually.\,. .\,.\,Right, yes, followed either by an integer followed by `l' (for `lower case'), or followed by simply `l' directly, if only one letter is to be changed. .\,.\,Oh, or followed by the end of the string; we don't need to write the final `l' of this code. For the `l' is a code for `lower case letters,' and this can also either be followed by a number of integers, or.\,. No, this doesn't work. Let's see.\,. (18:43) .\,.\,(18:44) Okay, we just have alternating numbers and codes (consisting of no numbers). We can start with a number, in which case this is essentially interpreted as starting with `l$n$', where `l' stands for `lower case,' but more precisely just means `no change.' A `c' means `capitalize.' So for instance, `example of a not very useful entity|1c1l1c1l1c1l1'.\,. Hm, maybe I should `n' instead of `l' for `no change,' shouldn't I, to avoid this mess. Okay, I'll do that. So  `example of a not very useful entity$|$c1n1c1n1c1n1c1n1c1n1c1n1c1n1c1n1c1n1c1n1c1n1c1n1c1n1c1n1c1' is rendered as `ExAmPlE oF a NoT vErY uSeFuL eNtItY.' .\,.\,And it's a good thing that this is \emph{not} something that the users have to code. Rather, it's coded automatically for them. And about accents, I want implement this until it becomes relevant, but the idea is then that one can add more symbol instead of or directly after the `c' to then code one or more accents. (ANd it's natural to use the symbol that are related to these accents. For instance, a tilde, as in `ñ,' might be encoded with a `$\sim$,' and the two dots in e.g.\ `ä' might be encoded by `..'.) (19:02)
.\,.\,And note that `$|$' is also gonna be a special character, i.e.\ one that needs escaping, otherwise.

.\,.\,I have considered adding a code for pluralization, but it just becomes way too complicated. I think users should just become used to searching for tags, that are singular and uncapitalized, instead of searching for categories. And let me therefore wave goodbye to this idea of rendering categories with plural nouns (for some).\,. (19:06)
%..Ha, what is wrong with me today, I just got dizzy again..

(21:26) I \emph{will} include templates, complete with the `\{\}' syntax and all. This is precisely done so that very specific things can be referenced, in a way where it is easy to achieve consistency as well as semantic precision, without having to worry much about the length of the boilerplate of the template. And these templates should be rendered a bit differently: There should be a link to the template itself, probably a pre-(super)script, like I've talked about, and there could also be an expand button in case the template includes the `\{\}' syntax. (21:26)

Furthermore, we will not be needing the sequences of types, like I've talked about. Each type in such a sequence should be self-contained anyway, so the last type in the sequence will hold all the information about the context/type anyway. So no types of the form `$<$type$>$, $<$type$>$,' I don't think. (Users \emph{can} of course make these, but I don't think it will be useful.) (21:32)
\ldots (22:11) Oh, but it could be useful for rendering/specifying the layout of the application column.\,. .\,.\,(22:18) Hm, there's something to think about here: Is it okay to.\,. or to what degree should the type specify what object we are talking about.\,.\,? .\,.\,(22:20) Hm, maybe one could make wrappers for objects, with the same syntax as the templates, but intended for only a specific use.\,. And then you could uprate all relevant specification wrappers for a given object.\,. .\,.\,Hm, I could be on to something here.\,. (22:22) .\,.\,(22:29) That didn't make sense, with the wrappers, but of course, it might be important to have a tab with the various versions of the word. But there's a danger that it will run amok, then, so we also need to be careful not to overspecify.\,. Hm.\,. .\,.\,Nah, it's fine.\,. If there are different things that the same word/title can refer to, the the users will be quick enough to notice this, so we shouldn't be too worried about loosing votes/ratings for under-specified objects, I think.\,. (22:33) .\,.\,Ooh, especially since we could just notify the raters, that they have previously rated an object, that is now deemed to be to ambiguous to rate, and asking them which of the version their ratings should be carried over to. (22:35) \ldots (22:50) And a way to measure this is to note when if a given object falls below some threshold in terms of its rating as the given type (e.g.\ `The Lord of the Rings' no longer being considered a `movie' if there becomes (or are) several movies called that).\,. .\,.\,Hm, so the type specifies the context, not the object/thing itself.\,. And so we shouldn't use e.g.\ (`movie', `The Lord of the Rings') and (`book', `The Lord of the Rings'), but rather make sure to specify book vs.\ movie in the object string?\,.\,. (22:56) .\,.\,No, that's not true; the object string just have to be pretty unambiguous when in the context of the given type.\,. .\,.\,Hm, but why not specify the object in the type string, then?\,.\,. (22:59) .\,.\,Hm, I guess because that doesn't work so well when you want to search for an object. Okay, there it is. So it's better to specify the object via the object string. And the type does not need to be specified beyond what.\,. hm, beyond what is a useful category of things.\,. Okay, I should think more about this.\,. .\,.\,Hm, for instance, `action movie' will be a useful type, surely, but if you rate an action movie as `good,' you naturally also want to rate it `good' as a movie.\,. .\,.\,Oh, but maybe you just always rate according to the outermost type---and then you actually do make big use of types defined by sequences of other (atomic) types.\,. (23:05) .\,.\,Hm, or maybe `action movie' is just \emph{not} very useful as a type when it comes to tags.\,. well, except for ``aggregation categories''.\,. Hm.\,. .\,.\,Yeah, so the useful types are only those who help specify tags, and also the layout of an app column. .\,.\,Well, no, other types are useful as well, but only in order to limit search results for optimized categories.\,. well, and also since they can be used as categories themselves. .\,.\,Hm, so I should have two tags, at least, about being a valid type for an untyped object. One tags says that the type `is a useful type for specifying tag semantics,' and one says simply `the object belongs to the given type.' Maybe there could also be more.\,. oh, something like `useful type for specifying the app column layout,' maybe.\,. (23:14) .\,.\,Hm, ah, but maybe I \emph{will} use type sequences, then, and make it so that the rating appearing in a given column automatically is given the type (context) of the.\,. outermost type, which then ought to be specific enough that the the vast majority of tags will have well-understood semantics in the context of that type.\,. (23:18) .\,.\,Sure. And tags that doesn't have that, well, they just need to specify themselves more, then. (23:19) .\,.\,Oh wait, this actually also means that these compound types (i.e.\ sequences) are \emph{not} needed after all. The column just always has this ``outermost type,'' and the column can then have tabs for any of its up-rated subtypes, where the sub-tabs (and the layout in general) can be determined by the given subtype tap that you have clicked on.\,. Hm, a bit complicated, but I think I'm on the right track.\,. (23:24)

(23:33) Hm, maybe I won't actually implement the templates.\,. For why do users need this help to make consistent names of objects?\,.\,. I'm not sure they do. And in terms of comments, the app constructs these automatically, anyway, so that's actually not an important example.\,. .\,.\,And again, ratings can be moved over if/when ambiguities appear.\,. (23:38) .\,.\,And when not using templates, it makes it easier to search for the objects.\,. (23:40) .\,.\,Oh, but I could still include the templates, but just make it so that they are exploded *(no, `instantiated' is a more fitting term.\,. .\,.\,or maybe better: `filled out'.\,.) \emph{before} the object string is uploaded (such that it bears no trace of coming from a template, other than its form, of course). (23:43) .\,.\,But I can then still make the same submission fields for these templates, where each input gets its own input field. (23:46)

(23:56) Maybe I should start using `category' again as a term for all type tags that are not ``main types''.\,. And then I could use the term `type' only for the ``main types''.\,.

(29.03.24, 10:28) We can do better than that `c1n1\ldots' encoding for the cap.\ code. The code could start with the first letter (lower case, always) that needs changing (capitalization and/or accenting), followed by the number of identical letters that you had to skip over to get to it. And for any of the subsequent letters that needs changing, you just do the same but where the number tells you how many identical letters to skip after the last position. If you had to skip 0 such letters, you write no number after the letter, i.e.\ instead of writing `0.' Now, if the letter only needs capitalization, you simply move on to the next letter. For instance, our example from before *(`ExAmPlE oF a NoT vErY uSeFuL eNtItY') would turn into `example of a not very useful entity$|$eapefnteysflniy'. And ``ExAmPlE of a NoT vEry uSeFuL eNtItY' would become `example of a not very useful entity$|$eapentesf1lniy1'. And then for accents, you append a single non-letter, non-number symbol after the `[a-z]([1-9][0-9]*)?' part, which determines the accent, followed potentially by the copyright symbol (I think), if the accented letter should also be capitalized. Note that I would now prefer that it is a single symbol to encode the accenting, since this makes the encoding/translation one-to-one (bijective). So `..' is not advised for making `ä' after all. (10:46) .\,.\,Ah, but `:' could be.\,:) (10:46)

Now on to some more important things. First of all, I'll probably stick to the idea of only using templates in the application layer to help make the submission fields. (And users can also up-rate templates for a given type.) But should we worry, that everything.\,. Wait, let me first say that I have also had the idea to get rid of the `\{\}' syntax, but still include a syntax to add expandable information to the object string. I could maybe just use the `$|$' delimiter, and then you just have to write `$|$$|$' if you want to go directly to the cap.\ code.\,. Hm, it's an option. But yeah, there we are. Hm, let's refer to it as a `subtitle,' why not?\,.\,.

Now, should we worry, that everything is precise.\,. that every object string is precise from the beginning?\,. No, I don't think so. It's nice to use the templates, especially now that they are actually completely searchable, since they are filled out before upload, and since the no longer use the `\{\}' syntax, on only have the specifying information last. But if an entity.\,. Hm, let me actually pause for a bit and say that, as they are now, templates should actually really be encouraged.\,. I should make it the standard option to choose from a list of templates when wanting to submit a new ent.\,. no, I mean a new object string. .\,.\,And I should really encourage using that `subtitle'/information appendix. For it will be really nice, if we can avoid having to set up the quite complicated system for recognizing duplicates and transferring rates from old objects to their more precise versions. Now, as I was getting at, this \emph{is} possible to do. We can use a `better duplicate$|$ (less ambiguous)' property. And if a certain mean rating exceeds some threshold, we could signal a bot (or several bots) to then recalculate the mean of all relevant tags (for the given type (that said property is rated in relation to as a tag)) where all ratings of the old object is then moved to the duplicate, except whenever the given user has a newer rating for the same tag for the `better duplicate' object. Another bot can then also at the same time submit a down-rating of the old object belonging to the given type. So all in all, it is possible to handle, but as one might be able to imagine, this will require a lot of effort, actually, not just to program the bots, which \emph{is} a complicated matter in (and of) itself, but also to then make the other bots around them use these bots as well, and to make the users take an active part in all this.\,. Yeah.\,. Complicated, for sure, but I believe that it's possible. But the thing is, however, it's probably way too complicated for the first version of the app, so it's actually (after all) so important that we are able to use the templates from the very beginning, and encourage users very much to use them as well, \emph{without} this hindering the user experience of searching for objects, and being able to see short versions of the object strings rendered, without the information appendix. (11:17)

.\,.\,Hm, by the way, it's nice that when we make sure that we always insert links (i.e.\ object references) into the templates (the app is responsible for ensuring this), it means that all the properties of the information appendix/subtitle can be typed, without it making the full title longer, when rendered. And if you are unsure of what property a certain value represents, e.g.\ if you are uncertain of the role of `Peter Jackson' in `The Lord of the Rings: The Fellowship of the Ring$|$, 2001, Peter Jackson', then you can just click---or maybe just hover, even better---on `Peter Jackson' to see the type that this link carries, since it will be rendered as a link (since instead of `Peter Jackson,' the object string would actually have a (typed) object reference).\,. Hm, oh, but according to what I thought about last night, `director' might not be a useful type.\,. Hm, is there anyway to.\,. Well, yes, we could make it so that columns where the object type does not have other layout date attached to it, looks in the property of `supercategories' in order to find a type which does have layout data attached to it.\,. Oh wait, no, you could also simply.\,. Hm.\,. (11:29) .\,.\,Before I forget it, let me also.\,. Hm, no, maybe not.\,. I was about to propose using `;' instead of `$|$' for the specifying appendix, but maybe not.\,. (11:31) .\,.\,Back to the other thought, the thing is, we could of course always just get to the (`director', `Peter Jackson') column, and then navigate to (`person', `Peter Jackson') from there. But.\,. (11:32) .\,.\,But nothing. You get to the `director'-typed column, and if this type is not yet implemented very thoroughly in terms of column layout, then you simply navigate to the (`person', `Peter Jackson') column from there (e.g.). And this is always possible, since you can at least always go via the (`thing', `Peter Jackson') column. (11:40) Hm, or could we say `anything' rather than.\,. no.\,. `Thing' is better.\,. Wait, or is it? You can also ask: it this object `anything?' And that would be the same as asking, is.\,. oh, wait, or we could use `something' instead.\,. Hm.\,. (11:42) .\,.\,Yeah, that's \emph{much} better.\,.

(12:08) Hm, since templates now have a different role, I guess it doesn't really make much sense to convert the type strings into string IDs.\,. No, probably not, so let me not do that. But I will of course still convert for the object references. (12:10)

\ldots\ (14:21) Hm, maybe the appendix delimiter should not actually be a symbol, but should come as a trailing code as well. This will nemlig make searching easier/better.\,. .\,.\,Let's introduce the adverb `nemly' to English.\,. .\,.\,Hm, should it then come before or after the cap.\ code?\,.\,. *(capitalization--accent code.) .\,.\,Hm, I guess it should actually come after, since this makes it least significant in the searches.\,. (14:25) .\,.\,No, that doesn't matter.\,. (14.27) .\,.\,Well, it could actually matter a little bit, so let me just say that it comes last (of these three parts of the string). (14:28) .\,.\,Oh, I forgot to say: and an empty string for this last part just means that there is no appendix.
.\,.\,(14:35) Hm, it could only matter if we.\,. No, never mind, it could matter a little bit: To prioritize a hit where the cap.\,. hm, or maybe it doesn't.\,. .\,.\,No it won't matter a bit, and I actually prefer to write the appendix start position first, before the cap.--accent code.\,. (14:39) .\,.\,'Cause you can always expand a bit on the specifics of your object, but the.\,. CA code is only used when there are proper nouns involved.

(16:41) Hm, let me call it an `object definition' rather than simply an `object string.' (An `object definition' \emph{is} a string, but so are the `object type.')

(17:05) Hm, maybe I will use `any' rather than `something' for template placeholders, where the user themselves are supposed to determine the type. This is then opposed to when `something' is \emph{supposed} to be the type given for the template.\,.

(18:58) I will of course just start out with having a single Index, governed by a bot, that user can search in. And I should then also create a FULLTEXT index on the str column of.\,. Oh wait, but can I do that if.\,. Well, if I only have one single idx\_id in the whole StringIndexKeys table, then yes, I certainly \emph{can} create a FULLTEXT index for the str column of that StringIndex.\,. .\,.\,(19:08) Yeah, and if we'll need more, we can create separate tables for it.\,.

(30.03.24, 11:55) I've realized that now that the Entities table are called Strings instead, I should definitely go back to calling what I have now been calling `objects' `entities' instead. We then have `typed entities'/`semantic entities' which consist of a type and a definition, both being entities themselves---the first with the type of `type' and the second one being untyped. And then we have the `untyped entities' which consist only of a string (and has an ID).

How ambiguous should we allow entity definitions to be?\,. Well, it depend: For tags/cat-egories etc., it perfectly fine to not be very specific. For instance, we shouldn't need to specify the `funny' tag any more than that. And the same goes for property tags, type/category tags, etc. But when it comes to entities that defines things of the ``real world,'' be it fictional things or non-fictional---in other words, the fundamental entities that we wish to rate with this system (never mind the fact that it can also be useful to rate the tags themselves)---their definitions should generally be very precise, enough that we know exactly what thing is beig rated. However, as an example, we might have specified `WWII' very precisely (in this case, hardly any specification is needed, 'cause there is only one WWII), but we still need an extra specification in the form of the entity's type in order to be able to interpret the tags correctly. We might for instance say the `WWII' is `good' as a `history subject,' but not thereby also saying that `WWII' is `good' as a `war.' So the type is generally meant to give this last touch of specification/context in order to make the semantic statement more precise. (12:09)

(12:50) It's actually quite important for the concept as a whole that users can rate anonymously \emph{while} also having some way of proving that they are not bots/spammers. And I guess the way to go \emph{would} be to persuade most users to make a public profile, which possibly does not upload anything. Users can of course use their public profiles, but they don't have to. In these public profiles, they define enough information about themselves that people either from their place of origin or their work, or so on, can identify them. And they also state how many profiles they have, as well as perhaps the degree of overlap in the statements that are rated by these profiles.\,. perhaps.\,. Hm, and the point is then to use some sort of game where several users from different locations around the world can link up and maybe anonymously post their non-public profiles to a pool of profiles.\,. Well, it should actually be large parts of the user community that does this at once (and it's just done automatically be a program, which means that users can hire/get third parties to do this for them, of course where these third parties then promises not to record any data from it). And when you have this large pool, you can.\,. hm, you should check that there are no collisions, but that means the groups can't be so large.\,. Hm.\,. .\,.\,Hm, let me get back to this, and then continue another thought, 'casue once you have a vast network of public users, then you can also make.\,. Friend of a Friend.\,. Hm, or would this work when you don't.\,. No. No, you have to make a public profile in order for your ratings to be taken seriously---\emph{after} a certain point, that is; this will not be necessary in the early stages of the network. But yeah, a fully public Friend of a Friend network, where each public profile state their number of non-public profiles.\,. .\,.\,And back to the cryptography game, how should this go again?\,.\,. (13:06) .\,.\,(16:08) Ooh, maybe if you first encrypt the non-public profiles' (NPP) IDs.\,. .\,.\,Then you can check for collisions. And if one happens, you can abort the procedure and ask users to reveal ``their hand'' in the game (which I have described somewhere before, perhaps in the out-commented notes below from 2022.\,.), which then makes it possible to identify the culprits. But even though all participants then have their submissions known, only they hold the key to decrypt their submission to reveal the \emph{actual} profile ID. So nothing is revealed about the participant, other than whether they followed to rules of the game or not. They culprits are then removed from the game, and their associated public profiles can now also be marked (unless it is actually the third party responsible that can be seen to be the culprit.\,. anyway.\,.). The game can then repeat, and hopefully it will then soon yield a pool of encrypted NPP IDs with no collision. (And you somehow have to make it so that.\,. Hm, let me postpone these thoughts for a bit.\,.) Then you play another game where the participant get to anonymously add decryption keys to a pool, and here it doesn't matter if you get collisions, or get more than you desire, 'cause they will only work (once) if they are right. Then you decrypt all the NPPs to get a pool of exactly the right number of NPPs mathcing the group. (That the number is exactly correct is required for the first part of the ``game'' to be excepted.) Most of the users then throws out the information that is used to unravel this whole game, i.e.\ ``their hand'' as I called it before. And even if some users (with malicious intend) does not throw out their ``hand''/data as they should (using a hacked version of the recommended program), they will not be able to unravel the game without the other users' data. Now, how do we ensure that the encryption of the NPPs are injective (one-to-one, basically)?\,.\,. .\,.\,The problem is we shouldn't, so what to do instead.\,.\,? (13:25) .\,.\,Hm, my legs need to be walked, so let me think about it outside (I bet I can figure it out)\ldots\ (13:28)
.\,.\,(13:35) Hm, it's not collisions that we need to prevent but that there are too many NPP IDs in the pool\ldots

\ldots\ (15:26) I recalled that in my idea from 2022 (I think), the the participants of the protocol uses it to share encryption keys first, and then publicly states that some of the keys was created by them, by which they will be thrown out. And everyone then has to do this until they only have the right amount of keys left in the pool that they should. No one knows who the remaining keys belong to, except the creators themselves. And if the protocol fails, it will be unraveled to find the culprits, and if it succeeds, the users throw their data away. Now, the important thing is then this: The the users can then afterwards verify their non-public profiles by uploading a cipher that can be decrypted with one of the keys \emph{via} the given non-public profile itself. So that's it. I won't repeat the protocol for creating that pool, nor for ``unraveling'' it here. See my (out-commented) notes from 2022 (I think). I'm pretty confident (although not 100\,\% sure, of course) that the idea there will work, maybe with only some few adjustments needed, who knows?\,. .\,. (15:35)

(15:36) And an alternative to all this, which might work for some, is of course to find a third party that one trusts *(and which the broader community also generally trusts) to govern all one's non-public profiles.\,.

(16:26) Oh, maybe for the sake of templates, it would be a good idea to go back to using `$|$' as a symbol within the string, or use another one, instead of having a trailing code for its position. So yeah, I guess I'll go back to that. .\,.\,And let me just use `$|$', indeed.\,.

(17:05) Hm, I should consider whether it is possible to make the subtitles/appendices ``semantic'' instead, and thus dividing the entity definition into two parts.\,. .\,.\,Well, couldn't we just make the entities consist of (type, name, specifying appendix)?\,.\,. (17:07) .\,.\,And just like each app column should have an `(other) types' tab, there should then also just be an `(other) specifications' tab.\,. (17:09) .\,.\,Hm, and then maybe I should also expand SemanticInputs thus. And when we at some point need to deal with duplicate specifications, this would then just be to ``tell'' a bot to merge any two `rated lists' where the specs are deemed as duplicates (for the entity in question), keeping the newest of the two ratings (if we have implemented RecordedInputs at that point) of a user has rated both.\,. (17:13) .\,.\,Not bad at all (it seems).\,. (17:14) .\,.\,Of course, this makes the entity references even longer (and they could be changed by using commas to also specify the specification *(within the square brackets)), but it might be worth it.\,. (17:16) .\,.\,Hm, and the specification is optional, just like using the `$|$' was optional prior to this potential change.\,.

.\,.\,(17:20) Hm, this might make me put templates completely out of commission again.\,. .\,.\,Yeah, 'cause the whole point is, that this allows us to be much more relaxed about the specifications (i.e.\ `subtitles'/`appendices'), and just write some that is `sufficient enough' in the beginning. This is because dealing with duplicates will now be something that can be implemented *(much more easily and) not long after the network takes off, I believe.\,. (17:24)

.\,.\,(17:29) Oh, but the `ent\_spec\_id' column would then have to be next to `ent\_def\_id', i.e.\ not being part of the PK, and that would expand the data. So I need to then instead make a new table, where these `specified entities' can get their own ID.\,. (This also means that the entity references will not grow (really) after all.\,.) .\,.\,Well, I guess this is what I must do, then.\,. (17:33) .\,.\,Hm, so a typed/semantic entity's definition is then either a `string/simple entity' or a `specified entity'.\,. .\,.\,Something like that.\,. (17:35)

.\,.\,(17:37) Okay, so this change is actually a must, now that I've thought about it. Since all it does is to put a clearer line between the buzzword/keyword part of an entity's name and its specifying part, which then makes it much easier to deal with duplicates, once we get to that point (and therefore means that we can be much more relaxed, luckily). (17:39)

.\,.\,In fact, maybe we can even be \emph{so} relaxed that we'll rarely need the specification part in the beginning of the website/network.\,.\,!\,.\,. (17:44) .\,.\,Hm, it seems like it.\,. .\,.\,Oh, and the thing about notifying the users if they have rated an entity which now either gets further specified (by creating or changing the specification), we shouldn't even really need that: We can just expect that the users in general talked about the same thing, nemely the thing that the entity is now being further specified as. (17:49)

.\,.\,Hm, is there any sense in letting SpecifiedEntities do double work and also serve as a way to implement concatenated strings?\,.\,. (17:52) .\,.\,Hm, but then, why on earth not just make a table of ConcatenatedStrings?\,.\,. (17:54) .\,.\,(also, that is.) \ldots(18:09) Yeah, I should make a ConcatenatedStrings table, and then drop the `\#123' syntax.\,. .\,.\,The StringIndexKeys table could then get a TINYINT column denoting which table the stringID.\,. oh wait, never mind about this part.\,. \ldots (18:25) Hm, maybe ConcatenatedStrings isn't so easy to work with (so I'm might consider going back to the `\#123' syntax, but let's see).\,. .\,.\,Hm, yeah, maybe it's better to use the `\#123' syntax.\,. .\,.\,Hm, or maybe ConcatenatedStrings is not so hard after all, and maybe it's better.\,. 

\ldots Oh wait, I probably need to have (Atomic)Strings, ConcatenatedStrings, and SpecifiedTitles all share the same ID space. So maybe, just maybe, it \emph{is} better to go back to using syntax for this.\,. Hm yeah, and using syntax instead should not make dealing with duplicates harder.\,. (18:56) .\,.\,And I could just make it so that only one instance of `\#$n$' can appear in a string (without being interpreted verbatim), and only in the beginning (such that `\#' is the first character). (18:58) .\,.\,Oh, and it can also appear as exactly the last part of the string, in which case it is interpreted as the specification!\,:) (19:00) .\,.\,Good.\,:) .\,.\,Oh, unless the syntax for specified titles should instead by `\#$n$\#$n$'.\,. .\,.\,That's probably better, isn't it?\,.\,. .\,.\,Yeah, let me say that.\,. (19:03)

(19:21) Oh, but what about entity references? Then they ought to be converted as well under a duplicate removal / specification.\,. .\,.\,And this could of course cause trouble in terms of the restricted string lengths and for concatenated strings.\,. .\,.\,(19:24) Oh, unless we just make an automatic redirect for the column of a discontinued entity. And such a ``redirect'' could even be made very seamlessly, since it could essentially just be a matter of searching which `specification tab' to open.\,:) (19:26) .\,.\,Phew, good.\,.\,:)

(31.03.24, 12:19) I think I will make it so that concatenated strings ought to end in `\#$n$', where $n$ is the ID of the first string.\,. .\,.\,Hm, at least for nested concatenations.\,. The idea is that the app can then fetch that immediately, and perhaps more importantly, when searching for strings, you don't get false hits of strings where there is supposed to be more to it (where the string doesn't refer to anything without its tail). .\,.\,Hm, and for single concatenations, it could then just end in `\#' (with the following $n$ being implicitly the same as the first). This then makes `\#' a special character always, like `@', bit that's also okay. Now, it would perhaps be nice, if we could find some way to chop concatenated strings up.\,. Oh wait, maybe compression on the Index table (i.e.\ StringIndexKeys) does help here, let me see.\,. (12:26) .\,.\,It does, since this table only has a PK. Of course, there is also the Strings table, but.\,. .\,.\,Hm, where it is the primary index that doesn't compress so well.\,. .\,.\,Yeah, but whatever. It's not a big problem. I'll just go ahead with this greedy algorithm for concatenation, i.e.\ where the heads (in each step) are always made as large as they can be. (12:33)

.\,.\,I was also going to mention that I think `\#$n$:$m$ is better for the specified titles, but now I'm thinking that maybe it would be nicer to just write out the appendix directly.\,. .\,.\,Hm, this is only a possibility due to that new idea of ending with.\,. Yeah, but that gets way too complicated when the appendices/specifications are concatenated themselves, so no, let me stick to `\#$n$:$m$' and only that for specified titles. This makes `:' a special character that needs escaping, but only if it comes directly after `\textasciicircum \#$n$', where `\textasciicircum' here is taken to denote the start of the string, not the symbol `\textasciicircum'. (12:40) .\,.\,And then `\#$n$\#$m$' as a full string will typically be referring to a concatenated string, starting with `\#$m$', and where the last string referred to by `\#$n$' couldn't fit the \#$m$' at the end of it, hence needing another nesting to complete it. (12:44)

.\,.\,(12:46) And let me also double down on thinking that it's totally fine, in the very early days of the network, to move users rates over from one entity to another, more specified entity without further ado, as long as we are not talking untyped/`something'-typed ratings, but are talking ratings where a specific type (such as e.g.\ `movie') is given. Then later on, we can be more careful to account for the fact that the network needs to function in a completely decentralized way, and where no party thus should take such liberties (anymore). Luckily, at that point, it will be easy enough to figure something else out, such as simply notifying the users and ask them to OK the transfer for example. (12:52) *[(14:04) Oh wait, we probably don't even need to transfer rates!\,.\,. It will not be so hard to simply tell a bot, when dealing with a certain specified entity to also look for similar rates for the unspecified entity, when the specification is the highest rated one.\,. Well.\,. Hm, it's harder when an entity is only \emph{further} specified.\,. So never mind, I guess.\,.]

.\,.\,Hm, that reminded me: I also need to think a bit more about untyped/`something'-typed lists (for instance property lists).\,. .\,.\,Oh, but the point, I guess, is just: never let the type determine what \emph{thing} we are talking about, only the \emph{function} of that thing in the context of the statements. For instance, `WWII' is always referring to \emph{the} war, and the type is not going to change that, but if we take e.g.\ `history subject' as the type, it then simply refers to \emph{the} war as a history subject. The type only qualifies the `as a' part, where the following type then does not change the \emph{thing} itself that we are talking about it, only in what context we are talking about it. More precisely, the types only specifies the tags/statements, not the thing itself. (13:00)

(13:42) Hm, maybe templates are still worth while to have (just as guidelines still, though).\,. But I'll see.\,.

(15:11) Hm, I'm reconsidering removing the `1's; it doesn't really seem worth it.\,. .\,.\,Since the ``null type'' is an actual type now (i.e.\ `something'), let us write the `1'.\,.

(19:00) Oh, I also need and end delimiter after the numbers of the head reference in the concatenated strings.\,.

(01.04.24, 13:18) About the end delimiter, let us just use a `.'.

I'm thinking of changing the system again. I think types are a bit too much trouble in comparison to what they achieve, so I think it might be better to just make some compound tags, maybe with a special syntax, and then just use those instead.\,.

.\,.\,I have also had some thought about postponing the types of the entity references for all custom, non-special types, but this might of course not be necessary at all, especially now that I might give types a lesser role.\,.

.\,.\,Hm, but it does kinda seem like the compound tag idea might be the way to go.\,. (13:24)

.\,.\,Hm, so in principle, all I'm changing now would/could just be that we introduce a syntax for making compound tags (i.e.\ tags that also tell the type of the instances), and then change SemanticInputs back, removing the ent\_type\_id column.\,. .\,.\,Hm.\,.

.\,.\,Hm, or maybe it makes sense to keep it like this, I' not sure.\,. (13:34)
.\,.\,But maybe typed references should just be for the special types, such as e.g.\ `user'/`u'.\,. (13:37) .\,.\,This might be a good idea, yes, and then an app column is also generally defined just by the entID; any additional type prop would just be auxiliary data.\,. (13:42) .\,.\,(13:42) The types would still be important for the system/network, but each entity will just have their relevant types queried for semantically.\,. .\,.\,(like I intended anyway.\,.)

.\,.\,Hm, I also considered using the SK for the non-special entity references, to make them more searchable.\,. .\,.\,(rather than using the ID *(so using the definition string instead).\,.) .\,.\,Hm, this would not work very well for specified entities, though.\,. (13:49) .\,.\,(13:55) No, there's no sense in that: If you are looking for a specific entity (a tag or some other kind), you are looking for that thing specifically. So no, I don't think I should use the SK for references.\,.

.\,.\,(14:02) Back to the question of compound tags vs.\ using the ent\_type\_id column, there is also the option of not using any special syntax for these compound tags, and then just.\,. .\,.\,Hm, maybe just use it for the aggregate (filtered) categories.\,. (14:04) .\,.\,Hm, or maybe the special syntax should just be `as a' or `as an', and if a tag includes this, followed by a reference/link (which are now untyped), then it is just automatically interpreted as a tag to be filtered by the various bots, meaning that they will only rate entities for that tag, if those entities are rates high enough as having the given type.\,. (14:13)
.\,.\,Hm, but it would be much more preferable with a single syntax for it, though.\,. .\,.\,A parenthesis could do the trick, but.\,. .\,.\,Hm, how about using `::'.\,. .\,.\,That could actually work quite well, where we then simply read `::' out loud as `as a/an'.\,. (14:22) .\,.\,`$<$entity$>$ fits $<$tag$>$ :: (as a/an) $<$type$>$'.\,. .\,.\,Hm, $\in$ could also be an option, but it probably won't save any bytes at all, and.\,. (14:26) .\,.\,(14:31) And it can also be read as `having the type,' which justifies the choice since it fits its meaning from Haskell.\,. .\,.\,(even though the syntax does not match that of Haskell at all for such tags, but that's okay.\,.)

.\,.\,(14:33) So `::' followed by a reference/link tells the bots to filter the given tag, and it also tells the application to suggest other types in the place of what comes after `::' to the user (and if there is no `::', the application can suggest types all the same).\,. (14:35)

.\,.\,For untyped references, which are now the only references possible for the non-special types, at least in the first version of the system/network/app as I imagine it now, I will let the syntax be like `@123.' instead of `@[123]'/`@1[123]'. For typed references, it will be like e.g.\ `@u123.' (we will stick to using only one symbol as long as we only have the special types for the typed references), where the type signifier is of course never a number (and never ends in one). (14:41)

.\,.\,Oh wait, going back to `::', we shouldn't even need to tell/program anything special to/for the bots, then.\,. So why not just let the users take care of defining tags that are semantically precise enough to also include the `as a' clause if necessary for the semantics?\,.\,. (14:45) \ldots (15:12) Never mind, it does make sense to tell/program the bots to filter these tags. And I actually also think I like using this special, brief syntax for `as a/an' clauses (and I think the users will find it useful as well). (15:13)

\ldots\ (18:16) I think I will embrace syntax like `::' even more, also letting property tags by constructed via syntax, especially. I'm thinking of using `-$>$' for property, and then perhaps also using `:' potentially after a property to denote that we are only interested in instances of the type coming after the `:'. This is much like `::', but not completely, since `::' also changes (specifies) the semantics of the tag, whereas `:' is purely there for practical reasons, namely to instruct the bots to filter the property tag.\,. (18:20) .\,.\,The first part of a property tag, before the `-$>$', should be a link/reference, but maybe the other parts shouldn't.\,. (18:21) .\,.\,Oh, and non-property type tags can also use the `:' syntax, namely if the type *(before the `:') is a subset of the type that comes after the `:'.\,. .\,.\,By the way, let me mention that that `::' syntax \emph{does} work a lot like in Haskell, namely if you thing of the tags as a kind of class (in programming terms).\,. (18:24) .\,.\,Oh, and I also might use `;' instead of `:', then, for the specified titles.\,.

.\,.\,Some of this can then be translated when rendered, if the user wants it, which might especially be a good idea in the beginning. However, the verbatim string should also be visible. When submitting entities, there should also be special forms to submit these kinds, where the user can then also see the verbatim string, and can also choose to write it verbatim. (18:31)

.\,.\,Let me also point out something else: `has good acting::movie' is not as good as `good acting::movie': Always try to make the tags as simple and easily searchable as possible. (18:33)

.\,.\,(18:41) Hm, I guess the other parts of the property tags should also be references/links as well.\,.
.\,.\,Hm, and then when you type in a property entity `verbatim,' it should actually \emph{not} be verbatim, not completely, as one should still be able to type in `@' and then continue to type in a link, where one chooses from some of the suggested entities, if any, and then when hitting enter, the link is inserted into the text field at/before the cursor position, replacing the `@' as well, seemingly.\,. (18:48)

(19:14) Oh, but I guess the superset type/supertype written after `:' should actually instead just be a semantic property of that tag. So if the users wants the bots to generally filter the instances of a tag by a certain (super-)type, they should just up-rate such a super-type/super-tag for that tag. .\,.\,Yeah.\,:) .\,.\,Hm, and what about the `as a' symbol, `::'? I guess this is different, since it also has another use, which is specifying the semantics of the tag. And then the bots might as well use it also, if that is helpful, why not?\,. (19:19)

(02.04.24, 12:47) I have slept poorly, but slept until late, so I got my sleep in the end. And in the middle of the night, I got the following good idea. I will give templates, or rather `formats,' a return, and then instead of using the `\{\}' syntax, I will let user write first a shortened format, followed (after a `$|$') by a longer, more specific version of the format. And (\emph{pause for effect}) I will use this to make (and allow users to make) the abbreviated syntax like the whole tag and property syntax I came up with yesterday.\,! (12:52) What a great idea.\,. .\,.\,Furthermore, for the formats, I will use a `\%$n$' syntax for the placeholders, and simple repeat these in both the abbreviated and the specific (parts of the) format. I will then recommend using language where the placeholders are always preceded by `the $<$type$>$', namely since this removes the problem of inflection. And for the form generated from a given template, I should then parse the first (function-identifier-valid) word before each placeholder, and put them as the label of the given field (in order of the placeholder numbers) in the submission form. (12:57)

In regards to my `::' syntax from yesterday, I think there should also generally be a `::' (`as a') clause before the `-$>$' in the property tag, such that the possessor has its type defined, i.e. (12:59) The order of the various part of such a syntax now does not matter too much, since the user will see the format above the form (be it a search (and/)or a submission form). For instance, it is okay that the property tags are written as e.g.\ `WWII::war-$>$end', even though the `war' part is postponed as the third placeholder input, such that you can search for the two others first (searching for `WWII' first, then `end'). (13:03)

I have by the way considering using `=$>$' rather than `fits' (in reverse, if you will). In that way `x =$>$ y' means that y is a quality of x, and `x -$>$ y' means that y is a property of x. (13:05)

And lastly, as I thought about in the bath this.\,. midday, when it comes to the specified titles---first of all, I will keep these, since I think that it is nice to be able to have this expandable subtitle. And in terms of dealing with duplicates, the users then simply up-rates a property tag `$<$entity$>$::$<$type$>$-$>$better duplicate,' first of all. The `$<$type$>$' part here is important, since you obviously don't want to move all votes/rates for the entity---for instance you don't want to move the rates for `$<$entity$>$::$<$type$>$-$>$better duplicate.' Now, when this tag has a high enough rating, and with enough users having voted. The app can indeed just notify all users who has rated $<$entity$>$ with tags where $<$entity$>$ (which could be `WWII' for example) was specified as a (`::') $<$type$>$ (e.g.\ `war'). And the notification can simply be `will you let you rates be automatically transferred to the `better duplicate' entity after the userbase has voted and decided (with enough margin, and enough votes) on which entity is the best replacement.' .\,.\,Oh, maybe the property tag should be.\,. hm, not a property tag, and be something like `needs specification/changing::$<$type$>$'.\,. Hm, but that's not completely right, since the semantics is understood with the entity being `as a' SDB entity, so what to do instead.\,.\,? .\,.\,Hm, could be simply: `needs specification/changing::SDB entity'.\,. .\,.\,Hm, and then the rate transfer should then simply concern all custom types at once, i.e.\ for all tags with an `::$<$type$>$' in them, where the type is not `SDB entity'.\,. .\,.\,Hm, that could work. So all in all, let us not be too worried about having underspecified entities in the early days of the SDB, nor about using consistent ways of specifying entities of various types, since this can all be mended without too much trouble later on.\,:) (13:23)

(14:09) I should also mention, references are ended by either `.', `@' (or `\#' for concatenated strings) or by the end of string. If a reference is succeeded by `@' (or `\#') or end-of-string, the dot should always be omitted. *(No.\,. (15:13))

(14:16) Hm, I'm now considering whether to just require that the syntax matches the `order of importance' as we might call it.\,. .\,.\,Or I could remap it in the specifying format.\,. .\,.\,(14:22) Or require that the syntax matches the order of importance.\,. .\,.\,Hm, I think so.\,. And then I will just use `$<$$>$' instead of `\%$n$'.\,. .\,.\,No, I'll use `\%e', where `e' stand for `entity.' (14:25) .\,.\,And the specifying format then gets to use `\%$n$' still, which makes to possible for it to explain it using a different order.

(14:40) Hm, but in regards to the `\%e::\%e-$>$\%e' syntax, maybe this order of importance (selecting the type before the property) is just as good as when selecting the property before the type.\,.

(15:12) Hm, maybe it's easier if references just always end in `.'. Yeah, I think it is.\,.
.\,.\,But maybe we can omit the `@' for format inputs.\,. .\,.\,Except that.\,. Oh no, the type is defined by the format, so yeah, we could do that. (15:15)

(16:03) Oh, I should probably use another delimiter for the formats rather than `$|$', since I'm using that for the capitalization--accent code as well.\,.
.\,.\,(16:06) Hm, but I could also go back to just using `$|$' as well for the subtitle/appendix/specification, and then there will just be two `$|$'s.\,. .\,.\,I think that might be better, and it also fits nicely with the format specification also being a specification, and one where it could also be nice if it were expandable. Yeah, so I think I'll do that.\,:) (16:09)

\ldots\ (18:32) I've had a couple of more good ideas. I'm not completely done thinking. But let me start by saying that I was considering using RegEx's for the search field where the app could continuously match the typed in query with the RegExp's, and if there is a match, it could make some relevant suggestions. In particular this could be used for the templates. Here the user might then type in a string that matches the rendered template string, and the app could then convert it and suggest going to a template column to finish the query there (or it could all happen in place). But I think that this idea is actually not necessary (and not useful enough to implement). Let me think some more about what else I have in mind, however.\,. (18:37)

\ldots Let me write something else first. I think we should use `::' (`as a') specifications only when necessary.\,:) So if you have a tag that you want to submit and rate for a certain entity, think about the other (up-rated) types of that entity, and think if it is necessary to specify the type among those, or if it can be understood implicitly without the specification, or if it has the same semantics anyway for all the types among these that the tag \emph{can} be understood implicitly in the context of. That was an ugly sentence, but I hope it makes sense. *(Oh, and the same goes for properties: Think if the type of the owner of the property has to be made implicit for the semantics to be clear, and otherwise just use the `\%e -$>$ \%e' format.)

And there's also always the `entity definition' type, which is a type that applies to all entities, and allow users to make meta statements about the entity, such as e.g.\ whether it has a better duplicate, etc. (This is similar to what I called the `null type' before.) Then if we go back to the question of dealing with duplicates, etc., the rates that are transferred automatically, if the user accepts it, should then simply be all tags where there is \emph{not} an explicit `:: entity definition' (`as an entity definition') declaration.\,:) (18:59) All other tags should have their instances of the relevant kind (if any) mapped to the relevant `better duplicates.'

.\,.\,(19:03) I have made a user--statement format of the (abbreviated) form `user who thinks \%e', but I think I will make it into a statement property instead: `$<$statement$>$ -$>$ user who thinks this is true.' For I think it is a good idea, as I've said before, I'm sure, to use the property construction as much as possible.\,. (19:06)

%This realization of not having to be strict at all about using types is really so important, I think.:) Just because types are often important for specifying the semaantics---in a very handy and easy way---doesn't mean that they are \emph{always} important to use.:) (19:10)

%(20:03) I have some more things to say, but also still some things to think about as well, and I was hit by tiredness, so maybe it'll first be tomorrow..

(22:33) Ah, we can just do something similar as with the duplicates: For property tags where the type isn't specified---or is/becomes underspecified---we can also just let it be up to some process involving user votes, which type should be chosen. In general, we shouldn't fear not being to strict about specifying the semantics of various things, as long as we think that other users will know in what way we meant, as such things can always be mended later on, when going from the early stages of the network to the more advanced stages. (22:37)
.\,.\,In a way, what's most important is that the early stages shows the way, so to speak, and leads to the following stages; it's not actually very important that we take great care in ensuring that the votes/rates from the early userbase it kept intact at all cost. (22:39)

(22:43) Maybe the order of the inputs in the formats won't matter very much\ldots

(23:06) Hm, wouldn't it make more sense to just make another SK for SemanticInputs rather than to use the statement\_user\_rater\_bot?\,.\,. .\,.\,That would also make the list include bots, but that's probably for the better overall, I think.\,.

(23:52) Maybe we can assume that most things have \emph{one} type that is naturally assumed (as the context in statements) without any further specification. And for this `main type' of an entity, we don't need the `as a' clause; for all others, we do.\,. (23:53) .\,.\,Yes, that's it.\,.\,:)

%(03.04.24, 12:26) Slept somewhat poorly. Went to bed very late and woke quite early. But snoozed and then lay wake for a long time, and then I had some good ideas about the tag syntaxes.

(03.04.24, 12:28) I've had some good ideas about the tag syntax. I will actually write it all of the property tag format using English words instead of special symbols. But I \emph{will} actually keep `::' for the general `fits $<$tag$>$ as a $<$type$>$' format. .\,.\,Hm, yeah, I think so.\,.(:)) (12:31) .\,.\,Sure.\,:) And the good thing is, it users can always expand and see what it means specifically, so they don't even have to memorize it. Now, for the property format, I think I will use `$<$property$>$ of $<$entity$>$ the $<$type$>$' instead.\,:) And again, the last part is only necessary to use if $<$type$>$ is anything other than the so-called main type of the entity, i.e.\ the one that is naturally assumed when speaking about it in most sentences. This so-called main type is then just a semantic property, specifically it is the type that is highest rates as the type property of the given entity---the entity definition. So let be more precise, for a given $<$entity$>$, the tag in question will be `type of $<$entity$>$ the entity definition'.\,. No wait, it seems that just using `type of $<$entity$>$' here would be fine.\,. no.\,. I should just call the meta type `entity' instead, rather than `entity definition'.\,:) (12:46)

I also had another idea% in the bath after getting up
, which is that when clicking on the text field of the search bar in the app, the first suggestions to immediately appear, before anything has been typed yet, should be the list of highest rated (as being useful) formats. The user can then select one (or make a search containing `\%e'-placeholders and then select one), whereafter they are lead to the template search(/submission) form field.

Now, there are many things we can do for the search field, including the mentioned thing about using RegEx matching, and so on. But I actually think that the first version of the app should just aim, apart from the thing mentioned in the previous paragraph, %..okay my typing speed right now really costs some time, actually.. :\
to search directly for the entity definitions. Then at some later point, it would be nice to also be able to type `@'---without a backslash first---and then do a sub-search essentially, where the search results while typing are temporarily exchanged for giving result searching just on what is typed after the `@'. When the link is then selected, it is inserted and the search continues for the whole string. This could be nice, but I will not implement it for the standard search bar in the first version of the app. The thing is, it will probably be much more common to use format for any such construction, by which the user should select that format first instead, anyway. And in regards to searching for e.g.\ properties and such, there are also the (good) options of doing what I call (and have been calling) a `semantic search' instead. (12:59)

For the template *(format) search/submission forms, however, there should also be search results shown while typing, and these should of course be for the individual text fields only. (And at least for the first version of the app, this should be enough). There is, however, the matter of searching directly on entity IDs, and of searching for.\,. oh no, searching for special types (like `user'/`u') is no further problem, 'cause this is always given by the format itself. .\,.\,And I guess when wanting to type in the ID instead of the entDef (the SK), we should just make it so that searches on straight numbers will give as the first suggestion, the entity with that number as the ID, before giving suggestions from searching on the given number as (the start of) an entity definition. (13:06)

So there we are.\,:) I also thought that maybe it would be a good idea to separate the bot and the user types, and then give SemanticInputs an extra tinyint to specify either `u' or `a' (for `aggregation bot', as to not be confused with `binary'), but then I recalled that aggregation bots don't have to be internal/native. They can be controlled by third parties as well, and simply appear as `users' in the system. (13:09)

About the statement\_user\_rater\_bot, I think.\,. Well, let me actually just think a bit more.\,. .\,.\,(13:12) Hm, I think I might indeed turn it into.\,. well, except that maybe one would want to have such sets but filtered in some way, perhaps by excluding dead user profiles (I think that might be a technical term for it, i.e.\ `dead,' but I'm not sure) for instance.\,. And it shouldn't really demand more space if we only use the statement\_user\_rater\_bot, should it?\,.\,. .\,.\,Hm, I'll keep the statement\_user\_rater\_bot for the reason given here (about the potential use of filtered lists of such users), so let us just keep in mind that we can also always just make another secondary index (another SK). (13:18)

And to round off, let me just repeat and underline how great it is to just have it like this where the users don't have to specify the type when it's the naturally assumed one, and where bots can just automatically make the aggregation categories.\,. oh, I feel like I had an idea last night in bed about this.\,. Let me think for a bit.\,. (13:20) \ldots (13:35) Hm, let me see, I think that such bots should look at the all the decently rated tags for any given type, taking all the types one at a time that is up-rated for filtering against. And then for each such type--tag pair, they should rate the `\%e :: \%e' version of that tag, where they first of all exclude all instances, that are not rated decently as being of the given type. And.\,. Hm.\,. .\,.\,And for each of the remaining entities, they first look at whether the `naturally assumed' type of that entity is the one in question. If it is, then the bot takes the rating directly from the tag.\,. Oh, but I \emph{am} missing something here, and I do feel like this is what the idea from last night was about.\,. .\,.\,Hm, if we for instance think of the types `film director' and `person,' then some entity might.\,. well the point is, the naturally assumed type might depend on the tag in question.\,.\,. (13:45) .\,.\,Okay this requires some thinking\ldots\ (13:46) %Luckily, I think I'm up for it; I don't feel tired at all (and actually haven't since I got those ideas in bed)... ..(13:50) Oh, I was about to write "..But isn't it trivial...", but it isn't. We uprate all the appropriate tags for a given type, but the problem is when a tag needs no type specification in most cases, but in some cases, they do. Hm, I might therefore just suggest that the users always specify the type, just in case, but I'm not sure... (13:52)
.\,.\,(13:54) Ooh, maybe we should think more about the `naturally assumed' type given a tag with no context of the entity, rather than the naturally assumed type of a given entity in the context of a tag.\,. Hm, and I guess this essentially leads to `always specify the type,' don't it.\,.\,? (13:57) \ldots (14:12) I guess the key might just be to always define the tags fully, using the (expandable) subtitle/appendix, preferably.\,.

\ldots\ (15:04) Okay, I have it now, finally. First of all, the `::' tags should not be meant for users to rate---and preferably not even to look for themselves---but should be meant for the bots to use only. So we should actually remove the specification from that format; no sense in specifying something that users in general are not really meant to use. When a user has `filter by $<$type$>$' for their ListGenerator search, the app can then see if it is possible to utilize such `$<$tag$>$ :: $<$type$>$' tags, where $<$tag$>$ is a relevant one for the given search. And instead of the bots just automatically creating these filtered lists for all relevant tags of a given type, the users instead up-rate exactly the relevant tags for a given type, where the tag can be used in a much wider sense as well. For instance, their is not much sense in filtering `action movies' by `movies' (since this can only ever reduce a little bit of spam, which shouldn't actually be a problem, since the userbase needs to actively down-rate spam (and down-rate the spammers)), but it makes sense to filter e.g.\ `entertaining' or `good' by `movies.' So that's how to do that.\,!\,:) And what's more, as I wrote at the end of the last paragraph, just before my walk here, the key \emph{is} to just to make a specification appendix/subtitle to all tags that isn't clearly understood on its own for the most part, but where said specification might often enough be helpful. And in general, one can probably say that it is good to specify most of the tags that are either abbreviated verbs or adjectives, more so than those that are just type nouns, essentially, such as `action movie.' .\,.\,So as a rule of thumb, specify all tags that either starts with `is' (with no `a'/`an' afterwords) or `has', or more likely, can have `is' (with no `a'/`an' afterwords) or `has' inserted before it. .\,.\,Well, that could be a rule of thumb, but I guess in general, just follow what seems natural: `which things can enough be helpful to have been specified, and which things really don't need it?' *(A better formulation of the rule of thumb, perhaps: `if you can put `is a(n)' in front of the tag without changing the meaning, then you may not need to specify, but otherwise it might very well be a good idea (I think.\,.).\,. (16:05))
As for properties, I will \emph{not} use the `$<$property$>$ of $<$entity$>$ the $<$type$>$' format, but only just the `$<$property$>$ of $<$entity$>$' format. And if this such be specified further, the users can specify the `property' entity here. Hm, and how to do that, actually.\,.\,? (15:24) .\,.\,Hm, by beginning the specification with `the property $<$property$>$\ldots'. For instance `movies directed$|$the property.\,.'.\,. Hm, or maybe it could by like `movies directed$|$as a film director'.\,. Hm.\,. .\,.\,Yeah, I guess that's fine, and I guess it's fine in general to just let it be up to the context of whether the specification is interpreted as a standalone sentence/lexical item, or if it is interpreted, like here, as continuing where the title left off. (15:32) .\,.\,One could.\,. Ah: If there's an ambiguity, one could.\,. use a capital letter, but maybe one might just want to rewrite the thing instead. Okay. moving on. Back to the properties: Sure, it is a good idea to specify the semantics of otherwise somewhat ambiguous properties, but it actually won't matter very much if users don't specify their properties. The point is, they are first of all rarely used for filtering/sorting EntLists (entity lists), and if they are, the user can just find the right property for the given thing that they are interested in. In terms of up-rating properties for what's essentially the `information page' of an entity column, sure, it might in principle happen that some entity has a property shown whose ranking was rated on the basis of different semantics than those it naturally has for the specific entity in question (of the column) (long sentence, but it should make sense), but what does that really matter?\,. Not much at all. So yes, it might be a good practice to specify properties, but it's not really needed, and one has to also weigh in the fact that using simply entities also has some benefits, since its more intuitive to look for the simple entities, rather than looking through the recommended specific ones.\,. (15:41) :)

So there we are!\,:)

I also have some other things that I need to mention. I talked about making the `users who thinks $<$statement$>$' into a property (and have done this), but I actually now think that `users who thinks $<$statement$>$' is a better choice. Hm, wasn't there more I needed to say?\,.\,. .\,.\,Oh well, I'll think of it if there was.\,:)

Ah, I really think that I'm landing/have landed now on something good.\,:) It's always such a long creative process when I need to make big changes like this, and of course, I might not be done yet: It so often feels like I'm done, and then they same day or the next, I realize that some other big changes (further ones and/or changes back to something previous) are needed. But it really does seem like I'm about to land / have landed on something good now.\,.\,:) (15:48)

.\,.\,Oh, I could mention that I think that when the users type in `$|$' in the search field, it might make them jump to the specification. This could be true both in the early and the later versions of the app. And in the early version, this will simply implemented by \emph{not} scaping the `$|$' character automatically---unless the user has chosen the verbatim option, which is handy e.g.\ for searching on URLs. (Note that the concatenated string syntax is always under the hood, however, so ``verbatim'' really means `verbatim, except for concatenated string syntax.') (15:59)

(16:40) Hm, I'm actually just gonna let the specification always be a continuation, and then you can just make a period or a colon, etc., if you want to begin a new sentence, e.g. This means that I will also make it so that spaces are written explicitly, meaning that many specifications will begin by a whitespace.

(17:05) And remember: It's (much) easier to go from an intuitive system over to a more efficient one, based on the first, rather than the other way around.\,:) So with that in mind, don't worry too much about efficiency of the early version.\,:) (Talking to myself.) Not that I \emph{think} that I will change the system---I actually don't---but it's still a comforting thing to remind oneself of.

(18:28) Hm, it almost seems that the entity references won't be necessary, and maybe not used much. But I'm not sure if I dare remove them.\,. .\,.\,(18:30) Hm, that would maybe be in order to actually prevent users from using them, forcing them to use the templates instead, which is probably best. For instance, it might be tempting to define a relational tag on one's own by writing it intuitively, but it might be much better to reformulate it as a property-of-entity tag, and thus use the property template (format) instead.\,. (18:33) .\,.\,And I can always add it later on.\,. at least if I still keep `@' as a special character for the whole string, not just the beginning of it (where it can represent the beginning of a format closure).\,. So I'll basically keep room for these inline entity references, but having them as `not implemented yet,' essentially, except without a clear promise that they will be implemented.\,. Hm, on the other hand, it would be nice only having to escape special characters at the beginning of a string, wouldn't it?.\,. Hm, but maybe not enough that I want to throw away the possibility to implement and include inline entity references (links).\,. (18:37) .\,.\,Oh, and `$|$' is also a special character throughout anyway.\,.

(19:03) I have until now forgotten about formats over entities with specifications, but I guess we can just escape `$|$' (once) in the format.\,. Wait, does this even make sense; the specification will just be exactly that of the template, after the placeholders have been substituted (but the cap.--accent code it interpreted before said substitution).\,. Hm.\,. .\,.\,Yeah, that's it, so I have already implemented formats where the specification is part of the format; it always is. (19:07)

.\,.\,(19:08) Let me repeat: I'm very happy with all these new changes (as a whole). It's for instance so nice that e.g.\ `movie' as both a `type' and a `tag' (formerly known as a `category') can share the same `subcategories' tab.\,:) And there's so many other nice things about these changes.\,:)

(22.17) The ability to specify properties is actually so important. It means that we can for example make properties like `early life$|$\ldots' etc., where the dots here then specifies that to property instances are of the type `article section'.\,!\,:)

%(23:31) Hm, one will probably not be able to log in / be logged in with the browser extension, while on other sites, as this poses a security risk, it seems. But I guess since everyone can query as a given user, we only need it for rating and submtting things. And for that, there can just be a link to the website itself.. Hm, although if I made a browser extension like feature on the website, where the user browses the web in an iframe.. Hm, let me see about something.. ..Hm, I don't think I can get the address bar in the iframe, but this maybe doesn't matter too much.. well, it matters some.. (23:38) ..Oh wait, no, we're free to just change the top url to anything we like, right.. Right?.. ..Oh, of course not, as a StackOverflow answer points out.. (23:41) ..Hm, but my site could just redirect to the page with the given URL if it receives the relevant reformatted version of the URL (with my domain in front). So this could all work; you could in principle make a browser extension via a website, I think. But do I want to?.. Hm, maybe.. (23:43) ..And there could also be a button to copy the real URL (of the iframe site) to clipboard.. (23:45) ..Ah, but not being able to use the address bar or one's bookmarks, and in general having to always go via my site.. Well, one could of course change one's bookmarks, but no. This is way to complicated, enough that it defeats the purpose of the browser extension, rather than just navigating to my site when you want to see data aboout a specific URL. (23:48) ..Oh wait, I forgot I could just use an iframe for the browser extension for loading a window from my domain, where the user can then be logged in. This was also my plan to begin with. So never mind all this: it could be possible to make a browser extension where the user can be logged in. (23:51)

(04.04.24, 14:13) Hm, since the specification part of a format is meant to be come the specification of the.\,. Oh, I'll stop myself there, 'cause there is actually just one answer. Yeas, the formats should only appear as the whole entity string (which means that I can and should get rid of the trailing dot for sure), and if you want to substitute it in the middle of a string, will then you just substitute the format instance entity that has the format closure as its whole definition.\,:) .\,.\,Hm, since templates will be the most common use of `@' references, and also to balance things out, wouldn't it be much better to use `@123' instead of `@f123' for templates, and then use `@e123' for entity references. I think so. (14:21) .\,.\,Hm, and this also means that `template' is just as good a name as `format string,' if not better.\,. .\,.\,Yeah, let me go with `template' instead, like I used to. (14:30)

\ldots\ (17:17) I quick note: When users submit any entity, they should immediately get the options to selects its main types and then up-rate both that the entity fits the tag and that the tag first the `type of $<$entity$>$' property tag.

Now, more importantly, I actually have to rethink the concatenated strings, and I've landed on the following, I think: There shouldn't have to be any support in the backend layer for searching for longer strings than of length 255. Not apart from being able to search for hits for strings that matches on the 255 bytes, and then maybe to let the application check the first couple of hits to see if the tail of the hit matches the tail of the query. .\,.\,Oh, but this obviously does not include the backend layer, so never mind. .\,.\,Hm, more to the point, I think I will implement longer strings for entities by simply letting an entity hold both a string a nullable text id, where the string is the first up to 255 bytes of the full text, and the text is, not the remaining text, but the full text itself, meaning that it starts by repeating the string. .\,.\,So there we are.\,. (17:28) .\,.\,(Note that this means that the entity's SK can be obtaining from knowing the textID and the text itself, the latter of which can be obtained from knowing the textID alone.\,.) .\,.\,Oh, but the text should simply be null if it is smaller than 255 bytes, and therefore contained fully in the string.\,. .\,.\,Hm, but I could also implement this in the app.\ layer alone, namely by using a trailing `\#$n$', where $n$ is the textID.\,. .\,.\,Hm, and if we instead let $n$ be a regular entity ID, then we are back to getting the opportunity to implement completely searchable entity strings, if we want to.\,. (17:35) .\,.\,Well, not \emph{completely} searchable, actually; only in the sense that each substring can be searched for.\,. .\,.\,Unless we implement it as a doubly linked list (with both leading and trailing `\#$n$'s).\,. (17:40) .\,.\,Hm, so should I just do that, and thus keep concatenated string up in the application layer (not counting initial\_inserts.sql, of course)?\,.\,. .\,.\,Hm, there is also the option of doing both things, I guess.\,. .\,.\,Oh, and if I choose to still implement it via syntax, I might also let the backend interface provide one query procedure for the purpose of getting all the parts of the concatenated string immediately, or rather up to some maximal number ($\sim$10--20).\,.
.\,.\,Hm, using syntax would actually be more efficient for the most part, at least if I stick to wanting the text to repeat the string.\,. (17:49) .\,.\,Oh, and if I use texts, I can't prevent identical entity duplicates, which is something I think is really important (only semantic duplicates are allowed (where their defs are different but their meaning the same)). So that settles it: I'll use this systax for concatenation, where I make it a doubly linked list, and where you are supposed to reference the head, not any parts of its tail. And let me actually set aside a fixed number of bytes to each `\#$n$' link, such that the maximal number of bytes that any string in the list can contain is fixed, and never depends on any of the $n$'s. (17:55) .\,.\,Hm, and this means that the tail end does not need an end delimiter, and the head does not need a start delimiter. Instead I should just make `\#' a special character always, I guess.\,. .\,.\,Sure. (17:58) .\,.\,Hm, with BIGINTs, I lose 21 bytes this way for the head and the tail end, and 42 bytes for all the middle parts, which is fair enough, I think.\,. .\,.\,No, 22 and 44, since I.\,. No, I need only and end delimiter for the leading link, so I could get away with 22, 43, 43, \ldots, 43, 21 *(no, the other way around), but is it perhaps better to round up?\,.\,. (18:04) .\,.\,Hm, I'm not gonna use hexadecimals, I think, since MySQL does not support conversions at all(!) as well as it thinks it does.\,. .\,.\,And it doesn't matter enough that we should worry about it. I don't know why, but I think I'll round up to 22, 44, \ldots, 44, 22.\,. (18:08) .\,.\,No, 22, 43, \ldots, 43, 22.\,. .\,.\,Oh, maybe I should pad the.\,. hm, no.\,. .\,.\,(18:17) Oh, because I round up the first 21 to 22, it actually means that one can also (perhaps later) implement concatenated strings using a text as the entire tail, namely by using (e.g.) a `\#t$n$' syntax instead. And this will work since `\#' is a special character that always needs escaping if it is not part of a (concatenation) code. (18:20) .\,.\,By the way, note that while the concatenated strings are implemented in the application layer, this implementation should still be almost completely hidden from the users: They should just essentially see the entities as having a higher limit than just 255 bytes, as if the backend simply supported that directly. (18:24) .\,.\,(18:25) Oh, wait, how can we implement linked lists when we don't control what IDs the submitted entities will get!?.\,. .\,.\,Hm, we can't.\,. .\,.\,Hm, so maybe I'll either need to use texts as the full tail, or the full text, repeating the head as well, or I should let the backend support inputting/submitting concatenated strings.\,. (18:29)
.\,.\,What to do.\,.\,? \ldots (18:47) Oh, the doubly linked lists actually doesn't help to make it more searchable.\,. Hm.\,. .\,.\,Hm, I guess it would work to use the idea I was gonna implement before I had the ideas of this paragraph.\,. Hm, why was it again that that way of doing it would not work.\,.\,? .\,.\,Yeah, why??.\,. (18:52) .\,.\,Oh, it works only when you know (almost) the whole string; if you only know e.g.\ the first part *(less than the 255 bytes.\,.), then you can't use that to search for the rest, that's why.\,. (18:55) .\,.\,Wait, that's not true, is it.\,. .\,.\,Oh, you could let the head and all other parts that is not the tail end end with `\#', without a number behind it, thus telling the application to look further for the search results. (18:58) .\,.\,Yes, and this actually makes the concatenated strings `completely searchable', i.e.\ basically as good as if the backend (entity SK) index allowed for any number of bytes.\,:) .\,.\,Yeah, essentially.\,:) (19:01)
.\,.\,Hm, and since the concatenated string you would want to render are always the tail ends, which begins with `\#', I'm now back to having that `\#' need only be special when it is the first character.\,. (19:05) \ldots\ (20:25) Oh no, it should always be a special character.

(05.04.24, 9:57) I should allow for using entity references as the type ``declarations'' of templates. In particular, I might use this to specify `property' further in the property (`$x$ of $y$') template, namely by writing `@e41'---well, I guess I should insert `property' earlier---instead of `property.' And then I will specify `property' further. This seems like an actual good use of `@e$n$' references (outside of template inputs).

I think my expandable EntityTitles could work, especially if there's a linkLevel prop/ state that decides what recursion level gets links. And then linkLevel could change when you toggle the expandable titles. But let me just implement it at first like I did before, where you simply have to click the link and go to the column to see the full title, and where the ``linkLevel'' is then 2. (10:06) *[(11:33) Or maybe I will implement it, and just make all links toggleable, only.\,.]

By the way, when implementing different languages, unless a translation algorithm can simply be used, it would probably be best to just try to relate each entity to a translated version, and then let the app automatically look up and replace the title of the entity, but make it so that users are still rating the original, English entities. In terms of translating the property (`$x$ of $y$') template, if I haven't mentioned it before (I might have), if the `$x$ of $y$' construction is not so easy in that language, maybe even if this is simply do to (possessive) inflection, I would recommend using a special `$x$ $<$- $y$' syntax instead, and just get used to that. (10:13)

(10:36) Oh, what about compound words for the template types; I probably can't just parse the word in front via whitespaces.\,.\,? .\,.\,Hm, how about using underscores instead?\,.\,. Oh, or one could also just use entity references.\,. (10:39) .\,.\,Oh, or I could just parse from the end of `the '.\,:) (10:40) .\,.\,Yes, let me just do that.\,.

(19:17) I should also mention: Note that tags can also break out of the standard semantics for what the rating scale means, and importantly, they can specify it more precisely. For instance, a `durable' tag might define (via the specification) how the rating should roughly translate to a time (e.g.\ `this item is durable for roughly three years,' where three here is then deduced from the rating).

(19:25) In regards to my whole `UPA idea,' why don't I just start by making a category of user submitted code suggestions, both for CSS and JS/React. These can then be explained in real text and shown haw to implement via code snippets. So if a can just find a code snippet React component online (like listings in LaTeX), and prepare the whole code suggestion format, then we can already get a taste of the overall idea, even though we it won't exactly be a UPA yet with just this. .\,.\,:)

(21:23) Hm, maybe the `as a' clause \emph{is} actually quite important, since in a way, all ratings are relative, and therefore it's best to always be able to compare to others when you rate.\,. So maybe I should focus more on trying to make it so that entities can be rated while viewed in a list, moving them up and down (I probably won't implement dragging entities, but maybe just make it so that one can copy--paste, kinda, or at least make it so that the list is updated immediately after---perhaps even before submitting a rate).\,. Hm, some food for thought.\,. (21:27) .\,.\,Yeah, good thing I (re-)realized this: It is quite important for the system to work well for the users to be able to rate entities on a list. And these list should in general be carefully selected for each *(predicate) tag where the spectrum is important (unlike e.g.\ most property or type tags), which is why the `as a' clause might be quite important after all (and filtering by bots less so, by the way.\,.).\,. (21:32) .\,.\,Oh, it could be nice if, especially for larger lists, we could pick out some good examples for the whole spectrum of the list, preferably prioritizing the most famous examples, and then show a given selected entity on this list, together with these example ones.\,.\,!\,.\,. (21:35) .\,.\,And by `we,' I mean the user community. .\,.\,Hm, this would mean creating a tag property of `part of a representative selection of some of the more well-known instances of this tag'.\,. (21:37) .\,.\,Perhaps with just `representative instance' as the short title of this property.\,. .\,.\,(Although I will probably try to avoid using the term `instance,' since I probably have special connotations for that word.) .\,.\,(And it doesn't fit for `tags' as well as for `categories'.\,.) .\,.\,Hm, and then the best thing would be to have bots copy the ratings of these representatives over to a.\,. Well, or just to have a bot that only rates representatives---hey, that'll actually be pretty easy after all.\,. (21:43) .\,.\,And then I just need for a ListCombiner (ListGenerator) to combine this list with one or a few selected entities, which also shouldn't be much trouble. .\,.\,Great.\,. (21:46) .\,.\,Oh, or even better---at least at some point---we'll just get a bot to make the representative list automatically, simply by selecting from as much of the spectrum as possible and also prioritizing entities which are rates highly as being `well-known'.\,. Yeah, good possibilities. And in the beginning, why not just make a bot that selects at random from the whole spectrum, and then we can always upgrade the same bot later to prioritize `well-known'/famous' entities.\,:) (21:50) .\,.\,That's actually great.\,.\,:) .\,.\,really great.\,. (21:53)

(06.04.24, 11:36) Okay, I'm not going to make the `as a' clause special. But we should just in general be careful not to use too wide-ranging tags. For instance, `funny$|$ as a movie' is generally better to use for a movie than `funny$|$ as a piece of media.' I will by the way divide the `Ratings' tab into types, such that the tab includes several EntityLists under/above one another, one for each of the most relevant types to the entity (if there is more than one (that exceeds the rating threshold)). (11:40)

(11:48) I think I will also make it possible for template specifications to introduce new placeholders, not just use the ones from the (short) title. I also have to consider the fact, that I should probably make `\%' a special character as well.\,. Oh no, maybe just.\,. hm, make `\%' followed by a number disallowed in specifications, as it will be replaced.\,. Hm.\,. .\,.\,Let's just make it a special character throughout, why not?\,.\,. .\,.\,And I should by the way also make sure to prevent single backslashes that does not escape a special character. (11:54)
\ldots (12:43) Ah, the ``excuse'' for making `\%' a special char could actually be that it is not actually rewritten as the EntityTitle of the entity that it refers to, but it is simply rendered as itself, only as a link (to the same entity as the link before it that it references). .\,.\,Hm, but shouldn't I use `@$n$' instead for this syntax. It doesn't clash with the template syntax, as they never make sense to have at the beginning of a def. (12:49) .\,.\,Okay, I'll do that, and I might then make `\%' only a special character in the context of templates, where it is then escaped, not by `\textbackslash', but by writing a double `\%\%' instead. (12:52)

%... (15:49) Havde en skøn løbetur. Og fik en ny idé:
\ldots\ (15:49) I had a good idea: I should also make a template for constructing tag types, and then we should also up-rate types for tags---and tags for types. In other words, tags should have semantically defined types, as I like to call it. Then bots can for each popular enough type take all the `relevant' enough tags for that type, make.\,. Hm, wait.\,. .\,.\,Oh, so maybe we don't really need this so much, since the `relevant tags (for this type)' property can just be understood to mean that these tags are also relevant to have bots filter by said type. .\,.\,Yeah, so we only need the `relevant tags' property, not necessarily the reversal, which is to have tag types.\,. (16:03)

(16:35) Hm, maybe I just want to repeat the EntityTitles for the `\%$n$'-back-references.\,.

(17:34) Hm, I just had the thought that maybe I could also make an input procedure for submitting/inputting template instances, and then I could go back to saying the templates are exploded/converted/filled out before being saved (but where it's still easy to insert template instances already in the backend layer).\,. .\,.\,Of course, it cost a bit extra space (but the space that Entities table requires is not really at an order where it matters much), but apart from that little thing, what would be the downside of doing this? And on the plus side, at makes these instances more searchable.\,. (17:39) .\,.\,At least it makes it a bit more searchable, although you still have to make a sub-search for each reference---well, unless I then go back to allowing for.\,. inserting strings (not references) directly into the template, but wait, I just had an idea, also: What about making the reference/links a trailing code instead, such that you control the title of the input entity and give it a reference underneath, so to speak, similarly to how links are made in HTML and in markdown, but where the link code is instead postponed as a trailing code.\,.\,!\,.\,.(?) And when you submit via a template, you can then either insert a non-link string, or a string with links in it, which are then moved back to the end of the entity def when this finally constructed.\,. (17:45) .\,.\,And again, we shouldn't worry about the extra space that the Entities will then require, and we also certainly shouldn't care about exceeding 255 bytes, since this method will prevent more subsequent queries (querying for references and template inputs) than it will cause (querying for the tail of the concatenated string).\,. (17:49) .\,.\,Oh, and I should make a query procedure for querying many substrings of a concatenated string at once, anyway.\,.

.\,.\,Hm, it seems that trailing link code and templates that are expanded/filled-in upon submission straight away in the backend, before being finally stored as Entities, would indeed be a good idea.\,.

%(17:55) Det var en god løbetur, og lang: 10 km (bare i roligt tempo)! Men har så ikke haft så meget energi her efter den, så jeg er virkeligt glad for, at jeg så endte med at få de her idéer i stedet (når nu jeg ikke får kodet rigtigt).:) ..Og ja, tilsyneladende nogle virkeligt vigtige idéer..!

.\,.\,Hm, maybe I should make the template insert procedure in PHP, rather than in SQL, though?\,.\,. (17:59) .\,.\,Nah, 'cause it's best if I can run it in the initial\_inserts program(s).\,. .\,.\,Hm, but maybe I should just copy--paste instead for making ``template instances,'' and then I can implement the template instance maker in JS instead.\,.

(18:20) Let me use the same kind of technique as for the cap.\ code to define where a link starts and ends. .\,.\,But where the symbol is then any UTF-8 symbol, not just (lowercase) letters. .\,.\,Oh, and by the way, I should make check for no non-lowercase letter in the PHP program.\,. Oh, I wonder how MySQL treats non-english letters like e.g.\ Æ, Ø, and Å. I should look into that (and make sure to choose the right character set/collation).\,. *(Oh right, I use the utf8mb4\_bin, so it should just treat all different characters as such.\,. .\,.\,Oh, but that means that I have to know all letter characters, if I want to make it searchable in other languages.\,.\,:\textbackslash.\,. (18:51) .\,.\,Ah, PHP has mb\_strtolower to the rescue.\,.\,:) (18:53) \ldots (19:35) Oh, and even better, MySQL has LOWER().)

.\,.\,Oh, if we include all symbol, then I can't use the numbers.\,. Oh, I can, if I just always write always write.\,. No.\,. But I can think of something else.\,. .\,.\,Oh, for instance, I could give the numbers an end delimiter (and since it is only one symbol always, we don't need a start delimiter (and it wouldn't work if we did)). So that's a possibility.\,. .\,.\,Yeah, so we could have e.g.\ a syntax like `hello world!!!$|$w.!2.123.' which would give us a link *(to entity 123, but I guess we should also have a type identifier first (like `e' or `u'), actually.\,.) located on `world!!!'.\,. Oh, but then I need to only treat the special characters.\,. as special characters before the second `$|$', but no, I can't do that either, since I need `$|$'.\,. Well, no, not necessarily. But I guess it's better to just convert from e.g.\ `\textbackslash$|$' to `$|$' before the link code is interpreted.\,. (18:37) .\,.\,Right, which means that both the link code and the cap.\ code \emph{can} actually use the special characters, namely if we just un-escape them before interpreting these codes. (18:39) .\,.\,(And if there are escaped before submission, i.e.)
.\,.\,`hello world!!!$|$e123.h.o.e124.w.!2.'.\,. (18:43) .\,.\,Let's separate the links with `;' rather than `.' to make it more clear: `hello world!!!$|$e123.h.o;e124.w.!2;'.\,. \ldots (where I have omitted the specification, including the extra `$|$' needed, as well as the cap.\ code if we decide to let that come before.\,.) (19:00)

(19:25) Hm, I might just use numbers for specifying positions instead for the links.\,. .\,.\,Something like `hello world!!!$|$e123.0.4;e124.6.13;', for instance.\,.

(19:29) Oh, I by the way also had the idea to use `anything' once again, rather than `entities' (or `something') as the everything type. For `anything' can be interpreted exactly the same as `something'.\,.

(19:48) Hm, would it make sense to drop---or delay implementing---templates, and then just let users do the copy pasting. And then I.\,. ``just'' have to make the thing where you can insert links by typing `@' (with out a backslash in front), and then start typing the title of the entity to insert, where the search results then temporarily is about the link that you are currently typing in.\,.\,? .\,.\,Hm, and this feature can then be used both for searching and for submitting.\,. (19:53) .\,.\,(And when it comes to searching, you then just don't \emph{have} to use this feature to search for entities with a link inserted, as you can also just start typing in the (short) title of that entity directly, without typing in the `@' first. .\,.\,i.e.\ now that I'll use trailing codes for making the links.) .\,.\,Hm, but even though you type in `@', this character is not actually used for it, then, which means that it doesn't have to be a special character anymore.\,. (19:59)

.\,.\,I think this is what I'll do.\,. (20:00)

(22:22) The nice thing about not using templates, in particular for properties, is that each property tag can then be given a specification that is tailored specifically to the property tag, and not just be given an instance of the property template's specification, which only defines the property tags' syntax broadly.\,.\,:)

(00:53) I could also use the `@'s, but just postpone the rest of the links to the trailing code (so that the trailing code would be e.g.\ `e123e124u123').\,.

(07.04.24, 11:24) I can make a kinda intermediate language for the app where we use `@', but no, in the database we should rather use the syntax like `e123.0.4;e124.6.13;', although I actually just had the thought of just using a syntax like `e123e124' instead, and then I could say that the link is just located the first place where the short title is repeated.\,. Well, or I could even also make skip codes to be absolutely precise.\,. Sure.\,. So this way, I'll then \emph{not} allow links that where the rendered text content of the link differs from the short title of the entity referenced, just like I had it before. And then I could maybe just make sure that the syntax is open for future modification (backward compatible) such that the links does get the ability to have a different text than the short title of the entity exactly.\,. (11:29) .\,.\,And that's actually trivial since the `e123e124\ldots' syntax is already open thus, as it expects one of the special type letters as the first symbol. So if you want to break out, just use a different symbol as the first character of the code. (11:32) .\,.\,And for the rare skipping of the first few matches of the short title (which might happen if the short title for instance is just a single letter or two long), just append a dot and another number after `e$n$', writing `e$n$.$m$' instead (but where the last part is optional), where $m$ is of course the number of occurrences to skip, just like for the cap.\ code. (11:35)

(11:36) And about templates, I do need them still, since how else are we going to uprate `relevant properties,' for instance?\,. .\,.\,But we could perhaps uprate single-input templates rather than properties.\,. \ldots I think I like this idea.\,. The alternative is to have properties that specify themselves as properties, but that doesn't seem as good.\,. (11:50) .\,.\,So property entities will now actually be a class of template entities.\,.

(12:01) Hm, maybe I should turn the first `$|$' into a trailing code as well instead.\,. .\,.\,Sure.\,. .\,.\,And let me just use the symbol-followed-by-skip-number syntax for this as well, as this is more compressed, and can be done by eye, which I find nice as well.\,. (12:04) .\,.\,Hm, and since the spec will often start with some punctuation, let's say that the chosen position is just before the given symbol.\,.

(12:10) Ooh, the fact the properties are now templates (as I think I've landed on) also means that we are no longer completely tied to `of' as the proposition.\,. although it will probably still be best with a convention of using `$<$property$>$ of $<$entity$>$' as the short titles of properties as much as possible.\,. (12:12)

.\,.\,(12:15) Hm, let me put the specification delimiter code last, preceded by the cap.\ code, preceded by the link code, preceded by the full title. This then fits the order of when the changes of these codes should be applied, from right to left, i.e.\,. .\,.\,Oh, that's not true, the link insertions should be done before the cap.\ code, now that.\,. they are written as text within the full title.\,. Hm, but what about when the short titles have capital letters in them.\,. oh, maybe I \emph{should} then let the capitalization be carried out before the link insertion, and require that the capitalization matches the capitalized (and accented, by the way) short title of the referenced entity.\,!\,.\,. (12:23) .\,.\,Of course, this could also be an opportunity to finally let the referee dictate the capitalization of the referenced entity.\,. (12:25) Hm.\,. .\,.\,Hm, and I should also still keep in mind that if I go back to using positions by links, this will make the referee be able to control inflection as well.\,. .\,.\,(finally.\,.) (12:28) .\,.\,Hm, but it doesn't work very well in regards to the properties being templates and all, which is important.\,. (12:29) .\,.\,No, let me stick to only implementing only these `e123e124\ldots' kinds of link insertions for the first version of the app, and let me indeed just say that the capitalization has to match as well.\,. (12:33) .\,.\,Or should it.\,.\,? (12:34) .\,.\,Yeah, actually, let me say that it should.\,. .\,.\,Hm, so yeah, let that be the order of the trailing codes.\,. .\,.\,the full title, the link code, the cap.\ code, and the spec.\ delimiter code last.\,. (12:38) .\,.\,Hm, or maybe I will reverse the order, such that are instead just carried out left to right.\,. (12:42) .\,.\,Yeah, that's nicer.\,. .\,.\,Oh, there will also often be a whitespace at the start of the spec.\,. Oh well, I guess that's also fine.\,. (12:44)

(12:54) Yeah, and I should also make an IL, so to speak, of these entity defs, which is easier to use for editing new entities, where the specification delimiter is a symbol/code inserted at its place, where links are also inserted at there place (e.g.\ using a `@$n$.' syntax), and where the capitalization is WYSIWYG.\,.

(13:00) Ooh, I just realized, having the full titles written out also opens up for FULLTEXT searches.\,!\,.\,. Of course, it won't work \emph{as} well for long titles that needs to be divided up and saved as a concatenated string, but thats fine: If you have some important buzzwords, they should come before the 255 bytes mark anyway, shouldn't they?\,.\,;)\,:)

\ldots\ (15:05) For the templates, I should use a delimiter, `$|$', for denoting where the specification starts, like how I did before (for the actual, stored entity def). I will also use the `\%e'--`\%u' syntax for template placeholders, and then use a `\%[$label$]' syntax for regular entity inputs that are then not labeled just as `entity $n$', but as $label$. In terms of the back-referencing placeholders, I don't think I will implement these for the first version of the app, when if they are ever implemented, they can use a `\%$n$' syntax, where $n\in\{1,\ldots, 9\}$, and potentially use a `\%[$n$]' syntax for larger $n$.\,. Hm, but that means that I would ought to disallow number labels for the first version of the app as well.\,. .\,.\,Yeah, nah, even \emph{if} we ever implement these, we will surely never need more than 9. (And if we ever do, we can just find another syntax that works.\,. ah, for instance simply by using another kind of brackets instead of the square ones. So no worries.) (15:14)

.\,.\,If there are links in a template, they are just implemented the same way, and then the app just has to.\,. combine the trailing codes.\,. Hm.\,. .\,.\,Hm, that will not be totally easy to program, but I guess it's worth it, so yeah, I guess so.\,. (15:17) .\,.\,Hm, but why not just also let the spec delimiter be defined the same way as well, then?.\,. .\,.\,Hm, I think I will let it be so, and then all of the three trailing codes then just have to be combined, from the inputs and the template itself, when submitting a template instance.\,. (15:20)

About the tag types, it could actually be useful if one for instance wanted to have bots filter e.g.\ property tags by a type. So yeah, maybe tag types could be useful at some point.\,. (15:21)

.\,.\,(15:25) Oh, the concatenated string don't have to end in the head ID, do they?\,. They just need to end not in `\#'. And then you always reference the head entity when you reference a concatenated string entity.\,.

%...(15:44) It's too good weather to be indoor and code. I'm going out again. It is actually too warm in here; my hands are sweaty.x) ..Time to use my curtains against the sun again..^^

\ldots\ (18:08) I \emph{will} remove the trailing spec.\ delimiter code and use `$|$' instead as the delimiter instead for templates---and for entity submissions in general. And I'll do the following for submission field. There should first of all be a field with the type, which is not part of the entity submission itself, but results in som immediate upratings of the entity as having that type (coming from the user). .\,.\,Hm, and the user should also see the relevant rating bars afterwards such that they can potentially adjust these uprates (if they want them to not be 10/10).\,. Then there's the main field, for which there are two options, determined by radio buttons: The field can be the entity definition straight up, or it can be a template. If the user chooses `template' as the option, only then will the app query the database for matches while the user is typing in the entity. Whenever the user makes a well-formed placeholder, using `\%$char$' or `\%[$label$]' syntax, there will appear a new field underneath, where the user can then type in an entity to search for.\,. Hm, and maybe there should also be radio buttons for each of the input fields to say whether the entity should be inserted as a link or not---and if the latter option is chosen, the app should not query the database for matches.\,. .\,.\,And that's it. .\,.\,And maybe the user should be forced to choose an existing template, or submit a new one. When templates include links, they template instances will inherit the same links. But if templates are the only way to make entities with links, then this requires template templates, i.e.\ in order to make the templates that include links in the first place. But this is possible; you just have to use double `\%\%' for the will-be placeholders, such that they are not interpreted as such in the template template, only in the (template) instances of said template. This is of course a bit complicated, but luckily, templates with links in them won't really be needed much---and I might even delay implementing them, meaning that submission ia them might fail in the very first version of the app.\,.

Now, this will mean that only `$|$' and `\#' are special characters in the stored format (in terms of the so-called full title). And in the app, the users will not see `\#' as a special character, since concatenation is hidden under the interface. So they'll only see `$|$' as a special character, unless we also count `\%', which is special only in the process of making a template instance. I will therefore just use double characters for all these instances, `\#', `$|$', and `\%' as well, as the way to escape these characters. So users will thus only have to write `$|$$|$' instead of `$|$', unless they use it to point where the spec.\ delimiter should be. And they'll sometimes have to write `\%\%' instead of `\%', but only for templates, i.e.\ whenever `\%' should not be interpreted as part of a placeholder.

When it comes to the trailing link code, I then have to be careful with the syntax that I'm planning on using, since it has to be clear whether the codes works before `$|$$|$' is turned into `$|$' or after. And here I will say: always \emph{before}. In other words, `$|$$|$' should only be turned into `$|$' as the very last thing before it is rendered and shown to the user. (18:32)

.\,.\,This multipurpose submission field can then be used t submit all kinds of entities, which is why it is the only submission field I will implement for the first version of the app.\,.\,:) (18:35)

.\,.\,And for searches, I will only implement direct Entity SK lookups for the first version of the app. So no full-text search, and also no typing in `@' to then get a sub-search for an entity. .\,.\,:) (18:39) .\,.\,And no use of Indexes for the first version.\,.

(22:06) Wait, there's something wrong with my idea for concatenated strings, let's see.\,. .\,.\,Hm, I guess we just \emph{do} need to reference the tail end instead.\,. .\,.\,And I could then make it end in a reference back to the head, but no, let me just implement a query proc that returns all the (up to 10--20) substrings at once.\,.

.\,.\,(22:17) I could remove the `\#' at the end of each substring, but no, it stays. And let me mention that when we implement searching on long, concatenated strings, we should then simply make the strings that end in `\#'---although the users should see in ellipsis instead of this `\#' (followed by a `expand down' button/symbol)---expandable, such that when clicking on them, the app then start querying for more possible continuations.\,. Oh, and this means that we don't need.\,. Oh, never mind, a query procedure to immediately get all (or a lot of) the substrings will still probably be useful for the EntityTitles. (22:22) .\,.\,Oh, and that query proc.\ also goes the other way around than when you search for concatenated entities (where you, in this latter case, start from the head and go towards the tail end).\,.

(08.04.24, 9:12) Oh, let me actually use backslashes to escape special characters still, especially since it won't do that you can't have empty trailing codes.

(11:31) Hm, I need to figure out exactly what to do when links are not of the `e' type.\,. .\,.\,Hm, would is make sense to reintroduce the `@'-references only for the non-regular entity types, such as the user type and the text type (`u' and `t').\,.\,? .\,.\,Hm, and when searching for entities, the users will then just use this syntax directly; only the search matches will have the this syntax translated to a link to the given non-regular entity.\,. Hm.\,. (11:39) .\,.\,Yeah, I think so.\,.

(11:41) Oh, is there any sense in letting this format with all the trailing codes and such be something that's implemented only for the Indexes, and then use a format like one of my earlier ones for the Entities themselves?\,.\,. .\,.\,Then me might want to give the IndexKeys table an ent\_id, but then again, we don't necessarily need to, since the entity SK might still be derivable from the given format.\,. Hm.\,. (11:45) .\,.\,Hm, this might actually be useful, yes.\,.\,!\,.\,. Hm.\,. (11:47) .\,.\,(11:49) Oh, and if I do include the ent\_id, then the capitalization code might not even need to be, 'cause I can then just convert to lower case (and non-accented) for the Index.\,.\,!\,.\,. (11:51) .\,.\,I would then let the specPos be defined by a `$|$' once again, just like how the users would write it when submitting an entity.\,. .\,.\,And I could back to references of the form `@$c$$n$.', where $c = \mathrm e$ is then the only type of reference that gets converted in the Indexes.\,. .\,.\,Hm, this does really make a lot of things a lot simpler.\,. (11:59) .\,.\,It would actually mean that I don't need trailing codes at all; not for the Entities and not for the IndexKeys.\,. .\,.\,(12:04) Oh, maybe I should call it IndexEntries instead.\,. .\,.\,Ooh, or I could also call the table IndexedEntities instead. (12:07)

.\,.\,(12:17) Hm, with this, I'm back to only expecting one instance of an unescaped `$|$'.\,. .\,.\,By the way, this doesn't change anything about the concatenated string; not for the entities nor for the indexes.\,. .\,.\,It also means that I no longer rely on implementing templates, at least not if I then go back to expecting the users the use the entity IDs, which, I guess, I couldn't get around anyway since they need to use them for the user and text (etc.) types, regardless.\,. (12:21) .\,.\,Hm, so should I implement templates.\,. well I certainly need them for the properties still.\,. wait, do I, let's see.\,. .\,.\,Hm, well, no, but I \emph{want} to use templates for properties now.\,. (12:23) .\,.\,So yeah, let me still implement template submissions, namely as a tab in the columns of template-types entities, but for the main submission column, the users can just get one input field, where they write the entity in, well, actually the exact syntax as it is stored in the Entities table.\,. (12:26) .\,.\,Hm, it's also nice that the app will then still function (and I can test it / try it out) without needing for the search field to be implemented.\,. .\,.\,Pretty great.\,.(!\,.\,.) (12:31) .\,.\,(Oh, but it would be quite easy to implement an index that just includes all entities, since all I need to do is really just to use LOWER()---and perhaps use a function to de-accent as well, if there is one at hand.\,.)

.\,.\,Hm, we can keep the same syntax for the templates as I had in mind *(and wrote about) before this realization (that we don't need trailing codes after all when we have the indexes).\,. (12:36)

\ldots (12:49) Oh, but there's also the option now of including template closures, i.e.\ entities of the form `@$n$.$m$\ldots', like I had before.\,. .\,.\,Hm, and maybe it would make sense to have properties be singular nouns again, with specifications that specifies how the are interpreted when appearing in a `$<$property$>$ of $<$entity$>$' clause.\,. (12:54) .\,.\,Yeah, but let me take a walk and think about it\ldots\ 
.\,.\,(12:57) Hm, I think I will go back to doing all that\ldots

\ldots\ Let me disallow numbers for placeholder labels, and then use the `\%$n$', $n=1,\ldots, 9$, and `\%[$n$]', $n\in \mathbb{N}$, syntaxes for back-referencing placeholders.\,. Maybe.\,. (15:38) *(And I might allow references as labels.\,.)

.\,.\,(15:48) Hm, maybe I will go back to a convention of capitalizing things where the `short title' is understood more as a title, rather than as a predicate. So basically almost all nouns, except something like `good acting,' where there's an implicit `has' before this title.\,.

.\,.\,Hm, about the labels, maybe I will also go back to how I parsed the labels from the specification instead.

.\,.\,I should by the way start calling it the `full definition' rather than the `full title'.\,. (16:00)

.\,.\,But yeah, maybe I shouldn't convert the `\%$n$' (and `\%[$n$]') back-references. They can just be stored that way.\,. Sure.\,. (16:04) .\,.\,Ah, I should give each link in an EntityTitle a className with the number $n$ contained in it, such that I can simply render to backreferences as numbers, perhaps like `($n$)', and when hovering over `($n$)', or the original reference further back in the full definition, then both (or rather all) these links with the same className (containing $n$) will get underlined!\,:) (16:08) .\,.\,Nice.\,.\,!

\ldots Hm, should I just capitalize everything, even adjectives and verbs?\,.\,. (16:38)
\ldots I'm a bit torn, but I might indeed do that: capitalize all entity titles.\,. (17:22)

(09.04.24, 9:38) I've thought a bit more about what it is that will bring users to using the SDB; the website and the browser extension. And being able to quickly see a lot of ratings for a given thing that you're browsing is of course one thing. And being able to easily find discussions and critique about the thing is another. But the thing that will really attract users is still the fact that we can make so much better Machine Learning algorithms based on this high-quality semantic data, so that users (anonymously) quickly can find where they are in a parameter space, just answering a few ``questions'' (or rather rating a few things), which can then be used to make so much more precise predictions of what they will like compared to existing technologies. However, this will only be the case once the system gets going a bit. So this grand thing will only be what attracts users if I can somehow make them excited about the promise (and if I can in particular attract other open source programmers). Now, if my mathematical discovery becomes big news, this may be possible, but even so, it will not be an easy task to get so many on board. Of course, I also have my general argument for why the open source community should get working on a Web 3.0 where the users are completely in charge. .\,.\,But still, it will not be an easy task.\,. (9:48) Hm, let me think a bit more about the angle of simply being able to see a lot of diverse ratings immediately for the things that you browse.\,. .\,.\,Hm, perhaps paired with just a simply algorithm to boost users that have an overall high `trust'/`agreeable' score.\,. .\,.\,Hm, and maybe I could also get the ML going at a somewhat early stage, let's see.\,. (9:52)

.\,.\,(9:59) But maybe focusing on the idea of a \emph{user}-driven Web 3.0, where algorithms are also a lot, lot better, due to semantic data, despite them being a lot less intrusive: They will not need to \emph{sniff} for data in order to be useful; the semantic data that the users actively volunteer themselves will be more than enough. (10:02) .\,.

.\,.\,But let me still think some more about what the chances are (and about the possibilities) that I can make an app that is able to attract a small userbase.\,.

.\,.\,Hm, maybe getting those user-to-user ratings at an early stage could actually be key.\,. (10:07) .\,.\,Especially if I also promise to make bots to save these user groups, i.e.\ such that the users are never in risk of losing their individual trust/likedness, nor the user group as a whole that they have built collectively.\,. (10:10) .\,.\,Yeah, this could really be it.\,.\,!\,.\,. (10:11)

.\,.\,Get `user groups' (I have a lot of specific (implicit) connotations when I use this term (see my earlier notes about it)) going as soon as possible!\,.\,.
.\,.\,(10:19) So I should make bots at some fitting intervals to save the user trust/likedness global scores.\,. Well, or maybe each new bots of these are actually based on the previous user group, such that popular users' votes count more in determining the new popular users. And I never have to delete these bots, since they won't require a lot of data *(and requires no maintenance, really, since their job is just to rate the users one time *(well, they might actually as well be active until some deadline, until another bot takes over.\,.)). I could even make their opposites as well, if I wanted to.\,. (but maybe not.\,.) This will prevent spam at an early stage, and more importantly, it will give users good incentive to be actively helping the system by rating things and other users---and commenting/discussing etc. And at a very early stage, I should then also make ML statistics on the user community and publish the a lot of the correlation vectors, together with a bot for each one that up-rates each user in terms of how they fit that correlation vector. .\,.\,These can then further more be combined with the trust/likedness scores to already give some very useful and interesting usergroups, which will give the users a good idea of the potential of this system, and which will give them even more incentive to rate things and be active, since this now also help define their.\,. belonging.\,. (I'm looking for a different word, but you know what I mean.\,.) to each of these interesting user groups. So maybe this is exactly how the userbase can grow from just a few interested users, to a decent amount of users.\,.\,:)\,.\,. (10:29) .\,.\,(Where the first users to join will then have a lot more to say in terms of helping the user groups evolve from there.\,.\,:)) (10:31)

\ldots (10:45) Yeah, early `user groups' (already from the first version) is what can make the thing grow in the first place; all the extra incentive that this gives, which makes the users \emph{want} to rate things and be active, is really paramount.\,. (10:48)

%(20:18) Jeg har tænkt lidt over det, og jeg tror måske jeg er landet på, at jeg endelig (efter fire år, kan man sige) vil give mig selv fri og så søge arbejde. For selvom jeg godt kunne arbejde videre på min SDB app, og selvom at jeg jo godt tror, at den vil kunne vokse sig stor, når først den kommer frem, så tror jeg ikke rigtigt jeg kan klare det uden nogle medarbejdere/partnere. Så det er nok bedst og komme ud og så forhåbentligt skabe et godt netværk. Så kan jeg stadig arbjede i weeknderne/fridagene, men jeg tror måske endda, jeg vil prøve at sætte mig selv for at opprioritere sociale arrangementer, og andre ting sociale ting, og dermed ingen gang arbjede alle mine fridage. Jeg tror bare jeg vil sigte efter, at jeg ikke slipper det helt af arbejdshukommelsen, og at jeg lige for lavet måske lidt hver uge. Men nok ikke mere end det.. ..Jeg tænker lige over det til i morgen tidlig også, men jeg tror altså mulgivis this is it.. ..Jeg kan også mærke, at der er nogen gange, hvor jeg egentligt burde være frisk på at programmere, men hvor motivationen halter (fordi jeg ikke længere har helt den samme (forestillede) gulerod foran mig, som jeg tænkte/følte, at jeg havde for et år siden). Så på den led er det måske så også fornuftigt nok med lidt luftforandring.. ..Det tror jeg, men jeg tænker lige noget mere over det.. (20:28)

(10.04.24, 10:11) Since we just render all substrings of any concatenated string as an InvalidEntityTitle, except the tail end which is interpreted as the full string, it means that re-mapping entities will be an easy matter, as we are always free to remap all references in entity definitions as well. And about the users accepting that their ratings are re-mapped, this can just be the standard setting, i.e.\ to accept all re-mappings greenlit of the central SDB organization, and then they just toggle off this setting if they want.

So for instance, if we suddenly want to remove templates again and substitute them for their filled-in versions, this will not be hard to do---except that the properties will then rely on having the templates, if we go by my current plan, so all these will need to be changed to templates (now without the special backend functionality) instead, but of course, we would only want to remove the (backend-supported) templates if users were already using (front-end supported) templates instead of the singular-noun-properties for constructing property tags.
.\,.\,Anyway, let me stick to templates with the full implementation (i.e.\ with the `@123.124.\ldots' syntax), and rest assured that we can always change it again, if it turns out that only-frontend-supported templates are better.\,. (10:21)

(10:21) About my thoughts mentioned in the source code comments after the last paragraph of yesterday: Yes, I think I will change lanes now and focus more on getting a (regular) work etc. Then I will puase this project somewhat: I will still work a bit on it in some of my free days, but only that, I think. (10:24)
%(10:24) Det er nemligt vigtigere for mig lige nu, mener jeg, at jeg får noget mere arbejds- og arbejdslivserfaring, for et forhåbentligt større netværk, og også at jeg lige for lidt socialt afrustning (og erfaring).. (10:26)

(15:26) I just continued programming a little bit this afternoon. I haven't considered the fact that because we need the template's definition in order to define the type, it means that the inputs can only be queried for after the template definition is fetched.\,. .\,.\,Hm, a simple fix would be to also include the type specifier in the template closure syntax.\,. (15:30) .\,.\,Another option could be to remove the type specifiers from the templates, and instead use numbers.\,. .\,.\,where repeated numbers is then treated like back-references.\,. Hm, and you could require them to be in order as well.\,. .\,.\,Or I could just use `\%e' always.\,. .\,.\,Hm, or I could just repeat the type character in the template closure, as I said.\,. (15:34) .\,.\,Maybe I'll do this, and then also check that the two declarations of the types match.\,. .\,.\,Hm, maybe except for `\%e' exactly, as this could be an inclusive placeholder.\,. Yeah.\,. (15:36)

(15:58) Oh wait, I actually haven't thought this thing of reintroducing fundamental types through. I need to either go back to then either include these types in SemanticInputs as well, or go back to letting all entities be included in the Entities table.\,. Hm, and I guess I could do the latter by sort of using some fundamental templates for entities of the special types; some that doesn't follow the rules of the regular templates, but who are rendered via special React components (not TemplateTitle).\,. (16:02) .\,.\,Hm, but maybe it's more pragmatic to add types to SemanticInputs.\,. (16:05)
.\,.\,(16:08) Oh, this is actually a kinda deep/big question; it actually makes me think of going back to using types again, and in particular the way I had in mind before where an entity can have multiple types, and where the full key to a column therefore consist both of a (nullable) type and the entity ID.\,. Hm, wild that it takes me so long to figure this out.\,. (16:11)

(16:20) .\,.\,Maybe the easiest thing would actually just be to make special template-closure-like syntax for making user and text, etc., entities, and then that's the only way that you can reference these: by referencing the entity with that special syntax. And then all other references will just always be references to the Entities table, including the inputs/placeholders of the templates.\,. .\,.\,(16:26) Hm, now that the def is close to WYSIWYG, and doesn't include the trailing codes, it makes sense to once again end all concatenated strings in a reference to the head substring. And that means that all concatenated string entities will have exactly two unescaped `\#'s in their def. So for template closures and special entity closures, I could just let these start with `\#' instead of `@'. And for all the non-template closures.\,. which are not really closures, actually, since they only contain one ID.\,. the `\#' could just be followed by a (special) type specifier first, instead of directly by a number. I think I might do this.\,. And note that then I can.\,. Oh no, I still cannot split the UserAndBots table just with this.\,. (16:31) .\,.\,Whatever, it's fine to just have an overloaded table like that; the extra one or two bytes per user is really not something to be concerned about.\,. (16:33)

.\,.\,`u\#123' makes more sense than `\#u123'.\,. (16:35)

(11.04.24, 12:51) Okay, I have a problem in the fact that template entities themselves (not talking about their instances) will have `invalid entity titles' as I have it now. (I'm ``procrastinating'' today by programming.) Maybe I should just render non-substituted placeholders and non-active back-references in a special way.\,. (12:55)

\ldots Hm, I can render the placeholder in a special way, but.\,. Hm, maybe it's better to.\,. Oh, maybe I could turn the `\%[1-9]' back-references into an active kind of back-reference, using `@', for template instances.\,. .\,.\,Hm, maybe with a /@n[1-9]/ syntax.\,. (13:16) .\,.\,Sure.\,. Unless I want to remove templates, but I think not.\,. .\,.\,No let me do this.\,. (13:18)

\ldots (13:46) It seems that it might be a very good idea to make sure that my new syntax and all that works well, before I move on to a new phase of my life: Think about if I had paused the programming completely (which might happen if I find (more regular) work and it takes up all my energy) before yesterday and not found all these bugs and problems: That would have made fixing it when I eventually got back a whole lot harder.\,. .\,.\,('Cause if I know my self correctly, I tend to start rethinking everything when I look at an old project with fresh eyes (and this is not what I need right now).\,.)

\ldots\ (16:05) If we at some point want to change Entities to divide it into two columns, the title and the specification, then we can also just do that, as long as either the app--control-server interface doesn't change or at least that the application user interface doesn't change. But alternatively, we could also just supply the control layer and/or the database layer with extra procedures that only returns the title part of the def, e.g. ('Cause right now you need to fetch the whole entity definition, even if you just need the title.) So no worries, if we want to be able to fetch only the title part of the definition, we can just implement an extra proc for this. (16:10)

.\,.\,(16:10) And going back to the question of whether to unpack the templates before storing, here we can also just make a proc that gets all the relevant defs at once. (And there's also always the option of remapping the syntax and use my idea for the trailing link code instead.) (16:12) .\,.\,There's even the option to make this remapping one-to-one by introducing a `was formerly a template instance' syntax.\,. wait, or one could just introduce an `is a template instance' instead, which future Entities might also use. So yeah, all in all, no worries.\,:) (16:15)



(16.04.24, 10:02) I have various things to write about and think more about. And now I just re-realized an idea, which I think I've had before. the idea is that users might want to be able to up-rate all the things/point about a certain thing that make this thing good. And also, under another tab, the things/points that make it bad. These are then up-rated according to importance, I think, and there might thus be a separate rating for how much they affect the `goodness' of the thing. But maybe it can also be done with one rating. Let me think some more.\,. \ldots Maybe the users could add and uprate these points as texts, and then for each text, there could be a property of relevant ratings.\,. And what to call these texts?\,.\,. .\,.\,`Points of praise,' maybe, and then the opposite could be `points of constructive criticism'.\,. .\,.\,Yeah, and I would wants these tabs anyway for thing-entities.\,. .\,.\,So these `points of praise/criticism'-type entities could then have a property, and tab, of `related ratings$|$ for a @[point of praise/criticism], i.e.\ ratings about the thing(s) that the @n1 points out'.\,. (10:34)
\ldots And maybe we don't need the importance rating: maybe the praises/criticisms need only be rated according to the `goodness'/`badness' that they cause.\,. (10:49) .\,.\,(10:55) Hm, maybe a way to do it would be to first have two tabs, one for praise and one for (constructive) criticism, and then rate these properties just for the goodness/badness they cause. Then the next step could be to also introduce a third, mixed tab where all points of praise/critique are rated according to importance rather then `goodness/badness that they cause,' but where the `goodness/badness that they cause' rating can also be fetched and shown for each element in this entity list. (10:58) And what the user community then decides to do / invents from there is up to them.\,:) .\,.
.\,.\,Hm, but I guess all these praise/criticism entities might as well get the supertype `tag' as well.\,. (11:02) .\,.\,No, 'cause the statement is really: `point of praise $x$ causes the entity $y$ to be good'.\,. Hm.\,. (11:04) .\,.\,Hm, which \emph{could} be reformulated as: `(is) good because of point $x$'.\,. (11:05) .\,.\,Hm, that's actually not a bad idea, i.e.\ to use such praise/criticism tag template, and it would also gives a good way to make the praises/criticisms ordered after importance in their respective tabs after all.\,. Hm, nice (it seems).\,. (11:08) .\,.\,Yeah, this is for sure what we ought to do.\,:) (11:09)

.\,.\,Hm, I think this little piece of technology will actually be so important for the app (thinking about the early browser extension especially).\,.\,:) (11:14)

%(11:32) Øv. Det er virkelig en flot bygning, der står i flammer nu.:(

(15:05) I ought to continue this subject in the next section as well, but I'm thinking that thing-entities should have a `points of praise' tab, a `point of constructive criticism' tab, a comment tab (for the normal positive engagement that you tend to see), a corrections tab, and a discussions tab. About discussions.\,. Oh no, I should write those points below.\,. .\,.\,I think that this system will provide a very good user experience of being able to review and discuss what makes various things good and bad, as well as other things. And especially if a make a bot that continuously calculates `user groups' via ML (dealing with correlations), such that the first users can immediately start to be grouped with the users that are most like them.

(15:13) I also have something else that I want to point out, concerning the monetary aspect of the idea. Since this system will very soon start to become a very important way for users to review products/services, media, and more, there will be a giant interest from companies to boost the visibility of their products etc. Now, commercials and biased search results are generally not something that users want. However, if the users can see how much this helps the community monetarily, they might still be willing to accept (as preferences/settings) that the sponsoring companies' products etc.\ have their visibility boosted. For if it is seen by the individual user as an alternative to donating money themselves, it they might very well be on board with that after all. This creates a giant vector for getting monetary towards the contributing `users' (including all open source programmers and inventors, of course). (15:20) The other vectors that I had in mind, i.e.\ attracting sponsors based on increased brand recognition for them, and because it supports a pool of user data about their products, these are still there. But they are probably not nearly as big as allowing companies to get their products boosted in terms of visibility (except for users who has turned this off, perhaps because they are donors themselves). So there we are, a potentially \emph{giant} source of income/funding for the project.\,.\,:) (15:23)

(15:37) I/we should also consider allowing tags where the semantics can depend on the user, i.e.\ such that the tag can.\,. Well, the statement, rather. Okay, maybe not, but I should think some more about this, still.\,.
\ldots (16:22) It could be done with a `tag that I, the rating user, would put on myself' tag (i.e.\ a tag for tags, where the rating tells you how much the user thinks that they fit the given user tag).\,. .\,.\,Or we could say: `predicate that fits me, the rating user'.\,.

.\,.\,Let me also mention: As part of the early app, the users should also already be able to up-rate statements as good ones for determining what type of user the ones that rates these statements are.\,. Hm, we could by the way also do the thing from the last paragraph with rating-user-referencing statements instead, uprated for the tag `True'.\,. Nah, the other idea is better. But it is worth considering this possible redundancy, i.e.\ if we utilize a `true$|$ statement' tag.\,. (16:32) .\,.\,Ah, well, if users just always up-rate tag--instance pairs, when they uprate these `good-for-type-determining statements,' they can just uprate a (`true', $<$text-based statement$>$) pair. (16:39)
.\,.\,(There should then of course be a bot that finds correlation vectors looking at only these statements (with a high enough `good for user-type-determining' score).)

(17.04.24, 9:03) About the praises/criticisms, these should also generally be spoiler-free as well (for movies, books, etc.). At some point we can then make another tab, or a switch to toggle within the initially spoiler-free tab, which then shows the praises/criticisms / review points \emph{with} spoilers.

(10:52) I'm considering a slight ``pivot'' in the sense that I might (with these latest thoughts, including those of the section below as well) try to focus the early app more on ratings, reviews, and discussions---and especially in the browser extension---more than about the `user-driven search/feed algorithm' part.\,. .\,.\,Well, I also need a lot of the other stuff to make it all work well, so I'll also have to implement a lot of that as well. But I could make do with ListGenerators that aren't were complicated, at least.\,. (11:00) .\,.\,But in terms of explaining/``selling'' the idea.\,. .\,.\,Yeah, I can focus more on just the browser extension part (with ratings, reviews (praises/criticisms), and discussions) .\,. (11:02)

(11:46) Hm, I'm looking at the definition of `Subcategory of Entities,' and now I'm thinking, perhaps the documentation should be in the form of (not searchable) texts instead. And come to think of it, we actually \emph{don't} want the documentation to be part of the search.\,. Hm, and maybe the exact `appendix'/`specification' should rather be determined when.\,. submitting an entry to an index.\,. Yeah, so such entries could be submitted instead.\,. .\,.\,(11:57) Hm, I have to make it more simple and clear somehow, I think, how the semantics of the various entities (especially tags and properties/relations).\,.

.\,.\,Hm, I think all tags should have documentations.\,. Then there is an initial documentation, but what the users will see, is whatever is rated highest as the `updated version' of that tag.\,. (12:06)
.\,.\,Hm, and the same could also be quite useful for ``thing entities.'' There could thus be an initial ``documentation'' (specification, rather), but the users will normally see the updated version instead, if there is one.\,. (12:08) .\,.\,For tags, the documentation will given to the template, not the tag instance, though.\,. Hm, but I guess any template instance could just have two documentations: one for the template, and potentially one that's more specific for the instance.\,. (12:10) .\,.\,And what about searchable specifications, should we then just make it so that a searchable string is uprated for each entity, and then the top-rated one can be used for the Index(es)? That could be an idea.\,. (12:12) .\,.\,So each entity just consist of a short title, potentially a template documentation (if being an instance of a template), and then.\,. \emph{potentially} also an individual specification as well.\,. And then users uprate the most fitting searchable string for each entity, which can then be used for the Indexes.\,. Hm, this sounds good.\,. (12:16)

\ldots\ (14:38) I've had several thought, and now I just had some wild additional ones about maybe letting all the defining properties of the entities, including even the title, be semantic.\,.(!\,.\,.) .\,.\,I've had such ideas before, once, but now they actually kinda makes sense again. But let's see.\,. .\,.\,And then you could just uprate the \emph{most} defining / the \emph{defining} ones, which will then be shown under the title and the ID in the Entity column.\,. .\,.\,(This of course requires an assisting bot that uprates what the creator chooses as the defining properties, and which is then generally given much weight.\,.) .\,.\,(14:45) Hm, I could let properties have a special type and/or make a special SemanticInputs table for them.\,.

%..Det er lidt vildt, men der er nu et eller andet ved de her seneste tanker, der tiltaler mig..

.\,.\,It is also easier to just uprate for instance one list of actors, instead of having to creating and maintaining a list by uprating each actor individually.\,. (14:52)

.\,.\,And with this, types can be both defining and semantic properties at the same time.\,.\,!\,.\,. (14:59)

.\,.\,Hm, so there's a fundamental tag of defining.\,. No, a fundamental property of properties.\,. (15:01) .\,.\,And a new entity--property--(rating--)instance SematicInputs table.\,. .\,.\,One which does not need to be optimized for fetching lists of entities (i.e.\ property instances), 'cause this table is meant for quite-constant property instances, which even for lists (like e.g. `actors') are defined only via single entities.\,. (15:07) .\,.\,These `properties' are thus meant only for factual data, or for data that defines entities (for instance such as tags, which are invented by the users, but which nevertheless need some fundamental properties to be settled before they get their intended meaning).\,. (15:11) .\,.\,Oh, which means that the (somewhat) fundamental `properties' relation, should actually be implemented as a normal tag, right?\,.\,. Unless we change it to `defining properties,' meaning that it can be a small list.\,. Yeah, we should probably do that.\,.

\ldots But general properties should be implemented more like the normal (in terms of what has been normal until now) SemanticInputs.\,. (15:26)

.\,.\,(15:32) Well, maybe never mind about the thing of saying that the properties have to be singular. For instance, having multiple types could be a good thing.\,. .\,.\,Well, but you could also make and uprate a (short) list of the types.\,. Hm.\,. .\,.\,And if I make a special List type of entities.\,. (15:34) .\,.\,(So that when a property value is a List, it is always interpreted as meaning that the elements of the list are the property values.\,.)

.\,.\,(15:40) I could also just add an importance rating directly to the Properties (or what we should call it) table.\,. That actually sounds smarter.\,.

.\,.\,(15:45) Maybe be I should just make yet another table such that I have two `Properties' tables: one for the one-to-one properties (including List entities), and one for the one-to-many properties.\,. .\,.\,Nah, I should probably implement the latter via tags, still.\,. (15:49) .\,.\,Yeah.\,. (15:49) .\,.\,And then the documentations of such tags will be a property of theirs. Note also that the `category title' of a tag could be a property as well, which will mean that we can get plural nouns for categories after all.\,. (15:52)

\ldots (16:17) Yeah, so this new Properties table is just for semantic facts and definitions. And the rest is then done in the same way as before, but where the documentation, and everything else, is now automatically updatable with this system. The factual/defining properties does not have a documentation themselves, however, but are just nouns (plural or singular depending on whether one expects a List entity or not).\,. Hm, and their names/titles actually does not need to exist outside of the Properties table.\,. (16:22) .\,.\,A List entity can just be implemented via a TEXT containing a comma-separated list of integers (IDs).\,. .\,.\,Oh, and let's actually not make it an Entity, but let us just make it something that is only used for the DefiningProperties.\,. .\,.\,Oh wait, all property values should just be texts (which are able to include references), so never mind about the List entities; these are just implemented via the Texts/Strings that are the fundamental building blocks of Entities (since they are what is used for all property values (including e.g.\ the title of the entity)). (16:32) .\,.\,`Defining properties' is a good word for it.\,. .\,.\,Oh, and then we could call the normal `properties'.\,. (I wish English said `call $x$ \emph{for} $y$, instead of just `call $x$ $y$'.\,.) .\,.\,call them `relational properties' instead.\,. (16:37) .\,.\,Oh, or even better: `subjective properties'.\,. (16:38) .\,.\,Or simply `ratable properties'.\,. .\,.\,One of those three options, I guess (if not just all at once).\,. (16:40) .\,.\,And simply `qualities' might also be a potential alternative.\,. (16:43)

\ldots\ (18:27) Okay, I can't get rid of the Entity defs if I also want to keep the possibility to search for an entity via its template, such as in e.g.\ my tag--instance statement template.\,. So maybe the def could be as it is now, and thus define the title of the entity, at least until a potential updated version of the title comes along.\,.
.\,.\,So the initial definition doesn't have to be \emph{completely} defining. It just needs to serve as a starting point for the definition, as well as a secondary key for the entity, which can be useful in some instances.\,. (18:37)

.\,.\,This might mean that I can go pretty much back to where I was in terms of the database implementation, I think (maybe), except that I might actually keep the thing about making at least one other SemanticInputs table, namely a entity--property--property-value one.\,. (18:43)

\ldots (18:55) The creating user should choose a title property along with the definition, which can, however, also be generated automatically from the definition.\,.

(19:59) Hm, or maybe it's easier to stick with tags only. In that case, the only change might then just be that Entity definitions, by this new convention, do not have to be very specifying, and that you instead treat the `semantic properties' as part of the definition.\,. .\,.\,Hm, then again, maybe a properties table for the factual/defining properties would be good.\,. I need to think more about this.\,.

\ldots (20:28) Hm, I think I will change it, i.e.\ such that we fundamentally have tag--instance statements as well as owner--property--value statements. I'll then make two tables for this, and for the factual/defining properties, we can just use.\,. .\,.\,Hm, never mind, all properties are entities, and can therefore always be documented at some point. And since the def is still always supposed to contain an initial title (written before the `$|$' still, actually), this means that we can bootstrap the whole thing, whithout having to define the `title' property of the `title' entity via itself, for instance. (20:34) .\,.\,So that might be the only new changes: Two types of fundamental statements instead of only the tag--instance one, and no longer with a convention that an entity's definition has to be completely defining: Any entity can be further defined `semantically' via its properties. (20:36) .\,.\,(Oh, and I will also cut a bit in the def syntax: Back-references will for instance no longer be needed.\,.)

.\,.\,Hm, I could perhaps also make.\,. .\,.\,Hm, I don't know.\,. .\,.\,Uh, maybe I could go back to an old thing of having a special, fundamental kind of template for property tags, and then always unpack these as the owner--property pair before storing in SemanticInputs, i.e.\ meaning that the tags of this special type cannot be used directly in SemanticInputs.\,. Hm.\,. .\,.\,Oh, or I could also just implement special database procs (query and input procs) where the owner--property pair is used instead of the tag, even though SemanticInputs then still only allows for tags.\,! That could be an idea!\,.\,. (20:49) .\,.\,Great.\,.

.\,.\,I should also keep the idea of using List entities sometimes instead of rating a whole list of factual properties one element at a time.\,.

\ldots (21:15) Instead of making a property tag template, let us just use a special syntax for it instead, and why not `@$<$\emph{owner ID}$>$\texttt{->}@$<$\emph{property (name) ID}$>$'?\,. .\,.\,e.g.\ like `\texttt{@123->@124}'. .\,.\,Or just `\texttt{@123->124}'.\,. (21:21) .\,.\,Or `\texttt{@123>124}'.\,. \,.\,Or `\texttt{@123/124}', alternatively.\,.

(18.04.24, 8:32) The syntax for the definition could be like `short title$|$type:type,tag$|$
prop2:something$|$\ldots', oh wait, or I might actually make it standard to use IDs (integers) for the property values, and then use quotation marks when wanting to type the thing in directly.\,. .\,.\,Oh, never mind, it should always just be IDs.\,. (8:38) .\,.\,Hm, and maybe it should also be IDs before the colon?\,.\,. .\,.\,But that would mean that e.g.\ the `type' entity cannot be specified further (or any other of the early entities) in their definition.\,. .\,.\,Hm, so let me actually allow non-ID words, both for properties and for values.\,. Hm, or maybe the parts of this $|$-(vbar-)separated list should just be free, but if any of the parts conform to the `123:124,125,\ldots' syntax, it means that the app knows to automatically uprate these properties as well for the created entity.\,. (8:44) .\,.\,Hm, or let us just make it `@123.:@124.,@125.,\ldots' instead.\,. .\,.\,And let us actually not prioritize implementing this functionality; users can just go to the newly created entity and uprate, first types for it, and subsequently other properties.\,. .\,.\,I should also store the year and month that the entity is created, since this allows the users to interpret the definitions according to the conventions of the given time.\,. (8:51) .\,.\,But yeah, let us start with a convention of writing definitions like `short title$|$type: type, tag$|$prop2: something$|$\ldots' .\,.\,Hm, without the spaces.\,. .\,.\,So: `short title$|$type:type,tag$|$prop2:something$|$\ldots'.\,. .\,.\,And one is then more than free to insert entity references (like `@123.') anywhere here instead of any word.

.\,.\,In terms of list entities, these could be implemented with a syntax like `@123,124,125, \ldots', which would then be rendered as an unordered list (and you could even also implement ordered lists, perhaps by using `;' as the separator instead). But let us actually not implement this for the early app, and instead just uprate each element individually for any one-to-many property.\,. (8:59)

.\,.\,Hm, maybe documentations can also be context-dependent.\,. This would then allow for the same ambiguity that I talked about liking when I removed the the type field/column for Entities.\,. (9:36)

.\,.\,Hm, maybe this is more complicated, than what I had before: Maybe the definition should just be sufficiently defining on its own for the Entities.\,. .\,.\,Hm, it's hard to say.\,. .\,.\,Hm, but I can just still parse the short title from the def always in the early app. That would make it a lot easier, yes. Let me say that.\,. (9:46) .\,.\,And let me also keep the same syntax where there's only one (expected) `$|$' (at most), and where the definition without the `$|$' is then interpreted as the full definition.\,.

\ldots Oh, we could actually just make a list (instance list) query proc where the secondary key, i.e.\ the def, is given instead of the ID. This allows this to be used for other tag templates as well, not just the property tag template.\,:) (10:06)

.\,.\,I will still make the special syntax for the property tags, rather than defining a template like I used to; that idea is really good, I think.\,:)

.\,.\,So maybe I should keep the definition syntax the same as it is now, even with the back-references, as long as I just either write them out or insert a `($n$)' after the original, referenced link.\,. (10:13) .\,.\,And then we'll just say that these definitions, including the (short) title, are not as set in stone, but if users uprate replacements for them highly enough, the app will show these replacements to the users (depending on their preferences) instead. (10:19)

(11:22) Maybe, I will remove the `@n[1-9]' back-references, but not the `\%[1-9]' back-reference placeholders for the templates, which are then just replaced with the given entity reference (like `@123.') instead.

(11:55) Hm, or maybe I actually will just use the property tag template, like I have done, instead of defining a special syntax for property tags.\,.

(12:00) Hm, maybe it would be a god idea to make a sharper line between tag entities and non-tag entities.\,. .\,.\,Hm, this made me consider introducing fundamental types again, but where these then are \emph{not} meant for the custom types.\,. Hm, or will it work with this high level of ambiguity.\,.\,?\,.\,.
.\,.\,It could also just be a leading character of the definition.\,. .\,.\,Perhaps followed by an initial `$|$' for clarity.\,. (12:06) .\,.\,Hm, 'cause it also doesn't make sense to use non-tag entities, like `LotR' or `WWII' as tags anyway, as it's way better to use more clearly defined properties instead. .\,.\,I should not make a separate `type' type (fundamental type/super-type), though, as this should just be a subtype of the `tag' (super-)type.\,. (12:10) .\,.\,`fundamental type' is better than `supertype'.\,. .\,.\,(12:14) Let me just make it a separate CHAR field/column in the Entities table, which are then just also a part of the secondary key. I will not implement typed tags and/or instance lists, so SemanticInputs need not be changed. But I can make a restriction of not allowing semantic inputs where the tag is not of the `tag' type. .\,.\,For e.g.\ 'user' or `text' entities, their defs will then just be an integer alone; no other surrounding syntax needed.\,. (12:18) .\,.\,I will also let `property' be a fundamental type, then.\,. (12:20) .\,.\,(12:25) Maybe I will call the standard/regular entities for (exactly) `regular entities,' and use the character `r' internally to denote this fundamental type.\,. .\,.\,Well, maybe `standard' is actually more fitting.\,. Sure.\,. .\,.\,(And then I'll just use `s' instead---which can also be thought of as standing for just `something').\,.

\ldots Hm, do I want a fundamental statement type?\,.\,. (12:44)

.\,.\,(12:45) Oh wait, if templates have to be able to.\,. determine the fundamental type.\,. well, I guess that could just be done with an initial character of the template.\,.
.\,.\,Hm, but why go to all this trouble of making it a separate field/column, when it might just as well simply be the first character, always, of the def.\,. .\,.\,Yeah, let me go back to just having the def as the, well, the full \emph{definition} of the entity.\,. (12:49) .\,.\,And let me just still keep the `$|$' as the mandatory second character for clarity's/visibility's sake.\,. (12:53) .\,.\,No, make it a `.' instead.\,. (12:54) .\,.\,Hm, or a `/', actually.\,. (12:56) .\,.\,Hm, but/and couldn't we just use single characters for the fundamental (reserved) types, but then also allow custom types.\,.\,? .\,.\,(13:01) Yes, I think this is a very good idea. It's a very good thing to have the overall type defined first of all in the definition (for better understanding). And since we won't really be searching for entities via their definitions, but via uprated index keys instead, we shouldn't worry about having something coming before the (short) title.\,. So yeah, let's probably do this. We could even also use `:' instead, but then allow for several types, separated by `/'s, such as in e.g.\ `Tag/Type:Entity'.\,. (13:05)

.\,.\,Okay, let me drop using `$|$' to cut out the title from the full definition. Let me then instead write whole sentences for the `specification,' with capitalization, of course, and I think I might then go back to using only lowercase for tags in general.\,. Hm.\,. .\,.\,Yeah, lowercase in general for tags, types, and properties, and all such meta-entities, except of course for proper nouns, and then for the `standard' (/`regular') entities, we might actually choose to capitalize as a standard, I'm not sure.\,. (13:18)

.\,.\,I'll probably just write `tag:' instead of `tag/type:' for all types.\,. .\,.\,And let me just separate the type declaration from the title with a `$|$' instead of a `:'. (13:22)

.\,.\,Great.\,.

\ldots\ (15:42) I will just use a list of integers for the type declarations (and bootstrap the types of the `tag' and `type' entities). I think I will separate them with `/' since this is actually how I intend them to be rendered, which means that I will also propose always having it so that each new type in such lists is always a subtype of the last one. Then when users create new entities, I \emph{will} make it so that these types are uprated by the user automatically. But I will also make the app cut corners and always add the types from the type declaration to the list of types, as well as any potential additional ones which are uprated highly enough as types for the entity. (15:46) .\,.\,Oh, and I \emph{will} thus use `tag/type' as the declared type for my types. And never mind about the reserved letters again, as we'll just use the IDs, as said. (15:47)

.\,.\,By the way, `questions' should also by a standard tab for the `standard' entities, which should then of course lead to (the `question' property of) `answers' as well.\,. (15:49)

(16:43) Hm, now that templates all have `4$|$' in front, we can just make `\%' a special character only for these, and not for any other entity. We can also treat only this first `$|$' as a special character for the templates, and just render all the rest of the template verbatim.\,. well, except for constant references.

(16:54) Hm, now that we don't need the definitions to be searchable, I might think about doing something else for the concatenated strings.\,. Maybe I'll even.\,. Hm, maybe I'll even just add a TEXT to entities.\,. .\,.\,Hm, I wonder if it could make sense to just make a manual check on the TEXT column, then, in order to prevent duplicates.\,. .\,.\,(17:00) But then again, one could just make a standard `documentation'/`full specification' after the specification part of the definition, and then require that this is always a text entity.\,. .\,.\,Hm, one could by the way uses hashes instead of BIGINTs for text IDs, if one really wanted to not have any duplicates.\,. .\,.\,Hm, that might not actually be a completely bad idea, as it also has the added effect of making texts and binaries searchable.\,.\,!\,.\,. (17:06) .\,.\,And I could make an input proc that just inserts/finds the given specification text automatically, as part of inserting the entity.\,:)\,.\,. (17:09)

.\,.\,(And of course, for the Indexes, I can still use the same concatenated strings as I had in mind before (those that are fully searchable).) Hm, this might actually not be a bad idea, about the hashes.\,. Hm, but I can probably make it even better, let's see.\,.
.\,.\,Hm, or maybe no; maybe the full specification / full definition / documentation / specification should just always be a text entity, which are then referenced by its hash (that serves as its primary key).\,. (Note that all texts also has a BIGINT ID as well, which comes when the `@t$<$\emph{hash}$>$' entity is created as part of inserting the text.) .\,.\,Hm, which means that one can actually also just use that ID for the def, instead of using the hash directly.\,.

.\,.\,Hm.\,. Oh, I can still bootstrap the first few entities, simply by adjusting the specification ID afterwards to the correct text entity ID.\,. (17:26)

(17:40) The Texts and Binaries tables can store their hashes, but I might do as https://
dev.mysql.com/doc/refman/8.0/en/optimize-blob.html
says and also do manual comparisons on top of that. And I can then also just use BIGINT UNSIGNED AUTO\_INCRE-MENT for the primary keys / IDs of these tables.\,.

(17:48) And let me just repeat, the specification/documentation, as well as the title and the type declaration, are never set in stone. They should preferable be pretty precise and well-formed, but they can always be ``replaced'' by the users, namely by them uprating replacements highly enough (in which case the app, depending on the individual user's preferences, can fetch and render those instead of the original ones). .\,.\,(And the types in the type declaration is not even meant to be an exhaustive list to begin with, not at all. The users are instead expected to uprate any other types `semantically' for the given entity---and might also at some point in the future be able to downrate the ones from the declaration, although this might also be achieved instead by uprating a replacement for the type declaration.\,.) (17:53)

(18:02) Oh, and let me then remove back-references completely, and let us for templates just use a general specification for all template instances at once, where the specification then uses some variables/pronouns/placeholders instead of back-references. This then means that the semantics of a template instance's specification is only fully understood once you also have the actual title part as a reference for what should be inserted in the place of the pronouns/placeholders. And.\,. Oh, and users could also.\,. No, let us just say that if the specification of a template instance is not enough to fully understand it, then you can.\,. Well, maybe users could uprate amendments to the general specification rather than replacing it in such instances, actually. (\emph{Or} they can always also just vote to replace it outright.\,.) (18:09) .\,.\,But generally it's better, like I do for the properties, to just make the input entities define how they should be interpreted in the context of the given template instead.\,. (18:11)

(19.04.24, 9:27) There are some things left to consider, so I've been thinking this morning. And now I just had a kinda wild idea: What if all factual and/or defining properties are uprated for an entity, not via a property tag for each property, but instead as whole batches of data in the form of XML documents. And when looking up factual data, you just look through all the high-enough-rated XML data documents in order until you find the first match for the property that you are looking for.\,.\,?\,!\,.\,. .\,.\,Well, or you could look through all and take all the property values that you can find, and serve them up as the ``instance list''.\,. Hm.\,.
.\,.\,(9:36) Hm, could there be a good way to combine this method with the one of rating each elements individually, more specifically such that we can combine two such different lists?\,.\,. .\,.\,If we for instance say that the elements from the XML document just automatically adopts the combined rating of the whole XML document as the individual rating.\,.\,? .\,.\,Hm, that sounds like quite a good idea.\,. (9:40)

.\,.\,Hm, maybe I will then use a special syntax for the property tags after all, since this would make `properties' something even more fundamental of the system.\,. (9:43)

.\,.\,Hm, and then we could perhaps just use XML TEXTs for the entity defs, making sure to also store the hash of this TEXT def in the Entities table as well, and to use it in order to check against existing duplicates before inserting any new entity.\,. (9:48) .\,.\,The stored def will then be the original one (by the creating user), and all subsequent ones will just be uprated as `property documents' for the entity. Then the original def will then just automatically start with a high rating.\,. (9:51)

.\,.\,Hm, but the Entities table should also store an original (short) title, which is the first thing that is typically fetched for an entity, and often times the only thing.\,. (9:54)

.\,.\,Hm, if this can be made to work, I might not need templates at all then.\,. (.\,.\,!\,.\,.) (9:56)

\ldots `Property document' will then of course be a reserved property that is not supposed to appear within a property document.\,. (10:14)

.\,.\,(10:19) The fact that the original property document is uprated as such as well of course means that it has to be an entity, which means the the Entities table should just have the ID of the text entity for its prop doc column.\,.

.\,.\,(10:22) Hm, alternatively to storing the short title, the app could also perhaps just query an Index for it instead.\,. Hm.\,. .\,.\,Well, might as well query for the top-rated instance of an instance list (in SemanticInputs), then. And then one could start out by using a bot that just always uprates the original title.\,. .\,.\,And one could then soon migrate to a bot that updates this if another property document with a different (short) title takes the lead (decided by some mean bot).\,. (10:26)

.\,.\,And let us underline: Property documents are meant for factual and/or defining properties only.\,. .\,.\,(Not for the more subjective properties, such as e.g.\ `related entities' or `subcategories,' etc.)

.\,.\,I'm by the way also considering going back to using `\#' instead of `@' for the references.\,. Oh, unless I want two kinds: one that is often expanded, and one that always stays as an ID.\,. Hm, maybe not.\,. (10:33)

(10:42) When people write texts within an XML tag, when they edit a new entity for upload, I.\,. Well, let me actually think, 'cause the typical inputs ought to be a list of entity IDs, right?\,.\,. .\,.\,Hm, and maybe they should be able to escape this when editing and write texts directly, where these will then just be stored separately, and the text content will then be replaced in the stored property document by an entity reference instead.\,. .\,.\,My point about these texts is that apart from redundant whitespace, the text should just be saved as is, with exactly as many newlines as the user types in between the given XML start and end tag. And then we can always decide on a standard format afterwards. Of course, any single newline should just be rendered as a space.\,. wait, no. I don't need that. The editor can just use wraparound, and then we'll let the users write there UTF-8 text almost verbatim, except for some special characters that needs escaping, for instance `$<$' and `$>$'.\,. (10:50) .\,.\,Well, and they can even get to write it completely verbatim if we implement an `add property' button to the form field, and make it so that each property is edited separately, and where the property name and the content for each property is also edited separately.\,. (10:53)
*(Oh, except for the entity references, which should not be verbatim, I think, but should be replaced with a link when the user has typed it in.\,. (12:40) .\,.\,Well, at least at some point when we implement this.\,.)

.\,.\,Hm, maybe I'll use JSON, then, instead of XML.\,. (10:54) .\,.\,Sure, that's easier/better, then.\,. .\,.\,And since we should not have nested tags anyway, JSON will also be at least as readable as XML. I will probably transform the property documents a bit, though, before rendering them for the user, but even if I do, the resulting format would then be more like JSON anyway, i.e.\ with a title for each property, followed by the content. (10:57) .\,.\,Yeah, and I will probably just render the JSON text verbatim for the first version of the app.\,. (10:59)

.\,.\,Hm, I might really make this change.\,. (10:59)

(12:03) I guess this will require text entities to be a separate kind of entity. .\,.\,So we could say that there are `data entities' and `semantic entities,' and also `user/bot entities' if these are not also considered `data entities'.\,. .\,.\,Okay, so I should probably let there be four fundamental types of entities, then: Texts, Binaries, Users (which includes both actual users and bots), and Semantic Entities.\,. (12:06) .\,.\,Hm, the user\_id of SemanticInputs (.\,.\,hm, which could by the way also be called something like RatedInstances/RatedEntities.\,.) will then always be of the User type, the tag\_id will always be of the Semantic type, but the inst\_id would need a CHAR/TINYINT to denote its type.\,. .\,.\,(I could use CHAR, and let the four characters be `t', `b', `u', and `s'.\,.) (12:11) .\,.\,Hm, but it would be smart, if I could let tags define their fundamental type instead. I could still.\,. well, I \emph{might} still store the (fundamental) type character in SemanticInputs, but I \emph{could} also just let this be defined by the tag alone.\,. .\,.\,Hm, let me include the type character, at least for the early version. It's also alright that this technically opens up for the possibility of type-overloaded tags, even though you'd still have to retrieve each instance list separately for each fundamental type. Okay, let me just include it, indeed.\,. (12:17)

.\,.\,Hm, this is a bit bothersome, so couldn't I just collect all these four types into one Entities table?\,.\,.
.\,.\,Hm, I'll do that somehow, yes, and one way to do it might just be to let Entities$\to$SemanticEntities, and then create a new Entities table that just hold a type character and an ID.\,. (And then I could make an Entity query proc that instead of fetching the type and ID, fetches the type and the data of the given.\,. sub-entity-table.\,.) (12:31)
.\,.\,The IDs of all these four sub-tables / helper tables can then be private to the database, not being visible above *(/in) the database (query/input proc) API.\,. (12:35)

\ldots\ (15:15) Okay, I had some more changes/corrections to these recent ideas, but now I also just realized, silly me, I forgot about how important it is that we can search for entities via a secondary key. So let me think about whether the ideas are salvageable or not.\,. .\,.\,(15:18) Hm, maybe if we somehow made `tag' another fundamental, and searchable, type.\,.

.\,.\,I by the way also decided that I would scrap the `property document' property, and then stick to uprating instances individually. And since this is the case anyway, I might go back to the defining the entities via a def, using a syntax like what I had just before these `property document' ideas from today.\,. (15:22)

.\,.\,Hm, but maybe the idea about making a searchable fundamental type for tags especially might also work, let's see.\,. (15:27) .\,.\,Hm, and maybe that's just it: Tags are.\,. Hm no.\,. .\,.\,Hm, how about property tags especially, then?\,.\,. .\,.\,Hm, or just having a WordEntities and SemanticEntities, where WordEntities are considered more abstract and more constant.\,. Hm, `ConstantEntities'.\,. .\,.\,Or AbstractEntities.\,. Hm.\,. (15:31) .\,.\,Hm, `abstract' and `specific' entities.\,. no, maybe not.\,. (15:33)

.\,.\,(15:34) Ooh, how about using the `as a' clauses for property documents / documentations?!\,.\,. .\,.\,So that the properties are made dependent on the type that the title is interpreted as.\,.\,!\,.\,. Hm, but then we're also back to having type-specifying tags and all that.\,. Hm, I should think about all this.\,. I feel like it could work fine with the def-defined entities that I had before the ideas of today, so I should keep that in mind.\,. (15:38) .\,.\,Hm, yeah, maybe I should go back to that.\,.

.\,.\,(15:47) Wait, back up to the `how about property tags especially, then?' idea.\,. .\,.\,Hm, by having a fundamental type that stores the owner entity's ID and the property entity's ID.\,. .\,.\,Oh, this might actually work.\,.\,!\,.\,. (15:51) .\,.\,Ah, or maybe we could even make it more general by going back to having CompoundEntities, where the `function' is then also stored.\,. (15:53) .\,.\,Ah, like templates, except that the input is not substituted in a text: Instead the function simply has a documentation that explains the semantics of the closures.\,. Well, and it should also define the (short) title.\,. (15:55) .\,.\,(15:56) Ooh, and maybe that title template could then simply be a (`semantic') property of the function.\,. .\,.\,Hm, I like this; it seems like a good idea.\,.(!) .\,.\,Hm, for this means that now I could in principle go all in on the JSON definitions, but still have searchable ``template instances'' (which are now actually `function closures/values').\,. (16:00) .\,.\,`Functional/compound entities'.\,.

.\,.\,On my walk a little while ago, I also thought that since JSON strings can also just be strings, we might also still have entities that are just defined by their title alone, without having to specify that the string is a value of the `title' property. And this would help making bootstrapping the semantics easier and more clear.\,. (16:04)

.\,.\,I also thought (since I'm going away from the thing about uprating property documents) that maybe the entity documentations/definitions don't have to be searchable, and maybe this could be the case.\,. .\,.\,Or I could indeed make a fundamental `word' type of entities, that are defined simply by searchable strings, rather than by documents.\,. .\,.\,(16:10) Hm, if I call it `derived entity' I could also group the `word' and the `functional' entities together.\,. .\,.\,Hm, `derived' and `semantic'/`standard' entities.\,. (16:12)
.\,.\,Hm, but I might also make separate types for the `word' and the `functional' entities.\,. .\,.\,Sure, that actually seems like the right thing to do.\,. (16:15)

(17:11) Maybe never mind about the `word' entities.\,. \ldots (18:09) Except they could perhaps be handy for properties, but I'm not completely sure what to do there.\,. .\,.\,Hm, maybe we should just rely on users selecting properties from a list (an instance list, i.e.), or at least until I implement the Indexes as well---which I of course could just make sure to do from the start.\,. .\,.\,Hm, yeah.\,. (18:15) .\,.\,And let me also just give `semantic entities' an (original) `title' column, by the way, instead of having to always define that via the JSON document. Hm, I might also give them a `types' column then, for good measure.\,. .\,.\,And with this, the `property document' column can be essentially nullbable, and I could make a unique index on these three columns.\,. (18:20) .\,.\,Hm, this then actually allows me to also store it as my previous three-part def varchar.\,. (But I could also store it as just three separate columns.\,.) (18:23) .\,.\,Hm, maybe it's cleaner to just use the one `property document' alone.\,.

.\,.\,Hm, if I make the procedure to take a property document and uprate the relevant tag--instance relations a private helper procedure, maybe I can then use it to bootstrap the first required entities a little easier.\,. Oh, well.\,. .\,.\,Oh, that \emph{will} actually be the easiest thing to do, so yes.\,. (18:31)

.\,.\,(18:32) Oh, there is a problem: We cannot have property documents that only use IDs instead of words---unless we for instance make the title its own, automatically understood column. So let me indeed do the thing about making both the type declarations and the title definition (i.e.\ the original ones) their own columns, next to the.\,. `other properties document'.\,. And let me indeed make that unique index, then.\,. (18:35)

.\,.\,Oh, and since the (other) property(ies) document will only consist of IDs, apart from boilerplate syntax, I will not make it JSON after all, since this would mean a lot of redundant `"'s.\,. .\,.\,(18:42) Yeah, let me just store it like `123:124,125.223:224.323:324,325,326', or maybe use newline characters instead of `.' And if we would like to convert it to JSON, then that would be a simple matter. .\,.\,(Just wrap it in `\{\}', insert a `"' after the first `\{' and after each newline, and then also change each `:' to `":[', and insert a `]' before each newline and before the last `\}'.\,.)

.\,.\,Hm, should I then store each prop doc as a stand-alone text entity, then, and store its entity ID in the prop doc column?\,.\,. I guess so.\,. .\,.\,And maybe I'll also keep the data\_hash column for texts and binaries, by the way, at least for now.\,. (18:55)

.\,.\,Oh, shouldn't I also make the `types' and the `title' columns essentially nullable for.\,. Hm, for entities with long titles, but no, maybe I should just change `title' to `short title' instead, and thereby cement that it might not be the full title.\,. .\,.\,Yeah.\,. (18:58)

.\,.\,Hm, let me reconsider having the `word' entities as well, which can be used especially for factual/defining properties.\,. .\,.\,Yeah, maybe I should do that after all.\,. (19:03) .\,.\,And then I might use actual JSON, and allow for non-integer strings as property names.\,. (19:04) .\,.\,Wait no, I can just use the semantic/standard entities where the prop doc is then null (or rather 0).\,. (19:06) .\,.\,Yeah, and then still just try to make it a convention not to use entities with non-null (non-0) prop docs for the factual/defining properties, such as e.g.\ `short title,' `title,' `description,' etc. (19:08) .\,.\,Oh, and I mean no to the thing about introducing `word' entities, not about using actual JSON. I'll still do that, then.\,.

.\,.\,Well, maybe it doesn't need to be a convention after all: Maybe we should just leave that up to the users. But I \emph{will} use such simple (without any `other properties document'/`prop doc') entities for the basic properties such as `short title,' `title,' `description,' etc. 'Cause how else do we bootstrap the semantics?\,.\,:) (19:13)

(20:35) Oh, if I use JSON exactly, then the IDs also have to be wrapped in quotation marks for the property values as well.\,. .\,.\,('Cause otherwise JS interprets it as 32-bit integers.)

(20.04.24, 9:15) Let me just not care about repeated texts for text entities (and likewise for binaries), at least for the early versions of the system.\,.

.\,.\,(9:19) I think `standard entities' is more clear than `semantic entities'.\,.

(9:36) Hm, to make the fundamental type part of the entity key or not to make the fundamental type part of the entity key.\,. .\,.\,(9:39) Hm, and/or to make the data key public or not.\,. .\,.\,About the former, I really don't want to add even a single byte more to SemanticInputs.\,.

.\,.\,Oh, I guess if we just make an overloaded Entities table, that would beat all (I think) of the other options I had in mind, at least in terms of efficiency.\,. And I could of course even overload some of the columns to make it.\,. well, slightly more efficient, but I guess not.\,. (9:47)
.\,.\,Oh, this might not work with the unique/secondary indexes.\,.
.\,.\,Hm, if I made the fun\_id in the FunctionalEntityData table into a string instead, and possible combined it with the input\_list, it could work.\,. .\,.\,Hm, but no need to combine it.\,. .\,.\,Hm, but I will lose too much clarity with this, so no, maybe I should keep the data tables separate.\,. (9:56)

.\,.\,Hm, but then again, what if I simply went back to the def I had before.\,.\,? .\,.\,But instead of an initial type, we just write an initial \emph{fundamental} type (which is \emph{not} the same, as two entities with different fundamental types can have the exact same types in principle---they can even refer to the same thing and thus be duplicates!\,.) in the front, and only continue with the.\,. \emph{semantic} types if `s' is the character.\,. (10:01) .\,.\,Hm, let me maybe call the `fundamental types' `meta types' instead, to make this (important) distinction more clear.\,.
.\,.\,Hm, but this doesn't work for the text or binary types, so no.\,.

.\,.\,Okay, so I'll keep doing what I'm doing now (having a global Entities table with just a (meta) type\_char and a data\_key, which is a foreign key to one of the `data tables').\,. (10:08)

(11:51) Hm, I just added a user\_type column to UsersAndBotData, i.e.\ a CHAR of either `u' or `b', but now I'm thinking, why don't we just use std entities for bots?\,.\,. Hm, and this makes me think.\,. Nah, let's still use the user ID, not the user `data key' in SemanticInputs.\,. .\,.\,(11:57) Hm, and let's keep bots as a user type, since.\,. Ah.\,. Yes, since it's good to have the bot data be controlled by the SDB, not the users. But I could divide them up into two tables now. I'll probably du that.\,. .\,.\,So I guess I'll use `a' for (aggregation) bots like I used to.\,.

(12:41) Oh, maybe the short title should be a constant, unlike any of the other properties.\,. Hm, or else I do need to include `word entities'\ldots

\ldots\ (14:21) Okay, I will indeed introduce the `word entities,' but I think I'll call then `vocabulary entities' instead. And these are supposed to be used for basic, defining/factual properties. For it doesn't make sense to insist of using a property-typed entity for this: We already know that it's a property when it appears as a property name.

Now, I \emph{could} then remove type\_list and sh(ort)\_title from the std entities table, but I might actually keep the short title, as it allows this initial/original short title to be served up after one query request instead of two. And I don't think it's necessary (and it would also be hard) for the functional entities, since functions are generally likely to be cached by the browser most of the time anyway. .\,.\,:)\,.\,. (14:27) .\,.\,(14:30) Ooh, and then we could also say that the short title is only replaced by something else, not if anything is simply rated higher as the short title, but if this is the case \emph{and} the rating value also exceeds some threshold.\,!\,:) (14:32) .\,.\,Great.\,.

Let me also mention that vocabulary entities are not really meant to be further defined (by their properties): There interpretation should be based on the word/title alone. (And if this title is abstract and/or ambiguous, then the semantics of the entity is simply \emph{supposed} to be equally ambiguous.) (14:35) .\,.\,This is not to say that users \emph{can't} add explanatory properties, as well as any other properties of course, but they are only \emph{meant} to add explanatory properties at most, not \emph{defining} ones. .\,.\,This is opposed to the standard entities and the functional entities whose semantics does not have to be nearly as constant (note that the semantics of the vocabulary entities is thus meant to be almost constant, only perhaps changing if the underlying natural language changes) as the vocabulary entities. Users are thus free to specify standard and functional entities further at any point \emph{after} they have been defined, by uprating different properties for them (although the original definition is still kept, and is supposed to always be honored as much as possible, i.e.\ such that specifying a std or functional entity is okay, but changing it is not okay, really).

.\,.\,Okay.\,:) (14:42) .\,.\,Oh, and maybe I should lose the `short,' and instead just use a convention of specifying it \emph{when} we're talking about a full title, and let it be implicitly understood that `title' normally simply refers to the thing that the entity is commonly called. Okay.\,:) (14:45)

(14:50) Hm, I'm considering simply joining StandardEntities and VocabularyEntities, saying that the former is equal to the latter when doc\_id = 0, but maybe not.\,. .\,.\,'Cause the app is not supposed to query at all for a title replacement for vocabulary entities.\,. Hm.\,. (14:53)
.\,.\,But you could also just say that it's not supposed to do so when doc\_id = 0.\,. .\,.\,Hm, I think that it might be a good thing to separate the two things more clearly. For again, vocabulary entities are not supposed to be specified (only explained at most).\,. (14:56)
.\,.\,Hm, and maybe you can also say that vocabulary entities are meant to generally be interpreted as the words themselves, not the thing that they are referring to, unless they are used in relations (commonly property relations, where they are thus used either as.\,. Hm.\,.).\,. Hm, should we then use vocab entities as property values---I guess we have to.\,. Hm.\,. .\,.\,Oh no, we maybe don't \emph{have} to.\,. (15:03) .\,.\,Hm, but maybe vocabulary entities just should generally refer to the thing, rather than the word itself, and when you want to define word entities, you could make them std entities.\,. maybe.\,. Hm.\,. (15:06) .\,.\,Hm, I guess so.\,. .\,.\,(15:09) Oh, how about simply removing VocabularyEntities again, and just make it a convention not to.\,. Yeah, maybe we should generally not specify entities beyond their original definition, but keep them.\,. deliberately vague/ambiguous.\,. Hm, I don't know.\,. (15:11) .\,.\,Yes, actually, I think so. If an entity can refer to a range of things, we keep it that way: General things are kept general (abstract things are kept abstract). Okay, so do I remove the vocab entities.\,. I guess so.\,. .\,.\,Yes.\,. (15:14)

\ldots\ (16:38) Hm, I'd like to make the prop doc a TEXT column rather than an entity ID (ulong) column, but then I do have to make the vocabulary entity / word entity table after all, since these still need to be searchable.\,. .\,.\,Let me do that, and maybe I'll go back to `word entity'.\,.

.\,.\,Hm, maybe I'll start calling them simple entities (instead of word entities) and prop-erty-defined entities, and just `defined entities' for short.\,. (16:48)

(18:47) It might indeed be a good idea to introduce ML bots early on, but another thing that I ought to do as well (perhaps even more so) is to introduce my `simple user groups,' as I believed I called them. These are then implemented first via the users uprating a pair consisting of a user/bot and then a tag, which is meant to have the instance type of `(actual) user' Then if such a pair gets highly enough uprated by all the users (and later on you could implement so that selected user groups are given more power to create new user groups), then the SDB first of all creates a bot that distributes a kind of tokens to the users rated highly enough as instances of the given tag (second element of the mentioned pair), by the given user (first element of the mentioned pair), such that the amount of tokens is at least kinda proportional to the rating. Then I should find a way to define how these users can then transfer said `tokens' to other users.\,. Oh, of course that just has to be another special tag, which is uprated as well along with the pair, making it a triple. And this allows users to take back their tokens, also. Then the bot continuously calculates each users effective amount of `tokens,' either in a way where givers of tokens lose some tokens as well, making the `token' metaphor more fitting, but perhaps even better where the original users of the `group' keeps the same voting power, and just has a constant amount of power that they can distribute to others.\,. well, or maybe the first option---one or the other. I'll figure that out. Okay, and finally, there's then a bot (maybe the same, if one will) that is essentially a `mean bot,' but where the mean/average is weighted according to the `token' distribution (and users with no such `tokens' are ignored by this bot). I think this could be a very good idea, to introduce these `simple user groups,' as I've called them, early on.\,.\,\texttt{:D} (19:02)

.\,.\,(19:05) Ooh, and maybe I'll start by introducing the following simple user group myself, which is the (mean bot, trust, trust) triple, which would thus.\,. Well, it has to then be a fluid one, instead of one where the initial group of users (`moderators' of the group, essentially) is constant. So I guess there could also be `dynamic simple user groups,' where.\,. Hm, where it is also a triple, but where the ``initial'' group of `moderators' are then allowed to vary as well. Ah, and I could also simply put a deadline on when the `moderator group' will stop varying and become constant, which I recall was also what I had in mind last time I thought about these `simple user groups.' So yeah, let's add that to the description: The `moderator group' for a `simple user group' can be allowed to vary, at least until some deadline is met. And about the (mean bot, trust, trust) simple user group, yes, I think that might actually be useful, since it would give a ``trust-squared'' user group. And since I'll set a deadline (which might be postponed if I (/ the community) see(s) fit), its usefulness will increase all the more, since it will make it a good guard against if trolls appear at some point after the early stage.\,.\,:) (19:14) .\,.\,A `trust of trusted users' user group.\,.

.\,.\,(19:18) I think the latter of the two options will create a better dynamic, i.e.\ where it doesn't ``cost'' anything for the moderators to distribute their tokens, and where the moderators thus retain a special status in the group.\,. (But we can of course always also just implement user groups with the former of those two options at some point, if somebody want those kinds of user groups as well.)

.\,.\,Oh, and `trust' is a little too vague: I think it should more precisely be a tag of `I think I respect the inputs of this user---and perhaps I also agree with the inputs on some level---and I think that the future contributions of that user is likely (according to my rating) to be constructive and valuable'.\,. Well, maybe we can just use the last part: `I believe.\,. no, `I think that the contributions from this user have been valuable so far (to the extend that my rating value tells), and I also (to the extend \ldots) believe that the future contributions of this user is likely to be valuable.' Yeah, and we can maybe shorten this down to just speak of the `contributions' overall, and then it's just implicit that it talks about both the future and the past.\,. wait, what am I saying, there's no need to shorten anything down now that tags can have whole ass TEXT descriptions to them, so never mind.\,\texttt{:D}\textasciicircum\textasciicircum\ (19:29)

(21.04.24, 8:59) Hm, would it make sense to make another text type where entity references are meant to be substituted if the SDB ever remaps its IDs (which it might if working together with other SDBs (other `nodes')).\,.\,? .\,.\,Hm, no maybe texts should just never be verbatim, and thus always have to escape the `\#'s (or the `@'s depending on what I go forward with) etc.\,. .\,.\,So whenever a user uploads a texts from a file.\,. they have to choose whether to interpret entity references as such, or to interpret the text verbatim (meaning that all `\#'s get escaped before uploading).\,. (9:04)

.\,.\,Hm, since some programming languages use `\#' for comments, would it make sense to choose `@' instead.\,.(?) .\,.\,Hm, and it also opens the door up for potentially adding more ways to reference a thing, i.e.\ not necessarily be ID. So yeah, maybe I'll use `@' as I've done up until now (i.e.\ the `@123.' syntax).\,. (9:09)

(9:15) Let us actually parse the title from the JSON document (if a user uploads from a file), meaning that the property document should always contain a 'title' entry (automatically understood as the common, perhaps shortened, title, as we recall).

(9:42) Hm, about not wanting to add ``even a single byte to SemanticInputs,'' should I actually turn the rat\_val into a tinyint? I know it want reduce the primary index significantly when compressed, which is why I have been fine with it, but the servers still have to send more bytes between each other and to the client.\,. .\,.\,Hm, but I could technically reduce this by transferring hexadecimals instead. And then you could even let two hexadecimals represent a smallint after padding to the right with `00'.\,. (9:48) .\,.\,And we could also convert the IDs to hexadecimals. MySQL just \emph{really} gave me a hard time when I tried to use hexadecimal, but then again, I only need to do this for selectInstanceList(); everything else can still just use decimals. This creates a somewhat asymmetric API, but so what? There will be a reason for this asymmetry and that is.\,. well.\,. that MySQL is apparently a dork when it comes to using hexadecimals (in certain instances).\,. Hm.\,. .\,.\,It's fine, simply consider the output of selectInstanceList() \emph{compressed} by default. (9:55) .\,.\,Ah no, there's actually a high chance that this will make the app slower, since it'll need for JS to run conversion procedures on all table elements. .\,.\,Hm, I should probably think a bit more about the costs and benefits of making rat\_val either smaller or bigger, and of using hexadecimals as well. But I'll just do this when it suits, no need to hurry.\,. (10:01) .\,.\,Hm, but no, I actually probably don't need to worry quite as much.\,. well, I'll think about it some more.\,. (10:08) .\,.\,(10:15) But yeah, it might very well be the frontend/client processing that's gonna take the most time, so maybe I shouldn't worry so much about.\,. Well, I could still worry somewhat about the speed of selectInstanceList() (but then I certainly should convert to hexadecimals).\,. .\,.\,No, so I should probably keep it like it is now (when I also compress the table, which includes compressing the primary index). (10:20)

(15:51) Hm, I need to think about if I shouldn't make initial\_inserts af PHP program instead, and where I can insert defined entities via JSON files (and not have to deal with CONCAT()'enated strings.\,.). And now I'm also considering whether I shouldn't just add another meta type for `property tags' to serve as nice starting-off point.\,. .\,.\,(15:56) Hm, I think I \emph{will} make this additional meta type as a help, indeed.\,. .\,.\,I guess, let me take a thinking walk to think about all these things, I feel like I need it\ldots\ (15:57)

\ldots\ (17:43) Yes, I will make that extra mate type. It means that the semantics doesn't have to bootstrap itself, but instead I just have a fundamental semantics of how the various meta types are interpreted, and then all other semantics can be derived from that.

(17:49) And I'll definitely implement initial\_inserts in JS instead. I'm thinking of making a function that takes a JSON object in the form of an array of plain objects, and then it should insertOrFind each entities in this array. Eah object will then denote it's own meta type, and will also have a key field, apart from the other fields that the given meta type needs. Also never mind about letting the prop doc / def hold the title also. It shouldn't. So a (property-)defined entity will thus have the JSON-object properties `key' (nullable), `metaType', `title', and `def', where `def' is then supposed to be a plain object itself. I will then make the insert function (in JS) return a Promise which resolves to an object with all the `key's of the JSON object, where the value for each key is then the outID (regardless of whether this was found or inserted).\,:) This means that I can then define the initial inserts in JS modules, and make then export and import these promises to each other, which can then be used for new inserts (orFinds).\,:)

Also, I think I will let the AncillaryBotData tables use the bot\_name, which I will make unique, rather than the bot\_id, since this will allow the bot events/procedures to work independently of what ID the bot is given as an entity.\,:) (18:00)

.\,.\,Oh, and let's of course just hand the JS insert (orFind) function an actual JS object (i.e.\ an array) rather than a JSON string. (18:01)

For inputting ratings, I will probably do something a bit similar (less complicated); that should be easy enough.\,. (18:03)

.\,.\,Great, I'll look forward to implementing all this tomorrow.\,:) .\,.\,And I might get started this evening.\,.

.\,.\,(18:11) Oh, and I didn't say, each objects to be inserted from an array should also be able to use the keys from a previous object. Oh, and I guess you also just hand an ID index object to the function as extra input, if is has to use some previous keys from an earlier call. (Or whatever way will work when I use Promises.\,.) And the way to reference a key should just be with a `@abc123.' syntax, or more precisely /@[a-zA-Z][\textbackslash w\_]$*$\textbackslash ./.

.\,.\,(18:19) Hm, for functional entities, let me just add a.\,. nullable text entity.\,. no, maybe I should just use a syntax where the last input can use a different separator than a comma, and if it does, it means that this last entity (a text entity, unless I'll also allow simple entities for this) will then be concatenated as a string to the input list. This allows arbitrary number of inputs, which might not be very useful, but it's nice to know that it's possible, I think. .\,.\,Yes. (18:23)

.\,.\,Let us just say text entities only for the functional input tails.\,. (18:28) .\,.\,Oh, and to help remapping (and make it possible), we should actually format the list in the text entity differently, making it a list of entity references, like `@123.,@124.,@125.', such that these will be automatically substituted under an ID remapping. (18:31)

(19:02) Besides the def field for the `defined' entities, I will also make an `other props' field for all meta types as well, where the JS funtion then also automatically up-rates these properties for the new entity (but without these properties being stored as part of the `defining properties' in the case of metaType=`d').

(22.04.24, 8:37) I think the (short) `titles' of simple and (property-)defined entities should actually be verbatim, and thus not be able to include links. The full title, on the other hand, which is a semantic property (possibly (and probably) being defined in the def text), can of course include links (i.e.\ `@123.' entity references). When it comes to the title templates for the function entities, I think they should simply have a `title template' property and a `full title template' property. This then also allows them to have another `title' (and `full title') themselves, which is actually a good thing, 'cause it might make more sense the see e.g.\ a comment function referred to as exactly `comment function' rather than something like `\%e, \%e, \%e', or whatever.\,:)

The fact that titles can't include references also means that I am okay to assume that they generally.\,. don't start with `@'.\,. Nah, let me just escape that somehow.\,. well, I should be able to use any kind of string as a property name in JS, so let's just escape it with a backslash. Then for my insertOrFind JS function, all strings are just interpreted as simple entities, unless they begin with an unescaped `@', in which case it is either an entity reference using the ID directly, or, more likely, it is an entity reference using a key instead, which the program then Promises to substitute for an ID.

If wanting to insert a text entity as a property, simply insert and object rather than a string. This object will then be inserted first, just as if it had been in the outer array, but where its key can then be anonymous. (8:50)

.\,.\,Let's perhaps boost the `initial user' ten times for the mean bot(s).\,. *((19:03) Oh, I actually don't need to, 'cause I can just run the same initial inserts several times for different users, I just realized.\,:))

(9:58) Hm, maybe I'll instead make a class with a key--callback store.\,. .\,.\,And when the key has finally gotten its ID, the callbacks can then be run, and maybe the array can be changed to the ID instead, or we could have a key--ID store next to it.\,. .\,.\,And my plan would then be that for texts with several key references, the callbacks are just inserted and waited on one at a time.\,. .\,.\,JS is single-threaded, so we can probably easily make sure that all the pending callbacks are called before changing to the ID, as I'm pretty sure that JS never switches program mid-code (and I've also used this for the queryQueue in the DBRequestManager).\,. .\,.\,Alright.\,. .\,.\,Let me make two stores, instead of overloading the one.\,. (10:06) .\,.\,Oh, but maybe I should still make the result store a key--ID-Promise store.\,. .\,.\,Hm, or I could just overload it, after all.\,. I think I will indeed do that. So if any subsequent insertOrFinds looks up a key and finds and array (of callback), they simply have to add their own callback instead of getting the ID immediately. .\,.\,(10:13) Ah, let's make a class that just always receives a key and a callback.
.\,.\,And let's give it a fusion() method that makes it absorb the underlying (private) idOrCallbackStore of the input to this method, which is another instance of the same class. This way we just import/export the class instances of this class, and never have to access the idOrCallbackStore field publicly. .\,.\,(Or we might call it absorb() or adopt() or something.\,.) .\,.\,`copyFrom()'.\,. .\,.\,Ah, but it's more complicated than that.\,. We should probably make a field that is a list of `absorbed' class instances, where the class then just queries each of these first.\,. Hm, no.\,. .\,.\,Uh, the absorbing class instance, which I think I'll be calling EntityInserter, can just add its own callback to the `absorbed' instance.\,!\,.\,. (10:30) .\,.\,Let me call it `adopt' instead.\,.

.\,.\,(10:39) Hm, let us just make all keys from an adopted.\,. Wait! Why am I doing this. Let me just use one instance in general, and scratch adopt(); it's really not worth it.\,!\,:)\,x)

\ldots\ (14:15) I'm just gonna store all simple entities automatically, with the title as the key, and then I'm just gonna prepend a `@' to all the user-defined keys.
.\,.\,Wait, maybe this should be a separate store.\,. \ldots\ No, let me just indeed append the `@' automatically to all.\,. keys.\,. Hm.\,. .\,.\,Hm, let us say that titles can also be key references instead of verbatim titles, meaning that titles always needs to have any initial `@' escaped (unless it is interpreted as a key to the ID store). (15:22)

(23.04.24, 9:18) I might for simple entities actually make it the standard tap to look for `instances' that the given `word' can refer to. This goes a little bit against the rule of \emph{not} conflicting `meta types' with the actual `types,' but still.\,. I might do it.\,. (Something to consider.\,.) .\,.\,Yeah, and maybe `simple entities' could be the exception to the rule, making them always be neutral/typeless in a sense. And one should note that this would \emph{not} mean that their type is simply `word,' for they are not always interpreted as referring to the word, far from it. Rather `neutral'/`typeless' means that they can refer to multiple things at once. They can for instance always refer to the word itself, but they can also refer to all the things (if there are more than one) that the word can refer to. I'll still consider it some more, but this kinda seems like a decent idea.\,. (9:25) .\,.\,Yeah, I think it's actually a good idea.\,. (9:26)

\ldots (9:47) Wait, this might mean that the `title' field should actually be an ID field for  defined entities'.\,. Hm.\,. .\,.\,Hm, that would actually make sense to do.\,. .\,.\,Hm, and selestEntityInfo() should then of course still output the title string, but I might as well then also make it output the ID, why not.\,. .\,.\,Sure, let me just do that. Okay, I think I will do this.\,:)\,.\,. (9:52) .\,.\,Ah, unless I want to make the field hold a data\_key instead.\,. .\,.\,Hm no, let us just make it hold the entity ID, I think.\,. (9:57)

(10:22) Hm, maybe it's easier if we keep it like it were, and just basically makes the.\,. Well.\,. .\,.\,Makes it conventional to upload a simple entity at the same time as a defined entity?\,.\,. \ldots (10:34) I might just make it store the data\_key instead. I can still look up.\,. wait.\,. .\,.\,Oh, I of course \emph{do} need the secondary index for the Entities table, so let me just in-comment that line.\,. And yeah, then I can still return the title ID if I want to for selestEntityInfo().\,. And do I want to?\,.\,. .\,.\,Probably not, actually.\,. (10:43)

\ldots\ (14:30) Hm, from the app's perspective, it would be slightly easier if\ldots\ .\,.\,Hm, what to do.\,.\,? \ldots (14:46) Hm, let me indeed change insertOrFindDefEntity() such that it takes an ID instead and just checks that this is indeed an ID of a simple entity. And I'll then change nothing else about the back-end.\,. .\,.\,Nah, this creates a weird asymmetry, so let me actually just change title\_data\_key to title\_id after all, and then simply query for and append the title as will for selectEntityInfo() when entity is a `defined' one.\,.

\ldots Now this case of selectEntityInfo() requires three lookups, but that's okay. One can also change the title\_id column back to title\_data\_key at some point in the future if one wants, without changing the API of the SDB.\,. (15:14)

(24.04.24, 8:31) I've had a couple of good ideas this morning. Let me start by a small one: I will call `functional' entities `formal' entities instead (as their main name), and then use `format' generally instead of `template.'

Another idea is that I \emph{will} use the `property documents' property after all, but users don't uprate the properties in a document by uprating the document under some tag. Rather I should implement a thumbs/arrow-up button that uprates all the properties in the document individually all at the same time. There should then also be a button to expand out a list RatingElements of all the relevant properties, such that a user can also give ratings individually (either adjusting some ratings down after having giving all properties a maximal rating by pressing the thumb.\,. or un-pressing it.\,. or before having pressed it). Then for the meta info page, users should first of all see the original/defining data, and then the should also be able to see a list of the most uprated property documents (unless we choose another page for this instead). Oh, and for defined entities, there should also be a button to uprate the original title. And of course, the original property document should also have this thumbs-up button as well (the same as the one for all the other (potential) uprated property documents for the entity).

Another great idea that I've just had is that I will actually let the set of meta types be an open one, and then in particular make it open for file formats! So instead of just having the `binary' type, there could be all kinds of file types, e.g.\ `gif', `jpg', `html', `php', etc.\ etc. This also means that I will make the meta type a VARCHAR(\ldots) instead of a CHAR, and why not just make it a VARCHAR(255) from the beginning (instead of starting low and then increasing on demand).

If the SDB wants to save a file format in a compressed way, this should simply just be reflected by the meta type name, e.g.\ by appending something to the uncompressed format name. *(e.g. `html.zip', but it doesn't have to be existing file ending; it can also be invented names, e.g.\ `json\_no\_whitespace') And the same goes for supersets and subsets of formats: Generally let this be reflected in the meta type name.

Like the `text' and `binary' entities (if I keep the latter.\,.), there should of course always be a UNIQUE data\_hash column for all such file format meta types.\,:) (8:53)

.\,.\,I should also say, I realized that JSON isn't meant for human readability (which is part of what got me thinking about this stuff). So I might just make my own format for the property documents.\,. .\,.\,And the point is, by defining this format as a meta type, we get a very good way of telling the app that it can render it a certain way. So in this case, the app will know that the property documents doesn't need to be rendered more verbatim as the texts they are, and instead they can thus be rendered more fittingly. So I could for instance just have a format like `123:124,125;234:345,12,1356;126:1;', and then the app can render it such that all these IDs are substituted with the appropriate EntityTitles. (9:01)

.\,.\,(9:02) Oh, and I also thought about adding a `relation tag' entity, why not, where instead of a noun, the `property' is replaced by a sentence, minus the leading subject of the sentence and the trailing object of the sentence. This might be handy in some cases, I think. For instance, a `might refer to' relation could be handy for the simple entities (for their main page), couldn't it?\,.\,.

.\,.\,Oh, let me by the way use meta type names like `text/json' instead, more like, what are they called.\,. .\,.\,the `content types' used in HTTP headers.\,. (9:11)
.\,.\,Hm, so I'll for instance let `t' $\to$ `text/plain'.\,. .\,.\,I think so.\,. (9:14)
.\,.\,And for the the non-data meta types like `d' and `p', I'll maybe just keep using the single characters, and at least I won't include any slashes.\,. (9:16) .\,.\,Hm, let's actually not keep the single characters and make them more verbose.\,. (9:17)

.\,.\,Ah, alternatively we could use formats for relation entities, and e.g.\ write `\%s might refer to \%o'.\,. well except that we don't generally have a specific object to insert.\,. (9:21) .\,.\,But we could just insert an ellipsis, then.\,. .\,.\,Hm, maybe I like it better that the subject is just meant to always come first and the object last for this meta type, and then one can always add another relation meta type if there is a desire.\,.

.\,.\,Alternatively, to limit redundancy (i.e.\ having too many choices for have to make a relational tag), we could also just add an `\%s'-placeholder syntax to the properties, where `\%s' can then be replaced by.\,. `this'.\,. Hm.\,. .\,.\,where `this' can then be rendered differently, perhaps by making it a link.\,. .\,.\,Hm, I'm wondering about this `\%s' syntax, but maybe it could just be exactly that.\,. (9:29) .\,.\,Hm, or we could write `[this]' instead, maybe I like that better.\,. .\,.\,It shouldn't be a link, I think.\,. .\,.\,Hm, maybe `[this]' could even just be rendered as is.\,. .\,.\,Sure, at least in the early versions of the app.\,:) (9:32) .\,.\,So this allows us to use the property of `entities that [this] might refer to', instead of using the `might refer to' relation.\,:) .\,.\,Hm, unless the latter (i.e.\ `might refer to') is preferable.\,.\,? (9:34) .\,.\,Hm, if so, we might as well just overload the `properties' by using a convention that they should generally be nouns if a suitable one can be found, and else they can also be these partial sentences, with the subject and the object cut out at the two ends, respectively. I think I might say that.\,. (9:37) .\,.\,Or they can also use `[this]' to refer to the subject (/ owner) entity if the other two options doesn't work so well (somehow.\,.).\,. (9:39)

(9:49) Wait, I should probably just let the text and the binary entities store their content/file type instead, giving these tables an extra column each. .\,.\,And the I could just let it be up to the app to verify that any received data has the correct format.\,.
.\,.\,Okay so these two tables will just get another varchar(255) column as part of their secondary index, and the backend will generally not promise to verify the given format.

(11:01) Okay, about the property docs, I can use a format like `123:124,125;234:345,\ldots' in the backend and the backend--app interface, but I might still want to encode the def in JSON for the frontend insertion files.\,. .\,.\,Yeah. And the JSON should then just encode an object with either a string or an array of strings as.\,. Hm.\,. .\,.\,(11:09) Ah, maybe I could just always carry around a `property document'-like string for all callbacks, together with the outID.\,. And that string will only ever have to be one level deep.\,. .\,.\,Not a bad idea.\,. .\,.\,Hm, or I could rather make a virtual meta type of `property document' to insert.\,. wait, it doesn't have to be a virtual type, since I need this meta type anyway.\,:) (Well, technically it would be a text meta type with a `property document' format.\,. but I guess that's also just as fine.\,.\,:)) (11:15)

\ldots (11:36) Ah, let the text entDefObj's always hold an array of strings to be concatenated, rather than holding the text as a single string (for its `text' property).\,:) .\,.\,Ah, and I can of course just concatenate before I call getSubstitutedText().\,.

.\,.\,I by the way think that I will simply let uprateProperties() always query the database for the property text, then upload. So I don't need to pass those strings along with the outID/entID for the callbacks, as I otherwise talked about. (11:41)

(12:00) Note that the actual HTTP header content type should NEVER be anything other than just text/plaintext or octetstream (or whatever it's called) when we let verification be up to the frontend. *[(19:02) Well, for images and videos to work, we have to set the content-type header, which means that the backend \emph{should} verify the binaries of these formats.]

(15:06) The ``text/plaintexts'' are nt so plain after all, since we convert entity references to links. So I should find another name.\,. Hm, and why not just `text'?\,. .\,.\,Well, I'll think about it.

(15:35) I had a good idea when I was out walking earlier about how to implement the property document findOrInsert(). The idea is to just make a call to findOrInsert() for all properties and property values at once from the beginning, and then just give each individual callback an index as well. I will then also record an array of all the delimiters that there should be in the final prop doc. And when each callback resolves, they add their entID to another array, together with the index. Then this latter array is finally sorted (once it is checked to be of full length) such that the index values match the index in the outer array. And finally all the IDs are then braided together with the delimiter array, given us the final property document.

(19:34) I just considered adding another unique index for SemanticInputs with rat\_val and then user\_id at last, instead of having the statement---user rater bot, but no. It's better to use bots here, since we want to be able to filter users out.

(19:39) Another kind of meta type(s) that we maybe \emph{do} want to implement at some point might be images etc.\ from external sources. These can then be shown by the app as the images themselves (instead of an URL), even though they are stored in the database as URLs.

(20:07) I think I might actually remove the InstListHeader (and therefore also not implement the ListGeneratorColumns) for the early version of the app. And then I will just implement one global menu where users can choose their queryUsers and adjust the weights of these. Then there should also just be an infinity weight, which can be given to oneself, as well as potentially any factual bots such as the statement--user rater bot. (20:10) .\,.\,This means that users won't be able to combine two (or more) tags in a combined search for the first version of the app, but maybe that's okay.\,. .\,.\,Hm, or else I can just make a new kind of search column specifically to combine tags, how about that?\,.\,:) I think this is what I will do.\,. (20:12)

(20:47) Ah, and then I will just put a link to the search column above the InstListDisplay when this is relevant, instead of having it as a part of the InstListDisplay.\,.

(25.04.24, 8:05) If the backend is not generally verifying the `intended formats,' then there's really no point in having them. They can just be semantic properties instead. Hm, but does this mean that property documents should be implemented via the property-defined or the formal entities?\,.\,.

But let me also continue on the meta types, 'cause I don't think that we will need the `image from external source' metatype(s) either. These can also just be implemented as property-defined or formal entities instead.\,.

.\,.\,Yeah, all these file type types should really be implemented via formal entities.\,.

.\,.\,Maybe `datatype' is a better name than `metatype'.\,. .\,.\,Ah, `data type,' rather.

.\,.\,Hm, is it worth considering making just the property document data type, just so that we don't have to rely on defining functions to get the semantics going?\,.\,. (8:26)

(8:34) The solution might be to simply not save the property document as a text entity, but as a text field for the property-defined entities, and simply add a data\_hash column to that table as well. I can then also make the php InputValidator verify the prop doc format.\,.

(8:49) Oh, I should also mention: The backend can still get to verify certain formats, such as image formats. Then the frontend just supplies the desired content type header as an input to the request. We can then add a table with VerifiedFormats to the SDB, containing the ID of the text or binary as well as any verified formats. If the correct format, matching the desired content type (`MIME type'), then the query succeeds and the control server gets to add the given content type header to its output back to the client.

(9:11) Hm, `associative entity' could also be a fine name for the property-defined entities, but then I have to rotate the symbols (since `a' clashes with `aggregation bot').\,. .\,.\,(Let me think about this.\,.)

(9:22) Oh wait, I did have a reason for wanting them to be their own entities, the property documents.\,. .\,.\,Hm, so do I just define it as another entity data type.\,.\,? .\,.\,It seems like the right thing to do.\,. (9:30) .\,.\,Oh, now I really get name collisions for the data type chars.\,. .\,.\,Well, if we just refer to aggregation bots as native bots instead---which is perhaps also better, and use `d' for `property documents,' then it still works out (without having being too confusing).\,.

(12:10) I just had the idea to maybe use property documents also for function inputs.\,.\,! This could then make it so that formats will get to use a `$<$my property$>$' syntax just like my templates before I changed up the SDB by introducing the ``metatypes,'' now called `(entity) data types'.\,.\,!\,.\,. .\,.\,I think this will make it easier both to use (from the users' perspective) and to implement.\,:)\,.\,. (12:14) .\,.\,Ah damn, but this destroys the whole reason to have the formal entities, which is searchability, and which is why we need the inputs to be ordered.\,. .\,.\,I might create a virtual `list' type, though.\,. Ah, and this could maybe also be used for property docs.\,. Hm, or perhaps not, but let me think about it.\,. .\,.\,Yeah, nah, it will be its own virtual data type, but I will implement it almost identically to the propDoc data type, perhaps be using a (reusable) helper method.\,. .\,.\,Okay, I'll do that, but let me take a walk first. Then I can also think some more about whether my `potential textID after a semicolon at the end' idea is good enough, or if I should do something else instead\ldots\ (12:23)

.\,.\,(12:25) I by the way also had an idea to add a `list' entity data type, but I don't think there's enough benefit from that\ldots

\ldots\ (13:39) There is reason to add a `list' entity data type to the SDB.\,:) It can be used exactly for plural properties, such as `actors' etc. And then I will also introduce a spread operator for the prop docs, which I will simply be a leading `s' that can be put before property value IDs. I will generally render lists as `(elem1)', `(elem1, elem2)', `(elem1, elem2, elem3)', `(elem1, elem2, elem3, ...)', but within property docs when the spread operator is in front, I will instead probably render the list (including the operator, which is thus never explicitly rendered itself) with gray/translucent square brackets, i.e.\ instead of these normal brackets. .\,.\,Note that I will not say that prop.\,. Oh wait.\,. Hm.\,. Does it make sense to introduce the spread operator for the list data types itself, rather than for the property document?\,.\,. (13:47) .\,.\,Well, and then also have it for the prop docs, but where there is then always either a single value for a property, or a spread operator followed by a single list?\,.\,. .\,.\,(13:51) Hm, I don't like having the spread operator for the list data types, since they need to be searchable. So no, I'll just add the `s' operator to my existing syntax for the property docs.\,. .\,.\,I will by the way also introduce a virtual `spread' data type for defining and inserting prop docs.

.\,.\,I should then of course also make InstListGenerators be able to use list entities as well, together with the instance lists, which could in particular be useful for blacklist/whitelist filters. So there are definitely benefits to introduce this entity data type.\,:) (13:58)

(14:02) I will also make a query proc for formal entities where the list text is provided directly instead of the list ID, i.e.\ such that the step of querying for the list ID can be skipped.

(17:26) I just had the idea that instead of reintroducing a link to the InstListGenerator (column) above (or as part of) the InstListDisplay, we could instead just add a menu where you can see all the active InstListGenerators, and click them to go to their column. That way we never need to clutter the app which InstListGenerator links, and we can comfortably design the app without these links (so with no InstListHeaders anywhere, or anything like it).\,:)

(26.04.24, 9:59) Maybe the prop docs should actually just always have one ID to the right of the colon, either with an `s' in front or not.\,. .\,.\,And then I just.\,. hm.\,. use the spread operator for the lists.\,. Hm.\,. .\,.\,Hm, no, maybe not.\,. .\,.\,(10:12) Yeah, maybe I should do that, actually. I think so.\,.

(10:52) Hm, is it a bit overkill with the lists, especially for the functions, which then requires hashing of the listText, and also requires four SELECTs. I could also go back to input\_list just being a varchar(255).\,. .\,.\,Hm, or I could implement lists via varchar(255)'s.\,. \ldots (11:09) Hm, I have to input the listText rather than the hash: Let MySQL do the hashing.\,.
.\,.\,Okay, running \texttt{SELECT REPEAT(SHA2(REPEAT('a', 65535), 224), 1000000) > "0";}
takes about 0.05 seconds.\,. .\,.\,Hm, weird, but anyway, I don't think I ought to be concerned---the four SELECTs are probably much more costly, then.\,.
.\,.\,(11:31) So I'll probably keep the current implementation and API (I have just corrected selectFormEntityIDFromText() to take a TEXT instead) as it is for now.\,. .\,.\,But my legs are itching for a walk, and I have several things to think about, so let me take that walk now\ldots

.\,.\,(11:39) I should make a virtual `concatenated list' data type instead of the current ``spread'' (virtual) data type\ldots

\ldots\ (13:24) I went on a good, energetic walk and had several good ideas.

First of all, I realized that the MySQL statements above only does one hash, of course, and then concatenates it with itself 1000000 times. But I also had the ideas that I will actually store the input text inline in the FormalEntityData table, iff it is less than or equal to 255 bytes in length. And selectEntityInfo() for formal entities should also return the textStart of the input listText, just like for some of the other entity data types.

I think I will let `:' $\to$ `=' for single-valued properties in the prop docs, and let `:s' $\to$ `:' for the many-valued properties.

I also thought some more about the `data' for the discussions, which I might call `evidence' instead. I will write about those ideas in the (`Structured discussions') section below.

And before I talk about some ideas for the tags and reviews (praises and criticisms), let me also mention the last idea of my walk, which was to let rating bars consist of to input fields with a shared value, namely a start slider (with some margins to the side, by the way, so that you can easily select 10.0 and 0.0 (which is an earlier idea of mine)) and then also a `type=``number'' step=``0.1''' input field, where you can press the plus and minus buttons to increase the number by one decimal at a time. (13:34)

Then we get to the ideas about tags and reviews. .\,.\,Well, it's really only one particular idea, apart from also just the small comment that the difference between using tags and reviews for uprating important qualities of an entity is that tags are generally meant to be generally applicable, whereas reviews are free to discuss thing that are specific to the entity in question, and are thus free to specify the entity as well, instead of just using a pronoun for it.

Okay, and the idea is this: I've already talked about having both the actual tag rating for each tag (or review point), and then also have an expandable menu with a rating of how much the fact that the entity has the given value means for its overall `goodness.' I think I will also add my old idea of `wish ratings' to this menu. And I've thought about calling it something else: `A+ expectancy,' which when explained more precisely means that `given that the entity in question had been such that it deserved a A+ grade for how it did in terms of the tag(/review point) in question, what would that rating be expected as?' Okay this sounds a bt complicated, but I think I can explain it better. It's just like: `if the creators of the entity (the movie, the product, etc.) where to redo/remake it in a way to try to get an A+ score in relation to the given tag/review point, what would be the rating value that they should try to achieve as their A+ goal?' Hm, I think that was a little better, and we can probably make it even better.\,. (13:46) .\,.\,Now this expandable menu of additional ratings, which includes two so far, might even also include a third one (such that there are four ratings in total when you count the tag itself as well), which is `how much it would boost the entity's `goodness' if the entity scored close to the `A+ expectancy' score instead of its actual tag score. This seems like a lot of things, but this is all important user data.\,!\,:) (13:49)

\ldots (14:08) I really think that this could make such a useful app, and I hope that a lot of users would even want to join simply by learning about these possibilities, and even before the network has fully taken off. So this is why I will try to give it my all with working on the new prototype for this app in the coming time, after all.\,.\,:)

%... Ret sjældent jeg har hold i nakken. Fik det sjovt nok efter min gåtur (selvom det nu nok må stamme fra i nats).. Det er rimeligt distraherende.. (Men jeg fik da alligvel kodet en lille smule (fik lavet insertOrFindConcatenatedList())..)

(16:18) Maybe I should just call it something like `wish.\,.' wait.\,. .\,.\,Hm, maybe it won't work so well: Maybe one would just, as a user, want to put in 10.0 for most of these `wish ratings'/``A+ expectancy'' ratings. So maybe it's not such a good idea after all. I'll think some more about it.\,.
\ldots (17:00) Ah, maybe we could just remove this `wish rating' but keep the forth one!\,.\,. .\,.\,So you keep the rating that says how good it would be if the rating of the given entity had been increased (or decreased, if the a rating below 5 it chosen) for the given tag.\,.\,:)\,.\,. .\,.\,This seems like a good idea.\,.\,!\,\texttt{:D}\textasciicircum\textasciicircum\ .\,.\,The good thing is that it really gives such a good avenue, I think, for giving constructive criticism without feeling like one is spouting negativity, at least in a lot of instances. For there's a lot of instance where you want to say `this is very good \emph{and} it could be even better,' and yeah, this just provides a good and very useful way of doing this, without either feeling like one is spreading negativity, or having to try to preface the criticism with a whole bunch of positive remark to balance it out---which probably doesn't really work anyway most of the time.\,.\,:) (17:09)

.\,.\,(17:18) Hm, this was probably also my idea with the `wish ratings,' at least at some point in time before.\,. And yeah, it makes a lot more sense this way. It also means that you don't necessarily have to agree on the rating value of the tag in question in order to still agree on the wish rating: you can agree on the latter while disagreeing on the former.\,.

.\,.\,I might make it so that there's a selction menu above this `wish rating' where the users can choose between four variants: `goodness of increased rating,' `goodness of decreased rating,' `badness of decreased rating,' and `badness of increased rating'.\,. (17:23) .\,.\,And I think it's fine that not all users will choose to rate the same one (and some users might rate several).\,.

(27.04.24, 8:59) Okay, I think I will finally go from SMALLINTs for the ratings to TINYINTs. And I think I might let the range truly be 0--256, and then handle deletions by.\,. Hm, wait.\,.
\ldots I'm thinking about removing live\_at\_time (I will, actually), and also about removing RecordedInputs.\,. .\,.\,Yeah, and by keeping Private\_RecentInputs, the database can still solve the mystery if a until-then trusted third party briefly changes a rating and changes it back. Also when it comes to security-sensitive things, users should never put their trust on single users, but always only on a group of independent users/bots.\,. .\,.\,So yeah, I'll remove RecordedInputs, and the whole live\_at\_time thing, I will let that up to the clients themselves to implement. For users that are going to care about this are going to use proxies *(VPNs) anyway, and then all they need to do is to find a service, that also allows them to put a delay on posts. (9:24) .\,.\,Oh, and I should then also make the RecentInputs table public.\,:)

.\,.\,Now, I could let.\,. Ah, wait.\,. Hm, nah.\,. .\,.\,Should I still just let 0 mean `delete,' or should I.\,. Yeah, let me add a byte to RecentInputs to denote deletion, perhaps imply by making the column nullable.\,. .\,.\,And then I will just let the `rat' data type in the control server (i.e.\ the input\_handler and InputValidator) be either a 0--256 number \emph{or} the string `del.' Then I'm going to convert that to a smallint (short) which is larger than 256 iff r=`del.' The input proc can then convert any number larger than 256 to NULL, meaning `deletion' (not `missing'), and take it from there. (9:35)

\ldots Hm, let's just make it a client-side thing to input a number larger than 256.\,. Or.\,.\,? .\,.\,No, I like the fact that the client--server API is the same as the database API. But I could just add a BOOL input after all, instead of using NULL.\,.

\ldots Hm, maybe I should make it so that number, not strings, are always return for the rating values for instance lists.\,. .\,.\,I'll look into this at another time.\,. (10:23) \ldots Hm, it's likely not worth it, actually, unless I can get PHP to do it directly (or something to the same effect).\,. (10:36)

(10:57) I thought about this yesterday evening: I will make all tabs be defined via schemata, which means that it will then be easy to open up for users being able to construct their own tabs, simply by creating such schemas themselves and uprating them.

(28.04.24, 9:27) Yesterday turned into a relaxed kind of thinking day. I've got a better grip on what the early version of the app should focus on. The `semantic searches' won't be important at the beginning, nor will the browser extension, actually, since so many people use phones anyway. So it's better to focus on making it work well on phone screens. And when I make the first browser extension, by the way, it will just be a button that queries and shows if there are comment and/or ratings available for the (top) URL, and when you click on it, a new tab is opened in the browser with the website, and the given URL as the opened entity. By the way, let us make sure to also always make a `text' search for the input of the search field, at least when the user hits `search.' That way, when users type in a full URL longer than 255 bytes, they can still find it is it is stored as a text (oh, and the app escapes the `@'s before making the text search.) I guess I should then also make a `might refer to' tab for texts.\,.\,? (9:37)

Let me get back to that. Now, what \emph{will}(/should) be an important part of the early app is certainly being able to rate and review various popular things, and to find out which user groups one belongs to, and to weigh the ratings by giving more weight to these user groups. By the way, when I have talked about ``correlation vectors'' in my notes, I have meant the so-called `principal components' from `principal component analysis' (PCA).

The PCs should be calculated with weight on the most `trusted' users, oh, which I by the way will change to `users give valuable contributions to the network.' And the should also be weighted by how important the statement is rated for the given subject tag. So here's how I imagine it:

The users will go to the site and by uploading useful entities to the network, their esteem (`users give valuable contributions to the network').\,. Hm, maybe we should actually refer to it as `esteem' when we want to be brief.\,. .\,.\,their `esteem' is then supposed to grow by this, namely since the user community is supposed to up-rate contributing users in terms of their `esteem,' such that the more valid contributions, the more uprates, and the higher uprates as well. One of the interesting things about having good esteem is that you get to be part of determining the PCA user groups. This happens in the following way. First the users define a `statements useful for determining a user's tastes and opinions in relation to the subject of $<$\emph{subject}$>$' tag and up-rate it for a `useful statement categories for PCA' tag. By the way, the default entity to open for the website should not by `entity'/`Entities,' but should instead be (something like) `useful categories and things,' which is then supposed to act as the category of useful frontpage links. And one of these should be the `useful statement categories for PCA.' Well, we could also just say `useful subjects for PCA' if we decide to always use the form `statements useful for determining a user's tastes and opinions in relation to the subject of $<$\emph{subject}$>$' anyway. But we'll see. Then for each of these `subjects,' the users can then up-rate statement that they think a useful for, well, determining a user's tastes and opinions in relation to that subject. This is then where the `esteemed' users can then first of all try to rate all of the most up-rated ones, if they want to be part of creating useful PCs (`principal components,' not `playable characters'). Then after the SDB have created the PCs for the set of statements, all users can then try to rate the same statements, and then have a bot automatically rate how well they fit each PC of the subject. Users can furthermore also try to describe what they think the PCs mean, i.e.\ `what are the underlying descriptors that users of a given PC matches?' It should furthermore be possible to have tags where the user is understood to really rate themselves when they rate these tags. And this can be used to create a statement of how well the users think they match certain descriptors. And such tags can then very well be up-rated as well as important ``statements'' (maybe it should actually be `statements and user tags,' then.\,.). This can happen before the first PCA is carried out, but it can also happen afterwards, since the PCA might happen continuously in several rounds for a given subject.

Of course, there also has to be an analysis s part of this, where the all other statements that the `esteemed' users have rated becomes judged on the PC spectrum, but this could be an analysis that happens more continuously more frequently than the analysis of the users themselves. So once the (esteemed) users have been located in the PCA spectrum, it's time to locate continuously maintain a spectrum for all entities that they rate (or perhaps not \emph{all}, but we'll see; it could be \emph{all}, at least at the beginning.\,.). (10:16)

(10:39) I've removed the recordCreater input and made so that the creators are just always recorded. The fact that the first user to rate a statement will typically create a statement entity in the process---and I should by the way make sure that \emph{they} are indeed recorded as the creators, not a bot---it means that users can also see which statements a user was the first one to rate. I might, however, also make it so that.\,. Well, I had the idea this morning to consider whether I should turn RecentInputs into just Inputs, and record them for all time. And then I could make a `rating' entity data type, and.\,. Hm, a few things to consider here.\,. (10:43) .\,.\,Yeah, nah, let's just make do with being able to see the first user to rate a statement, and not care very much about the first few that are not exactly first, even though this could be a good thing.\,. Hm.\,. (10:45) \ldots (11:00) Hm, there's a middle way where we don't make ratings entities with the same ID range as the normal ones, but where we still might give them all an ID (in RecordedInputs), and where a list over all ratings for a statement in chronological order might still be obtained.\,. .\,.\,Hm, this honestly seems worth it, since their probably won't be very many rating deletions vs.\ the count of given ratings. So yeah, I might turn RecentInputs into just RecordedInputs, containing all ratings of all time. And I'm just gonna not record a rating if it is the same as the previous one, then. (11:06)

So there we have it. This will be be the main part of the early app: Go and contribute with entities and rating to grow in `esteem,' if you want to, and even if not, you can still find out what PCs you following in regards to various subjects, and use it to boost ratings and review points from the `user groups' of that PC (possibly combined with the `esteem' as well). And then you can go and see ratings and review points for things that you are considering consuming/using in your life, with respect to all kinds of predicates, and where you can even weigh the user ratings to give more weight to groups of users that you tend to agree with. \emph{And} you can even adjust the weights to get a better picture of, which kinds of users like what about the thing, and how much, which might help to get a good insight into whether the thing is what you look for or not. (11:12)

.\,.\,There we are.\,.\,:)\,.\,.

.\,.\,But I have to also go back and think some more about the URLs.\,. .\,.\,(11:17) Ah, but the texts \emph{have} to have the `might refer to' tab as their main tab as well, 'cause what is a text worth without a context?\,.\,!\,:) .\,.\,So there we are.\,.\,\textasciicircum\textasciicircum\ (11:18)

(11:47) On this note, let me continue in a new section, below the `Structured discussions' one.\,.

*Oh, I also wanted to mention that open source contributors should also upload their code contributions as entities, such that users can also (greatly) up-rate these contributors' esteem. And in relation to that, I also ought to make a menu at an early stage, where users can select different CSS styles (themes) for the site. Their selected style/theme can just be saved in a cookie, why not.\,. And then a bit later on, their could also be another menu to select between different application react components, i.e.\ such that users get to change the website up completely to the structure that suits them best (out of the list of okay'ed components and themes). (13:21)






\section{Structured discussions}

(12.04.24, 9:05) I have written about some ideas before of how structured discussion could work, probably in my 2021 notes, if not the 2022 notes. Let me add some new thoughts here, and then also dedicate this section in general to ideas about the whole `structured comments and structured discussions' part of the SDB application idea.

In general, whether we are talking about my `debate/discussion website,' which I have wrote about before, or the comments discussion of the SDB app (note however, that the former might be implemented within the latter), it's a good idea if we can promote Bayesian logic (note that the `ian' suffix often gives associations to a \emph{persuasion}, and thus that Bayesian logic might not be true, but it is: It is a mathematic truth% Of course, there's semmingly the frequentist--bayesian debate, but I have disproved frequentism here in these notes.
) in the decision making process for discussions.
And this is by the way not just for discussion forums on the web, but it might might also be relevant to e.g.\ courts etc.

The idea is that the participants first give their overall opinion of how likely they think a given statement, i.e.\ the overall one that is being discussed, is to be true given only what was known prior to the event and/or the evidence gathering and discussion phases. Then the participants try to list all relevant pieces of evidence that can be used to inform us about the truth/falsity/probability of the statement. One could start with all the existing pieces of evidence when the discussion ``starts,'' but one could also make no sharp distinction between preexisting and future/subsequent data sources. But I think it might be a good idea, when we get to rate the likelihoods of these ``datasets,'' that each individual notes whether the rating was done before or after the data outcome was known to them. So for each ``dataset''/``experiment'' there is both a ``prior'' probability assignment done before the data is known, but also one that is done afterwards. This is important since it is likely that most participants, if they have a stake in the discussion in anyway (including simply a desire to ``win''), would want to change their ``predictions'' after the data is known. And instead of disallowing this, I think it is better to allow it, and then the community can instead just note which users have a habit of changing their ``prior probabilities'' a lot, and then for the future take their initial and/or their subsequent predictions all the more with a grain of salt (if the shift is often great from before to after, that is). But before I move on, let me try to make the terms less physics-like, and try to make them more realistic. For the ``experiment'' will often not actually be `experiments' done by `scientists,' but will often be something like predictions about what will happen in the future, predictions of what certain involved people might say or do in the future (or have done, if we are talking about the `post-prior probability' assignments, as we might call them), what experts might say about the topic (so we might also treat what experts/scientists might say about the topic as an ``experiment'' as participants of the discussion, and assign (post-)prior probabilities to these). Oh, and if you are unfamiliar with the term `prior probability,' look it up quickly, but also know that if I just wrote `probability' instead, it should also make sense.

So we first, as participants, rate the the overall prior probability of the statement being true, as if we knew no particulars about it. I guess we actually have to be specific about what we deem as this overall knowledge, and what is considered ``data''/``experiments.'' But in general we are talking just about `patterns in the past,' essentially. Yeah, `patterns in the past' is the keyword.\,. .\,.\,And the participant's overall beliefs regarding `patterns of the past and present.'

Then we try to come up with good ``experiments,'' which I will try to stop calling it now, and call it `data sources' instead, that could enlighten the question. These can be data sources that have already provided their data (an expert might already have given an opinion, an involved party might already have done/said something, etc.), or they can be sources that is expected to potentially provide data in the future (near or far). Each user then rates the probability of the data turning out one way if the statement of the discussion is true, as well as the probability of the data turning out one way if the statement of the discussion is false. And yeah, note that we should indeed try to treat the data outcomes as binary.\,. well, unless we want to implement a more complicated system where users assign ``prior'' probability curves to a whole interval/space of outcomes. But if not, then cut the interval/space in half and divide it two outcome groups. And if we do so, it means that we only have to assign two (post-)prior probabilities for each data source. But yeah, a more complicated/sophisticated implementation might also be useful in the future, and here it is a good thing that ``prior'' probabilities can also be assigned after the fact.

Of course the participants can cheat and give dishonest probability assignments, and they can also try to draw previous assignments back by making marginally different post-prior probability assignments, but the user community can just be on guard for this, and down-rate users that behaves in a fishy way.

Now, the great thing is that from here, it's basically all math. Of course, you have to take into account that users can be dishonest, and you also have to settle on conventions for how to use the actual-prior and the post-prior probability assignments by the participants (i.e.\ settle on some weighted mean of these two things), and have to figure out what to do with users who change their probability assignments to much. But after all this, the resulting probability for the statement of the discussion can be obtained mathematically. Oh, and you of course also have to factor in user trust values to begin with. Luckily this does not need to be done in a centralized way. Some users will assign different trust levels to different user groups, which might change the result of the discussion. And that can then just be calculated for their settings particularly, and other users with different trust settings will be able to see a different result (and will also be able to see the results for all the other (popular) user trust settings). (10:03)

(12:20) Oh, I must mention the ``just math'' \emph{can} be quite complicated, though, if there are correlations for the data sources.\,. But still, this process could still greatly improve our decision making.\,:) .\,.\,And online discourse in general, for sure.\,. .\,.\,:)


(15:24, 16.04.24) About the correlated statements, while this in principle could be the only tabs for a statement (as part of a discussion), this would be way to complicated (especially to understand). So the main tabs should still be `arguments for this' and `arguments against this'.\,.

\ldots Maybe I didn't (get to) mention the idea to make it a `correlated statements/proposi-tions' instead, but that's what I've been thinking---besides the fact that it should not take the place of the more natural `arguments for this' and `arguments against this' tab.\,.

(16:52) There's also the fact that for big enough discussions, the correlation between various statements can also be obtained (to some level of precision, not far worse than if the users were to rate this correlations manually) from analyzing the correlations in the predictions of the users (of the probability of each of the proposition in the group of what's been deemed as `correlated statements').\,. .\,.\,(I know that not a lot of what I've been writing here this afternoon will make a lot of sense when reading it: These notes are quite brainstormy.\,.)

(17:06) Oh, I just had a perhaps very good idea about always choosing an antecedent when making a \emph{probabilistic statement}, i.e.\ a statement/proposition that is meant to be rated in terms of probability by the users. And then sometimes, the antecedent can just be `(only) what we know overall about the subjects, without including any facts/details specific to this case' and other times it can be, well, also that, first of all, \emph{and} also another statement/proposition on top, which means that the user then rates the probability of the given consequent statement when it is assumed (without knowing anything else), that the added antecedent is true. (17:11) .\,.\,And thus the users can rate the correlation of two statements/propositions.\,. .\,.\,This is a.\,. \emph{very} good idea.\,. (17:15)

(9:52, 17.04.24) One can probably even think about these `correlated statements' more intuitively as ``data.'' And that might make it more easily understandable, and mean that this can be part of the discussions at a sooner point, perhaps from the beginning, even.\,!\,:)

And about the other tabs for the discussions, apart from a summary tab, where users can supply and uprate comments that tries to summarize all or parts of the discussion, I think there should be a sub-discussions tab, perhaps instead of having the two arguments and counterarguments tabs.\,. .\,.\,I'm thinking along the lines of rating the sub-discussions in terms of importance, first of all, where this `importance' rating is then a combination of how likely the sub-discussion is to be resolved (from the reader's perspective, of course), in a near future at least, and then of course also how important the answer to the sub-discussion is for the parent discussion.\,. Then apart from this `importance' rating (which determines the order of the list in the tab), there could then be separate ratings of `how important an answer to that sub-discussion would be in terms of how determining/guessing the answer to the parent discussion'.\,. Hm.\,. .\,.\,Well, this could also just double as the `importance' rating, meaning that.\,. wait, no.\,. .\,.\,Hm, or maybe yes.\,. .\,.\,Hm yeah, since this is not the ``data'' tab, we should not be worried about determining probability 
correlations or anything like that, so maybe `importance' is simply enough.\,. (10:06) .\,.\,Hm, yeah and you can say, \emph{if} the users are really serious about it, then the ultimate goal of the discussions would actually be to inform the readers in order to better be able to decide on how the ``data'' probability ratings should be given.\,. (10:11) .\,.\,Including what we can fittingly (as opposed to how I have used the term above in this section) call the `prior probability' of the discussed proposition to be true, i.e.\ when not accounting for any of the ``data'' at all. (And note that I use double quotation marks here to remind ourselves that this ``data'' can be much broader than measurements, which is typically associated with the term: It can also be what involved people and/or experts say and do in relation to the subject, and much more---it can be anything, really *(except for what we could call the `background knowledge' of the subject).) (10:16)

.\,.\,Hm, so that could actually be a quite simple discussion system, outside of the ``data'' tab. Then there would just be the summarizing comments tab, and the sub-discussions tab, which are mainly just rated according to importance---oh, and also how likely the users deem the statement of the given sub-discussion (each `discussion' entity is formulated as a statement (text-based, not tag--instance pairs)) to be true, of course.\,. (10.21) .\,.\,But this probability is only meant to.\,. or wait, could this be part of the ``data'' tab as well, or.\,.\,? \ldots (10:48) Ah, we could just use the same probability tag template for this rating, and then \emph{if} the given statement is also uprated as part of the ``data'' statements, then the rating automatically applies there as well.

(11:11) There should also either be a tab with comments about what the `background knowledge' is, or this should otherwise at least then be considered an important part of the job of the summary comments.


(13:50, 26.04.24) I'm thinking of calling the ``data'' `evidence' instead. And instead of rating all the correlations between each `piece of evidence' (which means four ratings for each pair among the group (since correlation are rated according to `if stmt 1 is true/false, what is the chance of stmt 2,' which is already two separate ratings, plus two more where stmt 1 and 2 are exchanged)), each statement should only have their correlation rated with respect to the main statement of the given discussion (which might be a sub-discussion, however). Then their should also be an attached, more loosely structured, discussion among the users of how the pieces of evidence might be correlated among each other. And then by taking the latter discussion into account, as well as all the individual correlations, and the prior likelihoods (i.e.\ the rated chances of the main statement being true or false given that one does not know/take into account any of the `evidence'), the user are then supposed to give a resulting rating of how likely they believe that the main statement is to be true (and they have to do this by being obviously informed by the numbers; the only reason for.\,.) Oh wait, no! Instead of rating the main statement as the `result' of the discussion (even though a discussion can always be reopened if new evidence appears---or background knowledge changes.\,.), the `result' should instead simply be a correlation rating for the \emph{combined} pieces of evidence (i.e.\ when what I called `stmt 2' before consists of the conjunction of all pieces of evidence being true there). (14:02)

So in other words, the users should only rate the correlations for each piece of evidence individually with respect to the main statement of the discussion, and then also---after having discussed the possibilities for the pieces of evidence being correlated between each other---the correlation for the combined evidence with respect to the main statement. (14:05) .\,.\,(And from there, the likelihoods of the main statement being true or false can be obtained mathematically, i.e.\ for each user individually *(i.e.\ in terms of that user's underlying beliefs).)


\subsection{*Continued on structured discussions (16.08.24)} \label{struct_disc_cont}

(16:04, 16.08.24) I guess a big question is: Do we want to aim for that idea of being able to deduce probabilities ``automatically'' from the sub-discussions, or should we just aim for having the related facts and discussions listed in order of how important they are to the truth/falsity---or rating---of the statement in question, and where the truth--falsity rating / subjective rating is then just shown for each one, which might inform the user in rating said statement.\,.\,? Hm, I guess we do want some kind of `rating the correlation between sub-statements and parent statement,' but how exactly.\,.\,?\,.\,. .\,.\,Hm, this calculation can then support the rating of the parent statement, but perhaps more importantly, it mean that it can be ``rated in advance''.\,. i.e.\ users can give ``dependent ratings''.\,. (16:11) .\,.\,Hm, and if we are thinking about subjective ratings (like most regular tags), then it could perhaps also signal the importance of the given aspect of the resource (movie, book, text, etc.).\,. (16:12) .\,.\,But I don't think that these two things should be mixed, actually.\,. .\,.\,(16:17) Well, if we are only talking about `importance,' then the systems could be mixed.\,. But `dependent ratings'.\,.(?) .\,.\,Hm, but couldn't you just not include the `dependent ratings' for subjective statements.\,.\,? (16:19) .\,.\,Yeah.\,. .\,.\,Hm.\,.

\ldots (16:30) I think the ``dependency ratings'' are actually always important, because it gives a user both the opportunity to reflect on the individual pieces that makes up a decision/rating, as well as the opportunity to show other users that their decision/rating is a reflected one. .\,.\,Yeah.\,:) Nice.\,. And then I think I should treat `predictions' in a separate way.\,. (16:32) .\,.\,Which therefore means that the `dependency ratings' should be about probabilities and predictions and such, but should just be.\,. well, they're a bit like the `importance ratings' except that they give a quantifier for \emph{why} the sub-statement is important, namely by giving a not-so-semantically-precise (either (like `importance')) value for the correlation between the sub-statement and the parent statement.

And then for `predictions,' this is a separate thing, on a level above the structured discussions (which now also includes subjective statements): A user group can declare themselves as being able to make predictions, and then go an and do that. And then other users/user groups can rate how good they are at making those predictions. .\,.\,Something like that.\,. And a user group can also use a `theory,' which means that the user group assumes that `theory' when making their predictions. This means that in practice, the same group of users can form several `prediction groups' at the same time, where each `prediction group' then assumes a different `theory' for how they should make their predictions. (A prediction group might also use a theory, even if they don't have more than that one.) An example of a theory, and a very short and abstract one at that (they can also be much more verbose), could be: ``all people generally always act selfishly on a fundamental level,'' or something like that (just to give an example). (16:44)
.\,.\,`Prediction groups' can of course also double as `discussion groups,' i.e.\ once we implement those structured discussions that I have in mind, where the `discussions' are dynamic things, that the participants have to take part in within a time frame, and where there are pre-agreed-upon referees that gives the participants points for how well they adhere to the rules and codes of conduct of the discussion, and for how well they argue their points, of course. (And not least how good they are at admitting when being wrong, which should count for a lot of code-of-conduct points.\,.) (16:48)

.\,.\,(16:52) Now, since `prediction groups' and such can wait, this then very much simplifies the `structured discussions' that I've been talking about in this overall section before (before this subsection). Then these are basically just an `importance rating,' which orders the list of sub-statements, plus a `correlation rating.' And this system can also even be used both for factual and subjective statements (e.g.\ `the dialogue of the movie is well-written,' etc.).
Now, how should I define this correlation rating, though? .\,.\,I guess: 10 (max rating) means that `this statement means everything for determining the parent statement.\,. in a positive way.\,. 5 is `it means nothing' and.\,. hm.\,. Isn't this just importance with two different signs?\,.\,. (16:58) .\,.\,Yeah, it seems like it.\,. .\,.\,Maybe `correlation' here should just be an estimate of how much each rating point for the sub-statement translates into points for the parent statement.\,. And do we then just assume that this correlation cannot be above 1, or below -1.\,.\,? That does actually make sense.\,.

Hm, on a not-too-related note, this makes me think about, should I go back to a -5 to 5 rating scale.\,.\,? (17:03) .\,.\,It does make more sense, actually, and the users are still just selecting a point on a scale of 10 stars, so yeah.\,. Why not use that scale instead?\,. And note that I don't have to change the database for this; I can still just use unsigned tinyints.\,. (17:05) .\,.\,Hm, and users can choose their own scales, at the end of the day, and then we can probably expect that a convention will be chosen at some point.\,.

Okay, so let me indeed say that for the correlation rating.\,. Well, but isn't this still just.\,. or rather, can `importance' then not be derived from this `correlation?' (17:09) .\,.\,Yeah, so the `importance' could just be derived by combining the descending-ordered `correlation' ratings with the ascending-ordered (same) `correlation' ratings. .\,.\,:) (17:11)

.\,.\,How simple is that?\,. So if we disregard the `prediction groups' and the dynamic structured discussions (i.e.\ what we could call `discussions contests'), then my `structured discussions' will now just be that for every statement, factual or subjective, there is a `correlation rating' (which denotes (a rough estimate of) the factor for how many rating points for the parent statement each point in the sub-statement is converted to), and the sub-discussions tabs (or whatever we want to call it) is then just ordered such that low-correlation and high-correlation sub-statements are shown first in the tab (and with the `correlation' rating visible and ready to be rated). (17:17)

\ldots (17:31) Oh, I guess we still need to consider correlations between sub-statements, and not least duplicates.\,. Hm.\,. .\,.\,Hm, how about voting for lists of `important sub-statements'?\,.\,. (17:36) .\,.\,That would take more work, but.\,. .\,.\,(17:39) Hm, another way would be to rate `correlated sub-statements' for each sub-statement, and if the rating exceeds a value, but has a lower ``importance rating,'' then we remove it or lower it in the tab.\,. Hm.\,. (17:41) .\,.\,Yeah, lower it in the tab, \emph{and} show a warning sign that it is correlated with sub-statements above it, with a button to then show what sub-statements these are.\,. (17:45)

So there are two ratings that determines the order: the `correlation' rating, and a `correlated statements' rating, which work more like a filter that drops a sub-statement further down in the tab if a correlated sub-statement (with a high enough correlation between the two) has already appeared above it in the tab. (17:49) .\,.\,(And the mentioned warning sign is also shown for these further-down-dropped sub-statements.)

(20:19) The sub-statements should also not be correlated to the parent statement in a way where the causality is reversed, so to speak. And in particular, it they shouldn't be a duplicate of the parent statement. We could use another rating to remove statements that are ``caused'' and/or semantically equivalent of the parent statement, but these should also just be given a `correlation rating' of 0 (5 on the 0--10 scale), always. (20:23)

.\,.\,(20:28) Ah, we should just use the `importance' rating on top of the `correlation rating' after all, which can then both help by removing the statements that depend on the parent statement, and also aid in removing some of the sub-statements that are correlated with more `important' ones.\,:) (20:31)

.\,.\,And the `importance rating' could by the way just actually be the `relevant sub-statements' (or whatever we call them) tab. (20:33)

(17:03, 17.08.24) By the way, on an unrelated note, the same kind of principle as the `correlated sub-statements' could also be used to filter e.g.\ correlated/overlapping subcategories (including semantic duplicates) in the subcategory tab of a category.

(20:18) The Bayesian calculation is still relevant, by the way, for factual statements. But this can just be a part of how the users are expected to rate the correlation score, i.e.\ from thinking about likelihood of the parent statement being true when the sub-statement is so, versus when the sub-statement is false.\,:)

(18.08.24, 10:35) In some cases it might actually be good to see several sub-arguments/ statements, even if they are much correlated. So I think it's better to just go by `importance,' and then have a expandable menu for each sub-argument/statement which also shows the same topmost important sub-statements, with the one in question subtracted, and where these are just ordered according to correlation with the sub-statement in question, and of course where this rating is shown.

(16:20) I just looked on www.kialo-edu.com, and the example on the front page was `Voting should be compulsory for all adult citizens' with one of the counterarguments being `Making voting compulsory would cost time and money.' This is not a very good sub-statement/-argument in my system, as it doesn't really give any idea of `if this statement is true, how will it affect the likelihood/rating of the parent statement?' In order for it to do that, it needs to be much more concrete. For instance `Making it compulsory would cost the USA around \$$x$--\$$y$.' (And the `costing time' argument could be its own separate sub-statement.) The there could be several ones of this kind of sub-statement, all with different \$$x$--\$$y$ ranges. Users can then discuss further the likelihood of each individual ones of these, and ultimately rate their likelihood. Then the most important ones of these will be the most likely ones. And note that this is also an example where sub-statements are correlated (in this case completely oppositely correlated), while it still makes sense to see several of them in the list of the most `important sub-statements,' at least until the cost has been more precisely estimated, in which case the sub-statements with the other ranges can drop of in `importance.'

Now, I also looked at www.kialo.com, and there seemed to be the same problem: The sub-statements/arguments are just overall topics, not precise claims that can really inform the decision of the parent statement if they are true or not.

So compared to Kialo, my system gives a much better opportunity to actually propagate decisions on likelihoods of sub-statements all the way slowly up to the top, where they can finally be used to also inform what likelihood/rating the root parent statement should be given. (16:34)

(16:42) Oh, I guess we should also for each sub-statement have a whole discussion page/tab for how that specific statement matters to the parent statement, rather than this just being an unspoken thing that each user has to decide on their own. So before you give the correlation rating (i.e.\ saying how many points the parent's rating grows for each point in the sub-statement (ignoring all its correlated sibling statements)), you might want to first look at the discussion page/tab about reasons why and how the given sub-statement impacts the parent statement. (16:46) .\,.\,(For some sub-statements, the correlation/impact is more clear, however, and users may not need to visit said discussion page/tab for these.) .\,.\,Note that this gives a little more room for some more abstract sub-statements, but it's still best to find as concrete ones as possible, where figuring out the correlation shouldn't require reading though much discussion (on said page/tab) first for the individual user. (16:50)

(17:53) Oh, the cost in this example can also be a rating on its own, i.e.\ where the rating denotes the cost rather than denoting a likelihood of a statement. This option could also work a lot of the times, i.e.\ when a statement has a parameter like that to it.

(19:39) The mentioned discussion page/tab from the last paragraph before the previous one can just be the statement that is automatically generated from the correlation property tag.

Another point: I think that it \emph{is} best to try as much as possible to make the list of the most important sub-statements be as independent(/uncorrelated) on each other as possible.

And a third point: Its also an important part of the system that the discussions aren't completely top down. Each sub-statement can also be seen as an independent discussion, and it can serve as a sub-statement for several parent statements. And it's also very important that a discussion graph is free to undergo a lot of changes, namely as users change up the various `important sub-statement' lists, substituting statements other ones in order to heighten precision, remove/reduce correlations, and of course also just to change less-important statements for more `important' (useful) ones. (19:47)

(19.08.24, 18:46) If some sub-statement ratings doesn't translate very linearly into the parent (statement) rating, then it is best to just try to redefine/reformulate them so that they are closer to having linear influence on the parent rating, i.e.\ in the eyes of most users (as many as possible).

(20.08.24, 15:03) Actually, the `sub-statements' should always be a tag--instance statement, so the user always have the ability to choose the right rating scale for the instance via the tag. If for example the instance is a true-or-false statement, then the tag should almost always be a `true' tag, where the rating is supposed to represent a likelihood. (So yes, you take a statement and construct a new ``statement is `true''' statement from it.) And if the instance is e.g.\ the cost of something, or another parameter like that, then the tag should be.\,. Well.\,. Hm, this actually begs the question: .\,.\,Should I replace `tag' with something else (`predicate' or something), where it is more natural to also have more complicated rating scales.\,. Or do we just e.g.\ define a `cost' tag, where the interpretation of the scale is specified in another field of the tag entity (other than its `title').\,. Hm, maybe we do this latter thing.\,. (15:10) .\,.\,Yeah, tags can have specifications of their own rating scales. And then for the `cost' tag in our example here, the rating scale could just be a linear one with a sufficiently high maximum (i.e.\ the highest cost on the scale) that is relevant for the given cost statement in question. (So a sufficiently high, yet not completely out-of-bounds, maximum cost should be estimated for the given argument, in this example.) (15:15)
.\,.\,Oh, and perhaps `likely cost' (on average) would be a better title to this tag. (15:22)






\section{Continued new thoughts}

(11:49, 28.04.24) I'm kinda considering adding a `statement' entity data type.\,. .\,.\,I could just choose `m' as the data type char, why not.\,. .\,.\,Hm, it could reduce the size of RecordedInputs slightly, as a small additional benefit.\,. .\,.\,Hm, let me just do it. I'm pretty sure, it's worth it.\,.

Hm, I guess we ought to remove the secondary index for SemanticInputs and just use RecordedInputs instead ad simply take the most recent rating.\,. oh wait, no, that's not possible unless we change the.\,. Nah, it's not possible: I should keep the secondary index for SemanticInputs.\,.

Hm, let me use the deleteRat bool for RecordedInputs instead of the nullable rat\_val, since this opens up for the potential of deleting user ratings, without deleting the knowledge that the users did upload a rating at that point.\,:)
.\,.\,Hm, should I actually just use the SMALLINT encoding, then?\,.\,. (12:20) .\,.\,I think I will.\,.

%(29.04.24, 12:46) On a very much unrealted note. Fumio Hiroshima has just written to me via ResearchGate and said that he thinks that my paper looks interesting, and that he will read it when he has the time. I'm very excited about this, needless to say.

(29.04.24, 14.59) I will actually drop the concatenated lists for the EntityInserter. We can instead just save the elemArr's in JS variables (and then concatenate them via JS instead), if we want to reuse parts of lists.

On a different note, in relation the thing about `giving it my all' with this project in the coming time: No. I will still work on it, but I think it's best for me to try to get some work experience in the job market (as a programmer) at this point, before I try to make the startup. So I'll try to maybe only program on the project in the afternoons/evenings from now on, until I get a job, and then I'll naturally have to do it even less than that. I'll still see, however, if this afternoon/evening plan will work out for me.\,. (15:06)


\section{SRC rather than open data?} \label{SRC_instead_of_open_data}

(03.05.24, 14:38) I'm actually considering another strategy rather than going for the open source-and-data strategy. The thing is, if the network really takes off enough to become an alternative to many of the mainstream websites, the users might be willing to accept a paywall, i.e.\ if it will also include music and film/video streaming.\,. %Hm, I'm a bit groggy right now, so I might just think some more and come back and continue writing later..

\ldots Or better than paywall, perhaps: Ads unless donating.\,. %..Wau (wow), jeg kan virkeligt ikke tænke i dag..

%(04.05.24, 7:43) Jeg skrev at jeg havde svært ved at tænke i går, og det havde jeg også, men jeg kom alligevel på nogle vildt gode og vigtige idéer. Først kom jeg på, at skaberne kunne udgive deres bidrag under forskellige licenser, som de selv vælger. Og om aftenen kom jeg på, at man kunne lave en slags dobbelt SRC, hvor skaberne er kunderne af den grundlæggende SRC, der skaber grundlag for det hele, og hvor de betalende/donerende/reklameseende brugere så kan regnes for kunder af skaberne, følgende de forskellige licenser, og hvor disse brugere så faktisk løbende også får en større og større rettighedsandel til det licenserede materiale/bidrag fra skaberne. Så i princippet er hvert lille bidrag på websites sin egen lille SRC.

(04.05.24, 7:50) I've had an awesome idea. Now I'm convinced that I should go for the SRC strategy rather than the open data one. The idea is actually to implement the website/network/website network as a kind of `double SRC,' where each contribution that a user (which might include the developers) makes to the site is given some license, which the contributing user chooses (from a variety of licenses). This could for instance be a license that the add money generated from the user watching ads in relation to also consuming the contribution, if this is a video or a music track e.g., goes towards the contributor/creator. Or it could be a license that says that only premium users of some level, i.e.\ behind some paywall, can consume the contribution, and then some of that money will go to the creator, perhaps based on time spent consuming the media (if it's a piece of media), or based on how well-liked the contribution is rated as, etc. I will return to what kind of licenses there could be, potentially. Now, the point is that every user that pays for using/consuming a contribution actually acquires a small fraction of the rights to that contribution, essentially in a way such that each contribution can be seen as its own little SRC---a micro SRC if you will. When users acquire some fraction of the right to a contribution, they thus also get a proportional voting power in terms of what license the contribution should be distributed under (and all such licenses can nemlig be changed over time, either at any time, or at frequent intervals). And they also get a proportional part of the income that the contribution generates. How fast the ``shares'' in the contribution is distributed out might depend on the deal between the creator and the main company that administrates all this and provides the web services etc., but it might also just be one rate for all creators.\,. Yeah, but then again, it might also be an individual deal between the creators and the administrating company, and might thus depend on what kind of contribution it is (is it a piece of music or a video, or is it a piece of code?, etc.). All the creators that thus earn money---or something else, but we'll get to that---via their contributions will then give a small cut to the administrating company. And, I've guessed it, the administrating company is itself an SRC, i.e.\ with the contributors/creators as its customers!\,:) (8:15)

Now, in the beginning of the whole thing, there might not be such a great income for the creators. .\,.\,Oh, but that doesn't matter in terms of how many shares are distributed to the creators, so that way we still get this nice situation, where the first contributors will be highly rewarded if the whole thing takes off, since they will have gotten a relatively large share in the main SRC without having done very much work for it, relatively to how it will be later on. And this is a good thing; it can help attract contributions early on.

Okay, back to the various licenses, and to the various contributions that could be on the website network. Let me start by saying that licenses which lets it be somewhat up to the users how the money should be distributed among the creators under to relevant license might turn out to become very popular among the users. %..Let me take a bit of food.. (8:23)
\ldots The thing is, the value of a given thing on the internet is not necessarily proportional to how many users view it and for how long. In fact, so many things, e.g.\ videos, articles, etc., are constructed in a way to sacrifice some of its usability/goodness at the cost of getting more users to view it, and for longer. This is how we have e.g.\ clickbate and needlessly long-winded articles, and more. And as another example, you might have some favorite songs that you savor, and thus maybe not listen too as much as e.g.\ the songs on your work playlist. The happiness/joy/good that the favorite songs give(/do) you might thus be more than the some other songs that you actually listen to more, and the goodness of these `contributions' (i.e.\ songs/tracks) might therefore not be proportional to the number of listens. So that's some of the reasons why it could be really good to have licenses where the users gets to vote on how the money should be distributed. This vote might then be completely determining of said distribution, or you could also have licenses where it's a mix: Creators may both get revenue from the number of listens, e.g., if we're talking music, or the number of views / the view time, etc., \emph{and} they may also get revenue based on user votes/ratings. (8:48)

So what kinds of licenses should the main company afford the creators? As many as possible, in principle, but of course, since these are complicated things, the company might start out with just a few, and perhaps quite simple, licenses. And these could work a lot similar as how existing web 2.0 sites reward their creators.\,. well, but hopefully less greedy.\,. But from there, the point is that since the main company always just earns a fixed cut of the creators revenue, these two parties should be pretty much on the same page in terms of what the main company should offer: They should try offer whatever will generate more revenue for both the creators and themselves, and that should of course include providing as many useful licenses that the creators can join as possible. (And just to clarify, when I say `licenses,' I'm talking about what deal the users, i.e.\ those that consume the contributions, should agree to in order to be able to view/use the contribution. For instance, they might have to pay by viewing some adds, either in the margins of the page, on the page, or during viewing the contribution (hopefully not), if we're talking e.g.\ a video, etc.)

What kinds of contributions are we then talking about? Well, everything! Even including the comments on the site, actually! So in a sense, every little comment on the site is its own little ``micro SRC'' in a way.\textasciicircum\textasciicircum\ And of course, we are also talking about videos and music, etc. And since the `semantic network' part of the idea should still be in play, we also talk about all entities created that make up the site. We might even talk about the ratings as well, now that I have the RecordedInputs table. And last but not least, we're talking about code contributions to the site. The site should be a UPA (or `UDA'), which means that over time, more and more of the source code that each user uses will likely be provided by other users in principle. In fact, the main company only have to create the foundation on which the UPA/UDA can be built, and from there, one might actually count all code contributions as `user contributions,' including ones that come from employees of the main company. (So you could actually have that developers hired by the main/administrating SRC is actually paid for their work both in regular salary, but also as a `creator' on the site/network.) (9:06)

I'm so happy for this idea to make it a ``double SRC.'' First of all, it's a massive idea in it's own right: This idea will be so important for how good the whole thing will be once it takes off. And in terms of selling the idea, I think this part of the idea might be even more sellable than the `ratings of all kinds of predicates' idea, and the `structured comments (and more)' idea. For unlike these other ideas, this double SRC (UPA) idea solves a problem that the users of the web \emph{already know that they have}. Users of the web already know that the conventional capitalistic structure always fucks them over repeatedly, since it always goes the same way: When enough users uses a website/network, they become reliant on it, and cannot just simply switch to another one, which then makes it possible for the company to try to squeeze as much money out of the users as possible, pretty much. But with the double SRC idea, the creators are first of all in more control of how they get their revenue from users, at least compared to mst conventional sites today, (in fact they are even in control over how the website looks and behaves, pretty much, since it's a UPA/UDA), and they can also rest assured that the company won't fuck them over.\,. well, first of all because they choose their own licenses, but also because the creators will own more and more of the company, and end up controlling a majority share after some 20--40 odd years (or maybe fewer than 20, depending on how fast shares are redistributed). (9:19) .\,.\,Oh, and another important thing that makes me happy about this idea, is that now it doesn't just offer excitement and a reasonable pay for me and my future cofounders, now it offers excitement \emph{and} we can even get rich (potentially very much so) from it. So all of a sudden, it's no longer a startup that's just visionary. It's also a good old startup with a great potential to get rich from it (both for cofounders and for potential investors).\,:) (9:23)

%(9:23) Forresten, andgående det med at jeg ikke kunne tænke i går: Jeg fik faktisk også en sjov lille idé til en klippemaskine i går aftes, som jeg så også lige forbedrede lidt her i min spisepause, nu her for lidt siden. Kan være jeg skriver om den her et sted i disse noter senere.

.\,.\,Oh, let me just mention briefly: If indeed there could be a difference in the share redistribution rate depending on the type of contribution, then I would think that music and films should be some off the things with a very low rate, whereas code contributions should be some of the things with a very high rate, i.e.\ such that the source code rather quickly changes ownership to the users, compared to many of the other kinds of contributions. (9:28)

(05.05.24, 9:03) I'm not so sure that this `double SRC' idea will work after all, as I described it, but I think I'm on the right track, going back to the SRC concept for the Web 3.0, rather than the alternative of just having `open source and data'.\,.

(10:10) Okay, how about this? It could be a single SRC where we have a group of owners, a group of contributors, including the providers of the web services, as well as the content creators and programmers of course, and then we have the end users. When the content creators and programmers upload something to the SRC organization, they get a period where they are able to remove their contribution again, but if and when this period is over, the SRC then owns the contribution from there. The end users, which are the counted as the `costumers' of the SRC (not the creators, actually), they pay, donate, and/or view ads, and when the do the latter, the money that the SRC generates from each ad view is counted as payment by that user/customer. Each user should be able to see exactly how much they have ``paid'' the SRC all in all, including when they watch ads, so that they can really start seeing ad viewing as the payment to the SRC that it is. How, does the SRC then pay its creators (including programmers) and its web service providers and other workers?\,. Well, however they want, in principle, but the shareholders cannot extract money out of the SRC, save for a fixed fraction of the total `sales,' i.e.\ the total amount of money that the users have paid, of course including the money that they generated from viewing ads (oh, and this means, by the way, that all deals with ad companies have to be view-based, in the sense that the ad companies and/or individual advertisers has to agree to a deal where the.\,. Hm, let me get back to this in a moment, actually.\,.).\,. So the shareholders always takes a fixed cut of the total sales, perhaps something like 2 \% a year, and the rest of the money has to distributed to the creators and the workers, or used for maintenance or expansion.\,. well, there should also probably be a fixed percentage that can be.\,. well, delayed.\,. .\,.\,Okay, simple enough: The SRC must not horde money. They can save up a reasonable amount, perhaps up to a certain factor of the total sales (averaged over the last couple of years, perhaps), plus some offset which will be beneficial at the early stages. And apart from that, all money has to be distributed. And how should the distribute it? Well, the shareholders decide in principle, but there should be an general commitment and a promise to take the users opinions into account of how the money should be distributed, even before the users actually become the owners (which they of course will over time, as is the concept of an `SRC'). And of course, the SRC should be free to take the opinions of the more paying users/customers than the less paying ones, and in fact, it maybe ought to promise to do this (giving a weight to the votes proportional to the money paid/generated). And back to the advertisers, the advertisers should essentially work as a service to the users that makes them able to pay and/or donate in an alternative way than by doing so directly. And the SRC is forbidden to sell ad space in any other way on its website(s), other than this way where all the money generated is counted as coming from the users who watches the ads. So there we have it, this could be one potential way to do it. But I will also think a bit more if there is a good way to.\,. hm, make each content/code contribution its own little SRC in a way.\,. hm, if this is even something that we want.\,. I'll think about, but otherwise, the above description might be a (very) good idea.\,. (10:45)

(10:54) Yeah, no, I think it's much better this way where the creators just give their contributions to the SRC organization after a certain period has ended (if the contribution has not been redrawn). And even though it means that the SRC then owns the content from there, it will still continue to pay the creators for it, with all likelihood, since this is first of all how to attract further contributions, and since this will probably also be the will of the users, whom they SRC has promised to listen actively too when it comes to the payment distribution, it will also be the way to gain and maintain favor with the users, second of all, which is also important of course, since these are the only source of income for the company. (11:00)

.\,.\,Oh, but maybe the SRC should also be able to distribute shares out to the creators?\,.\,. .\,.\,At least in the beginning, if not always.\,. Hm, how about just having a fixed amount of shares to redistribute the the `contributors' (including workers, etc.), apart from the shares redistributed to the users/costumers, and where the distribution of shares is just proportional to the distribution of pay.\,.\,? (11:04) .\,.\,Sounds like a good, and fair, idea.\,. .\,.\,(And it might also especially be a good way to attract contributions early on, of course, before the SRC has much money to pay the contributors.)

(13:33) Oh, we don't actually need to decide on a fixed protocol for how the creators give and/or license their content/works to the SRC. That can just up to the creators which deals they choose (and the SRC wants to offer a great variety of deals in order to attract as many contributions as possible). These deals are then just public, which means that both the users and the SRC can take them into account when they decide how much pay each creator should get each month (ultimately, this is a decision of the SRC, but recall that it should promise to be fair and to take the users' opinions (probably weighted with their money contribution) into account when determining the distribution).

And let me also mention, by the way, that the way that the SRC makes the content available to the users might also very well vary between the various types of content---and it might also very well depend on the deal/license with the given creator. .\,.\,Oh, and these deals might furthermore also include clauses about the payment, i.e.\ such that the SRC might commit to pay the creators a certain amount, perhaps depending how many views/reads/etc.\ the content generates, before it is free to distribute the rest of the money to the other users. So how the SRC procures and distributes content/code/etc.\ is thus completely free, as long as it of course doesn't favor contributors that are owners, and as long as it promises to listen to the opinions of its user base when deciding the money distribution, which, yeah, means that it is pretty much completely free. It just cannot show ads to the users unless the money coming from the advertisements are counted as coming from the users who watch/view the ads, it can only, and has to, give a fixed amount of money to its shareholders proportional to the total `sales,' it has to of course redistribute its shares to its users *(proportional to the money given/generated by the individual user), slowly, and as an added point, it also has to redistribute a small fixed amount of shares to the creators and workers as well, proportional to their pay, and lastly, it cannot save up too much money; it has to distribute most of its earnings to the workers and creators, i.e.\ after the shareholders has taken their small cut. (13:52)

I should also say that in the beginning, it's not super important to have restrictions of how the `costumer shares' can be traded on by the customers/users (but it should come later, in the true spirit of the SRC concept), which means that one might just implement the system early on, if this is easier, by having the company distribute out options to buy small amounts of the total earnings at zero cost. Each user and contributor can then just get a small option for each year in about 50--100 years going forward. This should not be too hard to achieve legally, I would imagine. (13:57)

%I also wanted to point out something else that I've thought about, and that is just basically that I hope we can get to a point in the future where companies sell/license plans for how to construct and operate their whole company, i.e.\ to make companies more open source, essentially. This is going back to my thoughts that I wrote about in relation to my old ITP idea. And yeah, I still really wish for this to be the future. Oh, and the same can be said about programming for that matter: Imagine a future where all this company/computer programming is part of a great, semantically structured database, where user can add all kinds of variations to each program (a ``company program'' or a computer/app/etc.\ program). Each contributor can be rewarded if their contributions are useful. And with this great database, it can then be very easy to make new startups, since they can just be built from all this knowledge. What a dream. But this was a bit of a digression, and I also don't think I managed to explain these thoughts too well, so let me actually just out-comment this paragraph.. (14:04)

(06.05.24, 9:31) Maybe it wouldn't make sense for the SRC to save up too much money anyway, for the shareholders who decides this are interested in increasing sales (over all the coming years), and not growing the company, which is what makes this type of SRC (where the share earnings per sales are fixed) special. *(Maybe it wouldn't be a bad rule to include for SRCs in general.\,. .\,.\,Ah, especially if the shareholders have the opportunity to vote to get a lower cut of earning each individual year (believing that this will get them more money in the long run).\,.) So maybe the clause about not being able to horde money is unnecessary.

(20:20) It's important that the contributions are structured semantically. It makes it possible to really see who came up with an idea first. And note that ideas for e.g.\ new design features doesn't need to be accompanied directly with code implementing it. Some users might contribute the ideas and other users might upload implementations for these ideas. And both of these groups should of course be paid for their contributions (as the users and the shareholders who listens to the users see fit).

Let me also mention, that the thing about making ad viewing count as payment/dona-tions to the website/SRC is so important. When the users have the UX of starting to see their ad viewing as payment, and when they can see exactly what it amounts to, I'm sure that this will change their view and how they feel about donating to the site greatly. I think that when the already get the experience of ``donating'' by watching ads, and can see how little they donate this way compared to if they just paid a monthly or yearly donation, I think so many users will be happy to just donate instead. And especially given that this donation also leads them to get more of a say in the network, by having their opinions count more in general, and also given that the donations also buys them shares (`customer shares') in the SRC. (20:31)

(8:58, 07.05.24) See the end of Paragraph 2 in Section \ref{SRCs} below.

I've thought about what my plans should be going forward, and maybe I've finally figured it out.\,. I worried that even though this `going back to the SRC principle' idea is very good, it might not be particularly easy to sell. (And the same can be said about the idea of start making ad viewing a kind of payment where the users can see how much they've paid/donated this way; this idea is also incredibly more potent than it might sound to a lot of listeners, I believe.) But I think I might be coming around on that. Maybe I should really focus on getting my SRC idea out there---now with the added point that workers can also be receivers customer shares to some degree---and then be open about wanting to implement the idea through a Web 3.0 company at the same time.\,. (9:05) .\,.\,If this idea could spread, and I think it might, then that could be exactly what could get the whole thing rolling (fast).\,.

(16.05.24, 9:08) I have to remember that (customer) shareholders cannot give themselves financial perks as customers, which also actually means that customers/users cannot get perks in general for having been a customer a long time etc. The only perks from having been a customer a long time should be the one that you get a part of the surplus, and voting power, in the company.\,.



(10:53, 18.06.24) I think it might be a very good idea to tokenize the contributions in this contributor--user (SRC-like) coop. And as a way of rewarding people who buy tokens, especially in the early stages, and also to increase the value of early contributions a little bit as well, the coop should have rate of interest for all the tokens such that the always increases by a fixed function time (as a factor) before the coop finally pays the token holder. So the idea is that the coop commits itself to always judge contribution from some rather abstract rules, which can however be changed over time, but where the contributions are never judged (in theory) based on \emph{when} they were done.\,. Well, you of course have to judge them in some ways in terms of `when,' 'cause the temporal order of the contributions matter.\,. Okay, let me think a little and get back to it.\,.

\ldots Ah, it doesn't matter. The coop will find out rules and procedures for determining the worth of the contributions. And at the end of the day, the donating users are the ones to ultimately decide, in a direct democracy (where the users \emph{can} use use representatives, or, more likely, subscribe to a set of rules and processes to determine the distribution). Bt no matter what they decide, there should be a interests on the contribution tokens, such that the longer it takes the coop to pay the contributor, the more they have to pay. This can then first of all attract token buyers at the early stages, and thus give us a way where the contributors can get a fitting pay before the coop has the means to pay all the early contributors. And at a later stage it also gives incentives not to take too long to pay the contributors.

Now, the tokens could be centralized tokens governed by the coop. Oh, and note that a tokens value is dependent on the contributor, who mints the token themselves (in principle), being the original source of the contribution. So any buyers also has to confirm that the contribution isn't copied from somewhere else in the public domain---or from something that's copyrighted. The coop should help with this process of validating contributors, at least at some point, but the whole point is that this allows the coop to postpone taking responsibility for this validation, until they have the means to help. It also means that contributors can stay anonymous, as long as their contributions is uploaded by a user profile that they can the keys for (passwords and/or encryption keys) to be able to authenticate themselves as the owner of that profile, until the time when the coop is ready to pay them. And only at that time do the users then need to contact the coop and give them their payment information---well, unless they simply sell their token on to someone who has a transaction channel set up with the coop. And if we implement the tokens via a blockchain (note that we can always change blockchain to mint the tokens on, and/or change to a centralized solution, etc.), the users can then sell their contribution tokens anonymously as well.

Then speaking of the potential to use a blockchain to mint the tokens on, this might actually also add an extra value of these tokens! For people might then collect them, even after they have been `paid' by the coop. Obviously the lose a lot of value when they are paid (generally an amount equal to the paid amount), but they might very well retain some value, especially the early (and important) contributions to the coop. For these are obviously more interesting to collect. And it only help the whole system that the very.\,. thing.\,. of collecting such tokens is actually something that does the world good. Here's the point: If part of the reason why this coop Web 3.0 takes off and grows is due to the.\,. collecting happiness of the token collectors, then doesn't this make the whole.\,. thing.\,. .\,.\,trend, rather! Doesn't this make the whole trend of collecting these tokens all the more exciting and joyous?\,.\,:) So that's the point why it might be a very good idea to implement these contribution tokens via a blockchain (perhaps Etherium). (12:13)

.\,.\,Let me also mention that useful reviews are important contributions as well. And when you tokenize the contributions, it boosts each user's interest in giving reviews that user users will subsequently like and deem as helpful. So that contribution token system could boost the users' will to upload helpful reviews of various things for each other (and of course also to upload helpful predicates, and helpful source code, etc., etc.) greatly.\,:)

%(12:44) Note that I still think that the idea of counting add watching as payment/donations \emph{from} the users, and to let the users see exactly the amount of money that they contribute by their add watching/viewing, is such an importent one. Okay, and with this latest tokenization idea as well, I think I'm now ready to first of all fix my vacuum paper and then get going with trying to argue about and find support for my ideas, while also starting to work professionally to earn some money and professional experience. (12:48)


%... (15:27) I went for a walk and had a few thoughts about the potential of a 'proof of public history' blockchain, or what we might call it.. ..I'm also considering `greenchain,' btw.. ...Something like 'proof of confidence (faith)' has also been on my mind, btw...
%(17:03) Hm, it's wild that PoS doesn't work at all, actually, without any reliance on a 'public history'.. ..Hm, but on the other hand, you can also just see PoS as implmenting PoPublicHistory in a roundabout way.. (17:10) ..As long as the users are aware of this and accepts it..
%..Yeah..

(20:45) Oh, it might also be worth holding on to a token if the first payment is later deemed to be too small, in which case the SRC should pay more money to whoever holds the token at the time.

%(19.06.24, 9:26) Okay, planen var at stå op og gå i gang med at fikse min vakuum-artikel. Men nu er der altså lige kommet nogle blockchain-tanker i vejen. (Jeg må også indrømme, jeg er lidt urolig indvendigt over at stå over for det her skel i mit liv, hvor jeg endeligt skal ud af denne lange (forlængede) idé-genererings-og-bearbejdelses-fase og i gang med at prøve at udbrede idéerne. Og jeg kan jo nu se, endeligt, at jeg ikke kommer nemt til det; der skal sikkert meget arbejde til. Så derfor kan jeg ikke lade være med at have tankerne kørende rundt omkring, om jeg nu har nok "skydts" med mig.. ..I går følte jeg, og jeg var nået til et punkt, hvor jeg var tilfreds, men tankerne kører altså stadig.. ..Nå, tilbage til blockchain:) Min angredsidé holder jo. Man vil kunne lave et særligt 51 \%-angreb, hvor man ikke behøver at spendere alle pengene, hvis man bare kan bevise over for andre brugere, at man har penge nok. Så kan man bare nøjes med at bruge nogen af dem, især i starten. Så man spreder bare en opdatering af softwaren, som er designet til at lade knuderne vurdere, om man skal mine på main chain, eller om man skal mine på en side chain. Uh, og man kunne endda i princippet starte med at give nogen penge væk gratis på en side chain, uden at det er et angreb, således at folk får lyst til at bruge softwaren. Og så er det ellers bare at lave.. Nå ja, softwaren skal så foresten kunne regne de minede penge.. Hm.. ..Hm nej, man kan jo betale for alle de minede penge på sidekæden, men hvor man så bare alrdig skal betale disse penge i virkeligheden, hvis angrebet lykkes. Og som ekstra-gevinster kan man så gøre så at medløberne kan, hvis de forinden har sørget for at oprette sig på the lightning network, få store ekstra pengesummer oven i hatten herved. Så dette angreb kan altså være rigtig potent, og det kan kun stoppes, ser det ud til, hvis brugerfælleskabet samarbejder om at forhindre det, hvilket vil være kosteligt for alle dem, der ikke står til at miste penge ved det.. Nå ja, og en måde at være medløber på kunne i ørigt også være bare at sælge coins på hovedkæden, hvorefter man så kan joine angrebet. ..Og ellers er den eneste alternative måde, man kan stoppe det på, ved at lave vedtægter omkring "offentlig historie," altså en konsensus omkring hvornår en kæde er så udbygget, eller rettere, hvor mange blokke (eller hvor meget arbejde, hvis vi skal være helt præcise) en kæde skal have bag på sig, før den bliver antaget som endegyldig sand fra brugerfælleskabet---givet at der ikke var andre nær så lange kæder kendt offentligt på daværende tidspunkt---og dermed ikke kan omstrides af et 51 \%-angreb senere. Men så ryger hele princippet om, at blockchainen består af ikke-samarbejdene, alle-mand-for-sig-selv-agtige individer. Og så er det jo, at man ligeså godt i stedet kunne bruge en dejlig, grøn PoPublicHistory (actual name TBD) -kæde. (9:57)
%..(10:02) Oh, by the way, if the community haven't yet planned how to deal with such an attack, but you think that a given blockchain community will figure it out at some point, then there could also be an incentive in attacking because it would likely drive the price of the currency down momentarily, due to the fear and uncertainty that the attack causes. Then you can buy currency once its value are down, and sell it again at a time when the blockchain has impemented an (automatic) way for the nodes to prevent 51 \% attacks in the future.
%(10:23) Okay, but reading a bit about Etherium again, I can see that this is actually already fully PoS now, and furthermore, the history of how ETC (Etherium Classic) came to be shows that not relying on the every-man-for-himself concept is not at all part of the whole philosophy that Etherium's community has. So Etherium seems to be the way forward, already..
%(10:32) Ah, the maximal script size of Bitcoin is of course limited (with no loops), and it's actually only 1650 bytes, so I'm not sure that it's even possible to make such an elaborate side chain, is it?.. ..Oh maybe it is if the same script can run for each subsequent block on the side chain.. ...(10:59) Oh, one \emph{should} be able to make arbitrarily complex sidechains, right?..
%(11:32) Oh, I might have mixed up what the Lightning Netowork really is. I do remember having read about it before, but I've somehow gotten the idea that it's a sidechain, which it is not really. Oops..
%(11:39) I think the attack should be possible, but now I've actually lost interest a bit again.. I'm just glad to learn that Etherium is already green, and going well..


(19.06.24, 11:51) Woah, I just got an idea for an alternative to commercials on the web.\,.\,!\,.\,. .\,.\,An alternative to watching ads.\,. .\,.\,Hm, which at least could work very well in my case where the ad viewing is seen as payment by the users.\,. .\,.\,Okay, this idea is worth a lot.\,.(!\,.\,.) \ldots (12:08) The idea is to have users be able to, instead of paying or watching/viewing ads, go to a catalog of products and select some they wouldn't mind trying to buy. Then they select the number that they want to buy. And depending on the implementation, they either get to buy it through a webshop, or they can go to a physical store and buy them. Then they have to be able to register their purchase.\,. Hm, okay maybe it requires some thought not to make this costly to implement.\,. But the idea is that the users can then confirm their purchase on the website and see their effective amount of ``paid/donated money'' go up (just like they do when they watch/view an ad).\,. .\,.\,(The money ``paid'' is then of course equal to the money that the company (in my case the SRC) receives from the given product-selling company for each such customer, as per an agreement (a deal) between the two companies.\,.) (12:18) .\,.\,Oh, and such a deal could also very well include a deal about not adding rival products to the catalog for the duration of the deal (otherwise it wouldn't make much sense, as the whole point is: buy \emph{this} company's version of this product, and you'll also support (and get perks from) this website).\,. (12:23) .\,.\,Hm, maybe a physical code that comes with the product, sealed in a way where you have to open/unseal the product (causing you to generally having to buy it (`if you break it, you buy it') from the store) to see the code, would be a good idea, actually, 'cause then it can also go the other way around, namely where you spot the product in the store, and decide to buy it due to the perks that you then get on the website. This way around is nothing new on its own, of course, i.e.\ to buy a product and get a code to redeem somewhere else, but the point is that this just adds to the value of the campaign, whose main focus is still the `first way around,' namely to attract buyers \emph{via} the website. (12:30)

%(20.06.24, 10:30) Some selling points:
% 1) Multiple and varied predicates for rating to better be able to express oneself, to get a better overview of any given thing, and also to be able to search more specifically on what you want.
% 2) Look at reviews of things and rate them for how they fit your opinions, then get your parameters for how you lie in the ML parameter space, and use it to in the future be able to see the reviews---and ratings---from the user groups that you fit the most first. Also, be able to view what various groups (segments) of users think of a given thing/topic, and get an overview of how the views differs between the various groups.
% 3) When you look up facts and explanations, don't just get the "top ones" first, as with e.g. Google, but get a rating (possible from various groups) of \emph{how much} the fact is believed to be true, or how good the explanation is thought to be. Also immediately be able to see all the sources, as well as the arguments and counterarguments for the facts, and not least also how much each individual argument/counterargument is believed to be true/valid by the community. The arguments etc. are then all ranked in terms of how relevant/important they are for the given fact/statement, and apart from this ranking, you then of course also see the rating of how believed those arguments/counterarguments/sources are.
% 4) A browser extension where you can immediately see ratings, relevant links and sources, and reviews/comments/discussions for the things that you browse on any other part of the external web, potentially.
% 5) A SRC-like creator--user coop where contributors are paid more, and where users get to pay more efficiently to the coop/company/SRC rather than watching long and boring ads which then amounts to almost no money per minute watched for the company. Also where the data, the algorithms, and the source code then becomes a shared thing, at least over time, rather than going to some greedy cooperation, and one that then has an incentive to be closed source. No: the SRC can be open source, pretty much, as long as the content itself just has somewhat restricted access, probably.

%(06.07.24, 17:18) I could also add a 6th point here about having a semantically structured internet, over things and their relations to each other, also including facts and discussions, not least. ..This makes it easier to find things, and it brings all people interested in a specific thing to the same place: You don't have to search for info and discussions about the same thing all over the internet; you can just search for it in one place. (17:23)


%(14:10, 17.07.24) Apart from paying/donating through veiwing/watching ads, users should also be able to pay by allowing companies to boost positive reviews and show commercials when you lookup things related to them and their products/services.



\ 

(13:21, 19.07.24) Continued from below. About the `backward payment' system: If the movement is slow politically, then my `Semantic Web as an SRC' idea here can hopefully help, as it is a way to get going with a similar kind of movement, only where all the intellectual products of the organization are not in the public domain, but are licensed instead. This limits the system a bit, but we could still very well get a situation were almost everyone will have a paying account for the software, knowledge, music, videos, and other kinds of digital creations that the organization controls the license to, simply because a person would lose out on too much if they don't have a paying account. (And remember that ad-viewing is also considered payment by the user.) So yeah, if the political movement is slow, this Web SRC movement can be the way to help the development along. It's not the absolute \emph{ideal}, but it might still work almost as well in practice, if we can get it going. (13:30)

By the way, let me just reiterate the point about using NFTs for the ``obligations'' of this organization (see what I have written below today). For people will then naturally be interested in collecting \emph{especially} the NFTs for the early contributions to the organization in the future, which means that clever people, who can see the future that this movement might bring about, will be much further interesting in contributing at the early stages of the development, even when there is not much general knowledge about. (13:33)

(20:46, 22.07.24) Let me also note that a Web 3.0 SRC could be the beginning, and then if at some point a governmental ``backward payment'' system is implemented, for enough countries (working together), then the SRC could just give its pool of IP out and make it open source, and then start to get backward-paid by governments rather than by the users directly.



\ 

(12.08.24) Let me just highlight these notes from the source comments:

\noindent``\%(20.06.24, 10:30) Some selling points:

\noindent\% 1) Multiple and varied predicates for rating to better be able to express oneself, to get a better overview of any given thing, and also to be able to search more specifically on what you want.

\noindent\% 2) Look at reviews of things and rate them for how they fit your opinions, then get your parameters for how you lie in the ML parameter space, and use it to in the future be able to see the reviews---and ratings---from the user groups that you fit the most first. Also, be able to view what various groups (segments) of users think of a given thing/topic, and get an overview of how the views differs between the various groups.

\noindent\% 3) When you look up facts and explanations, don't just get the "top ones" first, as with e.g. Google, but get a rating (possible from various groups) of \emph{how much} the fact is believed to be true, or how good the explanation is thought to be. Also immediately be able to see all the sources, as well as the arguments and counterarguments for the facts, and not least also how much each individual argument/counterargument is believed to be true/valid by the community. The arguments etc. are then all ranked in terms of how relevant/important they are for the given fact/statement, and apart from this ranking, you then of course also see the rating of how believed those arguments/counterarguments/sources are.

\noindent\% 4) A browser extension where you can immediately see ratings, relevant links and sources, and reviews/comments/discussions for the things that you browse on any other part of the external web, potentially.

\noindent\% 5) A SRC-like creator--user coop where contributors are paid more, and where users get to pay more efficiently to the coop/company/SRC rather than watching long and boring ads which then amounts to almost no money per minute watched for the company. Also where the data, the algorithms, and the source code then becomes a shared thing, at least over time, rather than going to some greedy cooperation, and one that then has an incentive to be closed source. No: the SRC can be open source, pretty much, as long as the content itself just has somewhat restricted access, probably.

\noindent\%(06.07.24, 17:18) I could also add a 6th point here about having a semantically structured internet, over things and their relations to each other, also including facts and discussions, not least. ..This makes it easier to find things, and it brings all people interested in a specific thing to the same place: You don't have to search for info and discussions about the same thing all over the internet; you can just search for it in one place. (17:23)''

(12:52, 23.09.24) I think I should actually not make it a SRC-\emph{like} company. I think it should be an SRC, straight-up, of course where we still define the customers as the users, even though a large part of the money will probably come from ad-selling companies, and thus only indirectly come from the users in principle. But yeah, maybe I shouldn't mess with the SRC concept. But! The shareholders can at any time make deals to also give out part of their shares to IP contributors.\,. Oh, so there do need to be a provision that you can.\,. Hm, vote on a part of the shares to give out to.\,. Hm, no.\,. But maybe the shareholders can just make some other contract, owing some part of the revenue in a period, and then these will thus become the `IP contributor shares'. That could work, I guess.\,. (12:59) .\,.\,Yeah, 'cause that is just an expenditure.\,. Wait, doesn't that mean that my SRC idea is flawed.\,.\,!\,.\,. .\,.\,For can't the initial shareholders not just.\,. Oh no, I forgot: The initial shareholders cannot make any deals that prioritizes themselves.\,. *(Nor can the customer shareholders when they get to rule the company, for that matter.) Hm, but what about shareholders that work in the company, such as a CEO, or something? (13:03) .\,.\,Their vote must then not be used as part of determining their wages. Okay, so that should be clarified for the SRC's, whenever there's a decision to vote on that can benefit all or part of the shareholders on the behalf of the company's expenditures, then the votes of that group should not count. (13:07) .\,.\,Oh wait! I forgot that the money owed on the shares are proportional to the \emph{sales}. So never mind; expenditures doesn't matter. If the expenditures increases, that means that.\,. wait.\,. .\,.\,No, never mind: If they go up, then you either raise the prize or cut down, of course after also using of the reserve capital. .\,.\,Yeah, that's all good. And therefore, the shareholders can at any time make deals of special payments.\,. Oh, but the whole point is about the votes.\,. Ah, but any shareholder can willingly lend their vote out to others, right?\,. So.\,. Hm.\,. .\,.\,Okay, forget about the votes, it's not about the votes. The `costumers' gets the votes (together with any initial shareholders and/or earlier costumers that still have shares). But the company can at any point promise out part of their sales paid to the IP contributors, and a similar kind of way as how future earnings is divided among customers via the `customer shares,' but where there's however no vote following along (since I think that that is too complicated: too much to think about when formulating the rules of the SRC). (13:27)

\ldots\ The initial shareholders are free to vote to redistribute their shares to (IP (and other things)) contributors, so that we can get the SRC system for contributors as well. But when the rule that customer shares can't be sold kick's in, the automatic redistribution to contributors will have to stop, and then be renegotiated and reorganized. (16:09)

.\,.\,And I \emph{do} think that we ought to do this in the beginning: Redistribute to the most upvoted contributors as well, alongside the users (measured in the revenue that these generate).

(9.57, 24.09.24) I do want to distribute shares to contributors as well in the early stages, and in an SRC, we are also free to do that, for any shares are free to be traded before a certain date, and the initial shares are always free to be traded. So we can just make a deal where we also distribute shares out to contributors (and where x amount of share has to be distributed for each month). However, we don't have to decide on the distribution right away; it can just be owed. And then I'm thinking that we could make the distribution slowly, such that at the end of the given month, the company has to decide on a distribution to hand out a small fraction of the owed contributor shares. Then after the next month, another fraction is handed out, potentially with the same distribution (and it often will be repeated), but the point is that now the distribution can also be changed, to account for the added knowledge of how useful the contributions has turned out to be. And so on, so this way, the contributors can still get a better and better idea of how much they will be payed in total for their contribution, and get a decent idea pretty early on, but we still get plenty of time overall to judge the usefulness of each contribution.

So yeah, I definitely want this. I'm certainly gonna try to do it with my own shares at least. (10:05)

(12:16) I forgot to write about how much I think the (extended) SRC idea could mean for the Semantic Network. The big point is this: When you either donate (or watch ads) and/or contribute to the site via scoring things, uploading new entities, and/or committing code/style/designs to the GitHub repo (or take active part in the development in anyway), then this will both yield you more costumer and/or contributor shares in the company (which will start to require the IP rights of the contributions, I think.\,. Not that you can easily copy a network, but still, it's good to guard a little against copycats in the very early stages, I think.\,.), \emph{and}, and this is the point, it will \emph{also} at the same time increase the value of those shares, since the prospects of the company will grow with each donation and contribution. So the value that the users get from contributing is having a positive feedback loop with itself, so to speak.

And since the total amount of shares redistributed (or owed) is constant, then it will likely really make the first contributors and user donors rich, since the will have easy access to something that will hopefully grow incredibly in value. And for those that do contribute, these will then have an interest in keeping the contributions up, both to get more shares, and also to make the ones that they have already gotten worth more (both then and not least in the future). Also, this will greatly deter users from trying to make useless contributions, and greatly motivate them to make as useful contributions as possible instead.

Once a large enough group of contributing users can see where things are going, then I don't think there's anything stopping the network, since these will then all have very high incentives to keep going, and not least also to do user-to-user PR for the network and try to get more users to join. So I think that the SRC idea really will make the critical mass much lower for the network. And the fact that the earnings will be distributed to the community rather than just go to the initial innovators will only much increase the interest in the network, I'm sure, and what's more, the prospect that the whole thing will be owned by the users (and contributors in the near future at least, unless the terms are renegotiated to keep the distribution `contributor shares' a part of the system) will also increase the interest greatly, I think. .\,.\,Yeah, I really think that this will boost morale (and confidence, of course) in the network be \emph{so} much, creating so much more interest and positive attitudes towards the whole network and company. (12:37)

.\,.\,And yeah, let me repeat: I think that this will make the critical mass really low for the network, which is everything that we can hope for.\,:)

.\,.\,Wait, I have to aks myself, then, should I really start to go close source, then, soon or at any point at all?\,.\,. (12:40) .\,.\,Ah, maybe not: The thing is, people would then have to copy the SRC idea to make it able to compete, in which case we can work \emph{with} such parties.\,. Of course, this is \emph{my} opinion (that they'll need to copy the SRC thing), so do we still need to do something to make the early contributors more confident?\,.\,. (12:42) .\,.\,Oh, we don't need to pool the IP! Now that the contributors (and donors) will have this great interest in the success of the network, they can just hold the IP themselves, and be confident that there will be no real reason for their fellow contributors to give away their IP to competitors as well. So by just making sure that the users keep the rights to their contributions, then we already.\,. have enough.\,. Hm, but we \emph{would} like to pool the IP still, don't we.\,. well, maybe not.\,. (12:46) .\,.\,Yeah: maybe, maybe not. We don't \emph{need} to do this since the contributors will be shareholders as well with the SRC idea, but we could do it anyway. We'll just see. It's very nice, however, that we don't need to pool the IP, and that the company.\,. Ah well, the early contributors could them still in principle mutiny against the initial innovators, so yeah, maybe we do want to get IP rights to all source code developed by any hired web developers, and maybe we would also prefer to require the contributors to agree that their IP of their contributions is pooled in the company in exchange for the right to earn contributor shares from those contributions. I think that might be best for all parties. (But I don't plan at all to try to make it more lucrative to be a hired web developer rather than a (freelance) user contributor, if the contributions are equally valuable (unless I need this in order to be able to attract hired web developers at all), and I don't plan on hoarding shares myself. So we should be fine: We should be able to make a network, where the contributors, especially the early ones, are very handsomely rewarded.) (12:53)








\section{*Continuation of the project}

(15:16 12.08.24) I think I will start working on the Semantic Network Project again every other day, between job searching. And I just had a delightful thought: Wouldn't it be both fun and nice to implement the AppColumns such that they are fully defined by `property documents' (or another object ML)?\,. (I by the way think that I will depart from the idea of showing several columns at once on computer screens (that are wide enough), and just always show one at the time, maybe showing gray/transparent versions of the neighboring columns to the side, if there is room, which the users can then click on to center and activate.)

And then already from there, I could then start designing things ``as a user'' (while of course also developing the possibilities for custom-designing columns further in the process).\,.\,:)

.\,.\,The users should then be able to give the modules and HTML elements classes, and should then also be able to upload (and up-rate) CSS documents (which are sanitized). Uploaded HTML is of course also sanitized. And I'll just make this sanitization very strict at the beginning, allowing only for pretty basic HTML and CSS. .\,.\,Modules should also be defined as semantic objects, such that they appear as such in the column mark-up property documents. (15:39) .\,.\,Data can be passed to the newly opened columns when clicking a link, i.e.\ similarly to REACT properties.

(16:03) Hm, the property documents should actually also have an explicit spread operator such that they can be saved more compactly by inheriting from an existing property document.\,.
.\,.\,(16:13) Hm, should I even add a nullable parent object to AssocEntityData?\,.\,. .\,.\,Yeah.\,. (16:14)

(20:43) Hm, I'm thinking about collecting a lot of my data types (formerly: ``metatypes'') into one, and that might actually especially be possible if I make the standard entities have inheritance, which I intend to.\,. .\,.\,Ooh, and even better: I could implement formal entities and such via \emph{classes}, which then has the searchable properties as input to a ``constructor''.\,. (20:49)

%\ldots (21:04) Hm, how about `EntityClasses' (which are also entities, in principle), together with the standard entities, which can perhaps (or perhaps not) be divided into formal and normal object entities.\,. and then how about this: What if simple entities just had their title as their ID/primary key in the database?\,.\,. \ldots\ (22:37) Nå nej, I can't do that.\,.

%(13.08.24, 9:46) Okay, jeg er lidt spændt på at komme i gang med dette project igen, og på hvilke ændringer jeg skal lave---og jeg tror det er en god idé at have et friskt hoved til de ændringer, så jeg udskyder altså lige jobsøgning (som jeg tænker at gøre hver anden dag, om formiddag--middagen, mindst) med én dag, og lover mig selv at jeg tager ordentligt fat med det i morgen.

(13.08.24, 9:54) It made good sense to just introduce as many `entity datatypes' as one wants, since it ensures that it all works well. However, it is also a little bit involved, and now I've come to think that there might be a simpler way. This is essentially going back to what I tried before with the type--context--defStr entities, i.e.\ making some one-size-fits-all data structure, but this time around I think I can do it better.\,. .\,.\,(10:00) I'm thinking that each object is defined via a prop doc, which I will btw rename to perhaps `struct,' which can have placeholders for searchable ID's (used to implement e.g.\ propTags and formal entities), as well as at least one placeholder in case the entity is defined by some stand-alone text or blob.\,. (10:03) .\,.\,Oh, and we have inheritance such that an entity can copy another entity's properties.\,. .\,.\,And then for the propDocs/structs, these \emph{shouldn't} consist of only numbers, but can use strings/simple entities directly via their secondary key, i.e.\ the string itself.\,. (10:10) .\,.\,Hm, do we even need simple entities, then?\,.\,. .\,.\,No, I don't think so.\,.\,:) (10:12) .\,.\,That's pretty rad.\,.

.\,.\,When you make e.g.\ a tag, you just make it inherit from a tag (class) entity, and then you write the string. Easy. Okay, this \emph{is} pretty awesome.\,.

.\,.\,I think I will use JS syntax for these structs (plain objects), thus wrapping string properties.\,. hm.\,. .\,.\,Yeah, maybe I will do that, and maybe I'll even use JSON.\,. .\,.\,Yeah, JSON is a very good option, actually.\,. (10:25) .\,.\,It would be nice if we could use numbers for IDs, well and we can in principle for small numbers, but for larger ones there also needs to be special string syntax for this, along with the data placeholders and searchable ID placeholders.\,.

.\,.\,Ooh, for Users and Bots why don't we just make a bot that rates who the actual (active) Users and Bots are, but let it be unrestricted for users to create their own (fake) User and Bot entities.\,!\,:) (10:32)

.\,.\,(10:37) I thought about whether searchable entities should get their own table/view/ index only of them, but then I recalled that we also need the other entities (i.e.\ with their own struct-defined properties (not defined by the separate searchable IDs column(s))) to be unique. So I should just make it all as \emph{one} Entities table.\,:) .\,.\,This table should have the columns: parent, searchable IDs (might be split up) (nullable), hash of own struct (nullable), hash of data (text/blob) (nullable), as well as the columns containing the actual struct and the actual data, if any. Apart from these latter columns, there should be a UNIQUE INDEX on all these columns in that order.\,. (10:44) Okay, let me get to it.\,.

(11:32) Oh, I also have to think about how to differentiate between one-to-many properties and list-valued properties.\,. .\,.\,How about just wrapping the array like this: `\{set: my\_array\}'? And then I can use a similar syntax for other things, such as e.g.\ overwriting or adding property values of/to the parent. (11:37)

(16:28) With this new way, I can't make restricted data types the same way, but that's fine. We can just either let the browser sanitize, and/or create bots that rates if some data is sanitized (i.e.\ has a given format) or not.

I think I ought to be minimalistic with the class entities. For instance, I will just let the User class be `\{type:``user'', username:``\%1''\}', and the simple tag class be `\{type:``tag'', title:``\%1''\}'.\,! This is instead of trying to be specific in order to prevent future confusion. No! It's better to make simple definition, and when future ambiguity arises, then it is just up for the newer classes to be more specific as to not be confused with the regular old ones.\,!\,:) (16:33)

(16:41) Oh, I don't get that nice automatic rating of all the properties.\,. Oh no, never mind, I can still create propTags automatically and rate them.\,. *Well, what about string-valued properties?\,.\,. (16:59)

(16:44) Hm, instead of the `\{set: my\_array\}' syntax, I could perhaps also use a double array for ordered lists.\,. .\,.\,Or use a `\{list: my\_array\}' syntax for that matter, but yeah, I think the normal interpretation of an array as a property value should be the one-to-many property interpretation. .\,.\,Let me indeed use the double array syntax, as it makes very good sense: If there are several arrays in the outer array, then there are just several lists that are true as values for the given property. (16:53)

.\,.\,``Well, what about string-valued properties?\,.\,.'' .\,.\,Oh, I'll just use a `\{title:``\%1''\}' class, why not?\,. (17:01) .\,.\,And then you are just not supposed to use the IDs of such ``simple entities'' in the prop structs; just use the title directly instead. (17:04)

%(14.08.24, 13:52) Hm, der er lige det ved det, at nu JSON-encoder jeg JSON (når det sendes til klienten).. ..Men gør det noget, rigtigt?..
%... (15:44) Jeg tror jeg vil lade spec_input være en '|'-spareret streng, og så vil jeg bare indføre en obj type i control-laget, som så skal encodes til JSON før det sendes videre til databasen, og som skal parses tilbage, før det igen encodes lige inden det hele sendes til klienten. ...(15:56) Nej, jeg kommer vist ikke uden om JSON-encoded JSON sådan heller.. ..Tjo, for input skal jeg jo ikke JSON-encode, så der går det fint. Og for query, der kan jeg bare indsætte parsing før den sidste encoding..

(15.08.24, 9:44) I'm thinking of implementing basically an ML (via entity propStructs) for making boilerplate REACT modules. This should just include some basic HTML (and choice of custom CSS classes, which can then be styled by other uploaded CSS entities), as well as ML for what props the module has and which of its child modules receive what props, as well as what contexts the module initializes, and what contexts it uses. All this is then transformed to an actual REACT module that does exactly these things. And whenever a module gets a prop or a context, the module then.\,. hm.\,. (9:50) .\,.\,Nah, that should be implemented via code anyway, i.e.\ that the module then ``uses'' both the prop and the setProp(). And this also means that the ML doesn't have to declare when a module \emph{uses} a context; it is just free to do this whenever (along with any global variables). It only has to state when a module initializes a new context. (9:52) .\,.\,Oh, but the setProp() is also always handed along together with a prop (despite if the module uses it or not). Well, I think so.\,. Or should the ML just also declare that?\,.\,. (9:54) .\,.\,Nah, just hand it along always, why not?\,. (9:56) .\,.\,Yeah, at least as the default behavior (unless otherwise specified, perhaps in a future version).

%(16:41) Hm, do I insert lists with all elements substituted for IDs, or do I keep the simple entities as strings?.. ... (18:22) Yeah, I \emph{will} just insert only the IDs (for lists, not for sets).
%(20:06) Hm, it doesn't.. often.. make sense to uprate lists, so I might not insert them automatically by default. Also, I'm almost thinking that maybe there's a possibility that the set/list interpretation could just depend on context.. But anyway, let me think.. ...(20:26) No the thing is, when you have a list of something, which is not just a set, then it is typically part of some other whole---I guess unless we are talking constant InstanceLists and such.. Hm.. ...(20:43) Oh, because only the outer array is interpreted as a list, it works fine: We don't need double '[[]]' for each nested list in a multidimensional array/list.
%..Okay, I think I shouldn't upload (or uprate) lists automatically.. ..Hm, and I also shouldn't upload the values inside lists automatically, then.. Okay..
%*(17.08.24, 9:11) I actually will insert the first lists, i.e. at "depth" 1. But I will not insert or uprate any properties for these lists.

%(16.08.24, 11:04) Okay, I actually \emph{need} to remake my prototype and get it up and working before I seek professional work (maybe a part-time job could work, but I don't think it's worth searching for..). 1) Then I can show it to whoever, which is something that I very much want, and 2) I can show it in the job interviews. ..And I probably have money enough for a few months more. *Maybe I'll search for a part-time job first.. (16:02)

(16.08.24, 13:32) Hm, what do I do about texts containing entity (ID) references?\,.\,. .\,.\,Well, I just make it a convention that '@' is a special character in texts, used to refer to entities, and perhaps even other links.\,. Hm, or maybe we should use some other mark-up, perhaps MD.\,. (13:42) .\,.\,Or a subset of HTML.\,. .\,.\,Yeah, in the long run, inserting URLs (including entity references) via some ML is a better option. (13:44)

*I have to still use the same `@$<$entID$>$' syntax from before, and indeed make `@' a special character. Then at a later point, we can always just add MD or HTML on top of this syntax. (12:03, 17.08.24)

(17.08.24, 11:56) I knew that my new SDB structure was in a lot of ways pretty close to my `type, cxt, defStr' one, but I have just realized that this means that I can reuse very much of my current prototype. So it won't be too much work to get back to where I were. The EntityTitle should then just be parsed from the full propStruct, first looking for a title, and if none is found, looking for the type instead, and just call it `$<$type$>$ \#$<$entID$>$.' .\,.\,And fot the submission fields, I will also just do something similar as before. I could at some point add a submission field that uses the EntityInserter in all its glory, and with a field to provide a entKeyIDStore as well, and where a new entKeyIDStore is then output after a successful submission. But that can wait, \emph{if} I do it at all.

Once I've gotten to where I were with my prototype, I should at some point do the following changes: I should make the interface use only 1 column as standard, and make the whole scroll experience better, and make it work on mobile. I should also replace the Info tab with a tab that show `sections' instead, which are entities that include their own section header, as well as content. I should also make it so that the tabs are looked up for the given type of the entity. When an entity has more than one type (just looking in the full propStruct alone for now, not in types that are up-rated later on), I will make the first line of tabs be a list of types, such that when you click a type, you see the tabs relevant for that type (and these are on the other hand up-rated ``semantically'').

I should also at some point make that thing about being able to mark-up whole REACT modules, give the elements classes, and not least upload and use custom CSS. But that can all wait a little bit.

So this means all means that it won't be long before I can get a new prototype up and running again. (And I will actually make a new website running it (with a new domain name).) And becasue I don't think that this will take long, it also means that I feel like I can start my search for professional work already now, like I had planned.\,:) (12:14)

%*(17:06) Okay, so I think that my plan now is actually to find a job while also focusing heavily, in the coming time, on my direct/e democracy party idea, and while then continuing this semantic web/network project in most of my remaining sparetime.

And once the prototype is up, I can just make it better from there one step at a time.\,:)

.\,.\,Oh, didn't I want to just uprate stuff like `relevant tabs' for the parent class instead of for the types now?\,.\,. .\,.\,Well, maybe nah; maybe types is still the thing to use there.\,. (12:19)

%(12:55) Hm, I want to be able to substitute entKeys \emph{within} a value string, but I guess I should use '[]' then for entKeys.. ..Hm, this is not a completely easy fix, let's see.. ..(13:00) Ah, I could use "" instead of [] and then JSON.stringify the string.. ..Yeah, and just remember that stringified strings start and end with ".. ..Yeah.. ... (16:10) I'll just use JSON string syntax for all entKeys.

(10:42, 18.08.24) I should also make tabs be defined from entities as one of the first things on the to-do list. But I'll just make it in a bit simple way at first, and thus not make that whole HTML-with-modules mark-up language the first time around. But note that the tab header should then be decided by this entity (as well as the relevant InstanceList and its props).

\ldots I think I will try to take the relevant tabs directly from that instance list, and then just try to make an expandable tab list, maybe one that expands downwards.

%(18:17) Hm, det bliver ret grimt hvis jeg har "" i entKeys, når disse er indkapslede.. nå nej, EntityInserter tager jo objekter, ikke JSON, så vi får ikke "" omkring f.eks. property keys, osv. Okay..

(21.08.24, 8:34) Okay, a few ideas. I think I will do the same for data\_input texts as for spec, which is to use `$|$' as a separator. And the placeholders will then be `\%t1' and `\%t2' etc. These can then be used for all `text field' properties, which are only meant to be read by humans, namely since these texts are not queried for in the initial construction of the (full) propStruct. I've also realized that I don't really have to sanitize data\_input server side, since React still sanitizes it always before printing. So that's nice. About binary data\_input, I'll use the placeholder `\%b', as to tell the app not to print it. Binary files should actually always be included in special format-specifying binary data classes as a standard (and only in those, i.e.). So users should actually just use some developer-made (/super-contributor-made) classes in the case of all binary entities. And if the then want to use that binary as (part of) a property value for another entity (including e.g.\ just a file entity with more meta data defined in the propStruct (although one can also define this (``semantically'') via semantic ratings)), then they just use the ID of the entity of the given binary-with-format class, rather than using the `\%b' placeholder and the data\_input field.

Another thing: I want to re-introduce categories as a special type of tags. I will then use a convention of capitalizing those. And for the class entity of these category entities, I will just add a (`text-field,' human-only-readable) description that specifies the rating, as well as the capitalization convention (and the purpose of categories). This rating is btw not trivial to explain, since it actually depend on what other tag instances there are in the instList (and the same can be said for its `subcategories' property).

I also want to reintroduce `full titles,' where as a standard, the last type in the property set (of the propStruct) will be displayed between parentheses. If a `disambiguation' flag is also set, the `disambiguation' property will also be printed first in said parenthesis, with a comma after. And if this flag is set when the `full title' flag (which I will call something else, like `showType' or something) is not set, then only the disambiguation is shown in the parenthesis. I then actually want to use this `showType' flag only when hovering over the entity, or perhaps when there are room, but not necessarily for AppColumn headers, as the types will also be displayed below in the first list of type tabs.\,. Well, but maybe it's nice to also show the last type in the header. Okay (I'll see, though).\,. .\,.\,Or do I drop the `disambiguation' and just use the last type in the set, like I have thought of?\,.\,. Hm, maybe I'll just combine the flag, and thus always print the `disambiguation' first in the parenthesis if one is there. Okay.\,. (9:02)

.\,.\,Hm, maybe I want to use entities rather than just strings for types.\,. (9:10) \ldots\ (10:46) Hm, alternatively, we uprate the descriptions semantically.\,. Hm, or make use of both options.\,. (10:47) .\,.\,(10:55) Maybe I'll just call it the `initial description' instead.\,. Ooh, and perhaps then make EntityInserter uprate it as the `description' as well, whenever it sees `initial description' as the property key. (10:57) \ldots (Yes, I think I will do this.\,:))
*Oh, this actually doesn't work so well, 'cause the description would be tied to a class, and classes are meta entities: A class' properties are never its own.(!) So we need to indeed use entities for types, and then we just give these types the initial description. Note that this also means that the App should generally use types.\,. Oh wait, class entities can still have semantic properties. And they can be the value of properties and instances of (tag-)instLists. But using types more so than classes to define e.g.\ EntityPages seems like the better option in most cases, still.\,. (11:36)
\ldots (11:59) Hm, now I can see why I've been gravitating towards using types in place of my new classes.\,. But I guess this new system is still better.\,.

Hm, what do I do with inner links in (full) EntityTitles now.\,.\,? (11:10)

\ldots Hm, would it make sense to define classes in a separate table.\,.\,? Or give the Entities table a class flag, that's probably better.\,. (12:03)

.\,.\,Hm, how about a system where entities have a field to specify their list of types, in order of abstract to more and more specific (at least as the standard convention). And then they also have the specs field, and then it is only the types that have the propStructs/ownStructs.\,.(?) (12:08) .\,.\,So a type both specifies the entity semantically, while also specifying what properties the given representation of that entity should include in its definition.\,. .\,.\,And then we eliminate the type--class redundancy once again.\,. .\,.\,I think I might actually do this, and then I will just make it a flag whether the entity is a type entity or not.\,. (12:13) .\,.\,Well, or this flag could actually just be whether the spec is NULL or not.\,. right?\,.\,. (12:14) .\,.\,Hm, the types field would be a list of ID integers, the spec\_input will be the same, and then should we just include another ``flag'' for data entities?\,.\,. (12:18) .\,.\,Hm, and I could gather the types and specs into one field by just separating the two integer lists by `;'.\,.

.\,.\,Hm, I could potentially have a varchar that is either `t' for `type entity,' `s' for `standard entity,' or some other string representing a file format for a `data entity.' And then the spec/def is just a BLOB field. The control server can then just divide both the input and the query requests up into three (or more) versions, where the data might be validated differently, and the output might be different---and they might also call a different MySQL proc each, but the data is still ultimately stored in the same table nonetheless. (12:26)

.\,.\,Hm, I had a really good system going here with these (parent, spec, ownStruct) Entities, but I think that it is worth it to eliminate this type--class redundancy. .\,.\,And also to make it more fundamental that data entities are always saved with just a format specification, and only that. (And this change also opens up for easier server-side validation of binary data formats.) (12:29)

.\,.\,Oh, maybe.\,. Hm, I want to be able to gather a type into one. .\,.\,Maybe we could just reserve a `super types' property.\,. no.\,. Better to just write the super types first in the field, followed by a `;', and then comes the JSON ``ownStruct'' (or what to call it now).\,. (12:32) .\,.\,The user then only needs to specify the previous type in the list, but I think the database should just repeat all the ancestor types directly in the field/column. (We can always change this at any point if we want to, as it is not visible to the app how the database stores it.)

Hm, should the specs of the standard entities then actually just be the values? .\,.\,Hm, no I do like the placeholder system, for instance such that we can define `\%1 of \%2' as the title of property tags. So let me keep the current system, including.\,. not having to save all strings as simple/string entities, and instead just by able to use a JSON string for a property value. Hm, but then what do I do exactly? Can I keep almost exactly the same system as now, perhaps where I might just construct and write out the propStruct of each individual type, rather than look up the whole line of ancestors.\,.\,? (12:41) .\,.\,Yeah, maybe I actually want exactly the same system as I have now, including having the ownStruct for standard entities, only where the type entities might save their whole propstruct (instead of relying on their ancestors), and where parentID is now replaced with the whole list of ancestor types (also so a recursive look-up is not needed here), both for standard entities and for type entities (where these types then represent the super-types). .\,.\,Hm, and then I only need a way to define the title of the type.\,. .\,.\,I'll just do that in the general spec/def (I think `def') field, of course, and maybe I'll just use JSON for the full `def,' both for type entities and standard entities.\,. (12:52)

\ldots\ (13:42) I think I will actually keep almost exactly the same system, and keep the SDB pretty much as is, and then only change class entities into type entities that just holds a (property struct) `template' property, which is the same (JSON) object that previously made up the whole class.\,. Hm, then the type property becomes redundant, so maybe the.\,. template can ignore that, although maybe this could exactly by the way to declare all the ancestor types, i.e.\ by just declaring them in the template.\,. Hm.\,. .\,.\,Nah, probably better to declare them in a `supertype' property of the type entity itself.\,. (13:47) .\,.\,I do actually kinda want to not have to do recursive lookups. But could we just define a new proc to get the whole entDataArr at once.\,. yeah, that is better.\,. .\,.\,(as it reduces the initial load time when a type is used for the first time by the app).\,. (13:49) .\,.\,Then I'll replace my EntityDataFetcher with that useQuery request.\,. .\,.\,Yeah, let me do that.\,. (13:52)

.\,.\,(13:52) And note that types don't have to include templates, namely since they don't have to even be standard entities. For instance `\,``type'':``type''\,' is a valid type declaration. (13:54) .\,.\,(13:56) Wait, if the type is always the parent, we should really just remove the type property from the propStruct, always, and let that be defined by the parentID, which I will then start calling `typeID.' And typeID = 0 then just means that it is a type entity.\,. .\,.\,Yeah.\,. (13:58)

%.\,.\,So a lot like my old way, only where the type entities now defines the ``context''/template (and also with the whole data\_input field and data placeholders).\,. (14:00)

(15:21) Oh, we still need to know if the parent is a type or a supertype of the entity.\,. .\,.\,Of course unless type entities just define their supertypes in a `supertype' property.\,. (15:23) .\,.\,Hm, but then the entDataArr (and the recursive proc) doesn't work.\,. (15:24) .\,.\,Oh, maybe I don't want types to have (super)typeIDs or specs, but want them to always only be defined by one propStruct, which then means that a type is then just \emph{supposed} to include the same properties as exists in all its supertypes (both the constant and the variable ones). I guess that might be it.\,. (15:29)

\ldots\ (16:15) I know: Types should be defined by three things always: A list of supertypes, its propStruct temple for its type instances, and its description. And would you look at that, that fits neatly into the spec\_input, the own\_struct, and the data\_input columns. It even opens up for potentially being able to query for a list of all subtypes of a given supertype list, if one wants to implement that at some point. But in any case, it is just nice that it fits so well in the same table. .\,.\,Oh, but how do you tell them.\,. Ah, the typeID, never mind. Okay. .\,.\,And this also means that I should make that recursive proc after all. I should just make a proc for querying (or inputting) type instances and one for querying (or inputting) standard entities. Well, unless I just want to use the same proc for both, actually.\,. .\,.\,I'll see. But no need for the recursive proc.\,. %Well, maybe.\,. no, never mind.\,. (16:23)
.\,.\,:) (16:24)

(19:49) About the EntityTitles, I'll just do the same as before: The internal references become the links only in the AppColumn header (and of course in the propStruct as well on the meta info page). *(20:54) I think I will also add a link button to the left of all EntityTitles such that you can always click all links directly in principle, plus you can also always see what are the individual entities---especially since these should be highlighted when hovering over the given link button (for computer users). .\,.\,Oh, and mobile users should click to highlight it and make it a link, before clicking a second time to open it. (20:57) 

(19:52) Oh.\,! I just realized how good the `correlated rating/sub-statement' ratings will be for the (ML) algorithms.\,.\,!

.\,.\,(And I should indeed call the property `correlated rating' instead.\,.\,:)) (19:57) .\,.\,(Well, or `correlated statement'.\,.)

(20:45) I think I need to have it so that tags can define their own statement templates.\,. .\,.\,Hm, yeah, they should define both the title/header of their instList, \emph{and} they should define how to take each instance and construct a statement with it.\,.

.\,.\,And I think properties should do something similar, i.e.\ define how they should normally be rendered in various contexts. (And then from there, you can always add more rendering methods ``semantically'' via new up-rated properties, and also even overwrite all these for special individuals, potentially.\,.) (20:49)

(20:58) Oh, I should also mention this idea: When showing the normal ratings under the `Ratings' tab, i.e.\ those meant for searching for (and filtering out) the entity, then the `((good fits $<$entity$>$)-$>$`correlated statement') fist $<$rating statement$>$' rating (this is why we need to have those better rendering options by the way.\,!) should always be shown just below (or above) the given rating. This will then make users used to the structure that all rating comes in twos: The rating itself, along with a rating of how relevant the first rating is for the most relevant overall statement, which will often be `good' (or `good fits $<$entity$>$'), but if in case of a submenu or page of another rating (such as the tab of `correlated ratings' to a given rating), then that will take the place of `good.' .\,.\,(I hope that made sense.\,.) (21:07)

(21:38) Maybe a tag should define its own standard parent tag (like `good,' but also maybe like `funny' etc.), but of course ``semantically'' rather than directly in its propStruct.\,.

.\,.\,I should mention: See Section \ref{struct_disc_cont} above for more on `correlated ratings/tags/sub-statements' and all that.

(22.08.24, 11:37) Okay, I have a few new ideas. First of all, tags should not only be able to define their own rating, but they should even define their scale. And it should actually be such that they can define both a formula (potentially with rounding symbols), and define the format string that results in the rating displayed on the right of the rating bar. And then I will actually just drop the stars above (or constituting) the rating bar, but always just have a colored not-too-thin bar, whose color change from red to/through green depending on the position. For regular 10-star ratings, and other $n$-star ratings, a number is just displayed followed by a star (i.e.\ right of the bar).

Left of the bar is shown an info sign that displays the description of the rating when clicked or hovered over. And above the bar is the tag displayed, followed by a symbol, I think, and then the subject/instance.

I also just looked at UTF-8 characters, and I found the LEFT and RIGHT MULTIMAP symbols. That looks very well to be repurposed for pinning a tag to an entity. .\,.\,That is, unless I still want to construct the whole sentence for each tag--instance pair.\,. .\,.\,For each `ratable statement,' we can also say.\,. (11:47)

.\,.\,Anyway, for each EntityTitle, whereever it appears, the should be a type symbol just left of it, possible with 1--2 characters shown (perhaps inside a square), at least until we start using pictures, perhaps. Hovering over these will then underline the entity and display the full type (and for mobile clicking them will do this). And clicking them will actually direct.\,. to the AppColumn/page of the entity?\,.\,. (11:52) (And for mobile, this would be clicking it again.\,.) .\,.\,I guess so, even though this is not too intuitive (but helpful).\,. (11:53)

.\,.\,Hm, maybe the type signs should only be shown when hovering over the entity.\,. (12:00)
.\,.\,Hm, or maybe we should use function-like syntax of `tag(instance)'.\,. .\,.\,Yeah.\,. (12:01)
.\,.\,(12:04) Hm, and we \emph{could} then also use a `subject[property]' syntax for property tags.\,.
.\,.\,Ah, then it is also language-independent, and thus sure to work in all languages, not just English. (12:07)

When hovering over such composite entities (or clicking the first time for mobile), the type-and-link signs should be shown to the right, but should also introduce parentheses around the entity if it is a composite entity. .\,.\,And all this can still be done with the same placeholder technology, right? We just change e.g.\ `\%1 of \%2' with `\%2[\%1]' and `\%1 fits \%2' with `\%1(\%2)'? It seems so.\,:) (12:11)

.\,.\,This is actually pretty nice.\,.(!) Sure this programming-like syntax will require some getting used to for a lot of users, but I actually don't even think that the alternative of constructing sentences is any better in that regard, ultimately. (12:13)

.\,.\,What great changes.\,.\,!\,:) (12:14)

.\,.\,For the type characters, we'll just use the first two characters of the type's title, unless there is a high enough rated alternative (when that is implemented). I actually just another though as well: the type sign could change to a link sign when hovered over (clicked once for mobile), and then a link to the type could be expanded (above the other HTML boxes, not moving anything, you know) below the sign, also of course displaying the full title of the type. (12:19) .\,.\,Hm, that would be three clicks in total, then, for mobile users to go to a link within an EntityTitle, but maybe that's okay.\,. (12:20) .\,.\,Yeah, actually it's fine 'cause it also serves to make sure that they don't click on the wrong sign within the EntityTiltle; they get to abort it if they do. And the sign and type display can just be expanded to make the third and final click easy to hit.\,:) (12:23)

(12:30) We can say about `tags': They are functions that returns a `rating' to be rated by the users. And the semantic of these ratings can depend on the real world, as well as what goes on on this particular side. They can even be dependent on itself, or rather on what instances have previously been rated for it, and what those ratings are. For instance, a subcategory of `progressive rock' might be a decent one for `music genres,' until `rock' comes along. When `rock' is highly rated on the list, then it might be better to have `progressive rock,' not as a subcategory directly of `music genres,' but have it as a subcategory under `rock.' The same can be said about `rock' as a subcategory of music, until the subcategory `music genres' comes along (and all of a sudden makes `rock' and unwanted subcategory to see on the list, since you would rather have that \emph{under} the `music genres' category). (12:39)

(12:47) I should mention that I'm thinking of using a scale from -100\% to 100\% for the `correlated ratings.'

(12:57) When an EntityTitle is within another EntityTitle, and has EntityTitle children itself, then parentheses should be rendered around it. Luckily, we can do this just with CSS.

.\,.\,So `my tag[correlated rating](my rating)' for the `correlated rating' ratings.\,. .\,.\,Well no, I guess: `my rating 1[correlated rating](my rating 2)'.\,. .\,.\,Hm, e.g.\ `good(The Lord of the Rings)[correlated rating](has good acting(The Lord of the Rings))'.\,. .\,.\,Hm, I might want to reintroduce ``short titles,'' first of all, and also it would be nice to be able to remove the repeated `Lord of the Rings' here, if possible.\,. Hm.\,. (13:07)

.\,.\,And maybe I will just drop internal links, at least from the start, and just write all internal links in a list under the haeder of an entity's AppColumn/page header title.\,.

.\,.\,Maybe I don't like the programming-like syntax after all.\,. (13:13) And maybe I should see if we could get a `this' keyword to insert.\,.

.\,.\,Hm, if we could mend it into something like: `good[(1)] $\langle$correlated rating$\rangle$ has good acting[(1)]'.\,.\,? (13:18) .\,.\,Or just `good(1) $\langle$correlated rating$\rangle$ has good acting(1)'.\,.\,? .\,.\,Or `$\langle$good$\rangle$(1) correlated rating $\langle$has good acting$\rangle$(1)'.\,.\,?

.\,.\,Hm, and we could also generate `short titles' automatically by taking all first letters (keeping capitalization) and punctuation, then potentially adding a number if several inputs have the same sequence of letters.\,. Hm, and for one-word titles, we could take one or two of the characters after the first.\,. But maybe I actually like just using number placeholders alone better.\,. (13:28)

(13:36) And clicking on a placeholder should just fold out its EntityTitle, and perhaps give some margin to the surrounding text as long as you hover over it.\,.

(13:40) I could then make a React context for making placeholders the same across several rating displays, e.g.\,.
.\,.\,I like this a lot, actually.\,. (13:42)

.\,.\,React children can then add any new IDs to the placeholder list (getting a small-numbered placeholder for that ID) of the context. This context should then in most cases be sitting at the top of the AppColumn/EntityPage, meaning that 1 will always be the placeholder of the top Entity of the given column. (13:46)

.\,.\,(13:51) Hm, InstanceList EntityElements can also provide their own placeholder context.\,. Perhaps we can prefix these by `\#' or something.\,. .\,.\,And when the ID appears in both lists, we just write both the placeholders, separated only by a comma, e.g.\ `3,\#44'.\,. .\,.\,Well, we should always add the placeholder to the EntityPage's context, I guess, so one could also just write them in reverse order of the contexts: `1,3', where 1 is then always the.\,. placeholder for the EntityElement.\,. Well, how about we instead just make sure to always print out the placeholder at the beginning of each EntityElement. That sounds better.\,:) (13:57) .\,.\,(Then we just have the one placeholder list per EntityPage/AppColumn, i.e.)

I like highlighting all tags in EntityTitles with angle brackets, like I just do above, but what should I do for properties (which serves as semantic relations).\,.\,?
`$\langle$good$\rangle$(1)$\rightarrow$corre-lated rating: $\langle$has good acting$\rangle$(1)'.\,.\,? .\,.\,Not bad.\,. How about: `$\langle$good$\rangle$[1] $\rightarrow$ correlated rating: $\langle$has good acting$\rangle$[1]'.\,.\,?
I like this. As long as we make the arrow a bit shorter and/or the spaces around it.\,:) (14:05)

.\,.\,Note that we should only start collapsing EntityTitles into placeholders when they are sufficiently deep within, \emph{and} I should find a way to somehow make.\,. Wait, first of all, it should actually be: `$\langle$$\langle$good$\rangle$[1]\,$\rightarrow$\,correlated rating$\rangle$[$\langle$has good acting$\rangle$[1]],' then, but that probably won't look very good.\,.\,? (14:08)
.\,.\,Hm, it is actually somehow not far from looking okay, but perhaps not good enough, still.\,. (14:10)

.\,.\,Hm, I guess we could do all this with CSS as well.\,.\,! And then we could also quite easily introduce syntactic sugar, such as `$\langle$$\langle$good$\rangle$[1]\,$\rightarrow$\,correlated rating$\rangle$[$\langle$has good acting$\rangle$[1]]' turning into `$\langle$good$\rangle$[1]\,$\rightarrow$\,correlated rating: $\langle$has good acting$\rangle$[1]'.\,.\,:) (14:12)

.\,.\,But to get back to my interrupted sentence, I guess I still need to figure out a way to make.\,. ah, to reduce the recLevel of a rating (statement) subject (like `$\langle$has good acting$\rangle$[1]' in this case), which should not be so hard, right?\,.\,. (14:15) .\,.\,No.\,.\,:)

(14:29) Hm, this means that we don't need that title-with-placeholders technology after all, since composite titles should actually be made with CSS instead.\,. Hm, it seems so.\,. .\,.\,And making the tags readable by machines to support blind people can be made in a layer above, I think, since it has to be language-dependent anyway (we should also support non-English speakers).\,. (14:34)

%..I'll make an ExtendedHover React wrapper for making this hover-or-click computer-or-mobile-dependent action, btw.

%..And when hovering over placeholders on computers, there should be inserted (by CSS) an 'expand' symbol.. ..The ExtendedHover can then therefore also get a computer-hover-specific action.. ..Or I'll perhaps call it HoverOrClickWrapper..

(14:45) ``Rating entities'' are virtual entities; they are only composite things.\,. no, that's not right. I need them to be actual entities. Well, they can be, but they are just always rated in the database by splitting them up into their two components (which they always have: a tag and a subject). (14:47)

.\,.\,So you might click on a rating entity, only to find that this entity haven't been inserted yet (and the app prompts you to do it), even though it has already been rated by users.

.\,.\,(14:57) Hm, it would be nice to eliminate the `rating as a thing to rate'--`rating as a user-provided number on a scale' ambiguity somehow.\,. .\,.\,Yeah, I think it's actually wrong to use as the `thing to rate' itself, rather than the measurement/estimate, so let me think of another word.\,. (15:01) .\,.\,An `estimator' would be a correct term, but.\,. .\,.\,(15:06) Nah, I think we might be able to use.\,. well, `rating' although we \emph{are} not talking about the actual estima\emph{tion}, but rather the `thing to rate'.\,. Hm.\,. (15:07) .\,.\,Oh, how about a `rate'?\,!\,.\,. (15:08) .\,.\,No.\,. (15:09) .\,.\,I guess I might just use `estimator,' actually; it is the right word when talking about statistics, so yeah.\,. .\,.\,So I'll e.g.\ say: `correlated estimator' instead.\,. (15:11)

(15:16) Okay, in terms of the composite titles, both the normal ones (for propTags and estimators) and all the custom ones, I think I will still use an (initial) title with placeholders. But then I will use named.\,. well, maybe not, actually.\,. `named placeholders' (like I did with my old Templates).\,. Maybe I'll just require that each placeholder appears first as the sole property value of a property member, before this placeholder is used anywhere else in the propStruct template.\,. (15:22)
.\,.\,Except that we sometimes want (ID-referenced) entities as property keys, which does make it more appealing with named placeholders.\,. .\,.\,Sure, let me just do that; let me use named placeholders instead, probably with a `\%[$<$name$>$]' syntax. (15:24)

Then for the EntityTitles, the placeholder substitutions are then wrapped in a span tag with a className that includes this name, after some prefix. And all the other text around these placeholders is also wrapped in span tags, that are then siblings with the substitutes' span tags elements. Then when using CSS to determine the actual rendering of the EntityTitle, the boilerplate text of the original/initial title can always just be made to be `display:``none''', and then the CSS can insert its own boilerplate, as well as even rearrange the substitutes, and make that syntactic sugar I was talking about just above. :) (15:31)

(15:37) Wait, we could actually even make it CSS whether to show the placeholder (i.e.\ from the given EntityPage context) or show an extended title. In fact, we \emph{should} do this! For then EntityTitle just needs to print out it all (down to some maxRecLevel), and then CSS can do the rest from there.\,! (15:39)

(15:45) Below the header title of an EntityPage, we can also show a list of the (first-level) substitutes, each preceded by `$<$name$>$: ', where $<$name$>$ is the placeholder name.

\ldots\ (16:38) We also have the word `score' to use. But I think I will stick to `estimator,' and then try to use `rating value/score' as much as possible for the estima\emph{tions}.

I don't actualy think I will use a context for those placeholders after all. Well, I might, but the point is that I think I will only let there be a placeholder for the columnEntID, as well as for each entity in an instance list, for which we can just use the `key' that React requires (recommends) them to have anyway. (So prop drilling is also possible here.) So the main point is to just not give placeholders to any old entity that appears in the column, but only use it for these special standard entities. Then I'll just use `\#$<$entID$>$' for the rest.

I think that it is very important to be able to see the whole list of `important correlated estimators' before one can give it a percentage score, so I'll probably only show those ratings on that list. But on the other hand, I will be very generous with showing that particular list, i.e.\ in a sub-menu to all ratings that you see.\,. Well, unless we get too much nesting, I guess.\,. Or maybe not; maybe the user should just be able to expand this ``discussion graph'' (which it essentially is in a lot of ways) as much as possible, always.\,. (16:47)
.\,.\,(Well, on the other hand, why not open up a new column?\,. I guess I'll just see.\,.) (16:48)

(17:16) EntityTitles should also have a className which denotes the typeID, preceded by a prefix as well, which can then be used in particular to make special rendering for tag, propTag and estimator entities (as well as other custom types later on, potentially).

(18:06) So we could actually drop the title property for propTags and estimators.\,. *(18:13) Nah, I think defining a title should still be a convention. But I will just ignore this title almost completely for the EntityTitles when it comes to propTags and estimators, and then only show it in the Definition tab of the EntityPage.\,. (18:14)

.\,.\,(18:09) Hm, I might actually not automatically uprate the properties from the propStruct after all with the EntityInserter class after all, since I think I like the convention to prioritize the defining properties over semantic ones, and then when you need e.g.\ to re-define a description, you just use a different property for that.\,. well, or you can use the same actually, since the app will not look it up by default if it is already present in the defining propStruct.\,. .\,.\,Anyway, I think I will not uprate the properties automatically after all, like I said.\,. (18:12)

(18:15) I also want disambiguations/sub-titles.\,. .\,.\,Hm, let me just use a `title' \emph{and} a `short title' property. Then if a short title is not too long, the app might choose to render it, even at a somewhat deep recLevel. I guess this might actually be possible in CSS alone as well, as I think we can get the width and/or character number, let's see.\,. (18:19) .\,.\,Yeah, you \emph{can} (of course) do that. Good. (18:21)

(18:22) Maybe I'll do something else instead of that 1--2 (or 1--3) character type sign: Maybe I'll just include a full `:: $<$type$>$' span element at the end of each EntityTitle (or `$<$type$>$: ' in front), and then let the CSS define when to display these. (18:24)

(18:27) Maybe I'll even use a `long title' property as well.

%(18:43) Let me just reiterate to myself that neither the types' ownStructs nor their template structs should include a 'type' property, as this is determined instead by the typeID field (previously the 'parentID' field).

(19:48) Oh, there is a thing, though. When it comes to e.g.\ movies, or any specific class of things like that, it often makes sense to rate the importance and, more importantly, the correlation of various tags for all entities of that class at once. For instance, `has good acting' might be equally correlated for a given user's perspective for all movies of a certain class (like `fantasy movies' or just `live action movies' in general). Whereas when we are talking about specific discussions, the correlated statements are often quite unique to that discussion. So I need to figure out what we could do to make correlation (and importance) ratings count for a whole class at once, in some cases.\,. Hm, the natural suggestion would be to say that the correlation ratings can be done for a whole category at once, rather than just the individual entity. So how would that be.\,.\,?\,.\,. (19:54)
\ldots (20:09) Well, we could uprate these ``(correlation) class'' categories for any individual movie (e.g.) and then make it so that the displayed correlation ratings are generally made by combining the ratings from this ``(correlation) class'' together with the specific ones for the individual movie.\,.
.\,.\,Or maybe we could pull it from the type instead.\,. (20:13) .\,.\,Hm, and perhaps pull the weight as well from there, i.e.\ the weight used when combining them.\,. .\,.\,Nah, not the weight. And I also think using categories is better than using the types.\,. .\,.\,maybe.\,.

.\,.\,Hm, I actually think that this might be the way, i.e.\ uprating categories for individual entities and then make it so that the correlation ratings are a combination, if a category is rated highly enough.\,. Hm, and.\,. wait.\,. (20:26) .\,.\,No, yeah, the correlation category is uprated for the individual (not the other way around, although you could insert that rating as well, why not.\,.), or perhaps for the type.\,. Hm.\,. (20:28)

.\,.\,Wait, and it isn't easier to just rate the impact, instead, i.e.\ such that you rate the tag itself and the impact it has on the overall tag in question (often `good'), instead of rating the tag itself and the \emph{correlation}, which is just impact divided by score of the rating tag itself. Would this not be a lot easier for the users?(!) (20:32) .\,.\,And then the correlations can be subtracted statistically for a given class from these ratings, rather than the other way around.\,:) It sounds good, but the only thing is that for discussions, where the score of the ``sub-statement''/sub-estimator itself might not be known at a time where it is still informative to rate and see the correlation rating (which again is ``impact'' divided by the score).\,. (20:35) .\,.\,But it of course doesn't make sense to try to make two different things, i.e.\ discussions and quality ratings, fit the same system; we should then rather just make two different systems.\,:) (20:37) .\,.\,For quality ratings, the correlation \emph{can} be rated in principle (somewhere), but it will probably be better to extract that instead, statistically. And for discussions, it is rather the impact that (very much in this case) ought to be extracted rather than rated directly, and the correlation that ought to be rated instead. :) (20:39)

.\,.\,Again, it \emph{does} also make sense for users to rate the correlations for quality estimators directly---maybe that \emph{could} actually help the statistics significantly, even---but again on the other hand, you are not supposed to rate the impact directly for discussions. (20:42)

.\,.\,As for the ``correlation classes,'' well, these are then just uprated and used in order to extract meaningful correlation ratings from ratings of all the different entities in a given class. (20:44)

(21:30) Ah, maybe there should always just be the three ratings: the tag itself, the impact rating, and the correlation rating (to the given overall estimator, e.g.\ `good'). And whenever you rate the second one out of the three, the third one will also automatically be rated. Then for quality ratings, the correlation rating can just be hidden.\,. well.\,. Nah, maybe it's better to just show all three ratings at the same time (so always show `impact' below the correlation rating), I think so.\,. (21:34) .\,.\,(For as long as `impact' is just dependent on the estimator's rating itself, then you it does make sense to rate it, as it is equivalent of rating the estimator itself, i.e.\ if the ``correlation'' has already been rated.)

.\,.\,Hm, in principle you could also do the same with the overall rating (e.g.\ `good[entity]' or the `likelihood[parent statement]') and all the ratings of the sub-estimators, together with their correlations (or impacts, equivalently). But maybe it's better to just show a slight warning if there is a discrepancy (and make this warning more pronounced if the discrepancy is larger).\,. (21:40)

(26.08.24, 9:01) I just had the idea to use `quality' instead of estimator. Before that I have also been thinking about `ratable parameter,' but no, (ratable) quality is much better, I think. I'm thinking then that we could let `properties' denote the standard noun-defined qualities, and thus then let `qualities' be a superset of `properties.' I think this might make sense.\,.\,:)

Another thing: We have to call types `classes' instead, if nothing else then due to how they are similar to classes in programming languages. Now, I'm currently thinking about exactly how to.\,. I think classes should be uprated more semantically; we should not use the propStruct as much.\,.

.\,.\,I also think that I need to rethink the whole ``semantic search'' using categories---in fact I think I might use the classes instead of the categories.\,. .\,.\,(9:11) The overall idea is that seaching class $\rightarrow$ subclass $\rightarrow$ sub-subclass makes sense when you just need to specify a class more and more for a search. But often, if you are looking for something that you don't have the keywords to search for or don't know exactly what you are actually searching for at all, then it would better and more natural to find the given overall subject, and then look in `related entities' (downward, horizontally, or upward), and/or in properties.\,. (9:16) .\,.\,And whenever you go to a new thing, the classes of that thing is then looked up, followed by what properties, etc., should be shown on the various tabs, and of course also what tabs should be there.\,.

.\,.\,(9:24) Maybe `quality' doesn't work so well with e.g.\ likelihood.\,. .\,.\,although `ratable quality' might.\,. .\,.\,Yeah.\,. (9:25) .\,.\,Yeah, `ratable quality'.\,. .\,.\,Also doesn't require further explanation but can be understood on its own, which makes it very useful. Okay, `ratable quality' it is.\,. (9:27)

.\,.\,Okay, so the only thing that is now troubling me a bit, is whether we need to e.g.\ make a kind of virtual classes without templates which isn't used to define/specify an entity, but more to categorize it.\,. Hm.\,. (9:30) .\,.\,Well, we could call these virtual classes `categories'.\,.

(9:40) Nah, it's actually bad to use classes (formerly `types') to anything other than defining/specifying, since there might be the exceptions that doesn't.\,. have the properties.\,. hm.\,. .\,.\,Well, we could of course just introduce a null value for properties, and we also already have the ownStruct for when an entity needs more properties to specify.\,.

(10:15) Hm, when specifying a subclass, you often need \emph{less} information to then specifying the entity from there.\,.

For all named entities, a list of supercategories followed by a name/title should be enough to define the entity (recall that semantic imprecision is allowed, and often good). And I guess for a lot of other entities, a list of supercategories followed by just one other property as well should be enough.\,. .\,.\,So maybe we could aim for that convention.\,. (10:19)

.\,.\,(10:24) Ah, but then we could just forget about subclasses needing to define all the properties of the parents.\,. no, better to define these.\,. .\,.\,But you have the option to define them as null for special cases.\,. .\,.\,Okay, so I think I'm back to wanting most entities to be defined as a list/set of supercategories, which I will now, however, call *(super)`classes,' as each class should also preferable have a propStruct template in order to define instances of that class.\,. (10:28) .\,.\,Except it would be cooler if those template were semantically uprated instead.\,. .\,.\,(10:31) Well, should templates then be their own things.\,. .\,.\,Hm, entities which holds a variable propStruct template, which defines.\,. oh, this is equivalent of just calling my previous classes/types `templates instead'.\,. .\,.\,I could then make templateID *(tmplID) = 0 just mean that there is no template, and that the entity is fully defined from its ownStruct.\,. .\,.\,And since I'm not calling the entities `objects,' there is no need to call the templates `classes;' we can just say that templates is for entities what classes are for objects (in programming languages). (10:37)

.\,.\,And I think I might use `categories' instead of `classes,' then.\,. (10:40)

.\,.\,(10:42) Now, you \emph{can} make semantic searches by going the `category $\rightarrow$ subcategory $\rightarrow$ sub-subcategory' route, but it would probably often be better to just normal-search for the given topic first, and then use properties like `related entities' or `related subjects/topics,' and more properties of that nature, from there. (10:45)

Okay, this seems very good; I think I know what to do now.\,.\,:) (10:46)

\ldots\ (12:32) Maybe I actually want to go back to calling it `types,' then, rather than `categories'.\,. .\,.\,Nah.\,. .\,.\,But maybe `class' in order to make it more natural to use singular nouns.\,. (12:32)

\ldots\ (15:16) Just came across the term `rating scale' when searching for UI/UX standards/knowledge. That might also be a good replacement for `ratable quality,' but let's see.\,. .\,.\,(15:27) Yeah, `rating scale' it is.

Hm, interesting to read about \emph{Likert scales} and \emph{semantic differential scales}.\,. And nice that 1--7 scales are also pretty standard, it seems.\,. (15:29)
.\,.\,But maybe I should just make 1--5 the standard scale.\,. (That's why I'm reading about this stuff now.) (15:34) Oh, it's a continuous scale, as opposed to normal surveys etc., so 1--5 must be the best option.\,.

(16:10) Oh, maybe I want a different syntax for sets and lists. The problem is that.\,. Well, I could manage, but it's not so nice when inserting a list or a set, or list elements, in place of a placeholder.\,. .\,.\,Hm, good question.\,. (What to do here.\,.\,?) (16:15) .\,.\,(16:16) Ah, can't we just say that the templates themselves don't have to be.\,. JSON.\,. No, that's not to good either.\,.

(17:21) Hm, I'm thinking of removing the redundant boilerplate around list entity propStructs (writing them instead simply directly as the arrays), and then just make a convention that list entities below a certain length ought to be written out.\,. oh wait, I forgot that sets are not entities.\,. .\,.\,(17:25) Hm, do I then try with a new array placeholder, or.\,.\,? .\,.\,Wouldn't it be better to introduce extra syntax to the spec/template input?\,.\,. (17:27) .\,.\,Well, it's not that simple. Hm.\,.
.\,.\, (17:37) Wait, I can just make sets a class of entities after all.\,! And then if you want to denote that a the set itself is the property value, and not all its elements, then you just have nest it in an array, like with the lists. Okay, so I just need to then figure out, it seems, if this can work when I only have the `[]' at my disposal (or should I try to use something else?).\,. (17:40)
.\,.\,Well, I could also just keep like it were, but introduce a `set' class alongside the `list' class, and then make a convention that short enough lists or sets are exploded/folded out directly in the propStructs.\,. (17:45) .\,.\,That sounds like a decent solution, although it would be nice to cut some redundant bytes out of the list and set entities, I guess.\,. (17:47) .\,.\,Oh, but this is also not that simple, namely since we have the strings that contain more than just the placeholder.\,. .\,.\,But then you could just parse and explode/fold out for when the value is a single placeholder.\,. (17:50) .\,.\,Wait, none of this makes sense as we are talking about entities that are stored with a spec/template input string.\,. .\,.\,I should actually call it something else, rather than `template input,' perhaps `searchable input,' and then I should also actually put it last in the secondary index.\,. (17:55)

\ldots\ (18:57) The propTag syntax should actually, I think, be more like the syntax for member declarations (inside objects) rather than for member accessing. .\,.\,So like `$<$property name$>$:$<$value$>$.' Now, I'm then also kind of thinking about using a list of tags as the definition, partly or perhaps fully (if that is possible (probably not)), or maybe just use it for something else handy.\,. .\,.\,Oh wait, I can't use that member syntax since the propTag consists of the subject and property (name), not the property and the value.\,. .\,.\,So never mind.\,. (19:09) .\,.\,But we could still let entity definitions be a list of tags, including such member tags.\,. .\,.\,Which is similar to just using boolean properties, which in JSON just means writing true as the value (with no quotation marks).\,. (19:12)

.\,.\,Hm, maybe I'll just drop the JSON restriction.\,. (19:13)

(21:18) Oh, it's not better to have expanded/exploded propStructs for normal, not-particularly-searchable entities.\,. .\,.\,Good.\,.

(21:54) Ooh, I could also use JSON numbers as whole-value placeholders.\,.
.\,.\,And a negative number could mean `get it from the data\_input,' and 0 could mean `\%b'.\,.

(22:10) I will use different placeholders for within strings, more specifically named placeholders, and I will then perhaps just use the numbers for the initial whole-value placeholders.\,.

(22:45) For meta relations/predicates, like `better duplicate,' you could just make entities defined like `entity \#$<$entID$>$'.\,. .\,.\,(22:49) Or use the SK instead.\,.
\ldots (23:19) Hm, I think it is better to just make it tag/relation-dependent, now that these are often thoroughly described.\,. .\,.\,So any entity can refer ether to the given thing, or itself, the representation of the given thing, depending on the context of the tag. (23:21) .\,.\,Well.\,.

(27.08.24, 8:46) I will by the way introduce relations again as a superset of properties, and then call properties `property relations' instead, which is more correct in regards to the standard use of the term `property' in programming languages (it seems). .\,.\,And these are then each ``\emph{represented} by a (singular) noun.'' (8:48)

About the meta entities, I think it's best to just let the tag determine the interpretation of the entity, as a representation or as the thing itself. And in terms of.\,. Wait, the problem is how do you know which ratings to transfer, then, if a better duplicate is found?\,.\,. .\,.\,So either we make meta entities, or we give tags a formal way of denoting that they are speaking about the representation.\,. Hm, I like the latter.\,.\,:) (8:54) .\,.\,Perhaps with an `is meta' property.\,.\,:) .\,.\,Which means that I should make a template for meta property tags as well. (8:58)

(9:00) I have another idea for writing lists and sets, namely by expanding the `@\ldots' syntax and use `@[\ldots]' and `@\{\ldots\},' which would make sense if we store them as searchable strings, and if we also say that two lists or sets right after one another means concatenation/union, but let me think.\,. .\,.\,For is it not better to just always store these lists/sets, and then reference them by ID?\,.\,. (9:04)
.\,.\,Hm, I want to do both, but it does create a redundancy where seemingly the same propStruct can be formulated in different ways.\,. (9:10) .\,.\,Unless I just render the EntityTitles a bit differently.\,. .\,.\,Maybe by not replacing this SK reference syntax, and then only make the `@' in `@[\ldots]' and `@\{\ldots\}' become a link (where the list/set entity is then almost-automatically (prompting the user first) inserted if it is missing).\,. .\,.\,Then I would also need to implement special `concatenated lists' and `unioned sets' classes.\,. But maybe I'd rather just always reference by ID (PK).\,. .\,.\,Yeah, I think so (maybe).\,. (9:16) .\,.\,Yes, let's just always store lists and sets as separate entities, and then always reference by (PK) ID.\,. (9:18)

(9:37) Oh, I should make it so that if an entity has no template, than data\_input is just always interpreted as the `description.' Then an entity never inserts into its own propStruct, only into templates, which are always the ones from the template\_id field.

(9:42) Hm, maybe I will not use named placeholders.\,. .\,.\,Nah, let me use the convention that each template input is just always introduced first as the whole value of a property, appearing before it is ever used within a larger string, e.g.\ for the title. (9:43)

(10:08) Maybe I actually \emph{will} introduce that `@[\ldots]' and `@\{\ldots\}' syntax. And then I will just make a clear convention of when a list/set is saved as a stand-alone list/set, or as a concatenated/unioned one, i.e.\ by making a max number of elements before this happens. .\,.\,Although what is the purpose if we are going to save them as entities anyway.\,. .\,.\,Well, when \emph{do} we actually need them in their entity-stored state.\,.\,? When do we need them to have an ID---only when they are rated, which I guess ought to happen for all lists.\,. Hm.\,. (10:13) .\,.\,Hm, I think I'll just make a list and a set class, and never write out these in stored propStructs or in tmplInput, but always just use their IDs.\,. (10:21)

.\,.\,(10:31) Hm, maybe I won't actually need sets and lists as much, since I think I will actually just always use one class, and then just rely on the superclass property. And when it comes to showing the superclasses to the users, I think it is better to pull these from an InstanceList, rather than from a set or a recursive lookup. .\,.\,It could even be such that the class ``path'' is automatically pieced together from the topmost class entities on this list.\,. (10:34)

.\,.\,And this makes storing sets less beneficial, so I don't think I will.\,. .\,.\,Instead I ought to make syntax for the templInput in order to insert sets, and perhaps lists, instead of just normal ID-referenced entities.\,. (10:39) .\,.\,Well.\,. .\,.\,Hm, no, maybe the template input should just always be stored entities, referenced by ID, and then I can go back to just using the ``\,`[]' means set, `[[]]' means list'' convention, and insert a comma-separated list of ID references into set or list templates.\,. Hm.\,. (10:44) .\,.\,Ah, and then list and set templates are the only ones, perhaps, that are allowed to break the convention of having to introduce the input by name first. Oh, or I could use a different kind of placeholder for.\,. string substitutions rather than IDs.\,. Oh, that could also save some space, by the way.\,. .\,.\,We could use `\%i$<$n$>$', then for ID inputs.\,. .\,.\,Or `\%e$<$n$>$', I think I like that better ('cause it should still get the `@' after the substitution). (10:52) .\,.\,Okay, I think I'll do that.\,. .\,.\,Or maybe not.\,. (10:54) .\,.\,Yeah, I think I will, actually, 'cause it's alright to sometimes require that the input is a stored entity.\,. (10:56)

.\,.\,And whenever you submit a new entity, you see the propStruct getting created, so you can see that you are formatting the input, e.g.\ a comma-separated list if ID references and/or strings, the right way. Okay, that takes care of that; now everything is possible, and most entities will be stored only with a template\_input, which will now be divided into two, however, but this is still about as space-efficient as it gets, and not least communication-efficient, since you should then only need to query for the tmplID and these two templInputs most of the time, and only really need the propStruct for templates, mostly, as well as some of the initial inserts, which one can however always hard-code into the app if one wants (and the same can be said for some of the common templates). (11:06)

.\,.\,Not that users might not make frequent use of the propStruct to disambiguate the submitted entity further than the given template allows, i.e.\ whenever your entity is a bit special and you don't want to make a whole new template just for that, but the good thing is that this disambiguation data only needs to be shown in the (meta) info tab. So one can delay fetching that data until necessary. (11:11)

\ldots\ (14:01) Note that missing template inputs just means (null) that they aren't specified. And I think I will take an empty string to always just mean null as well.

For any compound entities, I actually think that the title template shouldn't be defined by the entity (template) itself, but should instead just always be defined either hard-coded in the app, or be defined by uprating the best title template for the given class. (14:05)

When defining a title template, one can then use a syntax to access the various tmplInputs. And in a more advanced system, the title can even access the inputs of those inputs.\,. well, actually it can access all property values, and the property values of those, etc. (14:08)

(15:00) Hm, is it better to look up the properties ``semantically'' for the titles?\,.\,. .\,.\,Or should we be able to do both?\,.\,. (15:03) .\,.\,Yes, both, why not?\,. .\,.\,So I guss EntityTitles should first look up the title template as a semantic property of the class, and then I guess let us then construct the title recursively like we have done before, but where the each EntityTitle, and its potential helper modules just gets a className that denotes whether all its children are ready. Then we can make it a responsibility of CSS if we want to e.g.\ render a placeholder of the whole outer EntityTitle until all its children are ready. (15:09)

(15:14) Oh, you can even disable links via CSS alone! Nice.\,.\,!

(15:23) Hm, shouldn't I add a classID column to the Entities table as well, or.\,.\,?

(15:27) Hm, now I'm kinda thinking: wouldn't it also be nice if we could use relations directly, without having to look up or create the propTag first?\,.\,. .\,.\,We don't even have to make another ratings table, since we can just say that no subject means that the relation is a tag.\,. .\,.\,Hm, I really don't see why not.\,.(??\,.\,.) .\,.\,(15:37) It would only be a problem if you.\,. ah, if you need to look at `relevant tags,' which must be why I chose to do it this way. But let me see.\,. .\,.\,Hm, we can of course either just uprate a list of a subject and a relation instead of a tag, or we could uprate the tag, but just make sure that the app can see that it is a compound tag that should be split into two before rating.\,. (15:44) .\,.\,Hm, which would just require us to make a `compound tag' class instead of the `property tag' class, since now the relation can just know whether it is a `property relation' or not.\,. Hm, I know it would be a bit more complicated than this, but.\,. (15:47) .\,.\,But it's not any different that a `rating scale' entity also had to be split up into a tag and an instance (which I might start to call `object' again) before being rated. So yeah, I \emph{should} do this.\,. (15:48)

.\,.\,And then I surely need to add the classID field, don't I.\,.\,? (15:50) .\,.\,'Cause we can handle duplicates: If an entity should rather have another class, then we can just make sure that all users subscribed to the specific duplicate recognition user group can just have all their non-meta ratings about that tag, either as the subject, the tag/relation, or the object, be transferred to the better duplicate instead. (15:53)

\ldots\ I think we might just look up properties semantically.\,. .\,.\,And then we don't need to have the classID field.\,. .\,.\,(And we already shouldn't need that, since the app will most often know it from the template (templateID).) (17:32) .\,.\,I'm thinking, then, of using a bot to uprate all property values from the propStruct.\,. .\,.\,I mean, if we are only using that bot, then it's pretty much the same, only it's not template-dependent and/or template-reliant.\,. (17:36)

\ldots Hm, it seems more ``robust'' in some way to let the EntityTitles look first in the propStruct, but.\,. (17:53) .\,.\,So I'll implement that, and then we can also implement the other thing. Then when a compound title relies on a semantic property, the provided property is then an entity, whereas it is just a string when EntityTitle (or maybe a DataFetcher module) is supposed to look directly in the defining propStruct. (17:56)

.\,.\,Hm, a PropertyFetcher module.\,. (17:57) .\,.\,Or JS class.\,. \ldots Let me make it as a JS class.\,. (18:09)

(28.08.24, 17:10) It makes sense to still call relations tags: then there are just normal/monadic tags and relational/dyadic tags.

(21:06) With by impact--correlation system, it is actually best that one always tries to make the rating scales, at least of the most `important' ones for correlations, such that the middle rating value typically means that the impact is 0 (approximately, for most users (hopefully)) in relation to whatever overall rating is the relevant one (e.g.\ `good' or `likelihood').

(29.08.24, 8:51) Wait, wouldn't it actually be nice to also make the reverse index for the SemanticInputs? .\,.\,(which I might rename, btw.\,.) Then we automatically get rating values of e.g.\ ``obj is a subclass of subj''.\,. .\,.\,Ooh, or should we just make a special class of derived properties that is always interpreted as exactly the reverse and thus should never be rated themselves?\,.\,. (8:56) .\,.\,That requires a bot to do that.\,. no, it just requires all bots to.\,. .\,.\,to just rate the right derived tag.\,. but then isn't it just much easier to have the reversed index?\,.\,. (8:58)

.\,.\,Ooh, and for syntactic sugar, we of course just turn the arrow, e.g.\ to give us.\,. .\,.\,`relational tag\,$\leftarrow$\,subclass'.\,.\,? .\,.\,In English: `relational tag is a subclass of \ldots'.\,. (9:03) .\,.\,Sure, that could work.\,.

.\,.\,But then you get an ambiguity, though, which I don't much like. So in that case, it might be better to introduce the reversed relations. But then they.\,. Oh, they actually just need to state that they are reversions of another tag, and when bots go to rate them, they then just never look at these tag's own rating, but at the relation that they derive from. And the app also generally hinders users from rating these tags on their own, but automatically redirect these ratings to the relation it derives from, and also always pull the rating from there instead of from the relation itself. (9:10) .\,.\,Hm, but then we have the problem of how do bots then even know that the derived relation deserves attention.\,. Oh, they don't need to.\,. .\,.\,Yeah, they don't, it is all handled by the app. (9:12)
.\,.\,Well, the app still needs to know which are the reversed tags, then, and when to use them. But maybe we could.\,. Hm, let me just think for a bit.\,. (9:20) .\,.\,Hm, if we do make that reversed index, isn't it then just like I said: The app then just notes whenever a relation is derived, and acts accordingly?\,.\,. (9:23) .\,.\,That sounds good. Hm, but what about tags? Well, we can always divide it up into two tables if need be.\,. (9:26) .\,.\,Yeah, 'cause otherwise you just add a lot of useless instance lists with zeros at the end in the database. So yeah, let us divide it into two. (But we'll still require the all compound tags are then rated in the relational ratings table, and not as the tag; only monadic tags are rated in the monadic tag table.) (9:29)

(9:33) Hm, `my movie $\rightarrow$ director: John Doe' is easiest to formulate as `John Doe is the/a director of my movie,' so it makes sense to call my movie the `object' instead of the `subject,' which means that we can then also neatly call John Doe the `subject' in monadic ratings such as `good as a director' or `has directed many movies,' e.g. (9:37)

(9:57) Wait, now I'm thinking that reversed relations are not useful after all, since the rating will often depend on the direction. So maybe I'll turn it back.\,. .\,.\,On the other hand, it doesn't hurt to have the two tables divided. And then if we want to ever have that additional index, we can just add it, without having to split the table in two again.\,. (10:00)

(12:14) Nice, you can handle go back button events in HTML5 and in particular with the React Router, even such that when you click the go back button at the top left of the browser, the site isn't reloaded (unless you run out of previous pages in the router). .\,.\,So now that I don't want several columns at the same time anyway, let me move away from my whole AppColumn idea, and just use the React Router instead.

(12:23) When we make that browser extension, we can get it to query ad libitum but we cannot post inputs, unless we simply open an iframe to do so. And now I'm thinking, that could actually also be a standard on the regular website.\,. .\,.\,Yeah, one could do that, which then can make anyone able to make their own web app using the same data base; then they just also direct to my website particularly for these iframe insertion confirm prompts. And yeah, this means that it could even be very easy to copy the whole website and host it on a different domain (and then potentially start making changes from there). So yeah, I might add this prompt iframe for all requests where the user needs to be logged in (and thus needs to confirm their action). I should then also make this prompt very recognizable, so that users don't get tricked into clicking confirm on another webpage, thinking that they are clicking something else. Oh, and I should also add a restriction that not only should the whole iframe be in view by the user (and of its minimum size at least), but it should also have been in view in long enough time that users can react, perhaps almost a second. .\,.\,Oh, or even better, the mouse has to be outside of the iframe in order to activate the confirm button. .\,.\,And if not, it takes a least a second to activate. (12:34) .\,.\,And for user related requests, there is not just a confirm iframe, but the user has to do the whole action, including selecting the wanted action, inside that iframe (another page cannot just jump directly to the confirm prompt at the end). (12:36)

(12:59) By the way, it would also not be hard to let users download there own server, running on local host, to speed up the app, but I don't expect that that many will do that, at least not at first.

I think I will refactor to the React Router now. But before I start this, let me write that my plans for the EntityTitle is to query for the class first, and actually also query semantically.\,. well, let me actually not do this from the start.\,. Then we query for the best title temple, which is a whole new class of entities, where instead of the `\%' placeholders, the title placeholders are named ones. For each of these placeholders, the EntityTitle then queries both the propStruct as well as a semantic query. Based on the rating values, it then orders the first few found title template, and then begins a process of trying to fill in each one. .\,.\,Wait no, first a list of a few title templates is queried for, exclusively ``semantically'' (looking in the RelationalRatings table). Then it tries to query for the properties of the first placeholder, both in the propStruct and with semantic queries for one other user (accepting only ones that are rated above a certain threshold). Depending on the rating, either the propStruct or the semantically queried one is inserted (but maybe in the beginning we just always use that of the propStruct whenever it is available, I think so.\,.). If all properties can be inserted in place of the (named) placeholders (of the same names as the property), then the EntityTitle is printed. Else the next of the title temple in the short list is tried, and then the next, etc., until a default rendering is chosen instead if they all fail (and maybe one with a warning). (13:10)

\ldots\ (14:10) I just had a very nice idea. We can actually even make CSS figure out whether e.g.\ the subject (previously called the instance or object) of the tag needs to be rendered or not! For instance, if we are looking at relevant tags to e.g.\ the LotR movie, then we can just include said subject always, but let CSS figure out whether to write it or if it should simply write the tag itself (e.g.\ `good' or `funny' etc.). And the way CSS can do this is simply look for the first ancestor of the HTML element that has a `context' className. And if the entID of that context matches the ID of the subject in the given rating scale title, then it simply removed the `LotR' part of the title. And the same can also be said for the relevant class! We don't generally want to use tags that are just `good,' without any specification of the class. So the full tag is actually `good movie' here, but CSS can just hide the class part of the title, if the closest ancestor class context is of the same ID! (But it can render all unexpected classes, of course.) And we can also do the same for section headers. For instance, ander the page of John Doe, we can simply write the section header `Early life' for that section. But when viewing the same section in a different context, then the header can instead say the whole thing: `Early life of John Doe.'\,! (14:19)

As for property tags, we can perhaps also do a similar thing, i.e.\ removing the expected part in the right context. Now, I have more to say about properties as well: I still think that lower case, singular nouns should always be used. But I can just call a function that capitalizes and/or pluralizes the word in the right context, e.g.\ in a page section header. Pluralization will then depend on the usually expected number of that property (is it generally one-to-one, one-to-few, or one-to-many?). So here we can look that up (semantically, if not from the propStruct) to figure out whether to call the pluralization procedure. Now, I will just make a simply one to begin with, that just uses the most common (regular) rules. But since the database will at some point include all words with info on how to pluralize etc.\ anyway, i.e.\ since it will include dictionary info, then all we need to do is to look up that entry in the database at that point in order to right the exact right inflection (I think it's called, i.e.\ pluralization in this case). So there we are! This means that we can just use the property names directly for the page header on e.g.\ the `relevant properties' page, and I will just make a simple function to capitalize and potentially pluralize at first, which can then always be made more sophisticated at a later point.\,:) (14:27)

(15:08) Hm, about the react router, if I want to achieve the same thing, I cannot simply use the standard Links, unless I save all states of the modules on the page, which I won't.\,.

(15:41) Ah, Less CSS sounds nice. I think I will use that.

(16:27) Hm okay, maybe I don't want to use the React Router: I think my column idea was better: You just keep the previous pages in memory and then if you open a new column from within the row of columns, all columns to the right are just deleted. Also we can always just keep a max amount of open columns and then close ones that are too far to the left, meaning that they will be rerendered if you go back to them again. (And then the same thing happens in the other end, so you need to close the columns in between if you want to go back and forth between these column and not cause rerenders.) .\,.\,And then I will just use the HTML5 History API to push.\,. wait a minute.\,. (16:33) .\,.\,Oh, I could also just use the same route for all paths, right.\,. .\,.\,Yeah, I'm pretty sure that the state will be saved then. So I guess I'll just use the React Router for the easy API, but only have one route in it, which.\,. wait.\,. .\,.\,Yeah, the props of a component can change while the state(s) remain the same, as I thought, so yeah, I should be able to do this. (16:46)

%(17:16) Hm, why haven't I used template literals before in JS?.. It must have had something to do with working on that JS subset, but... ..Let me start using those from now on..

%(18:25) Good, I can see that even the focus/highlighting of the link doesn't change when I follow a link back to the same page. I don't know why InterfaceMain doesn't update though, since I should be changing its props, but there's probably a reason for that. I'm sure it will work.

(19:50) I think I will actually just let the rat\_val go from 0 to 200 (and then the app just needs to subtract 100 when it also parseInt()s anyway). Then I will actually make -100\%--100\% the standard rating scale.\,.\,! 'Cause it makes sense to rate most predicates like that---in fact, it makes great sense, since it also mitigates some of the bias that people often choose the max score. But if it says `this movie's funny score is 100\%,' I think many people are much more likely to pause a little and choose a more fitting score.(!\,!)\,:)

Now, some things makes more sense to rate on a 0--100\% scale, and I will therefore also allow that. And here I think I will just store this as precisely a 0--100 (tiny)int. And now I'm actually even considering making a separate table for this: MonadicPositiveRatings. The fact that I have already split SemanticInputs up make this more natural: why not just make as many versions of SemanticInputs as we need?\,. I'm therefore even thinking that if we ever want a SemanticInputs table back with a SMALLINT, or even larger ints, then, well, why don't we simply make that?\,.\,! So there we are: If that ever becomes beneficial, then we'll simply add it alongside the other SemanticInputs/Ratings tables. (20:01)

Another thing: I think I will actually do something a lot simpler for the EntityTitles. I think I will just write out the class, and then up to three of the next (assuming that `class' is first) properties in the propStruct, but stopping if a `title' property occurs. This makes the rendering not just propStruct- but also template-dependent, but this is fine: we must just make sure to make good templates where the properties that should (maybe) be used for the EntityTitle appears first in the propStruct. Now, when a property is an entity itself (i.e.\ of the form `@$<$n$>$'), then it is also rendered as an EntityTitle, with a recLevel of one less, and we'll stop pretty quickly here before we just write their IDs instead.

So when an EntityTitle is written out without further CSS, it will just be read as ``class: \ldots, prop1: \ldots, title: \ldots,'' e.g., but then we will simply rearrange this with CSS.\,. Well, this makes it less user dependent, 'cause it will take a while before users can upload and uprate their own CSS (and this will only be used after verification, possible human verification).\,. Hm.\,. .\,.\,But yeah, the idea is to ``just'' let CSS potentially rearrange these elements (which all have appropriate classNames), and probably hide some of them.\,. (20:11)

But yeah, is this what we want? Is it good to use CSS for all this?\,.\,. (20:11) .\,.\,Yeah, it still makes sense: Users \emph{will} be able to uplaod CSS, and we developers \emph{will} be able to find time to go though and authorize the most popular ones submitted. (20:14)

(20:20) I will make a sticky navigator header at the top with the side arrows, as well as with a row of buttons for each column. If a button moves so far to the left or right (from the one in focus) that it turns gray (before subsequently disappearing if you move further), then it will mean that its state is just about to be deleted (and will if it moves out of view). For small screens, I wil btw just say that these buttons don't \emph{need} to be clickable: I will rather remove the clickability rather than remove some of them from view.\,. But then again, if a users clicks on the wrong one, so what?\,. Then you just jump closer to it, and can use the arrows from there. Well, in fact, the user can't see which is which anyway, so even on a computer screen, they will often hit the wrong one and then use arrows (potentially the keyboard arrows) from there. (20:25)

(30.08.24, 10:59) I was awake for a while in the night and got a lot of ideas. The most important one is that I will introduce a class of `representations'.\,. Wait, let me first note an idea I got here in the bath, which is that I should introduce a `@this' reference such that any entity can now reference itself, like my `class' entity does. But back to `representations,' these hold nothing but an object that defines an entity, as well as potentially some data\_input which is meant to be substituted in the resulting propStruct of that object in the end. The idea is then that e.g.\ EntityTitle queries for the propStruct/metadata of the entity itself, and also for the highest rated representation entity for that entity, and if one is returned with a high enough rating, it actually queries for the metadata of that entity instead, and constructs the final propStruct from that instead (getting the data from the aforementioned object rather than the usual way). The representation is then supposed to hold all relevant classes, as well as all other relevant defining data that e.g.\ an EntityTitle might want to show, not necessarily in its initial form but perhaps in a dropout info menu as well. Hm, now I'm wondering if we should divide this into.\,. yes. Let us divide this into to searches, and then don't require that the representation necessarily holds all relevant classes, since new classes can be added frequently.\,. Hm, but some new classes then require new properties.\,. Hm.\,. (11:11) .\,.\,Hm, but this is meant to \emph{represent} (and thus define/specify) the entity, and not to show all relevant properties, so if we e.g.\ want \emph{all} relevant classes, for instance for the class tab menu (right below the page header), then we query the rated list of relevant classes, of course.\,. Hm, I think I will indeed call them `rated lists' from now on, i.e.\ instead of `instance lists.' So yeah, representation are meant for the EntityTitle as well as the metadata display (showing the data\_input, inserted into the propStruct as well), and that's really it. (11:17)
And EntityTitle should actually print out all the properties of the resulting propStruct (from the entity itself or from the best representation), not just a few, and not just until the `tilte' is reached. As said, it can then just hide most of these under a dropout menu (which I must find out how to implement somehow, potentially within span tags.\,.).

The representation thus allow us to edit entities in practice. But they should of course only be used if it is believed that they will not invalidate any significant fraction of the existing, non-meta (remember that tags, both monadic and relational, can have an `isMeta' boolean property (which \emph{must} be located in the entity's own propStruct, however)) ratings by other users. Otherwise one should rather create a new entity instead.

So when a user ``edits'' an entity, they.\,. well, first they should be directed to the list of representations to see if the edit already exists and just needs up-rating. And else a new edit simply means adding a new representation to this rated list, uprating it, and hoping that others will do the same.

And here's a related idea: I think that at some point in the early stage of the web app, I think we ought to extraordinarily allow the database to actually overwrite entities with their best representations (and then deleting the same representation from the relevant rated list). I think that could be a good idea. But we'll see. (11:28)

What else was there?\,.\,. .\,.\,I'm considering going back to just always using 1--255 underneath any given rating scale (at least until we implement SMALLINT (short int) rating scales).\,. .\,.\,Hm, alternatively: 0--200, and then we just shift when dividing with two for the positive rating scales such that the become rounded.\,. Hm.\,. .\,.\,Oh, I might as well use 0--254 instead, actually.\,. And shouldn't we just divide by 254, then? It doesn't hurt that a rating.\,. Oh, users won't even see a difference: if they rate a number, $n$, then they will always see $n$ again when calculating back, unless of course there is a calculation on top.\,. .\,.\,Yeah, it's fine: I'll use the 0--254 scale underneath (I still think that it is important to have the rating right in the middle, though, hence 254 and not 255.\,.).\,.

(11:56) Oh, I forgot: The EntityTitle should actually also query for and print out the rating of the `useful entity' (meta) tag, such that users can always see the popularity of the entity whenever they see it.\,:) (11:58)

%(13:20) Ah, instead of listining to History events (go back, go forward), and do a very complicated state handling, I'll just add an extra index as part of the key when a column is opened that already exists in the row of columns. And then the Link will always go to the new key.. Hm, that's it: the Link wrapper checks existing columns and add the index if necessary. And when SDBInterface sees the added index, it makes sure to open a new column instead. If the index is unecessary since there are no existing columns of that type, then we just make a redirect *(I meant 'replace,' using the terminology of the react router) and remove the redundant index. ...(13:42) Oh, the columnManager should make the redirect (in the terminology of the react router) instead, i.e. when calling openColumn. ..Yes, that's better. (13:50) ..And then SDBInterface still carries the responsibility of focussing the new column, after it is opened and subsequently redirected to (giving the reins to SDBInterface). And if the directed-to column somehow doesn't exist (even though it always should, I think), the SDBInterface just calls openCoulumn to open and redirect to that column.

(02.09.24, 9:45) I've gotten a good idea: I will still let the scale run from 0 to 254, and then let the scale in the app run from -100--100 in steps of 1, or 0--100 in steps of 0.5, but then I will also implement that users can choose intervals instead of precise rating value when they submit a rating. And I will then let these choices be denoted by some of the numbers that are not one of the nearest to the integers when dividing with 255/200. And they should preferably by somewhere in the middle of the interval, just so that when you look a a user's rated list, they still appear roughly in the right order, even if the user has rated in a mixed way. In terms of how to rate the entities, I then imagine that there's a plus--minus button, perhaps, next/under all ratings to chance the intervals. So in the lowest setting, you just choose between the upper or the lower intervals (split on 0 or 50 for the two types, respectively). Then you divide the scale into three, then four, then five, then seven, then.\,. .\,.\,eleven.\,. then twenty-one, perhaps. Let's see, that sums to.\,. .\,.53.\,. Well, that's perfect, isn't it.\,:) .\,.\,Of course, there will be many numbers around 0 that we'll need.\,. .\,.\,Oh and the last setting is of course 201 (0--200). .\,.\,So yeah, that almost sounds meant to be, but.\,. .\,.\,Hm, if we went with equal numbers, it would perhaps be possible, plus it stops people from gravitating as much towards exactly 0/50.\,. which is actually a very good thing.\,. Okay.\,. So 2, 3, 4, 6, 10, 20? .\,.\,Sounds good.\,.

.\,.\,Anyway, and I imagine that the rating bar slider then turns oblong to match the current interval, and thet the interval is just a global variable/context, by the way, such that all rating bars just share the same interval setting everywhere. And when selecting an interval rather than a (more) precise number, the rating should also displayed at the right as an interval.

I think I will just use the same system for both the positive and the polar scales, i.e.\ -100--100 and 0--100, but I'm pretty sure that I will also create a table with more precise ratings (e.g.\ SMALLINTs), at least at some point.

.\,.\,In terms of the discussions, I think I good instruction could simply be: make sure to parametrize all arguments (pro and con), such that they can be rated on a scale.

I've also gotten this idea which means that we don't need another ratings table, necessarily, in order to handle logarithmic rating scales. Say you don't know the cost of something at all, and that you therefore need a logarithmic scale in order to be sure that users can make their ratings, and that the scores can be contained on the scale. That's fine. All you need to do, then, is to make the `correlation' rating (not the `impact' rating) logarithmic as well, since it is then likely that the impact per cost will also vary greatly, then, if the actual cost can vary greatly. For instance, one user might thnk that the cost is 50 times more than another user, but also think that this matters 50 times less to the overall statement (and therefore end up with the same impact estimation). (10:18)

I think that all these parametrized arguments should also generally come with an explanation/elaboration. And here we might just use the text from the more loose discussion that should accompany any structured discussion: It is not the goal to immediately start making parametrized arguments first thing: First you discuss a bit loosely and try to flesh out what the various impactful/`important' arguments are, and try to discuss how they are important and such, just like you would do normally in an (internet) discussion. But then at the same time, the users should also slowly start selecting parametrized arguments, as this is the end-goal of this loose discussion in a sense, together with finding good written argument for why the various things are impactful. And this means that once the list of parametrized arguments are more fully formed, these should already also be some explanations/elaboration in the loose discussion thread---no not thread; it can still be a wall instead where the good inputs can be up-rated to appear higher---to take from. So when you make the parametrized argument, you can pick out some good texts to accompany it which summarizes the argument and also talk about why it may or may not be very impactful. (10:26)

.\,.\,(10:27) About the EntityTitles, I'm considering whether to let the title be a button for a drop down info menu as well, or to have that button to the left side, and let the title be a column link, or if that column link should instead be the button to the left.\,. .\,.\,Maybe a open-new-column link to the left, and big enough for mobile users to press it, and then the title itself becomes a drop down meta info display (which returns if you click the title again, or anywhere else).\,. That sounds good.\,. (10:31)

(10:32) Hm, I could let there be one more odd number.\,. 2, 3, 4, 7, 12, 20? .\,.\,Sure.\,. (10:33)

.\,.\,Oh, and let me talk about the fact that when bots calculate new ratings, they should make bins labels from 0 to 200, and record all the most precise ratings first (by adding them to this histogram (potentially weighted)). Then the take the intervals from the highest to lowest setting, and for the mean adds the ratings to the bin of the mean within that interval. (So if the mean of all precise ratings from 0 to 100 is 33, then you add all the 0--100 interval ratings to the 33 bin.) For median bots, you.\,. .\,.\,I guess you could do several things. But at the end of the day, you should end up with the number so that when all interval ratings for any intervals that contains the median number is removed, then there is an equal number of ratings to both sides of that number, also counting all the interval ratings where the intervals are strictly to one side of that median. (10:41)

%(10:54) Jeg skal bruge React Router Links alligevel, og så er det SDBInterface, der skal handle openColumn(), hvis den får en column-nøgle, som ikke allerede eksisterer i rækken.

(12:13) I should also note that I now want to show the `useful entity' rating as just a vertical colored (rating value-dependent) bar to the left of the title, with no number shown, but just with a maximum height of the text width. Now, should I combine this with the link button, then?\,.\,. .\,.\,Hm, how about a drop down menu button showing the first three letters *(3--5) of the class, with a rating to the left or right of it (before the title), and where the title is then the link?\,.\,. (12:19) .\,.\,That sounds better.\,.

(12:20) Oh, I should also quickly note: We can't do the context entity thing in CSS alone, since that would require regex's with placeholders. But we can just use a React context, and then make a class to denote whenever an EntityTitle sub-span is the same as the context entity, or the context class (and whatever else we might have). (12:22)

.\,.\,We could also use `useful instance of $<$class$>$' tags instead.\,. (12:25) .\,.\,Nah.\,. (12:26)

.\,.\,I want to show the last class in the class set, but then again, that will often be a long one.\,. .\,.\,I could also let CSS decide, and then just make it so that unknown classes get least priority, and then actually give priority to the first class in the list from there.\,. That actually sounds like a good plan.\,. .\,.\,Yeah, 'cause we also only need to prepend all the first letters of these classes; CSS can be responsible for rendering the first priority in an interesting way. (12:35) .\,.\,I guess I can even let CSS decide when to cut off the class name. (12:37)

(12:40) Hm, we don't want to task the users with uprating compound entities, do we?\,.\,. .\,.\,(12:43) Hm, but if I print out the sub-entities usefulness rating as well (in HTML), then I can just CSS-remove the outer rating for compound entities.\,. .\,.\,Hm, shouldn't I just move the usefulness ratings down in the drop down menu/display?\,.\,. (12:46) .\,.\,I think I'll do that, at least for now. (12:47)

(13:48) One could also display the usefulness rating and/or the class instance rating as a colored outline of the class button box. Then we can just divide the rating scale into, say, five intervals, and then display a different color for each interval. And when I say `\emph{and}/or,' I'm thinking that one \emph{could} even make two outlines for the two different ratings. I am, however, also considering postponing making such ratings part of the EntityTitle itself (and not the drop-down menu), as I've mentioned.\,. (13:51) .\,.\,Hm, there \emph{is} something tempting about using the class instance rating.\,. .\,.\,Hm, and about using both.\,. \ldots Hm, it's easier to just show it in the drop down menu; and that's only one click away.\,.

.\,.\,Yes, and if I ever include a rating to show next to the title, let me then just make a separate module for that. (14:16)

%(14:52) (Okay, I hoped I could get back from my midday walk and continue with the same freshness and productivity level, but I guess it was good while it lasted; still impressive (given that my last three nights have been: night bus, party, night bus).. I think I have more in me, actually, but we'll see..)

\ldots\ (16:04) I think I will use SASS rather than LESS. And even though one \emph{could} try to hack using CSS specificity, I will just use the not selector together with SASS in order to implement the logic flow of how to render the EntityTitles.

(16:39) While it might be a good idea to make it CSS-dependent which class to use from the entity's metadata, or it's `representation,' let us just cut corners for now by assuming that we always just want to use the \emph{first} class in the set array of classes.

%(16:42) Oh, and from there, it as actually also quite easy to extend, since you can just base you selectors around what class appears first in the set array, and then you only have to select between subclasses from there, if you in some cases want to make a specific rendering for a specific subclass. (16:43)

(17:07) Oh, you could also do something else, rather than using the left-over numbers when dividing with 255/200 for the intervals.\,.\,!\,.\,. You could perhaps use a multiple byte rating.\,. .\,.\,Adding a prefix byte just before the rat\_val in the PRIMARY KEY, but then perhaps make it so that.\,. Hm, this is almost the same as using SMALLINTs, hm, except that you can still output the ratings as TINYINTs if you want.\,. Hm.\,. (17:11)
\ldots (17:41) I think I will add another byte \emph{after} the rat\_val byte, used for denoting the interval radius of the rating (and potentially more in the future), at it almost cost us nothing: It shouldn't result in much more hard disk space used once compressed when it comes to the rated lists of the bots. And here we can also just remove it from the server response.\,. Hm, or use a different table.\,. Hm.\,. .\,.\,Oh, wouldn't it actually be nice if the bots also showed an estimated ``uncertainty'' as well, rather than just using an offset?\,:) I actually wanted to show the rough number of users who have rated the scale anyway in a future implementation. But here we jut get that opportunity from the get-go!\,:) (17:51) Okay, I'll do that.\,:) (I'll add that byte after rat\_val to show an interval radius (and perhaps more in the future, since we don't need 256 different interval radii).) (17:52)

\ldots Hm, if we plan on using different values of the rat\_modifier for most bots, then we could also put it last in the PK.\,. (18:14)

\ldots (18:23) Oh, we can also handle Gaussian errors in user-submitted ratings, since one can then just split up and distribute the rating into all the 200 (or 255, or 254) bins (see above).

.\,.\,Bots can then generally show the mean, or median, and the standard deviation of the resulting curve. And maybe a general error can be added to that which depends on the number of.\,. users who has rated, hm, this could be achieved by starting with a small offset value in each bin, by the way.\,. (18:30)

.\,.\,And then you can make use of ``pessimistic'' bots that always moves the mean/median, say, a standard deviation closer to the center of the scale, and then also adds further to the standard deviation (to sorta make up for this systematic shift).\,. (18:34) .\,.\,`Center-biased' is perhaps a more precise term, rather than `pessimistic'.\,.

.\,.\,Hm, but I think it's actually much preferable, if users are instructed and/or nudged (perhaps strongly) to use the flat intervals instead, which doesn't cause a greater error.\,. .\,.\,Oh, never mind, the Gaussian distributions for user-submitted ratings also doesn't mean that we won't get a smaller and smaller error, as long as we measure the standard deviation of the estimated mean, and not just the standard deviation on the distribution itself. (18:42)

\ldots (18:57) Hm, it's just better for the users to use those flat intervals.\,. I guess you could make a thing where the further away from the estimated mean, the more the individual rating ``will pull that rating in it's direction''.\,. sorta like how it is normally, but where you reduce the pull.\,. ah, but this is just the same as down-weighing your own rating (which is also what I landed on at some point in an earlier iteration).\,. Hm, so should we do that instead of the Gaussian user-submitted ratings?\,.\,. (19:00) .\,.\,I guess so.\,.

.\,.\,Well, we could, but the ``flat'' interval where you don't drag the rating at all if it's already within the interval (at the end after all votes/rates are counted and processed) is still much more useful in general, I think.\,.

\ldots\ (19:24) Something else that is nice with this rating modifier: It also potentially means that I don't have to make any offset bots at all, since this can just be taken care of by the app, mainly, can it not?\,. And this is good since the preferred offset would be very user-dependent, and one user might even want to use several settings in the same session. So much better to just---for the most part---take care of that by simply supplying the std.\ dev., and then let the app do the work from there. (19:27) .\,.\,(So in other words: the app is then (mostly) responsible for making ratings center-biased, to the degree that the user wants.) .\,.\,(Of course, for very large rating lists, where you don't want to query for a too big a chunk of the list, you might want bots to help, and not let it be up to the app alone (or you may want to divide the rated list into several smaller ones, for that matter).)

%(19:51) Forgot to note something earlier: When clicking back to column in the middle of the row, we should go() back that amount in the History. That way, we can never go back to a column to the right, and we can never go forward to a column that is not on the right.

(20:13) Hm, the idea with the ``flat'' distributions is also a little bit dangerous, since it gives a systematically smaller error.\,. .\,.\,So maybe the idea of just `pulling less when the rating is closer to the final mean'' is better, actually.\,. .\,.\,Hm, a distance-dependent weight.\,. But then you could also let this weight function be a step function.\,. .\,.\,But that's a decent tool: To have a curve that is 1 minus the self-submitted weight.\,. (20.20) .\,.\,Good.\,. (20:22) .\,.\,Well, it might also be quite complicated.\,. (20:25) .\,.\,Perhaps better to just be able to lower one's (self-submitted) weight.\,. (20:26) .\,.\,Nah, it's only usable for the common user if it's a distance-dependent weight.\,. (20:32) .\,.\,And I like my flat curves best for these functions. So let's do that. And then the bots can just use some other flags, denoting the approximate std.\ dev.\ of the mean instead. (20:34) .\,.\,(And I think I want to show an interval of two sigma, not one, since this convention of using one sigma for error bars are very confusing to many: it goes against our cultural intuition of what we mean be errors/deviation intervals (even a \emph{very} large portion of actual scientist seem to not get this convention, by the way).) (20:37) *We really ought, by the way, to make it a convention for data plots to show 2$\sigma$ in a ``backwards compatible'' way by extending the vertical line (or the horizontal line in the rare case of horizontal error bars) through the 1-$\sigma$ markers (i.e.\ the perpendicular lines) and continue all the way to $2\sigma$. As long as the 1-$\sigma$ markers are still just very clear, I suspect that most people would understand the meaning right away, without any big need for clarification, really, if they are already used to the present convention (of stopping the line at the 1-$\sigma$ markers).

(21:47) Ah, with the whole column system, I don't really need the drop-down info display.\,. Well, except when you have column in the front that you don't want to erase, but I guess I could just do that thing where the.\,. column is inserted.\,. maybe.\,. .\,.\,Yeah, well, it's a separate module anyway, so yes, I don't need to implement it now: I can just show the meta data either in the default tab or in the header of the column. (21:51)

(03.09.24, 10:08) We don't need to insert columns in the middle; users can just ctrl-click to open a new tab.\,. and on mobile?\,.\,. .\,.\,Hm, they can also open new tabs there, but it's not so nice. But I actually think that I will make a button to expand an EntityTitle, but put it at the end instead. And if the title is long, that button just turns into an ellipses, rather than a down arrowhead, which means that I'll also know that EntityTitles can always be rendered on one line if that suits the given situation. (10:12) .\,.\,I'll then make a drop-back-up button at the bottom when expanded, but a maybe a click outside will also.\,. yeah, will also put it back up, \emph{but} will probably also do the other action of the click, if any. And in order to make it more neat, I think the EntityTitle should float above all else when selected (and dropped down). Note that the title itself is then still a link to open a new column; it's the ellipses or down arrowhead button at the end that drops the menu down, and only that. (10:16)

(10:44) Ah, I was worried about going back to the website after having been on a another, and then having the app load new columns to the right whenever you go back. But because we have the history.action property, the app can make sure to open these columns to the left when you go back to a column that has disappeared.

(10:47) Hm, let me actually think about the ratings for the EntityElements (on rated lists); how do we show the rating bars?\,.\,. .\,.\,Hm, just a drop-down menu like I had it before?\,.\,. .\,.\,Sure. And then the unexpanded entity elements should just have a picture or class icon to the left, then the (single-line) title, and then the rating value of the most relevant tag to the right. Below or to the.\,. Or no, maybe when you click the rating part of the element, you get the drop-down menu of ratings. Cool. And as opposed to the EntityTitles, the elements expand to push down the element below in the list when this rating menu is shown. Okay.\,:) (10:55) .\,.\,(10:04) Well, the rating (or perhaps several, if the user wants) can be shown together with a $\pm$ sign followed by the error, where the plus--minus sign can have a short text above it denoting what kind of error it is, e.g.\ `($2\sigma$)' written above it.

(12:57) Did I ever note that when we start trusting other apps from third parties, we can CORS-allow other sites to input (and delete) data from users?\,. And no, I don't have to make a prompt each time the user submits an input, if I don't want that. (But making such a prompt available, still, can allow other apps to work, even before they are trusted (by the company and the community behind the SDB).)

(16:30) Hm, should I add (optional) types to the members of the propStructs? For instance: `class(@this):@this'.\,.\,?\,\ldots\  \ldots\ (17:54) Yes, I will have this, but I will not put it after the property names, but after the entity references. This also opens up for suppressing class clarifications in texts, so that you e.g.\ don't get `the movie's director is John Doe (director).' I will implement this by having an optional `c$<$classID$>$' suffix to the entity references. Note also that `\%e1c1' becomes e.g.\ `@123c1' after substitution, which is how we can define the classes of template inputs (and thereby suppress the class clarification). I will then let JS print out the class clarification if and only if the class of the entity does not match the given `c$<$classID$>$' suffix. But note that CSS is still free to hide (display:none) these class clarifications if we want to (e.g.\ for tags, where I'm thinking of wrapping them in angle brackets instead).

I'm also thinking of mainly using one class for each entity. For instance, if we have @123 := `Viggo Mortensen (actor)' and @456 := `person (class)', then it doesn't hurt to render `@123c456' as `Viggo Mortensen (actor).' In other words, it doesn't hurt to specify the class from `person' to `actor,' when expecting `person' and getting `actor.'

I will not do anything to warn against mismatched classes, for instance when expecting `tag' and receiving `person.' But the template can make sure to show the class clarification when it is not the expected, to-be-suppressed one, which means that the users can see themselves if an input in a compound entity looks out-of-place. (18:08)

%(19:45) I was adding a ClassClarification module to EntityTitle, but this means that I would have to fetch data for the 'c123' suffix as well. So now I'm thinking of using the flexbox order property instead.. ..Yeah, I'll do that for sure.. ..So what to do, do I then add an 'expected' className to the prop-member-class member?.. ..Something like that, I guess.. (19:49)

%(04.09.24, 10:14) Hm, I could perhaps also just put a of-class-<class> className on entOfClass references, and then do the hiding of the class name for each individual class.. That's just not so neat.\,. ...(10:27) Hm, now I'm actually considering changing it back to having classID as a column in Entities; if I'm only using one class anyway (and the rest are always just the superclasses of that).. ..And I can just make the input proc pare a "this" instead of a numeric classID, and immediately after the insertion change the classID column to itself.. (10:30) ..And the templates and representations can always include the class property on top of this if someody want them to, so it actually doens't hurt at all to have it like that. Okay, I'll do it! :) (10:31) (And with that, it will be pretty straight-forward to supress the class, since it comes from the classID column in the first place..)

%(10:44) Hm, that does mean that we ought to remove the class property from the templates as well, but I guess that's alright..

(04.09.24, 10:46) As noted out in the source comments, I think I will introduce the classID column to the Entities table once again.\,.

(10:59) ``And I can just make the input proc pare a "this" instead of a numeric classID, and immediately after the insertion change the classID column to itself.\,.'' I guess this is more relevant for the entity inputs, actually.\,. .\,.\,(11:08) Oh, I'll of course just store `this' entityInputs as exactly that, and then let it be up to the app to change `this' with a `thisEnt' reference.

(11:57) Maybe I'll just treat missing entities exactly like/as @null, meaning that we can actually also just use `@0' instead of `@null' (since @0 is always missing). .\,.\,But maybe we want to distinguish.\,. (12:03) .\,.\,Sure.\,. .\,.\,So `@null' it is.\,. *(15:50) Maybe `@0' could mean `none' instead.\,. (whereas `@null' means `undetermined').\,.

(14:07) I actually think I will go back to the SMALLINT rat\_val again, instead of the rat\_modifier. And then I want to be strict about not letting the first byte exceed 200, when interpreted as a TINYINT UNSIGNED. For the next (and least significant) byte, I think I will say that all intergers (from 0 to 255) ending in a 1-bit (i.e.\ all odd integers) will be interpreted as a extra precision in the convention that I will use as the standard one in the beginning of the app. (But in theory, the interpretation is completely up to the app layer.) When removing the string of all trailing 1-bits, we then get a decimal floating point number to append to the first byte, where the standard deviation is interpreted as being $1\sigma = 2^{-m-1}$, where $m$ is the number of bits before the string of trailing ones (following a convention here of `significant decimals'). And if it ends in a 0-bit, then number is instead divided by some power of 2 (perhaps 4), before being put in as the exponent of $2^x$ to yield the standard deviation. I guess this means that we still have some unused numbers left, since we won't need an error above 100.\,. Well, actually we might, since we do sometimes want more flat distributions than that. Okay, great. (14:20)

(14:37) Let me actually make it a trailing string of 0-bits for the precision modification, rather than 1-bits, just so that a normal rat\_val $\in \{-100,\ldots, 100\}$ with $1\sigma = 0.5$ gets a rat\_modifier of 0 (i.e.\ 0b00000000). And yes, I'm actually keeping the second byte as rat\_modifier instead of making it a SMALLINT again. (14:41)

\ldots\ (17:31) Okay, I've finally realized what to do with those curves. I should of course just treat is as a likelihood, i.e.\ that the users rates where they find it likely that their final rating would be if they had all the time in the world to study the matter and then make a precise rating. And for the ``mean,'' all we then do is to add all the users' curves together and take the maximum (the most likely value). Easy as that. Then when estimating the error, we can of course take.\,. Well, we can look at the std.\ dev.\ of the summed curve, and perhaps we can also divide with something like the square root of the number of users, but I'll see what makes sense there. But another great thing we can do is also to calculate the `average agreement' with the `most likely value,' namely be looking at how high the maximum is compared to how high it could have been if all chose the exact same value (with no spread of the curve). .\,.\,(Assuming that we don't care about smaller bins than of width 1.\,.) .\,.\,Hm, but then we don't even need.\,. .\,.\,need the more precise rating values for anything, namely if we always bin the values anyway.\,. Okay, but that's fine. I should by the way also note that I want to add flat curves to the user-provided rat\_modifier as well, and without the precise ratings, there is just all the more room for more curve shapes. (17:42) Now, back to estimating the error, we can of course also look at the mean square of all the center points of the user ratings. And when we want to to ML over this data, we might therefore potentially make correlation matrices that is based on some mix of the `combined agreement' (or \emph{dis}agreement, perhaps) between each user, as well as a (dis)agreement measured by looking at the squared difference in the middle/center points of the user ratings. (17:45)

.\,.\,How very nice.\,:) (17:46)

(17:52) When some users haven't answered a rating scale, we can just note the average correlation/(dis)agreement between the users in the given set (of users and of rating scales) instead.

(21:13) I had a thought: I'm considering whether it would make sense to not normalize the curves, but just let them take values from 0 to 1, where 1 then means `I don't disagree with this rating result,' and 0 means `I disagree completely with this result'.\,. (21:15) .\,.\,That actually does kinda make sense to me.\,. But of course, this makes it un-normalized as a likelihood function.\,. .\,.\,Oh, maybe this is the \emph{only} thing that makes sense; maybe normalizing it would mean that users essentially get to choose their own weight. (21:19) .\,.\,Yeah.\,. Great.\,.(!\,.\,.) (21:20)

%(22:16) And then it does indeed make sense to make an inverse-correlation matrix of the user-to-user \emph{dis}agreement, and do ML on that instead.

(05.09.24, 9:23) I think I will use that convention of making rat\_modifier either embody extra precision or an exponent that determines the standard deviation (SD) of a Gaussian curve. But I will let the precision codes end in 1-bits preceded by 0, where you then also remove the 0. And I will let the exponent (all numbers ending in a 0-bit) be interpreted as a signed number, that is then divided by some number and put as the exponent of a default SD , preferably. But in any case, as long as the app and the bots just mainly don't use too much precision for the SD code, then rated lists will generally mostly contain the same few rat\_modifier values, by which they can be compressed away most of the time on the hard disk.

In terms of showing the error curves to the users, I think I will actually just show my extended error bars (where the line extends out to $2\sigma$ on either side). Then I can just show examples of what that error bar means on a separate page. (9:32)

.\,.\,Oh, and when it comes to the bots estimating the SD of a ``mean''/`most likely value,' I think they ought to take the average user-SD, and then do statistics of.\,. Well, maybe no; maybe forget the user-provided SDs altogether, and just look at the SD of the center points instead, then calculate the SD of the ``mean''/`most likely value' based on that. .\,.\,And if there are too few data points to estimate a good SD of the center points, just use some default SD instead. (9:37)

.\,.\,Hm, and for some bots, maybe especially in the early times, we can just calculate and use the actual mean of the center points, rather than finding the `most likely value' from the curves. (9:39) .\,.\,But users should still see those (extended) error bars, and be able to adjust them, both because it will create a significantly better UX, I think (since you don't have to worry about not taking the time to make a very precise estimate), and it also helps with the ML later on (and the fact that these error bars serve a purpose is also necessary in order for the UX to increase as a result of them). (9:42)

.\,.\,Hm, and if we are using a lot of actual-mean bots anyway, then.\,. Oh, then users will just want to use max error bars to ``agree'' as much with other as possible.\,. perhaps.\,. Oh well, maybe I should just stick to the `most likely value' bots, still, then. They are also not hard/costly to maintain, since the bots can just keep their histogram data, and then subtract (potentially) and add to all bins whenever a user changes a rating or adds a new one. (9:47)

.\,.\,When submitting a rating, we can still show a range out on the right, but just make it go from $-2\sigma$ to $2\sigma$ (rounded to within the interval of the rating scale, of course).\,:) (9:49)

%..(Oh, and let me clarify: Now I'm thinking of only using Gaussian curves: never the flat step curves.) (9:53)

\ldots\ (11:55) Nice that you can use :where() and :is() to better control specificity, along with using :not() and SASS.

Never mind about determining the sigma from how many significant digits there are: If the curve represents disagreement, then it might very well happen that users want to make a precise rating, namely when they want to position an entity relative to others on a rated list, but still not `disagree strongly' with other users who has selected another rating that is still quite close to that of the user in question. So I'll just let the first four bits of rat\_modifier by a precision always, and let the last four bits denote the exponent in an SD calculation. And with this, I might as well just make rat\_val a SMALLINT again, and interpreted as a floating point number between -100 and 100, or 0 and 100, by dividing and potentially translating the SMALLINT UNSIGNED value (and also in principle after making the four last bits 0, but I probably won't really do that in practice, though). (12:02)

%(13:26) Hm, do I even want to use browser history when navigating columns? Don't I just want to "redirect" in order to change the top URL? I think I'm gonna use the React Router for other things as well so I might as well keep it, but I might not want to push and pop history pages when navigating the columns.. ..Hm, I'm also gonna use Links, since I like how they already opens other tabs on ctrl-clicks.. ... (14:25) Okay, I will do like I said before, and the way I will do it is to just add 'from=<colKey>' to the queryString whenever opening a new column. SDBInterface (/SDBApp) will then open a column and immedeately redirect to it afterwards (but not change its state to the new column before after the redirect). If the there is no caller/from column, or if it is unrecognized, the SDBInterface/SDBApp just looks at the history.action (if I can find it somehow..), and opens and either opens appends the given column at the end or prepends it at the beginning, depending on the action. (14:31)

%(18:36) Even if I make it possible to have several column open, I will still just make the left-most.. wait.. ..Or the right-most, except if at the beginning..(?..) ..I guess so, so I just need to know which one is in focus, right?.. ..I'll make the row of buttons more like browser tabs, btw.. ..But yeah, the focused one is always to the right. And I still need the ColumnContext in order to know which one of the columns called for a new column, i.e. becasue we might show several on large screens.. But yeah, I'll just comunicate which is the caller over the URL, which results in an immediate redirect.. .\,.\,But I don't need the ColumnManager: I just need a state with an array of colKeys, as well as an index showing which is the focused one. ...(19:09) Ah, kinda overlooked location.state and location.key. I \emph{could} also use location.key for the column keys; only specifics needs to be known when creating the new column, and they are always there, available in the URL whenever you go back or forward to a column. ...(19:43) Oh right, I can then just push the whole columnSpec/Key array. Ha.x) ..So no need to worry about going back to another list of tabs; we can preserve the tab list in History. (And I found it, btw: window.history.) (19:45) ..So we can and should also push what column is focused, and then all we need to do is to give that column the right CSS class, and also potentially close the content of the columns automatically that are too far away, and that haven't been pinned (by pressing a pin button on the tab). (19:47)

%(06.09.24, 10:40) I'm thinking a bit about the possibilities with history.pushState(). I just had an idea of letting each history state save its own state, as well as the states of one or two columns to its left. And then we could even save scroll positions in states, such that when you navigate back to a column, it can always reform, even if it had been closed.. ..Hm, and we could limit form input fields, demanding that for large inuts, users upload from files instead (that are then already safely stored).. (10:45) ..Oh, since other sites can also push a state, we must be careful never to record any unsafe state, for instance any state the is just before a submission. ..Ah, but we woud never do that anyway, since we never want the user to submit twice if they go back.. But still, we have to make sure of this, both for the sake of redundancy (and bad UX) and also for the sake of security.
%..Hm, I could make it so that you actually always navigate forward, even when you go back in the columns, in order to.. Hm.. (10:52) ..Hm, maybe all I need is.. ..Is a redirect whenever the column changes, and then I just save the state in history such that even if users navigate to ther sites, when they get back, they will always see all the same columns, ony some of them might need to reload (but gets to the same state, even in terms of scroll positions). And then I can also safely close *(no: in-activate) columns that are far from the focused one, since their state will just reload anyway if the user goes back to them. And with the on-page tabs over all the columns, users can easily close down columns for good that they don't need. ..And when they close down a column, we can even give them a short-lived regret option, at least at some point. (11:00) Okay, I like this a lot.

%(11:04) So how to manage this column state..?.. ..I need declarativeness, so I need to get rid of the ColumnManager.. (11:12) ..I will do the same '?from=colKey and then redirect' thing, by the way, and I can now just once nonces for the colKeys; the colSpecs are contained in the sate anyway.. ..And if the column list is empty (at the beginning), always check the path for what.. Hm.. ..Oh right, always check the path, and then open \emph{both} the HOME_ENTITY (to the left) and the one parsed from the path if any. And then if you close all the columns, the home entity will always pop back up.. Hm, this will not be too neat, though.. ..Let's just say that you can't close the.. Hm.. ..Yeah, you can't close the left-most column, at least for now. Okay. (11:24) ..No, not that, but by clicking Home, you alwasy direct back to the first column, and if it is not the HOME_ENTITY, then one is prepended to the column list. (11:26)
%..So a list of column keys, and a store of column specs, and the key or index of the focused column, this we need for our state, and then what about fst?.. (11:29) ..And the number of visible columns.. ..And we just redirect to the URL of the focused column whenever it doesn't match.. ..Let's make it the \emph{index} of the focused column, and then if it exceeds the array, we just make it the right-most index instead.. ..Oh, and fst (the fist visible column) is just changed automatically, if need be, whenever the focused index is changed. Good.. (11:34)

%..(11:35) Ah, can I make a hook for history states (useHistoryState() or something)? ..Sure.:) (11:36) ..How to generate the key, though..? (11:38) ..There is the useID() hook.. (11:40) ..No, useID() will not be persistent when reloading.. ..Oh, the id is generated from the "parent path," so maybe it will be persistent.. ... (12:13) Okay, I should be able to use useId()..
%...Hm, I want to clean up unmounted components, but then again, I would also like to be able to unmount components, and then restore their state after a mount.. ..So what to do..? (12:59) ...(13:10) Hm, it would be nice if useId() IDs always appends to the ID of the parent. But if this is not strictly a part of the API, then I might make my own.. ..It's not part of the API, it seems.. (13:13) ..I could drill parentKeys (pk for short, perhaps).. ..and make a useID hook.. ..Nah, there must be a way to useId().. (13:20) ..Oh, I should remember that useId should already be persistent between re-mounts.. right..? ..Yes, useId() is deterministic. Okay.. ..Ah, I can just supply a cleanup function for each state, I guess, unless the users don't flag that they want one, in which case the cleanup can just be automatic.. ..Okay, but if only I could make a parent cleanup its children.. ..Hm, I \emph{could} make a react context for each clean-up-parent module.. (13:31) ..(13:36) I should only update the history.state when navigating away from the site, I think, as well as at some not-too-frequent intervals (measured in seconds).. ..And yeah, I could make a context: one where the component is able to dismount and remove its content without deleting its state.. ..All useHistoryState() calls should then search this context for where in history.state.componentStates to place the data.. (13:41) ..And the context provider parent should then clean up after itself on its dismount.. ..Hm, so a hook to get the context.. Hm.. (13:43) ..Nah, not a hook, but a ContextProvider wrapper module.. (13:45) ..Which can then create that.. ..Ah, that queues up a callback to save to the history.state, but returns if one is already queued up, and also if.. Hm, or actually it should replace what's in the queue.. a one-element queue. And then there's a recurring effect that saves the queued state to history.state (or calls the callback). And then we might also block navigation just for as long as it takes to save all states to history.state (without waiting first).. (13:51) ..Ah, but then the ContextProvider should maintain a one-element queue for all children that uses useHistoryState(), where the key will just still be generated from useId(). So the setState returned from useHistoryState() should thus push the statechange to the ContextProvider.. ..Which therefore provides a method/function to do so. Great. (13:55)

%(18:25) I've changed it so that I just call history.replaceState on all state changes. If that is somehow slow, then I will go back and use the LazyDelayedPromise again (which I should then also go over again first, and potentially debug). But something else: Now I'm thinking: With useHistoryState, do I even need the central column handler on MainPage (formerly InterfaceMain)..? (18:28) ..Yeah, 'cause I need to handle opening new columns there.. ..And MainPage shoud of course useHistoryState() (and now I have an outer HistoryStateProvider, outside of the BrowserRouter)..

%(22:05) Hm, should I actually use localStorage instead, or perhaps together with the browser session history..? ..Nah, cause then we.. Hm, well if we use location.key, then we can have several different states in different browser tabs.. (22:07) ..Oh, there's also sessionStorage, btw (although one could also just clear localStorage manually, under normal circumstances).. ..Oh, but sessionStorage is already tab-specific.. Alright.. ..Oh, but I don't want users to go back to the same state if they go forward to the website in the same tab.. ..But again, we could just use location.key here.. ..Hm, and sessionStorage generally has more space as well than the history.state, it seems.. ..Yes, it seems so. Okay, might as well use sessionStorage, then.:) (22:19) ..Oh, something urelated that I haven't thought of: It's quite cool that you will be able to checkpoint-backup previous states by going to a different.. well.. by clicking some link that pushes a state rather than replaces a state.. So I could make such link.. Perhaps the home icon/logo, why not?:) ..Well, unless I want that to clear out the columns.. ..Whatever, it's not that important anyway. We can test it at some point if users say/seem to want it.. (22:25) ..(i.e. to checkpoint-backup there columns (in session history)..).. well, or we could just allow them to save a state to localStorage.. Hm, that actually a decent idea, 'cause then they can jump right back to a standard set of opened columns, that they like to start with. And a user can save several states in their browser. That's neat. (But it's still a only a potential feature in the future (and one also probably have to back these states up in the database, then, for the best UX..).) (22:29)

(07.09.24, 9:17) The ML that I described above gives us correlation vectors (whatever they are called again) of users. That can be used for creating ``user groups.'' But in order to interpret these groups, all we need to do is to take each rating scale and sum up the curves of each user with the weight of the given vector. Then we get our vector of opinions. And yeah, that is what gives meaning to each vector: users can then try to interpret it to see if it makes sense, and decide whether they want to use that vector (for ``user groups'' and other bots, and as a parameter that users can rate directly, i.e.\ ``how much do you think you align with this set of opinion (or rather rating scale curves), thus giving users a quick way to adjust their query settings). (9:23)

Something else that I thought of tonight: I think we \emph{can} make it so that users can include their own and others' scripts and style sheets. We just need to make it so that, the user first has to uprate their trust in those scripts/sheets, \emph{and} they also have to manually go to their list of uprated scripts/sheets and confirm them for the specific browser, i.e.\ where this confirm is stored in localStorage (and not in the database). And each confirm has to prompt a caution message before finally accepting the confirmation. And there we go, then users can include their own and others' scripts/sheets if the want to test something out (without even having to wait for the green light from the web developers). (9:28) .\,.\,Hm, but of course, it might not be so easy to add React modules.\,. I can think more about how this could be done.\,. But on the other hands, adding style sheets is decent start, and then we can forget about adding JS scripts for now. (9:39)

(08.09.24, 10:57) In terms of adding JS scripts, I think it is better if people can just easily copy the app and the database, or use the same database from a different domain, and in particular their localhost, such that they can easily try to develop the app further on their own computer, and still be able to try it on the actual data of the database. .\,.\,Hm, but it would nice if users could then subscribe to such changes.\,. .\,.\,Oh, I think that might be possible, at least if users just.\,. .\,.\,upload and download whole scripts for the app.\,. .\,.\,Oh, wait! Global variables shouldn't be changed when minifying: I should be able to just attach modules to the window object, and then we can excess them even after React's minification.\,.\,! (11:09) .\,.\,(11:17) And we can probably even just make a final module where we import all modules as whole objects, thus getting all exports at the same time, and then attaches them to window. Of course this prevents React from minifying a lot (at least it should), but I don't think we need to care much about that.\,. (11:19) .\,.\,Hm, or do we actually want to break out of the whole JS module concept.\,. .\,.\,Maybe making a preprocessor that converts imports and export into global variable getting and setting instead.\,. (11:23) .\,.\,(Basically what React does, by the way, at least in developer mode: gathering all modules into one and attaching all imports and exports to a global object instead.\,.)

(11:33) Okay, that is great. So we should be able to afford both custom style sheets and scripts (again, where users \emph{both} have to rate that they trust the script/style first, and then also for each individual browser (or app, I guess) that they use, they have to confirm the choice to use that script/style in that particular browser (after having also uprated for their user profile), and of course with a caution warning prompt for each such confirmation).\,! If only we could also make a nice way for users to create their own bots then, preferable even where \emph{they} are the ones governing the bots.\,. so basically a user that just tells the database, that it is a bot.\,. Well, doesn't that kind of answer itself then: We can just allow users to create bot profiles the same way that they create user profiles, but where bot profiles need to include a bot description, and where the upload/download volume limit can then be automatically dependent on how well-rated the given bot is (by.\,. trusted users.\,.).\,. (11:39) .\,.\,Yeah, the SDB in question then needs to assign a weight to each user, and for users that have several profiles for the same e-mail, the user in question can just choose which one should be the bot-rater (and maybe that is all that that profile does). Great, and then the SDB can also build on top of this system by potentially blacklisting users, and perhaps also boosting ones---well, for instance we might very well want to boost users based on their contributions (considering that whole SRC-like system, described above). So there we are.\,:) (.\,.\,!) (11:48)

.\,.\, (11:52) Hm, would also be cool if we made room for making it a distributed database as well already in the beginning. I'm think of a database prefix to all users and bots, both database-native bots, and bots that are simply user-governed clients of the given database.\,. (11:54) .\,.\,A prefix, then, in the app layer, of course.\,. (11:54) .\,.\,Well, it's more complicated than that.\,. no wait, it could basically just be a prefix on all references, right.\,.\,? Of course, this doesn't work too well when the classID column is not a `@123' reference anymore, but we could just add a database prefix column to the Entities table to solve that.\,. Hm, and do we then say that the Ratings table can only include Entities of the same database?\,.\,. Hm, and do we not try to.\,. have the same IDs across databases, let me think.\,. (12:00) .\,.\,(12:05) Hm, one could try to make Entity IDs as much a shared thing as possible, but give all users a database ID where they are clients.\,. Wait, shouldn't we instead try to make this a server-side thing.\,. Hm.\,. .\,.\,(12:10) Hm, could we perhaps make each database hold a list of all other databases (the recognized ones) that.\,. hm, or just one database, in case they remove the entity.\,. Hm.\,. .\,.\,Hm, how about a list of un-specified size over \emph{some} (if not all) of the other databases that stores the database for the entity, and in case of users and bots, also a list of other databases that governs their ratings. Then a database can quickly remove entity data or rated lists if ``it'' wants to, and just redirect to one on their list. And database nodes can then talk to each other in order to keep their various.\,. `potential redirect options' lists up to date. How about that?\,.\,. (12:17) .\,.\,Hm, and when each database could then also add prefixes in all propStructs---and potentially in classID columns---that already communicates to the app that a request of this entity, or in case of a user or bot, a request also of any of its rated lists, will result in a redirect (so the app can just ask the other database straight way). If the other database has removed the entity in the meantime, however, and that this hasn't been updated, well, then the app will just get the actual redirect of that database (if it knows yet another database (node) that keeps the entity), in which case the app can just make the redirect to that third database (node). (12:22)

%(14:07) Det går lidt langsomt i dag. Sjovt når man sammenligner med i mandags.. Jeg er lidt usikker: umiddelbart kunne man godt mene, at det skulle være rent server-side, men alligvel.. Tænker lidt på om man kunne gøre ulige ID'er til foreign keys, som databasen slår op i et separat index, medmindre man på et tidspunkt erstatter det med et native ID (eller dem modsatte vej).. ..Og brugere skal i øvrigt godkende nye databaser, medmindre de er pre-godkendte.. ..Bots kan være delt på tværs a databaser, selvom de er stadig er særlig klient how én ad gangen, og så kan databaserne sørge for at den rated lists bliver gemt i alle de bot-subscribende databaser, men potentialt med forskellige external IDs.. (14:11) ..Og databaser der ikke subscriber til en bot, eller rettere til at gemme en bots data lokalt (.. lover at gemme en bots data lokalt..), de kan stadig have botten som en entitet, hvor det bare vides, at rated lists fra den bot skal hentes eksternt.. ..Hm, og når man så henter en rated list, jamen så må man også query'e samme eksterne database for metadata omkring entiteterne, nemlig fordi ID'erne så kan være mappet anderledes til entiteter.. (14:18) ..Hm, så hver database (node) kan faktisk have helt deres egen ID--entitets-mapping, kan de ikke?.. (14:20) ..Jo, hvis vi netop også query'er samme database for alle propStruct- (og classID-)referencer.. ..Hm, og vi kan eventelt sørge for at en bot selv up-rater (eller får up-rated for sig), de databaser, som subscriber til / lover at gemme dens rated lists, for så kan dette nemlig ændre sig dynamisk, selvom botten selv (og dens formål og væremåde) er konstant.. Lyder godt..:).. (14:24) ..Lyder rigtig nice.. (14:25) ..Hm, jeg burde næsten bare gøre at own_prop_struct kan erstattes med en JSON streng i stedet der benævner en liste a ekterne databaser, hvor entiteten menes at kunne findes, samt hvilket ID det har der (i.e. ét ID for hvert element i denne liste). Og så tømmer man selvfølgelig også bare data_input-feltet hver gang entiteten findes eksternt. (14:29)

%... (15:24) Åh okay, man smelter totalt.. Jeg har et par flere tanker, men ligenu tænker jeg: Bør der være en god måde at komme tilbage til ens egen primære database? Eventuelt noget med at slå.. Nej.. Hm.. ...(15:36) Ah, og hvordan blander man lige bots/brugere (til sammenblandede rating lists)?. Det går jo ikke.. ..Ej, jeg er ristet i den her varme.. ..Hm, vi skal altså sørge for at mappe de samme ID'er til de samme entiteter.. ..Kunne man give et suffix på ID'et, ift. hvilken node, der har ansvar for entiteten?.. ..Hm, eller skulle man bare lave en præfiks-bit til nye entiteter, og så gælder det bare om senere hen at ommappe dem i fælleskab i den distribuerede database?.. (15:45)

(16:02) It would be nice if the aforementioned preprocessor was located server-side, such that user just upload JSX modules, and then the server takes care of how to turn that into a script that it can serve the client to add to their HTML header. I by the way think that these modules could always be paired with a style sheet as well as a standard, which then \emph{can} just be null. (16:05)

%...(16:18) Ah, måske kan jeg se en løsning nu, som vil være rimelig nem at bruge.. ..(16:23) Ja, jeg tror, jeg har det nu..

(16:25) Hm, I guess there's also a way (a hack, really) to simply toggle off `development mode' in react, without minifying the code, and in that case, we might just do that, and then use React's own preprocessor (in development mode) to generate the added scripts.\,. Well, or at least to make the preproccessing easier.\,. perhaps.\,. or perhaps not.\,.

%(19:29) Klasse-brugergrupper... ...Factual, UI/UX (relation flags)... (19:58)

(09.09.24, 9:16) I don't recall if I have thought of this before, but the `representations' of entities is also a great way to have the site support multiple languages: Just choose a relational tag for choosing good representations that specifies your language (and hope that somebody has translated the entities that you are interested in at the given point in time).

(9:20) Okay, so I can happily make it so that users are free to totally change the app, both with CSS and even with React JSX (including JS). And with my input prompt meant for iframes, people can also host their own apps on their own domains using the same database, even before.\,. Ooh, and couldn't we make it so that a user can record their trust in another domain, and then the server can return CORS headers that match the users trust?\,.\,!\,:) (9:23) Great, and in that case, we don't even need that meant-for-an-iframe prompt.\,:) (9:24) .\,.\,But as can be seen in the source comments, I've also thought about how to even make the database a distributed (very much decentralized) one. And in order to make sure that entities get the same IDs across the d.\ database, I have come up with this solution, which is that at the ``top'' you have $n \geq 1$ amount of big, independent databases that have agreed to share their entity IDs (why not, if the common goal is decentralization). These then agree to a set of individual quotas that each database (node) in this set can add in terms of entities. And we then simply make it so that the $i$th of these $n$ databases then has to only use integers where ID mod $n$ = $i - 1$ (or ID $\equiv i - 1 \mod n$). If a database node exceeds its quota, then it just potentially has to wait a turn before the others are willing to record the entities that exceeded the limit. But if a database has a margin before the quota, then the unused IDs can go to the other databases that are the furthest in terms of their maximum ID integer. Note that when $n$ changes, you can just change the protocol. And while $n=1$, the given database can just use regularly incrementing IDs ($n$ mod 1 = 0). Then, below this level are smaller databases that are clients of these ``top-level'' databases, or even clients of clients, etc. A client database just has an agreement with their patron database (node) that it can send new entities to this patron, at a certain maximum volume each day/week/month. The cleint then waits for the patron to determine the ID of each new entity, and then simply copies that in its own database, before returning it as the outID to the user. And there we are. This creates a system where even very small (maybe run on a private PC) databases can in principle join, as long as there is some client (including a client's client, etc.) there is willing to agree to a quota for adding new entities each day/week/month. (9:44)

Users can then choose to use databases that are closer to them geographically, in order to increase the bandwidth, and choose the ones they trust the most in terms of e.g.\ not logging any data that they don't want to be logged, and such.

I should also talk about users and bots, and in particular their rated lists, here in the rendered text.\,. .\,.\,Oh, but I should also talk about the fact that the top-level databases don't need to store the propStruct or inputData of the other databases' entities, nor does the patron database need to store that of their clients. Instead we the databases can just store a list of databases that they know/think store this data, perhaps in place of the own\_prop\_struct. .\,.\,Oh, and now that the IDs are shared, that's it: The app can then just fetch the data from the other database instead, but only if the user has confirmed that they don't mind querying from that database (node), for instance in terms of trusting that that database does not try to log their queries in an unwanted way by the user. (9:54)

(10:08) About the users and bots, and their rated lists.\,. well, first of all: A client database doesn't \emph{have} to upload their user entities to their patron. I just wanted to mention that. Oh, and let me also note something else, which is that all databases/database nodes gets their own entities as well (shared by all databases, of course: they are important enough). When it comes to the rated lists, each database can subscribe to.\,. wait, let us talk about users' rated lists first: These are generally only stored by the database that the user has subscribed to, unless the user moves to another database, potentially, or if the databases has a protocol to share user ratings for some reason, perhaps to prevent users making repeated accounts at several databases.\,. although we can make a system where.\,. Hm, where the databases stores the number of profiles per e-mail, which is hashed.\,.\,? (10:15) .\,.\,Or maybe it's encrypted, and then they databases can ask for the key in order to momentarily decrypt the e-mail, or the hash of the e-mail, actually, in order to compare it with other databases to find out the total number of profiles.\,. Sure, but then again, I've also figured out that ``game'' where public profile get to anonymously add their backing of / anonymously ``claim'' there other profiles, so no need to worry, I guess.

About the bots and their rated lists, a bot is mainly governed by one database (at a time), or by a third party user, but then that user is still a main client, as a bot, to one particular database (at a time). But other databases can still subscribe to a bot, and then also receive updates whenever this bot updates it rated lists. (10:21) Hm, and what else to say here.\,.\,? .\,.\,No, that's actually just it, it seems. How easy things are now that all IDs are shared across the whole distributed database. (10:24)

And last but not least in terms of all this distributed-ness, decentralization, and user-driven-ness, we should of course also implement both that users can uprate bots for a given database to govern and maintain, which by the way ought to include an SQL (MySQL or whatever the database uses) code implementation in the description of the entity, why not; better to help the developers so that they don't carry all the responsibility of implementing the bot. (10:29) *(Oh, and the bot designers also ideally ought to provide unit testing of their implementation as well, although it should also be an option just to upload a link to a GitHub folder with the relevant code, including the implementation itself, rather than having to include it all in the bot entity itself (so to just include the GitHub link in the bot entity instead.) (10:44)) But we should, as I've probably mentioned, also make it so that third party users can upload their own bots that \emph{they} promise to govern and maintain, and where other users can then uprate these user-governed bots as well. And just like how the database ought to try to implement the databse-governed (native) bots that are the most popular, it should also accept the most popular user-governed bots, which on their part then just means to accept that the given user gets to upload and download a greater volume of data.\,. Hm, of course the third party user then also has to have a small database over at least all the relevant data to the function of the bot, and maybe that doesn't always make sense; maybe a lot of the times it just make more sense with the database governed bots, but we'll see.\,:) (10:34)

So there we are about this whole distributed-ness, decentralization, and user-driven-ness. How great.\,:) (10:35)

Now, I also had another idea last evening, which I will write about in a minute.\,. \ldots (10:49) The idea is first of all to uprate users and/or ``user groups'' (i.e.\ bots) for classes in order for their ratings to be boosted (by other bots or by the app (making it query for mixed/combined rated lists)) in terms of either factual data about entities of that class, or in terms of UI/UX enhancing relations about that class, such as e.g.\ which tabs should be shown, what are the `relevant tags' for instances of that class, etc. I think in the beginning of the app, it will actually be helpful to up-rate even single users for such ``class moderators,'' you could call them. These `moderators'' can then e.g.\ apply themselves to upload all relevant entities of that class (that they can think of/find on the internet), and to upload good data for them, either in their own propStruct or in their `representation'.\,. Oh, and more importantly also to uprate relevant factual property relations for the entities. And/or they can apply themselves for the more UI/UX-related stuff like determining tabs and instance-relevant tags and relations, etc. This will mean that instead of users having to give their rating to all these things in order to show agreement, they can instead just up-rate the various ``moderators,'' or class governors, we could also call them, and then they will hereby boost all those factual relations and/or UI/UX-related relations in one go.\,! Now that I'm writing about it, this sounds like an absolute must.(!)\,:) (11:02)

And of course, this system doesn't need to only extend to the classes, if there turns out to be some other useful cases that are similar, i.e.\ where it helps to bundle up a lot of ratings, such that users can pick between these large bundles and basically up-rate all included ratings at once. Now, I was even thinking along the lines of giving relations flags to hold that tells the app whether they are `factual' relations, or if they are `UI/UX-related' relations, or what, but let me think some more about this.\,. (11:05) .\,.\,(11:10) Well, isn't this part of it? Shouldn't we just make the relations hold flags, or properties, rather, that define whether they are `factual' or `UI/UX-related,' etc., and then when boosting a ``moderator''/``class governor,'' then it just means that \emph{all} their ratings of the relations with the flag in question (`factual' or `UI/UX-related,' etc.) will get boosted for when the given class is the subject of those relational ratings. That does make great sense, doesn't it?\,.\,!\,:) (11:14)

(12:14) In terms of money, let me also mention that using promo codes, essentially, of sponsoring companies could be another great avenue for users to ``donate''/``pay'' to the website. .\,.\,Or rather to the whole project, for if the system is to be very much decentralized, then we also want an (central, somewhat ``paradoxically'') organization that takes care of gathering ``donations''/``payments'' (often indirectly via sponsors and companies that buy adds etc.) for all users of the combined database (and websites), and makes sure to distribute them the right way.\,. Hm, and what is the `right way,' let me think about that for a bit.\,.(?) (12:23) .\,.\,Hm, do we even actually want the.\,. no, actually not; never mind about the central organization, at least in terms of making deals with companies. That part at least should not be centralized, not at all.\,. (12:25) .\,.\,Hm, the contributing users should obviously have an organization.\,. or several, that they can pool their IP together in, and demand payment from the various.\,. hm, nodes and websites that deals with the companies.\,. Or should these organizations deal with the companies directly.\,.\,? (12:29) .\,.\,Well the IP organization deal with the websites that uses and shows their contribution, and the websites then deal with both the companies (i.e.\ sponsors and add-buyers, etc.) and the client users that uses the systems, which can also be contributing members of the organizations, which might then also be counted by the website towards the ``payment''/``donation'' of the individual user. But yeah, other than that, then it is just a matter of.\,. well, in short: The websites deal with IP pool organizations, with companies, and with users, in order to get the IP, get the to get money (which is dependent both on the companies and the users who needs to watch the adds, etc., in order for the sponsorship/add buy to make sense for the companies). And then, if the website company is SRC-like, which it ought to be in my opinion, then it should count only its users as customers, I think, and not the companies, but instead count all money coming from the companies as coming from the ad-viewing, etc., users instead. And when I say SRC-\emph{like}, this is because I would also like for the IP pool organizations and/or their client `contributing users' to be part of who gets shares redistributed to them in the SRC-like system. (12:42)


%(14:45) Ah, jeg tror jeg laver useSessionState om, så cleanup-functionerne altid bare kigger i forælderen, om state'en skal slettes eller ej.. ..Hm, og det må kunne gøres ved at lade alle Providers holde et link til forælderen, således at hver komponent bare tjekker hele vejen op langs denne liste, når den unmountes.. ..Og så kan vi vel ellers bare.. Nå ja, vi kan bare gøre så at alle komponenter sletter sin egen data, så længe vi bare husker at denne også kan være slettet af forælderen.. ..Ah, og hvis vi bare bruger globale useId()-ID'er så, så skal hver komponent jo dermed kun lave et tjek: hvis data under useId()-nøglen er gemt, så slet det, ellers gør ingenting.. ..Ah, og delete brokker sig vist ikke, hvis property'en ikke findes, så vi behøver ingen gang lave et tjek her.. Hm, men hvor skal selve slet-eller-behold-træet gemmes? Skal det også gemmes i session? (14:53) ..Nej, forælderen skal jo bare gemmes i hver komponent-data-objekt, og hvis forælderen så mangler, jamen så skal dataen jo altid bare slettes. ..Hm, Og forælderne er jo så bare en wrapper-komponent, der context-provider forældernøglen, og som også har en property, der afgør, om børnene skal fjernes eller ej, hvor børnenes tilstande altså dog ikke slettes, selvom de fjernes.. (14:57)
%Hm, jeg har egentligt også lyst til at.. Ah, kunne man ikke bare.. automatisk påklistre.. Ja, vi gemmer både.. Nå nej, vi må ikke gemme funktioner i sessionStorage.. Ah, men vi kan gemme funktions\emph{nøgler}.. Så jeg kan lave en global.. ja, klasse faktisk, som.. ja, som vi både kan give en metode til at slette-medmindre-forælderen-siger-'gem,' og også en metode til at finde den nærmeste forælder af en vis type (så min useCreateParentSessionNode(), eller hvad vi skal holde den, skal altså også have en type-string..).. Lad os bare gøre denne klasse statisk, for ved flere end én React appiktion, bør vi alligevel sætte identifierPrefix. Ok.. (15:08)
%... (16:24) Okay, I'm just going to drill SSKs (session state keys) for all session-stateful components. (And I'll call the properties 'pssk' for 'parent session state key.') ..Or maybe psKey and sKey..
%(10.09.24, 9:16) Hm, useId() doesn't seem so trustworthy.. Let me see.. ..Holy shit, no, it's not to be trusted at all! Even if you render a list of AppColumns with different (string) keys, then the IDs are still just like ":r6:" or ":r7:", etc. So I can't use that!.. (9:21)
%(9:24) Hm, does it make sense keep a list over all elements for list-like parents? I guess so; it could just help get us the list quicker.. Oh, and when I implement the rate-by-moving-elements-around thing, then we need it anyway. Okay, let me think.. ..So a parents stores a list of its children, and maybe I can sometimes hide this.. ...(9:49) Okay, if I start at the root, then that just makes a ParentSessionState context with a setState() for the children to add their own.. IDs, I guess.. ..No, let's see.. ..No, I guess the parent need to distribute its keys.. ..Hm, I could make it semi-automatic, and then throw an error when a parent has multiple children of the same class/type that haven't gotten an sKey.. (9:54) ..Hm, Or how about a.. hm..
%(10:28) Okay, one can seemingly pass the key "prop" via a {...props} syntax, so that's good.. ..Hm, so one way could be to pass key and sKey (which can be the same) via a {...keys[ind]} expression.. ..Hm, keys(ind).. ..Hm, keys().. ..Ah yes, but maybe {...keys(x)} with an optional string input, x, that should never be "none" (or something like that).. Okay.. (10:39) ..Or better yet, just with a variable number of inputs.. ..Yes, and then just never with no input.. Okay.. (10:41) ..Damn! No, we can't pass keys like that, sadly.. ..No.. ..Oh well, I guess {...keys(x)} and then key={x} {...keys(x)} for lists could also work.. (10:56) ..Sure, something like that.. ..(11:01) Hm, key={key(x)} {...keys(x)}, where x is optional..? ..I like that.. (11:04) ..Hm, or I guess {...props(x)}.. (11:06) ..Nah.. ..Yeah, nah, just 'key={key(x)} {...keys(x)}' (and '{...keys(x)}' when React key is not required).. ..(11:12) Hm, but it would actually be nice with a props function.. ..So I guess 'key={key(x)} {...props(x)}' indeed.. ..Then we make a useGetProps() hook, that takes a function and returns an extended getProps() function (or just 'props(),' if we want..).. (11:16) ..Nah, make it part of the returns of the same useSessionState hook.. (11:17) ...I'm gonna take a walk and think about how to implement this... (11:35) ... (12:27) Hm, I should drill psKey and sKey in keys(x)/sKeys, always.. ..Well, but then I only need to drill a suffix to the psKey as the sKey, so maybe the API user is just responsible for choosing this suffix.. ..'s' for 'session state key suffix'.. ..Well, or sKey could just be a pair of psKey and suffix.. ..And then I can perhaps do it much like I had, only where I generate my own IDs based off of this. (And we should not double the React contexts: only make a useSetAncestorState() hook (that doesn't access the state itself).) (12:36) ..Hm, and the passed psKey should be a recursive join() of the parent's sKey.. Hm.. (12:38) ..Well, I should just pass the sKey as a ':' separated string, and parse the psKey off of that, and I guess useSessionState should then.. generate the sKey() function..? (12:40) ..let key = createKey();.. (12:41) .. let [key, sKey] = createKeys();.. (12:43) ..Ah, I should make a separate hook, useSessionKeys(), for that, such that we don't have to useSessionState() for each component. (12:45) ..Hm no, let me see.. (12:49) ..No about "let [key, sKey] = createKeys();".. Hm.. (12:57) ..(13:02) I need to pick myself up, and figure this out, and also figure out if it is worth doing (I should use much more time on figuring this out..)
%(13:25) I could also use useEffect() (which typically runs after the DOM has been "painted") to make my own useID() hook.. ..No.. ..No, drilling the keys must be the best way.. ..But let me just put this aside for now: No reason to spend more time on this at this point. It's not too impressive anyway, either.. ..Yeah, never mind about this for now. And I can just let external links be opened in new tabs by default in order to prevent me and others from accidentally navigating away from the app. Okay fine. (13:41)

%... (14:36) Hm, pis, jeg har mistet energien..
%Jeg er dog kommet på et rating bars skal bestå af to bars med gradient-farver, hvor hvis man trykker på den øverste, så flytter man midtpunktet (og gradienten representerer skalaen), og hvis man trykker på den nederste flytter man 2\sigma, og gradienten her ændrer sig så svarende til Gauss-kurven. *(Jeg skal omtænke dette lidt, for man skal jo kunne have sigmaer, der er langt større end skalaen.. ..Ah, man kan bare trække og slippe, og så skal sigma udvide sig eller formindskes tilvarende. (20:47) *Nej, det er faktisk bedre bare at lave plus-minus-knapper.. Nah, det er ikke bedre, men jeg bør også lave disse knapper, lige bag sigma-tallet.)
%Men ja, jeg er altså desværre gået lidt død ligenu. Jeg må prøve at samle mig op---eller også har jeg bare brug for lige at tænke over mine mål med den første prototype, det kan også godt være..
%..Det er ikke vigtigt at implementere custom JS i starten, og ingen gang nødvendigvis custom CSS, selvom dette dog klart ville komme først på prioritetslisten af de to ting.. ..Jeg vil implementere at man slår op for det givne relations-tag-flag, hvem man skal query'e med, både ift. overordnet set, men også specifikt til den relevante klasse. Og så samler man altså bare disse rating/rated lists.. ..Jeg skal self. også have correlated ratings.. Hm.. ..Hm, eksterne links, kommentarer, correlated ratings, \emph{og} relevant ratings?.. (14:48) ..I princippet også en Info tab, men det kan måske være sidste-prioritet af disse..?.. ..Hm, hvad med arguments, pro and con, for ratings..?.. ..Hm, en bot der udregner brugernes type-rating, og udregner en off-set sigma baseret på spredningen i brugernes indsendte midtpunkter.. Hm, og det kunne også være rart med eksternt hentede images.. (14:55) ..Hvad med et developer-maintained fulltext index over entiteter?.. ..Hm, eller måske tager man bare propStruct fra alle højt-nok-ratede useful entiteter, og så bare laver et fulltext index direkte over dette. Ikke dumt for en start.. (14:58) ..Hm, arguments, pro and con, for rating entiteter, og et discussions tab i stedet for alle andre.. ..Så både 'comments' og 'discussions' tabs for de fleste entiteter (hvis ikke alle).. (15:00) ..Discussions-elementer vil så også altid formuleres som ratings, og så er man i gang.. (15:02) ..Hm, skal jeg ikke bare lave 'correlated ratings' om til 'arguments,' og så er det altid bare underforstået at argument entities altid er rating scales (enten direkte, eller som en monade over en rating scale)..? (15:07) ..Jo, og så skal det bare understreges, at 'gode/useful arguments,' som er den liste man viser fra, ikke er useful hvis de er korreleret med et eksisterende, bedre 'argument' på listen. ..:) (15:08) ..'Arguments'.. great.. ..Og her snakker vi altså de her ratings, som også altid følges af 'correlation' og 'impact'---ah, så vi skal faktisk bruge monader til arguments, for de skal jo vide, hvilket overargument, vi snakker om, før at de rigtige correlation- og impact rating scales kan findes frem. Ok. (15:11) ..Hm, but then again, vi kan også få dette fra konteksten, det er måske bedre.. ..Ja.. Og for Discussions, der har vi i hvert fald to ratings: hvor relevante de er, samt en rating for den pågældende skala.. ..Hm, men her trykker man bare på det display, der viser ratingen for selve elementet på den pågældende liste. Og det andre ratings må så være noget man folder ud fra selve elementet, som så er uafhængigt af, hvorhenne denne entitet vises (så altså uafhægigt af den omkringliggende rated list).. (15:16) ..Okay, dette virker faktisk ikke så uoverskueligt..!.. (15:17) ..Ah, og for ikke-rating entiteter, der har vi jo ikke en Arguments tab, så det er jo her vi så har en Relevant ratings tab i stedet..!.. (15:18) ..Det lyder da til at overskue..!.. ..Hm, bør 'impact' egentligt ikke være en virtual rating..? (15:20) ..Jo..! ..Og så bør vi næsten lave en 'derived rating,' hvor man tager all (sub-)arguments, der er rated over et vist niveau, og så prøver at bruge deres correlation og deres rating. Vi gør så dog ikke dette rekursivt: Vi bruger de faktiske ratings til derived ratings, og ikke de afledte (derived) ratings nedenunder. Nice..! (15:23)
%(15:29) Ah, og det er jo i øvrigt også rigtig nice, at lige under de mest up-ratede arguments, af hvilke man bør forvente, at de er nærmest helt ukorrelerede, så har man en liste over arguments generelt, som ikke behøver at være ukorreleret. Så vi behøver altså kun én Arguments tab. Og den behøves heller ikke deles op i pro og con, for det er jo det vi har correlation-/impact-ratingerne til (hvor impact altså nu er en virtuel rating, der i virkeligheden rater correlation (efter at man har rated selve ratingen, så man får altså ikke lov til at rate impact, før man har det)). Og igen: Det gør ikke noget, at argumenterne her er korrelerede, det må de kun ikke være, hvis de er ratet over et vist niveau på listen.:) (15:34) ..Og bum, så får jeg jo rigtig nemt allerede mine struktuerede diskussioner herved.! Hvor er det nice, hvis det kan gøres så let, og så relativt let forståeligt oven i købet! Og ja, jeg behøver ingen gang nødvendigvis at implementere andet en ratings hvor skalaen går fra totalt uenig til uenig til at starte med.. Hm.. ..Men anyway, det bliver faktisk sjovt at implementere (f.eks. 'cost'-rating-skalaer, etc.), så det gør jeg nu nok bare alligevel. ..Okay, virkeligt nice, nu lyder det ikke som om, jeg behøver at være særligt langt fra en fin prototype, især ikke hvis den eneste ikke-bruger-bot, jeg behøver at implementere i starten er den der type-rating-og-midtpunkt-spedning-bot, hvilket det jo lidt virker til..!.. (15:39)
%Lad mig gå en (\emph{god}, for en gang skyld (det er lang tid siden)) tur og tænke noget mere over det.:)... (15:40)

%... (18:30) Jeg har fået tænkt videre, og nu kom jeg også lige på to ting: For det første skal 'importance'/'useful argument' rating-skalaen nok være sammensat af to forskellige ratings i stedet. Og for det andet, så bør vi også indføre et tag-flag, der siger at ratingen er afhængig af listens nuværende indhold selv, eller af andres. Så kan man nemlig sørge for at få bots til at kræve, at brugerne opdaterer deres ratings, medmindre de så ellers udløber, om ikke andet så hvis der har været meget andet aktivitet i listen.. Ah, så "flaget" skal faktisk være mere end bare det: det skal også indeholde referencer til \emph{hvilke} andre tags, det afhænger af, muligvis inklusiv sig selv. (18:34)

%..(18:44) Hm, classID-feltet er bare hoved-klassen. Og alle dennes super-klasser vil så også være nogen entiteten tilhører. Og derudover kan dens propStruct også selv indeholde yderligere klasser, enten ved 'class'-property'en, eller ved en 'yderligere klasser' property, hvis det gør det nemmere ikke at blande dem (men man kan altså godt bruge 'class'-property'en).

%(18:52) Ellers tænker jeg altså, at man først slår alle klasserne ud fra metadataen, og så kan vi vente med at implementere 'repræsentationer' til senere. Man slår så tab data op for alle disse klasser. Og hertil bruger man i starten bare initial_user, og så når jeg får genimplementeret login, så spørger man altså også brugeren selv. Alle disse tab specificationer slås så sammen, så man får alle tabs'ne. I øvrigt er der altid en 'Meta' tab, som omhandler metadata og meta-tags om entiteten. ..Hm, ja så i næsten-starten bruger jeg bare initial_user til næsten alt, og også brugeren selv, på nær.. ..nej, når det kommer til alle UI-relaterede relationer.. ..Tabs kan bestå af én relations-liste, eller af under-tabs (med en default start under-tab), som så hver især enten kan være en relations-liste, eller selv kan bestå af under-under-tabs, osv. I øvrigt vil jeg nu dele 'comments' op i 'reactions,' 'informative comments,' og 'discussions,' hvor sidstnævnte så bare er identisk med den 'discussions' tab, der står ved siden af 'comments.'
%Når det kommer til rated lists, så vil jeg faktisk gøre noget meget lig det, jeg havde gang i før, nemlig med SetCombiners osv., og hvor hver RatingList/RatedList har en header, som CSS så bare kan skjule nogen gange, og/eller skjule alt andet end en link-til-'rated list'-kolonne selv, f.eks. når det kommer til lister i en liste-header selv. Og hvad skal der så være i denne header. Jo, jeg har faktisk tænkt mig at lave et lille sprog, således at brugerne (i starten; så kan vi altid ændre det) faktisk skriver et lille program selv for, hvordan listerne skal sammensættes (men hvor der selvfølgelig altid er en startindstilling for hver rated-liste).
%Særligt skal der være en operation til at summe kurverne af flere brugere (med vægt), især for samme tag. Og der skal også være hvor man lægger ratingsne sammen i forlængelse af hinanden (med vægt), således at man får en sammensat rating af flere individuelle rating (og her gør det ikke noget, at den endelige skala for nogen helt andre min- og max-grænser; det er bare nogen blandede scorer, som brugeren selv ved, hvad betyder).. Nå ja, og en god idé: Brugere skal kunne definere bruger-lister (med vægte) som en slags variable, således at de kan bruges flere steder i "programmet." (19:11)
%..Og jeg skal i øvrigt gentage: hvor er det smart, og vigtigt, at brugere kan up-rate bundles af faktuelt---og UI-relateret---rating på én gang, essentielt set, ved bare at up-rate faktuelle (eller UI-relaterede) ekspert-brugere, således at disse brugeres ratings boostes alle på én gang (ikke mindst for brugeren selv). (19:14)
%..På et tidspunkt skal brugerne også kunne uprate entiteter, der udgør bruger-med-vægte-lister, som så kan bruges automatisk som default i rated-listerne, i stedet for hvad der ellers er default. (19:16)
%..Kommentarer skal kunne få up-rated edits til sig, fra alle brugere (ikke bare ophavsmanden), og så kan der bare så en parentes, "edited," når det er, som så også er et link til den oprindelige tekst. ..(Muligvis kan der stå "edited by ...", men det kan vi se, hvad vi har lyst til..) (19:18)

(11.09.24, 8:55) I have some great ideas written in the source comments (in Danish) above this paragraph, especially the idea the the `correlation' tab can be renamed as an `Arguments' tab, and that this tab can also do the job of my former `arguments and counterarguments' tab. For the correlation rating is just what shows whether the argument is pro or con the context argument (which is actually a rating scale: all ``arguments'' here are). And it is only the topmost elements on the list which need to be uncorrelated. Oh, and also this list is a combined rated list of two predicates (relations, actually): `importance' and `not correlated with any more useful argument on the importance rating list.' (Maybe I should use `rating list' more so than `rated list'.\,.) And I'm btw gonna make another tag flag denoting that the tag depends on the rating lists of other tags, which also then expects a list of these other tags. (9:02)

.\,.\,There are also some other ideas, but I won't repeat those now. Instead I will note a new idea, which is to use a.\,. FLOAT as the rat\_val, and then reintroduce the rat\_modifier $\to$ rat\_error\_exponent at the end of the primary index. (9:05) .\,.\,The idea is that the exponent (when UNSIGNED) can typically be compressed away, and the same goes for the tailing zeros in the precision part. Plus it's a good thing if the server outputs floating point numbers as well. (9:08) .\,.\,And with the extra precision, we also in principle need a whole byte for rat\_error\_exponent (TINYINT (signed)), which is good.

.\,.\,It's hard to overstate how nice it is that I've gotten this `Arguments' tab idea: All of a sudden it all seems very much within reach again.\,! Oh, and by the way, the idea to uprate expert users (factual or UI-related) rather than up-rating each fact individually by all users also deserves to be underlined once more. (9:16)


%(11:38) Okay, jeg er lige hoppet lidt tilbage i kaninhullet, desværre, måske.., omkring state management. Jeg var lige ved at tro at Redux var sejt, men hold da kæft, det er dårligt.. Det svarer bare til at have én tilstand på sin ydre App component, og så bare definere en række rene funtioner der, som man så kan bruge til at ændre denne globale tilstand, sende setState videre gennem en context, og så kræve at man kun bruger omtalte funktioner som input for alle kald til setState(). Det er ikke det, jeg gerne vil have.. Hm.. ..(11:49) Og jeg kan ikke finde nogen bedre alternativer.. Okay, nu får jeg altså lidt blod på tanden alligevel med min løsning med at drille psKey og (s)key.. ..Hm, jeg driller psKey og key, som i reglen bare skal være den samme som key.. ..Hov, kunne jeg ikke extende.. Reacts Component-klasse.. Hm.. (10:54) ..Hov, glemte at kigge på Recoil.. ...(11:25) Nej, det er også basalt set mere af det samme.. Hm.. ..Man kunne godt lave en funktion til udvide de normale funktioner brugt til at lave React-komponenter.. ...Min computer er begyndt at blive langsom, og det er begyndt at blive et problem.. (Har også for lidt plads, men jeg ved ikke umiddelbart, hvordan jeg lige får mere på en nem måde..) *(Hjalp en hel del at ryde Spotify chache. ..Wow, kørte lige autoremove, og der står: "After this operation, 14.0 GB disk space will be freed."!.. ...Jep.) ..(11:51) Hm, man kunne faktisk godt lave en funktions-wrapper, som så viderefører psKey og sKey=key automatisk.:).. ..På nær hvis det fucker med hooks'ne.. ..Og det gør det vist, kan jeg se.. ..Men man kunne wrappe alle returns, til gengæld. Og så kunne man sørge for altid at bruge props, og så gette specfikke (ikke-sKey-relaterede) props på første linje i stedet.. Hm.. ..Og jeg kunne lade useSessionState returnere den specifikke wrapper, hvor psKey er kendt.. ..Så useSessionState() tager props, tager en state, tager en optional context-nøgle, og returnerer en returnWrapper(), en state, en setState().. Tja, eller en reducer list, måske, hvor den så også skal tage en reducer list.. ..Tja, den tager en reducer-liste (array eller object), og returnerer en dispatch(). Man kan så selv bruge sin dispatch, men alle komponentens børn får også adgang til samme dispatch() gennem nøglen.. Ah, så dispatch() skal tage en nøgle først, hvor "self" betyder 'ændr din egen tilstand' og <some ancestor context key> betyder 'ændr din nærmeste forfader af angivne type---og så kan man i øvrigt også have et optionalt 'skip' input (træd over n antal forfædre med den nøgle). Okay, dette lyder altså umiddelbart godt.. og lidt sejt, også (i forhold til alle de ikke-modulære state management-løsninger, man kan finde..).. (12:12) ...(12:46) I think it is better to pass setState(), and then just.. Hm.. ..Ah no, I'll still do the dispatch thing, but just let setState() be a default reducer, accesses always by the key (since I'll let reducers be an object) "setState"..
%(15:02) Hm, det er umiddelbart et problem at man nogen gange sætter React keys på ydre komponenter.. ..Hm.. ..Hm, jeg kunne bruge konstante componentKey, som exporteres, og så måske lave en store over.. Nej.. Hm.. ..Ah, jeg kunne.. Nå nej, det svarer bare til at bruge.. Nej, jeg kunne lave en boolsk prop, der siger at komponenten skal have en sKey!.. ..Ah, og hvad bedre er, jeg kan godt lave en store over alle component klasser-eller-funktioner, sammensat med deres componentID.. ..Og så bliver det pludseligt helt automatisk med de nøgler.. Hm.. ..Ah, og endnu bedre.. ..Ah, jeg kan bruge Function.name!.. Nice, så kan jeg lave en getComponentName() funktion, som kan bruges til at opdatere og vedligeholde componentID-storen på tværs af.. navigationer.. ..:).. (15:17) ..Og så skal brugeren altså ikke selv sætte komponentID'er; det gøres automatisk. Og prepareJSX() bør så selv kunne danne alle sKeys automatisk ud fra React keys (med prioritet) og fra getComponentName() (med lavere prioritet, i hvert fald for samme komponenter)..:).. (15:20) ..Hm, men er det ikke nemmere bare at.. Ah, at bruge en sKey prop, og hvis denne sættes til true, så kopieres key bare.. Tja, tjo, det er spørgsmålet.. (15:24) ..Sjovere at disse bare bliver genereret automatisk..:).. ..(15:26) Ah, og funktionerne er jo altid defineret, før de bruges, så ja, vi har selvfølgelig altid navnet lige der i JSX-elementet.. ..Ah, og de defineres ved runtime, så.. Nå nej.. ..Men ja, jeg kan stadig lave en name--ID store, og så skal denne bare opdateres for hver.. refresh, hvilket jo er nemt nok: bare gør det i en useMemo() i starten af React-appen. Nice nok.. Så nu får vi altså automatiske sKeys.. Ah, jeg skal dog også.. ..Hm.. Vent, hvorfor sætter jeg ikke bare keys på indre komponenter.. ..Ja, det kan jeg jo sagtens.. ..Ah, og så kan jeg bare sørge for at prepende alle forfædrenes keys til hver key inde i JSX-elementet, som jeg prepare'er..!.. (15:36) ..Altså til deres sKeys, rettere sagt.:) ..Hm, okay, men hvad er bedst?.. ..Det er jo sjovere, at det gøres automatisk, synes jeg stadig.. (15:38) ..Ah, og vi kan bare bruge '*' til alle HTML tags. Og så kan sKeys'ne, de fulde, være a la: 'my_root/ID1/*.2/ID3.0/*.0/*/*.6'. ..Ja, nogen a la det..:) (15:45) ..Og når der er keys?.. ..Hm, vi kunne starte med et 'i' eller et 'k' alt efter om elementet har en key eller ej. Og så kan vi også lade * \to e for de elementer, der hverken har nøgle eller er en React-komponent. (15:48)
%(16:54) Hm, jeg kunne sige at componentKey altid bare skal være komponentens navn (for navne skal alligevel være unikke), eller.. Hov, vent.. ..Hm, navnet ændrer sig ikke ved imports med as-omdøbelser.. Men ja, jeg kan stadig sige, at man \emph{kan} bruge strenge, som så skal være lig navnet, eller man kan definere sin komponent med en 'function'-keyword-funktion, og så passe this til useSessionState().. (17:00)
%..Nå ja, og jeg glemte at sige: Glem *-delene af sKey-nøglen. Slet dem, og gør til gengæld at ID altid bliver sat på, også hvis key-prop'en sidder på selve React-komponenten; så får vi bare to led i sKey-nøglen for den komponent. Og alle komponenter får så bare et indeks alt efter rækkefølgen det optræder i i JSX-elementet, altså siden sidste key-prop, eller siden starten. (17:06)
%(17:08) Hov, man får ikke funktionen fra this, heller ikke med function-keyword'et.. ..Ah, men jeg kan dog også bare lade prepareJSX() give navnet videre som props._name til komponenten selv. Ok.:)
%... (21:04) Never mind at den skal have både key-prop \emph{og} componentID som (sidste) del af nøglen, hvis den har begge; den skal kun have key-prop som dens sidste del a sin sKey, hvis den har sådan en. ..Tja, og dog.. Hvad gør je.. Nå nej, den \emph{skal} bare rigtignok have begge dele.. ..Ah ja, men den skal bare aldrig have et indeks i forlængelse, hvis den har en key. Okay.:) (21:08) ..(For eller for vi unødvendige opdateringer, hvis man rykker rundt på elementer.)
%(12.09.24, 10:34) Hm, kan og bør jeg lave det så at children kan være 'shouldBeSaved'..? ..Hm, eller er det ikke.. hm, jeg kunne måske bare lave det som en mulig fremtidig todo..? ..Hm, jeg kunne i princippet drille shouldBeSaved som props (altså et mere anonymt prop).. Hm, hvordan ville det være.. ..Hm, og det ville endda være mere effektivt i princippet, end hvis hvert barn skulle ringe op til sin forælder under hver unmount.. ..Så man kunne lave en useSaveChildren() hook, der bare returnerer end modificeret prepareJSX.. ..Nå nej, useSessionState() kører hver gang, også selvom tilstanden ikke ændres, så jeg kan bare give den et flag. Okay, så er det næsten det værd.. ..Ah, og jeg kunne give.. Ah, og når man sætter saveChildren-flaget, jamen så returnerer.. vent.. Hm, jeg var ved at sige, at det returnerer et tomt element, og ja, det må jeg implementere (:)), men jeg skal så lige sørge for at børnene får besked først, eller at de ringer til.. roden.. Nej, til den pågældende forælder.. Hm.. (10:55) ..Hm, én idé, nu skriver jeg den bare, er at drille en prop, der så får alle.. Nej.. ..Jeg kunne også sætte et ydre flag i.. Ah nej, eller i AuxallaryStore, hvor jeg siger 'ring til forfædrene.' Og så kan de gøre det, og hvis de har den pågældende forfader, så forhindres cleanup-processen.. Ja, det var måske ikke dumt.. (11:03) ..Og hvordan sætter jeg det flag tilbage, så?.. ..Hm, hvorfor kopierer forfaderen ikke bare børnenes data selv. Det er da en meget bedre idé.. Og så skal den bare genindsætte det igen, hvis flaget ændres tilbage.. Ok, lad ig se.. (11:07) ..Ja, det bliver sådan, jeg gør det.. ..backUpAndRemove = true|falsy.. Og så skal jeg bare gemme børnenes data inde i forælderens.. ..Tja, eller man kan bare parse alle nøgler og gemme dem, der er afledt af forælderens.. (11:09) ..Lad mig faktisk gøre det for nu (for jeg tror, det er mest effektivt, og så kan man jo altid bare huske den anden implementation (som også er den mest naturlige idé)). Ok.:) (11:11) ...Hov, jeg kan ikke iterere igennem dem, medmindre jeg gemmer dem i et objekt, så det må jeg lige gøre..

(12.09.24, 12:07) Arguments, and ratings in general, should also have a Discussions tab, as well as a Comments tab, btw (which includes Discussions), which is where users then get to argue more loosely, if they want to: It's not a linear thread, but a tree, which is only good, but here one.. Wait.. Hm, maybe we don't even need the more ``loose discussion'' now, now that arguments below the threshold don't need to be uncorrelated.\,. Right.\,. .\,.\,Yes, true: The Arguments tab of any rating/argument/sub-argument should feel much like a regular tree-structured discussion ``thread'' in the beginning, until people at some point start rating the `uncorrelated' relation as well, by which we will start to see these arguments appear on the top.

.\,.\,Hm, should we even add a rating of something like `interesting because it has some interesting discussion attached to it?\,.\,. Well, maybe no, since arguments can be reused. Okay, never mind.

%(15:26) Nice at https://react.dev/learn/preserving-and-resetting-state beskriver lige præcis hvornår tilstandene er bevaret, og hvornår de ikke er. Samme princip skal jeg bare holde mig til.. ..Men så bør jeg jo i princippet netop beholde HTML-tag-nøglerne.. ..Tja, medmindre.. Hm, måske medmindre jeg ved at cleanup-funktionen bliver kaldt forinden.. (15:29) ..(15:31) Hm, der kan forresten ske memory leaks ligenu, hvis nu en bruger ændrer komponentens returnerede element samtidigt med at backUpAndRemove sattes til falsy igen.. ..Her kunne jeg op-bakke hele elementet.. Nej, for hvad er så pointen?. .. (15:32) ..Men men kunne dog nemt implementere et automatisk cleanup af descendant-tilstande, der ikke kommer i brug efter restore.. Ok.. (15:34) ..Ah, som kan kaldes lige efter at komponenten her opdateret sig (og er blevet malet). Ok.. (15:35) ..(15:45) Yeah, I should add the position each time, except for with keys. ..So /<root id>(/(<pos>|k<':'-escaped key>):(<componentID>))*/.. (15:49) ..Hm, and maybe tags should also get "component IDs" as well, such that we follow the React (https://react.dev/learn/preserving-and-resetting-state) rules.. (15:51) ..Yes, we must (probably) do that.. (15:52)

(15:59) Users should just be advised that you ought to feel free to formulate argument in a loose manner in the beginning. For even though their argument is then bound to be replaced by a more precise one at some point (at least if it is very loosely written), all sub-arguments that other users come with can just be very quickly ``imported'' and reused for the substitute. So the initial discussions won't be lost; they will only be ``edited,'' essentially, and bettered. (16:02)

%(16:05) The weather is too good rn, so I'm gonna take another walk, think about how to implement prepareJSX() some more, and just hope that I will have good energy to continue when I get back...
%(19:44) Jeg bør faktisk kun sætte sessionStorage-state'en in setState()-kaldet, og ikke i useState()-kaldet..:) ..Og så bør jeg jo gemme hvilken komponent-instans, der skal have ryddet garbage op efter sig (i tilfælde af sådant).. ..Jeg skal også huske ikke at sætte _sKey på ikke-komponent-elemeter.. ..(19:57) Hm, man kunne også sige: Hvis du genopstår fra session, så kryds lige dig selv af i backup'en (inden den så ryddes op efterfølgende).. Hm, tja.. ..Hm, jeg kunne da egentligt også bare gøre så at cleanup-funktionen også rydder børn op efter sig. Det koster jo ikke så meget, for det kører alligevel bare efter at komponenten er blevet malet. ..Hov vent, det er jo mig, der kontrollerer.. state-ændringer, ja, men det kan også forekomme prop-ændringer.. ..Anyway, jeg mener ikke, at det gør noget, hvis en slumrende backed-up komponent pludselig genfinder sin tilstand sent, hvis den alligevel bliver renderet på et tidspunkt igen, og hvis cleanups også altid bare rydder sine børn op for en god ordens skyld, om ikke andet så hvis den har været backed up før, jamen så behøver vi ikke tage os yderligere af dette problem, for så har vi ingen reelle memory leaks. (20:08) ..Nå ja, og jeg skal nok lave usePrepareJSX() om til 'useSessionStateless(),' og i øvrigt vil jeg nok kalde prepareJSX() for 'passKeys()' i stedet, således at man bedre kan fornemme, at man ikke behøver at bruge den, når det returnerede element ikke indeholder nogen session-stateful komponenter. (20:10)

(20:21) We need to also be better at defining the most ``basic'' and standard predicate: good. And in fact, there are completely different versions of this predicate. There is `what an great and awesome job; 9/10!' good, and then there's `I would recommend others to try this; 9/10!' I personally have a hard time not using the former version when rating things; you don't want to rate e.g.\ 3/5 or 4/5 if the creators, etc., did a great job!\,. But in reality, the latter is much more useful for all other users, apart from the creators. But then consider having both options!\,. Then you can tell the creators: `what an awesome job! 9.5/10: Almost nothing to do better!' while at the same time also giving a score of 6/10 in terms of `would recommend others to watch/try. .\,.\,(I just thought of this because YouTube just asked me to rate a video that I finished yesterday, and I just can't, 'cause it was a 5/5 in terms of being awesome and well-made, but it was also kind of long and not too exciting/engaging all the while, and certainly not to a 5/5.\,. .\,.\,So I'm just not going to rate it, 'cause it would feel like I'm not appreciating great work. There we are.) (20:33)

%(21:10) Uh, fik lige den idé at lave _sKey \to sKey, og så faktisk have det som en prop som brugerne godt i princippet må bruge---måske også til at give til useSessionState() i stedet for props, faktisk---men ellers så også til potentielt set at overføre en tilstand fra en tidligere komponent til en ny, som befinder sig et andet sted I React-træet (hvor man så får en re-rendering af komponenten, men hvor komponentens tilstand, og dens børns tilstand alligevel beholdes). Så skal jeg bare sørge for aldrig at overføre sKeys i prepareJSX()/passKeys() til børn af React-komponenter, hvilket i øvrigt også vil være redundant alligevel. Og så skal jeg altså nemlig også bare lade være med at overskrive nogen sKeys, der allerede er sat af brugeren før passKeys()/prepareJSx() tages.:) (21:15) ..Eller setKeys(), eller setSKeys().. (21:18)
%..(21:24) Ah, og så kan man i princippet endda bruge det til at kopiere helt andre tilstande.. Tja, men det kan man nu også gøre på andre, mindre side-effekt-fyldte måder. Men ja ja, stadig en lille smule sjovt, at man kan..

%(13.09.24, 11:32) Never mind about copying states, and about exposing sKey prop to the user..

%(16:31) I only implemented backUpAndRemove because it didn't cost any real execution time for when one is not using it. But now I think it \emph{is} justified to have, 'cause it seems that the browser doesn't care about CSS in terms of when to fetch and store images in memory, at least with the standard <image> tag. So it does actually make sense for me to use this feature myself.

%(14.09.24, 9:33) If one only wanted the dispatch functionality, one could also make a hook that is just const [dispatch] = useGetDispatch(initState, reducers), which makes sure that dispatch is memorized such that is can be passed via React contexts. *Maybe I will actually also make this hook, why not.. ..Well, but then again: why? It's enough to note to myself that it can be done, namely for other projects/works.. (11:24)

(14.09.24, 9:38) I do like the option to have tags that depends on the present configuration of other tags' rated lists. But a solution that is a bit more pragmatic is to instead have a tag that measures what we could broadly call the ``collision'' (e.g.\ `correlation' or `overlap as a category'), and then implement something neat where users see the elements listed according to the rated list in question, but have a main rating.\,. .\,.\,that says how correlated/``colliding'' the element is with the entity in question (which is typically also on this list, but of course you don't rate an entity's collision with itself). Then users can get to rate coll.\,. Ah, it could be cool if one could just press a button on an entity element to activate this functionality.\,. ooh, and that woud actually just be to change a context!\,.\,. A context about which element in the list we want the relevant ``collision rating'' to be about. Yes, so that is a somewhat harder, but more pragmatic solution.\,. (9:47) .\,.\,And when we flick that switch, each element that is loaded on the screen should search for that rating and display it directly without need for expending the element first (either in place of the list's rating, or right next to that).\,. (9:50) .\,.\,(9:55) Ah, and then there's actually a really simple option for the algorithm that determines the derived rating for the overall `Argument' (which is always just a rating, and can be any rating, in fact, so `Argument' just means `Rating,' really). And note, before I explain it, that the derived rating is just a guide for the users, both to help guide there rating of the overall argument/rating, as well as to guide adjusting the various impact/correlation ratings, which can very well also be informative on there own (in fact, these can be become very useful in the (user-driven) ML, which is obviously a very important thing for the users (to get better algorithms and search results)). The thing I just thought of to simply have the algorithm go the each Argument on the list and then use the rating and the correlation (with the ``overall argument'') rating to add to the derived rating, and then when it comes to an element that is deemed as correlated with one of the elements above, then the algorithm can simply subtract the above elements rating times the correlation from that rating.\,:)(!) (10:03) .\,.\,How elegant and easy.\,:)\textasciicircum\textasciicircum\ (10:04)

.\,.\,Oh, and about the times.\,. where we want to also filter/change the list according to the collision, for instance with categories.\,. Hm, and I also just remembered that I should mention: One could then make bots to change.\,. Oh, this is exactly related to this question: One could potentially make bots that subtracts the collision with a better rated element above an element from its rating such that the element below is sent further down. Now, you could also try to do this client-side.\,. Oh, and that would actually not be too costly, since you can also just do this one element at a time: You can just make an algorithm that after the initial list is formed goes down through the list and further down-rates element that have collision with any elements above. Since the lists in question are typically not very long (as the whole point is to create a good, curated selection of choices at the top s that the user \emph{don't} have to scroll and search very long), this could be a good option, especially at an early stage when we want to keep the number of bots that we rely on low, preferably. Great.\,.\,!\,:) (10:13)

.\,.\,This is really nice. Now, a remaining question (that is not very pressing, since it might ultimately depend on user preferences in the end) is how do we try to sort the Arguments lists, now that correlation is not necessarily something to be shunned completely?\,.\,. (10:18) .\,.\,Hm, the easiest thing is still to make an `important/useful (sub-)arguments' rating, but I just had the thought: why not do both thing?\,. Why not have the `important/useful (sub-)arguments' rating as a very important help, especially at an initial stage, but also in reality at later stages, namely in order prevent unwanted flips.\,. .\,.\,and then also go by the virtual impact rating, which means combining the rating itself and the correlation rating, as well as the algorithm that down-rates elements --- ah, to some user-determined degree, since this is happening client-side!\,\texttt{:D} --- that are correlated with ones above them. Nice.\,.\,! (10:25)

.\,.\,Great, and then the only thing is that ultimately, you would rather want to use a bot for all this, at least perhaps except for the last correlation down-grade algorithm, but then again, perhaps also that.\,. .\,.\,But yeah, the nice thing is then that we still get the `important/useful (sub-)arguments' rating (that is dependent on its own rated list, and should therefore have the given flag that I've talked about above) to rely on in the beginning when new Argument lists are formed, and in the early stages of the app, we might just want to rely purely on that, why not?\,.\,:) Then we can always enhance things by starting to also use the more sophisticated techniques, where we don't rely purely on this essentially circular (and meta) tags.\,.\,:) (10:34)

(10:51) Hm, even the `factual' relations are also ``UI-related'' to some extend.\,. Hm.\,. \ldots (11:03) Yeah, I think it will actually be a really beneficial, widely used thing to make a standard rating scale where the top, whatever, 10, 20, or 50 rating points is over the threshold, for instance meaning `true' for factual tags, but then the remaining distance to the top determines the UI-related importance. Actors of a movie is a very good example. Here all actors are equally true as being `actors of the given movie,' but you of course still want to show the leading roles at the top of the list. So yeah, how to make this a standard thing.\,.\,? (11:07) .\,.\,Well, why, with a flag denoting this, of course.\,. (11:08) .\,.\,And the flag could just be accompanied by a parameter to determine the given threshold.\,. .\,.\,Hm, but maybe the last 50 percentage points is a decent idea.\,. Well, or the last 25, actually.\,. (11:11) .\,.\,Well, the good thing is that this doesn't matter so much, now that I've come up with this system of taking the most ``frequent'' (accounting for weights) rating as a standard.\,.(:)) .\,.\,Hm, but maybe where to best put the threshold actually does depend on the relation in question: Is it a very sure thing or is it generally a more debatable thing (where it is better to then have a lower threshold (compared to how one perceives the tag, especially when going by its title alone). So yeah, let me just generally make the threshold a part of the input parameters when making templates for such relational tags.\,. (11:18)

.\,.\,Hm, and maybe the `factual' and `UI-related' tags should also come with some parameters for cases where e.g.\ a thing might b mainly factual, but a bit dependent on opinions as well.\,. Hm.\,. (11:19) .\,.\,(11:25) Hm, well, maybe the `factual' parameter is enough, then, as all ratings are.\,. Yeah, \emph{all} ratings are UI-related.\,. Well, but some are more central then others, though.\,. .\,.\,Hm, as long as we by `UI-related' then mean `is supposed to change what tabs appear in column, and not just the lists in those tabs.\,. Hm, so what do we do; how do we classify the tags (relations, in particular).\,.\,? (11:29) .\,.\,Oh, and note that the `factual' parameter can of course always be ``edited'' later on by voting (rating) about it. (11:31) .\,.\,(11:37) and/or by changing its representation. But maybe we mainly just want true/false (or 1/0---which can then naturally always be changed to floating point numbers, if it becomes useful), since the main purpose of this is just to have a way to give power to some users (or `user groups,' later on), as a way to essentially bundle a multitude of ratings together into one.\,. (11:40) .\,.\,So by `UI-related,' we mean \emph{structurally} so, i.e.\ relations that are supposed to determine the structure of the app columns and their tabs (but not the lists themselves that are generally shown in each leaf tab). And by factual, we mean things like e.g.\ Wikipedia shows for its semantic data in its information columns: fact. But then the ``factual users'' are then still also supposed to keep the order of the lists in mind, when a property as one-to-many, e.g.\ in case of things like `actors,' where the order matters as well. Okay.\,. (11:45) .\,.\,A `related to app UI structure' flag, to be more verbose.\,.

.\,.\,So these threshold ratings, they essentially have one set of semantics before the threshold, where it is of course part of the semantics that these are all entities below the threshold (and \emph{what} the threshold defines is of course defined), and that the ones at the top of that interval is close to meeting the threshold. Then after the threshold, a different set of semantics for the rating goes into effect, of course still with the underlying semantics of being above the given threshold. .\,.\,So the threshold needs to be defined, then the scale leading up to the threshold needs t be described (but a constant is that entities close to the threshold are close to meeting the requirements). After the threshold, the same thing can not be said.\,. Well, this is not so great.\,. We can't have a complete jump in semantics, where entities just beyond the threshold aren't necessarily close to being below the threshold.\,. .\,.\,So either they have to be, or one needs to use another method, which would generally involve using more than one rating scale to get a combined rating.\,. But it's also fine to have it like this, 'cause whenever an entity is just above the threshold of being relevant, then we would alwasy want it at the end of the list, and conversely, if an entity is as low as it can be in terms of the rating for how high it should appear on the list, well, then it makes sense that it is then on the threshold of being removed from the list. So this system makes sense after all.\,:) But then for the upper interval, the rating scale can continue from the tipping point of being relevant, i.e.\ the threshold, with a different set of semantics that determines what each point means. And typically it will just mean how high the entity should appear in the list, without necessarily meaning anything related to what determines that scale below the threshold. Sure, I think it makes sense.\,.\,:) (11:59) .\,.\,One can think of it a bit like a singular but continuous point on a curve, in mathematics: The semantics are continuous around the threshold, but semantics of what each increment in the scale means from there changes.\,:) (12:02)


%Hm, my solution doesn't work with React Router, since JSX elements are passed through non-children props. But I actually \emph{could} make it work, if I wanted to, by providing an optional id to passKeys, that adds that id to the sKey.. I think I will do this.. And I might as well test it out on the React Router.. (14:48)

\ldots\ (17:11) Ooh, along with `list' and `set' entities, I should also make a fundamental `struct' class, which when saved in the propStructs of the database simply takes the form of a JSON object. (So this will then technically make the format property of my temples into such `struct' entities.) (17:13)
.\,.\,Oh, and I'm then actually thinking of using these for all boolean flags, in order to not break the fundamental semantics that property names are always nouns that describe the value(s), and even when they are property entities, the same thing applies, but here these names (titles) can just be accompanied by a description.\,. (17:20) .\,.\,Hm, but then again, it seems better to just allow property names to be verbs as well, in which case the value is normally expected to be either true or false, unless perhaps it can be something in-between, where the value might then be a floating point number instead between 0 and 1.\,. Yeah, let us say that: Property names can also be (compound) verbs (where we for instance also take verb + object to also be a (compound) verb).\,. (17:24) .\,.\,Hm, but still: Structs can fully follow their own rules, in principle, as these can just be defined in the description of the property.\,. But then again, it's still nice to have a good standard, so yeah: Property names can also be (compound) verbs. (17:26)

(18:27) But since we are free to let entities have several classes (the classID just denotes the ``main class,' which is the one that the app is expected to mostly treat the entity as), I think is is better to use classes in place of boolean flags. So for instance, the `is meta' property can be replaced by a `meta-tag' class (and potential subclasses of that). And the `is factual' flag can be replaced by `factual tag' class. (18:30)

(18:30) Hm, when calculating the disagreement between two users' ratings, then we ought to take go by the been where there is the maximal overlap, right?\,.\,. Or should we take the midpoint of the ratings and then calculate the disagreement in both directions and combine them somehow.\,. probably not.\,. Hm.\,. (18:33) .\,.\,I guess taking the bin where there's maximal ``agreement'' must be the right way to go.\,. (18:34) .\,.\,Yes.\,. (18:35) .\,.\,Which must mean multiplying the two values for each individual bin, I guess.\,. (18:36) .\,.\,Sure.\,.

(22:20) Hm, why even put a maximum (or minimum, in case of polar ratings) on most rating scales?\,.\,. .\,.\,Since we are using the most ``frequent'' rating (when accounting for the weight of each bin) now rather than the means, we are also no longer required to put a maximum on the rating scales.\,.\,! .\,.\,! (22:22)

%(15.09.24, 11:58) Jeg har godt nok sovet længe.. Gik i seng lidt efter tolv, tror jeg, og stod først op kl. femogtyve over elleve..!

(15.09.24, 11:59) The two-part rating scales does break the rule of trying to be linear in relation to.\,. Well, it's not in relation to anything, so I guess it's okay.\,. Hm.\,. Otherwise I was thinking about something else.\,. .\,.\,Nah, two part rating scales are fine, just not for Arguments.\,. .\,.\,(Otherwise I was thinking about compound rating scales, perhaps even where you make one of the atomic ratings virtual, if you are not planning on using it anyway.\,. Or you can just have it as non-virtual and then just be okay with storing ratings even though they can be calculated from others.\,. But anyway, we can do that if it ever becomes useful; I will probably stick to those two-part ratings.\,.) (12:05)

%(12:06) A gentle remider that with my useSessionState() system, you can also very easily save a state to localStorage, which is very nice; I want that.

\ldots\ (16:12) It's fine to let rat\_val be signed.

About the two-part ratings, no, I think there \emph{is} a better solution. I think it is better to instead introduce `derived ratings' as a class of ratings---not to be confused with the `derived rating' calculated from the Arguments and their correlations, since this require something else. But these other `derived ratings' .\,.\,Or maybe we could them `formal ratings' to be more specific, then.\,. These are ratings where the `Arguments' and their correlations are defined beforehand, so to speak, cause the very semantics of these formal tags is to be a compound derived from these, let's not actually call them Arguments, but let's call just them something like `inputs'/`input ratings.' Now, I think I will actually just let the app submit rating values for such `formal rating scales,' which can then be done after the user has rated all the inputs. But at some point, we might implement it so that the back-end can recognize these `formal ratings,' both in terms of queries and inputs, and then delete these ratings from the Ratings tables, and instead implement them as database views instead (and just throw away any inputs for the formal rating scales). So there we are: it doesn't hurt to implement these formal rating scales like normal ones in the beginning.

A good thing about these formal rating scales is that they can also be helpful, not just for the relations (e.g.\ `actors' or `factual users to query facts from,' etc.), but also for normal tags.\,! They can thus serve as a way for users to define very specific compound tags and hope to make it a convention to use these. To see this, let's consider the Arguments and correlations for normal tags (such as `funny,' `well-made,' etc.). Normally, these Arguments are created dynamically for each individual rating for a specific entity (although we can at least recommend tags to use for such Arguments from a rated list belonging to the class of the given entity that we are rating). And that's great when you don't know beforehand what the things that impact the overall rating of the given entity is. But in other times, it might be helpful to have a tag that just straight-away combines several ratings into one. (Although for Arguments, one might want to rather use all the `input rating scales' instead.) .\,.\,Yeah, I don't think I need to explain it anymore: It could be helpful.

And then for ratings like `actors' or `factual users,' etc., we can also use these formal rating scales, rather than use e.g.\ ``two-part ratings'' (i.e.\ the continuous bt singular ones). (16:32) .\,.\,Also, part of why I thought that `two-part ratings' was smart was because I'm still thinking about using mean (average) bots. But as I realized last evening (/night), I no longer need to be concerned with this push-and-pull situation: With my `most frequent rating' bots, we don't even really need to limit the intervals of the ratings.

Of course, limiting the intervals might still be a good thing to do for some tags, such as e.g.\ likelihood, in particular. So I will also let tags include data about if they should be limited. Oh, and I haven't even mentioned: I should also make it a convention to try to describe the semantics of different points of the scale, at least at the two ``ends'' of the scale, although these ``ends'' might then not be actual ends, as the rating scale can be blown, unless otherwise specified. And note that it should be considered bad practice, actually, to use examples as these defining points. It is much better to try to describe the feeling that the point represents, i.e.\ when the tag is feeling-related, or something to this effect for other kinds of tags.

Of course, if a tags semantics is not too well described initially, then we can use.\,. `representations'!\,.\,;) As long as the representation isn't believed to go against a previous conception of the semantics, at least not by too significant a portion of the users, then it is absolutely okay to vote in a better `representation' of an entity, including a tag. But if it seems to go against what has previously been thought about the tag, then it's always much better to just make a new entity instead with the given (more precise) semantics, and then start from scratch with that. (Otherwise you get the unfortunate situation where a significant portion of the ratings from users in the past are now wrongly interpreted.) (16:45)

%(16:53) If I were to use a less radical version of my state management system without session storage, I would just make a similar hook that just returns state and dispatch() alone, and then either use the same React context to pass dispatch() to the descendants, or drill it. Well, but how.. ..Ah, I guess the hook might then also take the dispatch of the nearest stateful ancestor (either drilled or from the context) as an optional input in order to be able to access that. Cool. (16:57)

%..(16:59) Ah, vejret er så dejligt.. Lige før jeg ville ønske mig noget mere efterårs-agtigt vejr igen, så man ikke bliver så fristet til at gå ud i det. Ligenu kan jeg næsten ikke lade være.. ..Okay, jeg kan ikke lade være. Jeg går en tur, og så må jeg lige tænke på, hvad jeg skal begynde at implementere som det næste... (17:01)

\ldots\ (17:55) Okay, the rating error (rat\_err) should actually not denote the `disagreement' with other users: It should simply denote the uncertainty, plain and simple. But this still allows us to use the same `most ``frequent'' rating' bot (although I should to do something else for its error). But first of all, let me say that when calculating the correlation between two users, the calculation should essentially be where you take the averaged distance squared between each two bin, weighted by there values multiplied, and then also normalize by looking at the average correlation between all the users of the set. And since we have Gaussian curves, this should be analytically solvable, let's see.\,. .\,.\,Hm, well, yeah, it is analytically solvable. So there we are about that. And then for the `most ``frequent'' rating,' we want to.\,. let's see. I'm both thinking minimizing the $\sigma^{-1}$-weighted distance squared, and also taking the maximum of the product of all the curves, which is similar to taking the maximum of the sum of all the exponents, which is indeed the same as the first thing, so there we are.\,. wait, is it the same, let's be sure.\,. (18:08) .\,.\,Yes, it is. (18:09) So that's what we'll do, and then for the sigma (the rat\_err, i.e.) of this bot, I was thinking if it makes sense to multiply the curves together and calculate the spread.\,. .\,.\,the standard deviation, rather.\,. (18:10) .\,.\,I think it makes good sense, but I'm looking to justify it a bit more.\,. .\,.\,(18:16) Hm, the bot's curve wants to be in a way where if one where to sample from that curve, then add the user-provided uncertainty on top of each sample, then the data, well, particularly the midpoints.\,. would best fit that curve.\,. .\,.\,Hm.\,. (18:19) .\,.\,Well, I ought to think about this later, and try to do some programming instead.\,. \ldots (19:07) Okay, let me think about it now.\,. \ldots (19:31) It's not true about wanting that curve from which to sample.\,. .\,.\,We want an error on the ``most frequent rating'' when considering.\,. Hm.\,. .\,.\,Hm, if we went by the mean, then we could get $\sigma$ from the Central Limit Theorem. And is it so bad to just show the error on the mean rather than on the.\,. Hm, I guess it would make the outliers add more to $\sigma$ than we would ideally want to.\,. (19:37) .\,.\,Oh, how about simply cutting out the outliers as a simple hack to make it work.\,.\,? (19:39) .\,.\,Hm, one way would be to first take the Poisson error on the tallest peak, and then make a cutoff below that with enough of these Poisson $\sigma$'s to be happy. Then you take the remaining peaks count.\,. Hm, how many users are ``within'' a peak.\,. Well, I guess you could go by the hight here.\,. Ah, but you need to subtract the base of the peak, then.\,. Hm.\,. .\,.\,Hm, is the user even more interested in the uncertainty of the peak, rather than the spread of all.\,. non-outlier users.\,. Hm.\,. (19:48) .\,.\,Hm, but anyway, I guess cutting off the highest peak(s) with enough Poisson $\sigma$'s (each equal to the square root of the height), then removing all ratings outside of those peaks, and then use the CLT, that should get you a pretty good estimate on the uncertainty of the maximum, I believe.\,. (19:52) .\,.\,And then we might indeed let out bot use that as its rat\_err, and then we can always make another/others that shows a $\sigma$ (rat\_err) calculated from just the std.\ deviation of the rating values.\,. Oh, and possibly with outliers cut out, potentially by a similar technique of cutting off from a number of Poisson errors below the tallest peak, or another procedure. (19:56)

(16.09.24, 10:31) Wait, treating the rat\_err curve as `agreement' rather than `certainty' is exactly what allows us to use the most frequent rating, isn't it?\,.\,. .\,.\,Yes, I think so. And `agreement' is generally just a wider `certainty' curve (certainty of the users \emph{own} opinion, i.e.). So yeah, forget what I said yesterday; we need to use `agreement' curves instead.\,:) (10:37) .\,.\,The last part from yesterday of how to estimate the error on the tallest peak then still counts.\,. Hm, I should by the way \emph{also} make a mean bot, only where we cut off outliers.\,. Hm.\,. Well, maybe.\,. .\,.\,And for measuring user correlations, then we actually ought to go back to another algorithm, right?\,.\,. .\,.\,We ought to go back to just looking at the maximal bin when multiplying the two curves, right?\,.\,. .\,.\,Yes, I think so.\,. (10:44)

%(11:19) Hm, I don't like that my sKeys already get so long at this (early) point.:\ Is there a better solution?.. ..I might just make the other reducer--dispatch solution that I was talking about, though, passing each dispatch through a context instead.. (I've already spent quite long on this project, so I think it's a bad idea to try reimplementing it differently, at this point..) (11:22) ..A professional solution ought to just use the React tree directly as well... (11:24) ..Hm, I could in principle use a HTML data attribute instead.. (11:26) ..Oh, in principle, one could also save the state in a process where all components are rerendered, and then creates a new object in the process that depends on the rendering order.. Nah, that doesn't sound robust.. (11:30) ..Hm, couldn't you also just store the parentID in auxDataStore, and.. let me think.. ..(11:33) Ooh, you could let each parent keep track of the sID/sKey (= useId() | = getNonce()) of the children, as well as their nodeIdentifier.. ..And then let each one hold the sKey of their parent.. ..Yes, you could do that.. ..One would not generally need to worry about that kind of memory usage, except for the fact that it grows quadratically, and I kinda fear that now.. (11:39) ..Oh, this needs to be saved localStorage instead.. Hm.. ..Hm, I could make the SessionStatesHandler sync lazily with sessionStorage, I think I will do that..

%(11:51) Okay, I will make a plan now for how to remake useSessionState. But then I will actually not do this right now, but instead just make a pair of useStateAndDispatch() and useDispatch() hooks, similar to useSessionState() and useSessionStateless(). And then instead of doing the obvious thing of wrapping all returns in a Context.Provider, I will just keep the same pattern of returning a passData() function to wrap the returned JSX elments instead! This both makes it easier, \emph{and} it means less refactoring once I remake useSessionState(less)(), since I'll make it so that I'll only need to change the names (of the hooks and perhaps of the passData() function).:) (11:55)

%..And for my plan for the remake of the useSessionState: As I alluded to, we could still let all session states be contained in one object in sessionStorage, but use nonces instead for the sKeys. Then each parent state just has to hold a store over all the nodeIdentifiers (gotten from getNodeIdentifier()) of its React component children, paired with their nonce IDs, i.e. their sKeys. ..And it of course has to also hold the sKey of its parent. And that's it, really. 
%Hm, it doesn't sound too hard to implement, actually. But neither does the more simple useStateAndDispatch(), so let me just implement and use those first. (12:01)

%... (15:59) Puh, blev lige totalt ramt af træthed. Mærkeligt.. Har fundet ud af, at det jo også er totalt smart at bruge FLOAT (signed, this time) til rat_err. For så kan exponenten bare fortælle om størrelsen, samt de næste par bits, og så har vi resten til at opgive yderligere data, hvis det er. Hm, måske skulle man endda bruge DOUBLE. ..Anyway, og jeg er kommet frem til, at enhver SessionState bare skal holde et index med alle dens Komponents-børns nodeIdentifier-sti. Og så kalder hvert af disse børn bare useID(), og sørger for at indsætte dette for forælderen.. Hm, men så skal de også vide deres egen sti.. Men det er måske også okay.. Ja.. (16:04) ..Så vi sender hver pSKey videre, samt den relative nodeIdentifier-sti fra forælderen til barnet. ..Og så, så laver jeg altså bare auxDataStore om så det er en klasse, der så med jævne mellemrum backer sig selv op i sessionStorage (men dog kun hvis den bliver triggeret til det af, at der sker ændringer). (16:06)
%..Hm, man kunne egentligt også lave rat_val en DOUBLE nærmest gratis.. (16:08)
%..Ej, hvor blev jeg træt lige pludselig..

\ldots\ (19:13) I thought I got a good idea (on a walk just now) about being able to rate example entities to define a rating scale dynamically. But then I realized that the thing about using descriptive examples is bad in the first place, at least if we are talking about feelings, which I was. But here's a much better idea: Let's use the F--A rating scale for all the normal, often adjective-defined/-represented tags, and then let's say that there's no limit to that scale, since it can continue into A+, A++, A+$\times$3, etc.\ on one side, and F-, F--, etc.\ on the other. Then A means `among the best,' B means `among the good ones,' C means `middling,' and so on with D and F. And then below and above A, you just try to go by the fact that the semantics ought to be linear, so if an entity has double the score of another, then it is double as good as meeting the given adjective/predicate. And there we are. Then we should just at to this at some point, that users can click on any of these segments on the scale and then thereby extend an expanded interval with well-known examples of entities whose rating lie in that interval (but not necessarily with value markings on it). And that's it, really. Then the only other kind of rating scale that I will use in the beginning, I think, is one that simply defines the scale in terms of some quantity, and potentially with limits on the scale. And an important instance of this kind of scale will then by the likelihood scale that goes from 0 to 1 (but we can display this quantity in percentages).\,:) (19:24)

.\,.\,I also got another idea, which is that we can make a simple, separate system in order for users to be able to uprate entities as ``hot''/relevant, or whatever you want to call it. A simple system where users can press a button to boost any given entity, and then they can only boost a limited number of entities each day. The app can then make a front page where it shows all these boosted (or whatever we want to call it) entities in a list, sorted according to their count in how many has ``boosted'' them, of course. (19:28) .\,.\,And then we have ourselves a (decent enough) front page feed, simple as that.\,.\,!\,:) (19:29)

.\,.\,Oh, and the users should also be able to select the classed with respect to which they are boosting the given entity. So they can boost it for the `entity' class for the most basic front page feed, but they can also boost it for other classes, such as e.g.\ arguments (which will be quite important, especially for the early stages of the app, I think).\,. (19:32)

(19:48) I should add `Reviews' next to the `Reaction comments,' by the way.

(17.09.24, 9:28) How is it awesome that there might be such a short route to getting what I have called p-models before: We will get such a great way to discuss things and weigh arguments with this system, and it doesn't seem to take a lot to implement.\,.\,!

For the EntityTitles, I will let the expand button always be a down- or up-arrow. And for the expanded-out box, I think I will show a full title first, which is just an EntityTitle with a CSS class set to `full.' (And the title might just be repeated.) Then I will make a metadata box. And for this I will actually show the metadata pretty much in the same way that it is stored. I will thus first show the main class, then.\,. oh, so this means that we can set this class as ``expected'' for the full title. Then I will show the entity inputs, followed by the string inputs, and for the former, I will put a number, e.g.\ `1)', in front of each one. Then comes the template, where I will actually use number references, rather than repeating the title of the input entities. But for the string inputs, I will just render the strings perhaps in a green color (perhaps dark), both for the substituted strings, and for the same strings in the string input section/field/entry. And note that if any of these fields are missing, then I will still include it, but just show nothing in the field. And then we get to the `own struct,' which I can call.\,. Hm, how about `own members'.\,.\,? .\,.\,Perhaps.\,. .\,.\,Or maybe just `other properties'.\,. ah, and then instead of having the `other' be misplaced when template (and template inputs are missing), we just call it `properties' instead. Ooh, and maybe we actually remove the template-related fields (first three after the `main class') when the are empty, and then indeed write `Properties' rather than `Other properties.' For the data, we should just let all text placeholders be links to the text, oh and with a URL-fragment that scrolls down to the specific entry.\,.\,:) Hm, so ideally I should make a separate page to show data texts.\,. Except for descriptions: When there is no data placeholders, but there is data, then we should add a description field.\,. (9:46) .\,.\,(9:51) Yeah, add a Description field and fetch as least part of the data (up to some max length) as a text and show that in this field. For binary data.\,. Well, that should only be used in special templates, used to define the binary formats in order to give the binary meaning, and that is not relevant for the early stages of the app, since we will just fetch all images from URLs instead (with the image (or picture, etc.) tag). (9:55)

For the `representations,' which we could call.\,. Oh, forgot what I had in mind.\,. Anyway, for these we can at some point make a query proc that takes the first element of a rated list and then returns its metadata straight away.\,. and perhaps also its entity ID. I might then make a bot that automatically up-rates an entity as its own `representation' and then just query that bot always as well when querying for the `best metadata representation,' og there it was, what I was thinking about. (9:59) Now, I shouldn't forget the `better duplicate' relation, or rather `more popular duplicate,' since this relation should then be used, not to say that a duplicate has better (more clear and useful) metadata---that is the ``good metadata representation' relation's job---but instead be used to say that another duplicate entity has more ratings about it, and more activity related to it. Then for the AppColumn, we should also query for `better duplicates', and if one is rated high enough, we should add a link (with a warning-like, attention-getting header text) to that duplicate. (10:05)

Technically, we ought to extract the baseline from the peak before we calculate the CLT error, or any (vertical) Poisson errors, for that matter. (10:08)

.\,.\,(10:09) Oh, and about the topic of the ``p-model,'' I don't think we will even really need my whole (gamified) `debate site' idea, when we can get such good structured discussion this other way.\,. Oh wait, I was thinking more about my earlier ideas for `structured discussions,' where it is still also a bit gamified.\,. And what about discussion/debate groups in general?\,.\,. (10:13) .\,.\,Is it enough to just have our `prediction groups' instead.\,.\,? .\,.\,Ooh, maybe we could do one thing.\,. Maybe we could allow users of user groups that allows this to color a comment or argument in.\,. Hm, a color that says that it comes from that group, and then we can also just make sure to always show several ratings of the various relevant user groups for any given argument.\,. (10:18) .\,.\,Yes indeed, but yeah, I think it is a good idea that a user can upload arguments on behalf of a user group, such that the user group can be seen (by themselves and others) a team that helps, not just to rate, but also specifically to \emph{come up with} good arguments. And then if they do a good job, the esteem of that user group will naturally rise in the eyes of other users. (10:24) .\,.\,(Oh, and they of course also get to mark it as a contribution which they can then get a token for, on which they are owed later payment, according to my whole idea of making it an SRC-like company, but whether the contribution then belongs to the specific user or to the group (as a kind of small IP pool), that is then a matter for the user group to figure out (and it also has to be implemented by the $\sim$SRC that user groups can claim contributions together). Cool cool.\,:) (10:29)

(13:15) I forgot to mention: When displaying properties with sets as there values, we then simply remove the square brackets, and just list the values of that array. And for lists (arrays inside other arrays) and struct (JSON objects), we simply render them inside square brackets and curly brackets, respectively. Also, we never render the outer curly brackets of the propStructs.

(18:59) The likelihood rating scale should have buttons to give either a standard true or a standard false, probably with rat\_err set to 0.

The standard factual properties will consist of two ratings: `is a property' (likelihood rating) and `is a useful/important/remarkable instance of tag $x$' (an F--A rating). (19:02) .\,.\,Hm, I should also have simply `is a well-known instance of tag $x$' relation, which can then be used to select examples for the rating scales. (19:04)

.\,.\,And I will actually implement virtual ratings such that the app actually both queries both ratings (and requires them both), and also requires both ratings to be rated for inputs. (19:06)

I think I will drop the browser-like column tab header for now, and give room in the page header for a search field instead (using the full-text index on all propstructs, like I talked about, perhaps further filtered/resorted by querying the entities popularity/usefulness rating).\,. (19:08)

.\,.\,By the way, the first browser extension should just be a button that informs the user if there are entities related to the top URL of the page that the user is visiting (of course without logging any queries). (19:14) .\,.\,When clicking it, a new tab with the SDB website should be opened, and on it should be a column with the list of all the relevant entities related to said URL. (19:15)

%(18.09.24, 11:47) Ah, as a response to my commit message: I can just make it possible to overwrite the state in dispatch() (with the new state of the caller reducer). ..Oh no, it's the other way around.. Well, then dispatch just has to return the state, that should make it work.. (11:49) ..Oh, I should do both.. (11:49) ... (12:23) No, I can just access other "self"-reducers via `this.' And a child calling an action of a parent then just needs to not overwrite any state changes that the parent makes to the child if it does so. *Never mind, a parent can never change the state of its children using dispatch(). (Note also that returning nullish will mean that changes are not overwritten.) (16:23)
%(13:40) Hm, is it okay to save a lot of states for what should happen next (as long as they are never dangerous things to happen next, such as database inputs, and such)..?
%...Hm, I do indeed just have to only react on mouseUp event when it comes to reacting to user's scroll.. ..Ah, and let me just let REACT_TO_SCROLL reducer check that the final position isn't to close to the position of currInd column.:).. (13:55)

(18.09.24, 19:19) I thought about introducing an `I, the rating user' entity, and then I thought about introducing `@I' instead, but no, the user should just rate themselves according to predicative (that \emph{is} a word, luckily!\,:)) tags, since this is the most intuitive thing to do. And then an ML bot might just query especially for a users own rating of themselves. It is just a happy circumstance, then, that other users, and not least bots, can then also rate the user w.r.t.\ the same user descriptor tags. (19:23)

I've also thought, and am thinking, about ML. It could be nice if we could get something going pretty early. And I've thought that it should actually then not just be a list of questions to include in the ``questionnaire'' (and in the ML, of course) that users has to rate, but there should also be a rating of the weight of \emph{how much} the given ``questions'' (ratings, in reality) should way in terms of `correlation' in the ML algorithm. For instance, one might choose to give a relatively high weight the the users own ratings of which user descriptors suits themselves (when it comes to their tastes and opinions). So the ``questions'' that users uprate for the ML algorithm should therefore first of all include predicative tags (and relation + subject tags), and also.\,. and also the same thing but with a user as well, and where the object entity is them always the user in question themselves, and where the user entity might very well be the user themselves, but it might also be e.g.\ a bot (namely such that a bot's and/or moderator's rating of the user can count in the ML calculation, as well of course as the user's own rating of themselves). I hope this makes sense.\,. Okay, but now I'm thinking about how often we might do these ML calculations, then (at an early stage), and such things.\,. (19:34)

\ldots (20:07) Maybe propositions, propositional tags, and PropositionalRatings is actually a good idea, for convenience, along with predicative and relational tags.\,. I think so.\,.

.\,.\,(20:13) Oh, but that means that we can potentially reformulate the user descriptors as statements about `what is important in a movie,' etc.\,. .\,.\,That \emph{does} make sense.\,. (20:14) .\,.\,Yes, I think I will try to do that. But of course, it is a choice; you can also do the other thing. But yeah, I should introduce propositional tags, as well, although `tag' doesn't then fit so well in this case.\,. .\,.\,(20:20) Whatever, you just ``tag'' it to everything.\,.

(20:24) Hm, about the ML, I think that we should just do it not too frequently, and then just ask the users to update their descriptions of the.\,. ``correlation vectors'' (I've forgotten what they are really called) each time. For that's what I'm thinking: asking the users to try to describe each ``correlation vector'' after a round of ML user group creations. And yeah, a user group (bot) should be created for each of the largest vectors, which users can then subsequently use in their queries. This will not, I think, hopefully, %7, 9, 13,
be too hard to implement, and then we already get to showcase (some of) the idea of user-driven algorithms.\,:) (20:29)

(23:29) I should just keep all the user groups (bots) for each ML round at the start. And then we can always discontinue old ones when the aren't really being used.\,:)

%(19.09.24, 13:46) Hm, hvordan løser man det scroll-problem uden mouseUp.. ..Well, perhaps with drag-and-drop.. ..Hm, kan man ikke tjekke for mouse \emph{is} down.. ..Jo, by tracking its state..

(19.09.24, 13:59) Hm, it would make sense to call tags `relations' instead. Even the ``propositional tags'' can be though of as at least an unary relation if we also count the rating value as part of the relation. .\,.\,And then I could call them.\,. well, either `0-ary, unary, and binary relations,' if we don't count the rating value, or `unary, binary, and ternary'.\,. .\,.\,Hm, that does make sense.\,. (14:04)

%(14:07) Let me let the scroll behavior be for now, and then just click on columns in order to center them for now.. Hm, so do I keep track on scroll position then? ..I could also use scroll snap.. ..Oh, I could keep track of the actual velocity (divided by time) of the columns / the scroll, and then also listen on mouseUp as well as scroll in order to update the velocity. ..But only update velocity on mouse up if some time has eleapsed.. Hm, but what to trigger the reaction, that is still the problem.. ..Well, the mouseUp event.. ..Ah, so the mauseUp event triggers REACT_TO_SCROLL, but also checks that it isn't too long since the last scroll. (14:14) ..Oh, I should.. Oh, the scroll event only fires after scroll-end on mobile.. And one should use touchStart and touchEnd as well instead, as I was about to note. But yeah, this changes things.. ... (15:10) Yeah, so I should just think of implementing the app for desktop browsers only for now..

\ldots\ (15:10) I should make `@subj' and `@obj' keywords, then, also. When displaying the relation entity itself (maybe it's full title), these can then be rendered as placeholders, but in the context of a rating scale, the subject and object, if any, should then be inserted in place of these placeholders. Now, one might then render them a bit differently, when they are substitutions of such placeholders. But whether to render the EntityTitle, or render reference to it instead, that should just depend on the context entities, like normal. (So in other words, we always substitute these placeholders for rating scales.)

I've also thought about Categories/Subjects. I think `Subjects' (as in `topics') is still the better choice. And then for all entities, I will make a tab of `related entities' with sub-tabs `related entities under @subj' and `related entities next to @subj.' Then it should just be explained and understood what is meant here by `related entities \emph{under}' and `related entities \emph{next to}.' This can then be used to navigate subjects, as well as other things. Relevant links should of course also be a (very important and central) tab, and here I think we might call it `related entities about @subj' and.\,. .\,.\,perhaps.\,. Nah, we will not have a.\,. pendant, no.\,. A tab similar to the `next to' one for links, only the `about' tab, which is then just the whole link tab.

Hm, about `relations'.\,. Oh first of all, you can always include the user as well as an `input,' so `ternary relations' absolutely makes sense. But then, should I still call it rating scales, the thing they produce, or call it propositions.\,. Well, when rated it becomes propositions.\,. (15:25) .\,.\,And without the users and the rating, it's a `rating scale.' I guess that makes good sense.\,. (15:26)

.\,.\,(15:28) Hm, we could also call them `relational statements,' `predicative statements,' and `propositional statements,' and then perhaps group them together under `variable statements'.\,. Hm.\,. .\,.\,Hm, `relations' is probably better.\,. (15:31) .\,.\,Oh, why do we even need a term for the grouping?\,.\,:) I could just call it `propositions/statements,' `predicates,' and `relations'.\,.\,:) .\,.\,And if we want a grouping, then we could just call it `variable statements'.\,. But I don't think we really \emph{need} a class the includes all three subclasses.\,.\,:) (15:34) .\,.\,Yes! And we can also call it propositional function, or whatever. And also note that we will still often use the term `properties' as well, which is then just taken as a subclass of `relations.' (15:37) .\,.\,Oh, and `finitary relation' could also potentially serve as a superclass.\,. But let me just refrain from defining and using that superclass myself, I think.\,. (15:39)

\ldots\ (17:21) I've actually come up with some new naming conventions.\,.\,! Okay, first of all, I'm now thinking of calling `rating scales' `qualities' instead. So qualities are basically number properties of entities (sometimes more or less implicit entities in terms of what I previously called `propositional tags'). And note that `qualities' includes `quantities,' which means that we don't need `quantities' as a class and/or a thing, really. 

Then I will actually call `tags' `quality functions' instead. And in case of 0-ary quality functions, well, those are just qualities.

And `ratings' I will actually call `scores' instead, since it makes more sense to sy that a user `scores a cost of something,' rather than `rates it,' and it also makes more sense to say that bots `scores' qualities rather than `rates' them.

`Rated (entity) lists' I will call `scored entity lists,' `score-ordered lists,' or `score lists.' I'll decide on that.\,. (17:28)

.\,.\,And I will call the `propStructs' `defProps' instead, and call the `ownDefProps' `otherDefProps' instead, and then just, as I've said, make the label simply `defining properties' rather than `other defining properties' when displaying the metadata. I will still call `struct' property values the same thing, though: `structs.' (17:31)

.\,.\,And I will stop using `properties' to describe `relations,' but simply use `relations' much more instead. I will of course still make it a convention to use nouns for the short EntityTitles of relations, though; not much has changed in terms of how I want to render what was before `tags' and `statements'/`rating scales.' (17:34)

.\,.\,(17:36) We can btw call it `grading scores' and `grading qualities' when it comes to the F--A (and below and beyond) scales.

I've also thought about making compound qualities a purely app-implemented thing, where the app simply shows all the rating bars of the atoms/input qualities, and then makes sure that if all but one is rating, when including the compound itself as well, then the last one gets, not rated automatically, but the bar gets set automatically. Whether to input each rating or not is however up to the user. Each individual bar, including for the compound rating at the top, just has its own submission button that the user can press at any time. And no, we don't care if they choose to submit the ratings one at a time, after adjusting in-between, and then submits mismatching ratings (that doesn't add up to the compound like they should). (17:42)

.\,.\,And I think I will make it standard for relations to use a compound quality consisting of `is true' with a weight of 75 \% and of `is an important member of.\,. you know what I mean.\,. of the tag $x$' with a weight of 25 \%. (17:44)

(22:52) Hm `parameterized statement'.\,. .\,.\,Or `variable statement'.\,. .\,.\,Ooh, or simply `measure'.\,.\,!\,.\,. (22:59) .\,.\,Or simply `scale,' without `rating' in front (necessarily).\,.\,!\,.\,. (23:01) .\,.\,I like that.\,.\,! (23:02)

.\,.\,Hm, and then maybe I could just call the `quality functions' simply `functions' instead (since I'm not using that term for entities anyway; there we talk about `compound entities' and.\,. .\,.\,and templates instead).\,. .\,.\,Or yeah, a `scale function'.\,. .\,.\,(Of course I'm still gonna use `relations' and `predicates,' but these are then technically a subclass of `scale functions,' or what to call them.\,.) (23:07) .\,.\,Well `scale template' or `scale function,' yeah.\,. (23:09) .\,.\,Hm, let's use the latter.\,. .\,.\,i.e.\ `function'.\,. .\,.\,But again, we don't really need to use this class very much---perhaps I don't even need to define it.\,. (23:11)

.\,.\,Hm, and instead of `grading scale,' we can also just simply say `grading'.\,. (23:13)

.\,.\,Oh, Arguments should be scales, of course, except that they might also be wrapped in a short sentence about why.\,. it matters.\,. Nah, better to have that as a.\,. wait.\,. .\,.\,Well, you seem to need a wrapper, indeed, if you want to uprate such an explanation for the argument ('cause the argument itself doesn't ``know'' its parent).\,. .\,.\,Ah, but this will be one of those `derived entities' (compound entities) where we don't actually need to upload the entity before displaying it in a list: We can wait doing this until someone has something to rate about the entity.\,:) (23:19) .\,.\,(So no, the explanation follow along; it isn't mandatory. But explanations can be submitted and up-rated by users (of why the Argument is relevant to the overall scale statement.\,.).\,.) Hm, `scalar statement'.\,. interesting.\,.\,!\,.\,. (23:22) .\,.\,I think I like that a lot.\,.\,!\,.\,. (23:23)

.\,.\,And then you could also say `scalar predicate' and `scalar relation,' which are then superclasses of respectively `predicate' and `relation' (which assumes a likelihood scale).\,. (23:31)

.\,.\,Let us use both: Let us use `scalar statement' \emph{and} simply `scale' for short. (23:34)

(20.09.24, 10:02) I think I will just use `scale' more so than `scalar statement.' But I will still then call it `scalar relations' and `scalar predicates.' (I've also thought about using `scalar' instead of `scale,' by the way, but no, we don't want the score/value--scale ambiguity.)

Now I'm thinking about the `@subj' and `@obj' keywords: Do they make sense?\,.\,. .\,.\,(10:07) Ooh, we could also make a more general `@[$<$property name$>$]' syntax.\,.\,! .\,.\,Yep! (10:08)
.\,.\,(10:17) Well, and what then, should the scalar functions (formerly `tags') then be templates, or what?\,.\,. .\,.\,Hm, I guess I haven't really though about this any time recently.\,. .\,.\,Maybe the `scalar functions' \emph{are} actually scale templates.\,. (10:23) .\,.\,And then scalar predicates should just always include a `subject' property, and `scalar relations' should include an `object' property as well.\,. (10:24) .\,.\,Oh, we \emph{can} by the way also still refer to scalar predicates as `tags' as well.\,. .\,.\,Tags are the scalar predicates that uses a grading scale, not a likelihood sc.\,. well, relations use a compound scale, which is a compound of a likelihood and a grading scale, so what to display on the scale there? (10:27) .\,.\,Oh, maybe I'll just use `predicate' instead of `scalar predicate,' and the same with relations, and let that be implicit such that the two shortened terms are the general terms.\,. .\,.\,Okay, many questions.\,. (10:30)

.\,.\,Oh yeah, predicates/tags and relations \emph{have} to be templates, as scales they produce have to be derived (formal) compounds. So yes, `scale templates' it is. And yeah, let's say that.\,. Oh wait, first of all, I was about to say that subject and object (if there) have to be the first and the second inputs, respectively (and should be the only inputs). But this reminds me: We do already have the input placeholders, then. So we don't need the `@[$<$property name$>$]' syntax, no.\,. We can just use the convention of always introducing each placeholder as the only value of a property bearing the given name instead.\,. (10:37) .\,.\,Hm, and I could then render the placeholder from there as.\,. the name.\,. well, maybe we just want numbers.\,. But yeah, you could also potentially name it. But that's a matter for the application layer. (10:39)

.\,.\,And yeah, I'll call the unary and the binary scale templates `predicates' and `relations,' respectively, and yeah, the `subject' should always be the first input, the `object' (if there) should be the second, and there should be no other input placeholders than that, or any other type of placeholder for that matter.

.\,.\,There we are.\,:) And now the remaining question is just: How to render a compound scale?\,.\,. .\,.\,Well, that's just something to think about.\,.\,:) (10:59)

%(12:30) Learning a bit more about CSS. I just found layers..!.. ...And scopes. Cool. So you don't ever need to hack you way to getting these layers; they are already there. And the can be managed nicely via imports. So there we are.:) (12:44)

\ldots\ (14:19) Okay, I will not refer to the unary scale templates as `predicates,' but rather as `qualities.' And `relations' (binary scale templates) I will call `relational qualities' instead, but still call the normal relations `relations.' The scale of the normal relations should actually be a grading scale, and instead of using a weighted compound, we should instead introduce \emph{filtered} compounds (compound scale, i.e.). Now, I thought a bit about introducing a null score (rating value) (perhaps the minimal score possible), which then states `doesn't belong on this list.' But no, while the weighted compound scales should still just be implemented in the app.\ layer at the early stage of the app, the filtered compound scales have to be implemented by the aggregation bots. So to make this easier, we just need to make sure that a scale/quality derived from a filter template gets a filtered scale/quality/relational quality class as its main class.\,. .\,.\,Oh, I should start calling it `main class' in the template defStructs.\,. So there we are, then these bots should just check the filter quality first, and only rate the entity according to the filtered compound if the filter quality gets a score above a specified threshold.\,. Oh, so the bot needs to look at more properties of the entity, so maybe never mind about needing the main class to be `filtered whatever'.\,. (14:29)

So for the normal ratings, we just have a useful/importance grading scale (deja vu, of course.\,.) that is then filtered by the likelihood of the entity even belonging the the list in the first place (e.g.\ `is the person even an actor of that movie,' etc.). So I don't need to consider how we can have scales of weighted compound qualities---'cause we don't even seem to need these compounds now in the early stage of the app.\,.\,:) (14:32) .\,.\,Wait, if the normal relation have grading scales, then I should just refer to all `relational qualities' as `relations' for short.\,.\,:) (14:34) .\,.\,Nah, wait, no.\,. No, but we'll hardly ever use `relational quality' anyway, I think.\,. .\,.\,No, let me introduce the class for good measure, but then never use it myself for the early stage, and never really speak much of it either, except a brief mention when introducing the `relation' class.\,:) (14:37)

\ldots\ (17:05) I \emph{am} actually going to introduce the `@[$<$property name$>$]' syntax, since it will make it easier for descriptions to refer to their own properties in their description(s) in a neat way. I will then let the match for $<$property name$>$ be case-insensitive, such that we e.g.\ get to write `Subject' and `Object' in quality descriptions. .\,.\,Hm, let me by the way still refer to the standard predicate- or verb-defined/titled unary qualities as `tags'.\,. But yeah, isn't that nice?\,.\,:) (17:09) .\,.\,Oh, and you can then still choose lower-case references in some instances, e.g.\ for a parameter $x$, or whatever. Note also that it's nice to use short (perhaps a little bit abbreviated) property names in general, and then elaborate in the descriptions, and/or in other properties (e.g.\ `full title' or whatever). (17:11)

I've also considered using star rating scales, 0--5, for relations, but no, the grading scale is actually better. (And it's a really good scale; it was an important idea to use that.) And then I will just make sure to let the underlying number behind the scale, which can also be displayed when adjusting it (and submitting it, etc.), go from 0--100 \% (0--1). That also makes it appropriate for e.g.\ weights, and such. I don't think I'll actually use polar scales then, after all. Instead the lower limit of the `F' grade interval will be 0, and the upper limit of `A' will be 1. Of course, these scales can be broken, as I've talked about, which means that you can also give a query user a weight of more than 1, if that is useful to you. (17:16)

I've also thought about the trouble that using floats introduce in relation to binning the curves. But this is actually easy to solve: Just start by grouping scores according to the exponent, and then cut off the outliers, and then that get's you the scale: The largest exponent that is not part of the outliers. (17:18) .\,.\,(When displaying the combined curves to the users (and we \emph{should} do this, btw), it is by the way important to show the full interval, after the outlier cutoff.) (17:19)

.\,.\,It's by the way funny that tags, and other `qualities,' are now templates.\,. So now templates are much more than just something meta.\,. .\,.\,But yeah, that's absolutely fine.\,. .\,.\,That doesn't mean that tags are now meta entities: They semantic meaning is still.\,. well, the quality. And then if you rate/score them in relation to meta tags (or meta relations), then you talk about the database-stored representation of that quality. So yeah, templates don't need to be considered meta, not at all.\,:) (17:25)

\ldots\ (18:43) No, I don't at all like that tags, etc., should be templates, after all. So instead I will use a bare ApplyRelationToObject() and ApplyQualityToSubject() templates to combine relations or predicates/qualities with object/subject. And entities with these templates can then never be used at column formerly known as the `tag' (tag\_id) column (instead the sale should be exploded before rating). I will by the way also reintroduce `properties' as the \emph{most} standard subclass of `relation' (which is a pretty standard subclass of relational qualities). I've also wondered about if the Apply() templates should define the scales, but no, these should be pretty bare, as I said, and the scale should thus be defined be the relation/quality. .\,.\,(Hm, by the way, I might change my mind again about my recent naming conventions.\,.) .\,.\,Hm, and there was a last thing.\,. (18:50) .\,.\,Yeah, and that was that I \emph{should} then introduce the `@subj' and `@obj' keywords (but also keep the `@[$<$property name$>$]' syntax as well, I think).\,. (18:53) .\,.\,Hm, yes to the former, indeed, 'cause it also allows us to make the relation define the title of the scale and/or the unary quality (when one or both the Apply()'s have been applied).\,. And yeah, I also like the `@[$<$property name$>$]' syntax as well, such that we get a neat way that an entity can refer to its own (metadata-) properties. (18:57)

.\,.\,(19:05) Maybe `measure,' `predicative measure' and `relational measure'.\,. .\,.\,Well, or we can also replace `measure' with `scale' here, I guess.\,. (19:07) .\,.\,Ah, but when we rely on Apply() anyway, it isn't too important to define the classes of these scalar/scalable, finitary relations anyway.\,. (19:11)

.\,.\,That is absolutely true, and I could even just call then unary and binary (scalar) functions, but you know what? How about `scalar statements,' `scalar predicates,' and `scalar relations,' actually? I think that that's actually nice, and I even think I might \emph{want} to say `scalar statement' rather than just `statement' when talking about e.g.\ Arguments.\,. (19:18)

(21.09.24, 8:12) I almost think that `scalar statements,' `scalar predicates,' and `scalar relations' is very close to working, but it just doesn't work in the end. So what I'll do instead, I've now decided, is to simply use `statements,' `predicates,' and `relations,' and then actually assume that quantity qualities are always formulated as statements(/predicates). So for instance, `the  cost of $x$' becomes `$x$ costs money' instead, and then you just use a quantity scale (particularly with units of money) to specify this statement. (8:16) .\,.\,I've also thought about whether to use `tags' and `properties' for the particularly formatted predicates and relations, but then again, I think that pretty much all predicates and relation can be at least summarized as (compound) words, so there's no real sense in making that distinction. I therefore think that it's actually much better to call it the same thing all over: `predicates' and `relations.' (8:21)

.\,.\,And this means that most predicates are formed simply by the user choosing a (compound) adjective or verb to use for the given predicate, and then most often inserting it into the same predicate template, namely the grading scale one. But they might also choose a likelihood scale, or some quantity scale, in which case they might also have to choose some parameters of that scale themselves in the template. So yeah, generally the just have to pick a template (where they'll often pick the same), then formulate the title (as a compound adjective or verb), and then in rare cases also adjust some other parameter(s). And for relations, it's exactly the same thing, of course just with nouns instead, and where they'll also have to pick an class subject class.\,. Wait, all predicates and relations need to have their subject class defined (and maybe you also want to define the object class in some instances for relations.\,.). So yeah, the users also always has to choose that as well. (8:30)

The ``Apply()'' templates that I talked about yesterday will then of course be respectively a `compound statement' template and a `compound predicate' template. (8:32) .\,.\,And what I meant by ``bare'' is this: They are each only defined by two inputs: the predicate/relation and the subject/object. (8:33)

.\,.\,The likelihood scale should have buttons to say either 0\% or 100\% with negligible uncertainty, but the grading scales, and quantity scales should of course not.

Oh, and one could, by the way, talk about `quantity statement,' which is then the statements like `$x$ cost money,' where you expect/want to see a quantity scale accompanying that statement. (8:37) .\,.\,(But this is just a term that we \emph{might} use, and not something that consider making a class for.) (8:38)

\ldots (8:50) We can say that a score `qualifies' the statement. .\,.\,(The score and the scale does.\,.) (8:54)

%(9:24) Hm, floats do have the NaN option, but.. ..But no, I can just use that to delete ratings instead. ..Oh, maybe MySQL doesn't handle NaN. So let me make a separate procedure to delete scores.. Why didn't I do this anyway?..

(10:10) Oh! I could make a keyword, like `@score' perhaps, that references the score of the statement, meaning that quantity statements can be formulated as `movie $x$ cost $score$ $<$score unit$>$'.\,. Hm, yeah, couldn't we automatically import the unit this way?\,.\,. (10:13) .\,.\,Yeah, 'cause `movie $x$ has a length' just doesn't work (and neither does `movie $x$ is long'). So yeah, couldn't we do that?\,!\,:) (10:16) .\,.\,And yeah, we will import the unit of the score automatically.\,. Hm, and how.\,.\,? .\,.\,We will by the way just use $x$ as the standard variable to display for the score, so e.g.\ `LoTR has a length of $x$.\,.\, hours'.\,. Yeah, it's fine that we write it like this, 'cause $x$ is never substituted in this sentence. For the scale itself, the statement can then code for more sophisticated units, such as $\lfloor x \rfloor$ h, $(x - \lfloor x \rfloor) \times 60$ min, \ldots, to show next to the bar. (10:23) .\,.\,And we also form the full statement for any scale, even if they entity hasn't actually been inserted yet (and thus has no ID yet, but it still ``exists'' all the same, you can say), so we can just read the unit directly off of a property. (10:25) .\,.\,Great! That solves that!\,.\,. Oh, but \emph{could} we then do the same for grading and likelihood scales, and any other scales for that matter?\,.\,. .\,.\,`Movie $y$ is an animated movie with likelihood $x$'.\,. Hm, here it does seem a bit redundant, so I'd rather that we just let the score, and how it qualifies the statement be implicit in terms of the entity title, and then only be specified explicitly in the description(s) of the statement. .\,.\,And the same for the grading scale, of course. Okay.\,:) (10:30)

But I said that we can read it ``directly off of a property,'' which is not quite true for compound statements, no. But we can still read it off of the properties of the inputs of that compound, so effectively we can. (10:32)

.\,.\,Great!\,.\,.\,:) (10:32)

(10:39) Oh, we don't need to read off the unit automatically: the user defining the statement (or predicate, or relation) can just insert the appropriate unit themselves, why not. Okay.\,:) (10:40)

(10:44) Hm, I'll use capital first letters for classes.\,. .\,.\,Or will I?\,.\,. (10:47) *(10:57) Hm, I think capitalization is preferable, indeed.\,.

(10:49) Oh, and we must also at some point add syntax that makes us able to transform the score value, perhaps introducing a syntax like `@score($<$calculation formula$>$)'.

(11:14) Predicates and relations must have Predicate and Relation, respectively, as their main class.

(11:18) Oh, instead of `metadata' I could say `defining data'.\,. .\,.\,That makes more sense.\,.\,:)

(11:48) Hm, it would actually be nice if we could make statements more modular and use entities as rating scales, e.g.\,. .\,.\,Yeah, of course, modularity for the win.\,.

(12:12) Okay, so what does a statement need?\,.\,. .\,.\,A title?\,.\,. .\,.\,Hm, for compounds I will use that special syntax for rendering them.\,. And atomic statement should mostly be whole sentences, or several, meaning that they should be Texts.\,. (12:15) .\,.\,Yeah, and ones that can include the `@score' keyword. So for atomic statement: A text and a scale definition.\,. .\,.\,And I'll let Scale definitions, or `Scale format,' be a class of entities, which I will figure out as I go along. So let's move on to predicates. These should have a subject class, and a title, which should preferably be an adjective or a verb. And then they should have a Scale spec.\ as well, and that's it, isn't it?\,. Then for relations, we might then just have an optional object class as well. (Recall that an empty string as a property value is interpreted as null/undefined.\,.) Yeah.\,.

Okay, so let me then start to think about the scale format entities.\,. .\,.\,`Scale specification' entity, rather.\,. (12:21) .\,.\,Hm, is lower-case better?\,.\,. .\,.\,Hm no, since we won't always be able to highlight entities in titles (and texts), so the least we can do is to capitalize entities that often needs highlighting.\,. Hm, but don't they all, then?\,.\,. (12:23) .\,.\,Okay, I will highlight them, and I might also make them clickable, where the click then just don't bubble up to the parent, and where I'll also set some small markers at the start.\,. Well, that all depends on the context; in some contexts I might do this, in other we might not. (12:31) But about capitalization, then?\,.\,. .\,.\,I think whenever can entity can hide some details underneath the title, like basically all classes, then they ought to be capitalized. Great.\,:) That was a good realization of why we should capitalize entities like classes.\,:) (And note that I \emph{can} still say `classes' here with lower case, since the reader knows what kind of classes that I am refering to, but the entity should be capitalized when referenced directly.) (12:34)

What about the ``meta flag,'' which should actually be a class, and the other ``flags''.\,.\,? (12:42) .\,.\,Well even though the class of a predicate should always be the fundamental `Predicate' class, they can still have other classes.\,. .\,.\,So yeah, a list of classes as a common property as well.\,. (12:44)

(12:46) Oh, Class specifications should not just be data structures after all, like was otherwise thinking. But they can also just be descriptions.\,.\,! .\,.\,And a mix, of course.\,.\,! .\,.\,Yeah, so I will probably let the ones I have in mind be descriptions for the most part, and then only use other properties for the potential units, and the potential limits.\,. Well, the ``limits'' don't need to be limits; they can also just be markers of where the app should initially make the intervals (if it doesn't later overwrite this with some other user preferences). For the grading scale, I'll let it go from F to A, meaning that the user has to click `zoom out' in order to access A+, A++, etc. (12:52) .\,.\,Oh, and a property to define whether there should be button to give min or max scores with negligible uncertainties?\,.\,. Sure.\,. (12:54)

(13:10) Hm, maybe we'll actually change me convention and start mostly using plural nouns for relations' subject nouns, and then only use singular nouns when you expect there to be only one.\,. .\,.\,I mean, why not?\,.\,:) (13:11)

\ldots\ (14:29) I think I will actually not include the unit property, as this can just be defined by the.\,. text.\,. Hm, and how to know to express likelihoods in percents, then?\,.\,. Anyway, let me point out that we can always ``edit'' the representations afterwards. I have probably said this before, but I should make a proc to query the metadata of the best representation right away, of course given the query user as input.

Let me also note that I want the scale to zoom out automatically if the user places the slider at the end of the current interval. But I don't want this for likelihood ratings in particular. So there should be a property that can limit the interval, indeed. Oh, and I don't think that I need the buttons to rate 0 or 1 with negligible uncertainty after all. For as long as the center point of the curve is at the maximum value precisely, then the uncertainty doesn't matter much for the most frequent score.

.\,.\,Hm, I guess I should add the (optional) unit property. And maybe I should write `$<$Movie$>$ has a length of $x$' rather than `\ldots of $x$ h.' Hm, that is actually a bad example.\,. Oh no, it's not.\,. .\,.\,Ah, not as long as we can mark such statements as being `factual' as well, such that authoritative users can be queried. Oh, and that reminds me: I think I should make the meta statements a separate class from the statements.\,. .\,.\,Hm.\,. (14:43) .\,.\,But yes to the unit property, btw.\,.

.\,.\,But `factual statement' can just be a subclass, for sure.\,. .\,.\,And so can the meta statement. Then Statement should just state that unless otherwise specified, the statements are about the thing that the entity represents, not the representation itself. .\,.\,Hm, let me call it `Data statements' instead.\,. (14:53)

(15:10) I personally only plan on using two Data relations, and perhaps one Data predicate (saying `is well-formed'), but only maybe. So let me not define a template for Data relations, etc. Let me instead just define these relations via other\_props.

(15:25) I forgot to say that I wanted the grading scale to extend a bit into A+ and F- on both sides by default, but maybe it should actually only extend to mid A and mid F instead by default.\,. .\,.\,Hm, and C should be at 0, I think, rather than at 0.5 (or 2.5).\,. .\,.\,Hm, let us say mid A+ and mid F- after all.\,.

.\,.\,Hm, So let mid A have a value of 1, and let mid F have a value of $-1$, and for the unit, let us actually measure it in multipla of A, such that A is the unit. Well, and make it percentages, so `\% A' is the unit.\,. Yeah, that works.\,. .\,.\,Except: Do I want F at the bottom instead?\,.\,. .\,.\,I could also let the `unit' be `grades (above F),' and let F be placed at 0, and A at 4, and then make the intervals rendered as gradients instead.\,. .\,.\,Or we could measure it in grades above C.\,. Hm.\,. .\,.\,Hm\,.\,. (15:39) .\,.\,Hm, maybe a star rating scale is just more practical, and while it doesn't have exactly the same immediate associations, we could still write out very clearly `among the best' when hovering over the A/5-star interval, etc.\,. Hm.\,. (15:41) .\,.\,Hm, is there a way to fuse the two together?\,.\,. (15:44) .\,.\,That's it, I'm gonna try to fuse them together.\,. (15:49) .\,.\,And I'm gonna do it exactly by showing the grades under the interval, accompanied by a text only when hovering over the interval.\,. Hm, but where to show the usual row of stars?\,.\,. It gets a bit cluttered if I'm not careful.\,. (15:57) .\,.\,Hm, okay, maybe we'll just show the grades.\,. .\,.\,And I could render the value a e.g. `B + 0.5' and.\,. wait the plus sign doesn't mix well with A+ etc.\,. .\,.\,Hm, B.5.\,. .\,.\,Oh, we could just use `A+3.46', then, as an example. That is actually a good idea.\,.\,:) (16:02) .\,.\,And then A++ becomes A+1, and A+$\times$3 becomes A+2, etc. :) (16:03) .\,.\,Hm, ah, we could also add some superscript unit at the end like.\,. .\,.\,Like the degree symbol, or the apostrophes that is used for e.g.\ feet--inches and minute--seconds. Great.\,:) (16:07) .\,.\,`A+3.46\textdegree'. Nice.\,:)

(17:30) Wow!\,.\,. What if the grade was automatically calculated instead from the curve!\,.\,. .\,.\,Well, or maybe we should just make a habit of showing the curve of all entities of the given class to the users!\,.\,. (17:32) .\,.\,That all of a sudden eliminates the ``need'' of using that F--A grading scale.\,. (17:34)

(17:53) Hm, and since the curve is generated from the whole subject class, the proportions of the doesn't matter, so we could even let the stars just be an initial suggestion for the scores, in principle.\,. .\,.\,But I still want to write e.g.\ `among the best' when users hover over the neighborhood of the 5-star score.\,. (17:56) .\,.\,Hm, and then we could also show a normalized bell curve that is supposed to serve as a standard for how most curves ought to go. And we could just.\,. Nah, maybe we'll always show a silhouette of that in the background when showing the actual curve.\,. .\,.\,This is great.\,. (18:00) .\,.\,Well, except that we can't assume all qualities to fit a bell curve.\,. .\,.\,Hm, but I think that it's fine anyway: We can still show it in the background to be of some assist.\,.

(18:09) Hm, I could make this guiding bell curve be.\,. .\,.\,have faded edges such that it's not a clear line, but the curve it just where some color is strongest. This could give it a more loose feel, making it just a loose guide rather than a thing to necessarily follow. Ooh, it could also be a silhouette with gradient edges, I like that better. (18:12) (So having one color inside and another outside, with a gradient on the curve.\,.) Well, but then you can't see where the curve actually is, unless the gradient starts from it.\,. .\,.\,Or unless the gradient is short enough.\,. .\,.\,Okay.\,. I'll see.\,. (18:15) .\,.\,Hm, I could also just do a stippled line type for the curve. That's also a fine option.\,.\,:) (18:17)

(18:53) The offset of the scales actually shouldn't matter, so I'm thinking of putting the center of the bell curve at 5, and then let 10 be at $2\sigma$, I think.\,. So no units, not even stars.\,.\,:)\,.\,. (18:55)

(22.09.24, 10.15) I will still draw the F--A intervals, and probably beyond, on the axis (below it). But I will actually measure the score in stars, making the unit a star emoji. I will not draw a row of stars, though. The users must imagine that themselves. I will draw the stippled bell curve, though. And if we really want to be serious, we also fill out the area under the curve after the slider, away from center, and then draw a percentage denoting that area, just as a small guideline: nothing imposing at all. (10:19) And of course, I should draw the, not `uncertainty,' but `agreement' curve. And I should draw the ``error bars'' as well, and make them adjustable with pointer drags. (10:21) And at the right side, I draw the score value, in input fields that can be written to.\,. Well, maybe no. Maybe I'll just start letting it be something to adjust with the cursor (/touch).\,. (10:23) I should also make zoom buttons (which can be hidden, in particular for the likelihood scale).\,. .\,.\,And these can also be on the right, first thing, if they are there.

(10:25) I had a cool idea as well last evening. We could actually make the aggregation bots normalize the scale dynamically such that it is made to fit a bell (Gaussian) curve as well as possible (only when there is enough entities (with enough scorings each) in it). I actually like this. It does mess with the correlation scores a little bit, but the normalizations aren't meant to change the scale too much at each time, and correlations to grading scores aren't typically too important themselves, so I think it will work out nicely anyway. So I actually do think that I will implement this idea at some point.

.\,.\,Oh, and a think I forgot from yesterday is also that I think I should make my predicate and relation templates include a statement text property as well, which can then be used for the `full title' of the compound statements. And by the way, `full titles' are just implemented by giving the EntityTitles a `full' class, and then let CSS do the rest (because that's simpler, and also more easily adjustable afterwards (e.g.\ by User Sheets)). But I will only render the full titles in the defining data pop-out display, where all the other props can be seen as well. And this means that we are free, then, to use the `@[$Property\ name$]' syntax in full titles, since the entities that these will then be placeholders for will thus always be displayed close to the full title. (10:38)

.\,.\,By the way, since we are using capital first letters for the classes, it now means that it makes sense for e.g.\ a Template entity to hold a `template' property member, and for a Predicate to hold a `predicate' property. .\,.\,Hm, should we then capitalize for.\,. e.g.\ for compound Predicates holding a Relation property, or should we reference.\,.\,? (10:42) .\,.\,Hm, or should we just write lower case `relation,' and then just let it be up to the description to explain that that property should take a Relation entity, I think so.\,. (10:44)

%(12:03) Oh, I should remember to use the @123c456 syntax..

(12:07) Hm, maybe we should \emph{also} be able to denote a class context at the property name.\,. Hm, but that will mainly be used by set properties---unless we do it for aesthetic reasons.\,. .\,.\,Yeah, let us introduce a `@c123' syntax (with no numbers before `c') to use at the end of property names. (12:12)

(12:26) Hm, maybe singular nouns are better.\,. .\,.\,Oh wait, we could just make two different relation templates. And that makes sense---actually we have to!---since we generally want likelihood scales for one-to-one relations, and grading scales for one-to-many relations. (12:29)

(12:36) Hm, I think I will just define my likelihood scale and grading scale purely from their description for now. And for the quantity scale, we just need the `unit' property, don't we (since we'll just assume that the quantities in question are always limitless for now (as it doesn't matter very much if some limited scales are treated as unlimited)). Oh, we also need the default range specifiers. So maybe I'll add the `limited scale' class or property if it is easy to do (when I'm already implementing it for the likelihood scales anyway). (12:40)

(13:01) Maybe I'll drop the star (emoji) units, and just let the grading scale scores have numerical values.

(13:32) I could also just write `@[Subject]' rather than `@subj', and then just say that an entity can also make such references to properties that are not their own, but which are properties of the compounds in which they appear.\,. .\,.\,Ah, and the description can just explain what the reference refers to. So these are just `non-substituted (and implicit) references'.\,. Sounds good.\,.\,:)\,.\,. (13:36)

(13:38) Hm, and let me.\,. wait.\,. .\,.\,Hm, do I want to.\,. yeah, hm.\,. .\,.\,Ah, I'll just define both the `statement' \emph{and} a `predicate' property for Relations.

(13:49) Hm, sometimes one might want to insert the other properties, right.\,. .\,.\,Well, I can always add e.g.\ a `@($<$Property name$>$)' syntax later if we want that. (13:52) \ldots (14:20) If we ever implement this, be careful about users writing `@(\_prototype)' (and always refrain to access user-specified properties directly for the same reason).

\ldots\ (15:35) The scales' zoom buttons can be up in the right corner instead.

(15:50) Hm, Templates can have descriptions of how to use them, but these will not be a part of the derived entities. So I guess Templates should just have description properties as well.\,. .\,.\,Alternatively, we can make the entities inherit the description from the template.\,. Hm.\,. .\,.\,Automatically, without needing to store it twice of course.\,. Well, that's almost the same thing as using the `description' property.\,. Hm.\,. .\,.\,Okay, I actually feel like I ought to change it a little bit to make descriptions smarter, maybe by factoring them out as a description column, only meant for that.\,. .\,.\,Ooh, and then the template could hold a.\,. `instance description' property.\,. Hm.\,. .\,.\,No.\,. (11:58) .\,.\,That would almost have to be a instance description column.\,. .\,.\,Is that it?\,.\,. .\,.\,Adding two new columns.\,. .\,.\,And then we might as well split up data\_input into a text and a binary data column.\,. .\,.\,I guess I should.\,. (16:03) .\,.\,Oh no, that gives us too many hashes.\,. So I do actually need to multi-purpose columns.\,. .\,.\,Hm, I could make Templates' descriptions two-part, where the second part is the instance description.\,. .\,.\,Or I could make data\_input into a struct that is free to specify its own data.\,. That does seem a bit neat.\,. Oh, and that would also allow for binary entities with metadata, or other fields that are not part of the format of the file itself.\,. .\,.\,Yeah, meaning that.\,. Nah, never mind, it doesn't give us any new powers/possibilities that we didn't have before. But it still sounds neat with that kind of data\_input column anyway.\,. .\,.\,Except it would then have to be JSON. So how about we instead just say the the data column \emph{can} be a JSON object, that sounds better.\,. (16:11) .\,.\,Alright, then I need to make data property placeholders, and just disallowing any leading `\_' for these properties. And then Templates just assumes a specific format for the data column, without needing to specify this in their defProps.\,. .\,.\,Hm, let me call it the other\_data column.\,. (16:17)

(16:24) Oh.\,.\,! I was just thinking: maybe I don't need to worry about duplicate entities, really, since I have the `better duplicate' relation. And when the database gets distributed, we can't avoid duplicates in real time anyway, so maybe it just adds an unwanted and perhaps redundant layer of complications. Now, of course I still need the derived entities to be searchable. Or how much do I actually do that, is the question that I just realized to ask myself.\,? I don't really need lookups for compound predicates and statements anymore, or do I? (16:28) .\,.\,Yeah, I still do, otherwise the compounds have no use. But what to do, then?\,.\,. (16:29) .\,.\,Well, let me just do exactly the thing I have been doing, and then maybe we could just concatenate the non-searchable columns before hashing if we want many such columns. Then I should just technically.\,. Oh, or we could just hash the hashes. Easy.\,. (16:33)

.\,.\,Oh, hm, maybe I don't need the data placeholders, then.\,. Maybe each data sequence should just have its own property name.\,. Hm.\,. (16:37) .\,.\,Yeah, I can make an other\_main\_props column and an other\_props column, which can be much longer, and it ot fetched for the EntityTitles alone, but which is also just JSON property struct. And then binary\_data can be the last column, meaning that there should still be the `\%b' placeholder, which can be used by both the two props columns. Hm, that sounds like it.\,. (16:41)

.\,.\,If a non-searchable (unless you know the full thing, so practically still as searchable.\,. which means I should find another term to use.\,.) column is missing, I should then still pad the hash *(before combining the hashes into one), for otherwise we risk collisions. If and only if all these columns are missing should the stored combined hash be the empty string. (16:45)

\ldots\ (17:38) It shouldn't be a `\%b' placeholder; I should just make a `@bin' keyword to use instead.

(18:27) Hm, do I want to divide other\_props up, having a special description and instance description column.\,. yes.\,.

%(21:44) I don't think I will actually implement and use that sessionState thing at any point after all. So I guess some waste of time, except I learned something from it. But now I've actually even gotten an idea to get rid of passData() as well: Why not just put use the DOM nodes to communicate and dispatch actions to their parents, similar to what I did with my ContentLoader system?. That way I can have almost the same API, apart from getting rid of the passData() function. I think I will implement this soon.

(23.09.24, 9:29) Hm, is it better to store files separately, and then make some syntax for file references, perhaps an extendable one that allows for a table for each file type potentially?\,.\,. .\,.\,Nah, we want the files to be entities as well, why not, and it shouldn't really make much difference otherwise. Instead of file tables, we just use a template and a BLOB (and potentially metadata as well, as inputs to the template, or stored in other\_props), and then the control layer can just query for that instead. So yeah, binary\_data stays.\,.

(10:35) I should probably also at some point make a convention, perhaps with accompanying syntax, for how an entity can refer to its parent, meaning that its semantics can be dependent on the `parent entity' in which the first entity appears as part of the defProps.

\ldots\ (12:51) I should also make a template\_text\_input column, like the string one, only larger, and not considered as part of the mainData.

\ldots\ (16:01) I was about to posit that we maybe no longer need the sets, now that we use plural nouns for one-to-many properties. But then I also thought about that we should maybe ad template\_list\_inputs. And then the sets will actually be handy once again, namely to concatenate lists, essentially. .\,.\,But I could call it `concat'.\,. .\,.\,Sure.\,. And yeah, lists inputs should work the same way as string inputs, only where the strings can here only be comma-separated integers. (16:06)

(19:22) Hm, the idea of using user--user agreement as correlation matrix entries for ML is an interesting one, but I'm not at all sure that it will be as efficient *(it will be more efficient in terms of computation time, for sure, but it might then---naturally, actually---not be as precise and as useful) as ML where we simply take the score\_val (disregarding their agreement curve) for each scale of the set and do regular ML with that. But I will definitely try it out. It is supposed to be user-driven anyway, so we are free to experiment.\,:)

(24.09.24, 9:38) I thought of another possibility. One could also make the user--user agreement ML first, and then instead of making the regular ML with all the users of that set, simply take the user vectors with the highest agreement (a good number of them, but still not at all all of them) and then do the regular ML with the averages, or probably actually the most frequent values, of these user vectors. Then we already automatically filter out users who has many outlier scores. And yeah, it sounds like it could be much more efficient, while perhaps still getting a good fidelity/precision. (And we get a good way to make use of the agreement curves, which therefore might add some to the precision compared to the regular ML method (even if the regular method might still be more precise overall, who knows).) Pretty exciting to experiment with.\,:) (9:45) .\,.\,Oh wait, maybe this method would not work as is, since you might then get too few points for the subsequent regular-method ML with the user vectors.\,. Hm.\,. (9:48) .\,.\,Hm, nah, it \emph{might} still work (to some degree), especially if we also remember to weigh the user vectors according to (combined) agreement, I think. It's worth trying, and it's also worth investigation further, how to potentially combine the two methods ('cause the user--user agreement ML \emph{could} in principle also stand alone), and also in generally what ML techniques one might invest (where we have these extra agreement curves in play, compared most other dataset). (9:57)

(10:55) Let us turn template text placeholders into text references instead.\,.

%(14:24) I think I will reimplement useStateAndReducers() now, actually..

(20:34) I don't \emph{think} that the user--user agreement ML is gonna help much, actually. But the regular ML (where we don't use the agreement curves) is gonna be very powerful anyway, especially since the users can curate list of good scales to score in order to do efficient ML (i.e.\ scales that seems to be very telling of a users tastes/opinions (potentially including scales where the user rate their own tastes/opinions, of course)).

(25.09.24, 8:03) I still think they might be worth it, but I wish my agreement curves had a little more utility than being a tad better than the median, apart of course from giving user more ease at scoring something.\,. .\,.\,Wait, what happens if you plot the agreement curves on all axes and then.\,. hm.\,. (8:06) .\,.\,Hm, no.\,. .\,.\,Hm, maybe we could just add the agreement as another parameter to the regular ML, for then.\,. hm.\,. (8:09) .\,.\,Nah.\,. But maybe we could use these curves to adjust all scores before going into the ML, meaning that we draw then towards the mean, but of course only if the have agreement with the mean. (8:14) I could do it just with the mean, or we could do it with the local peak, hm.\,. .\,.\,Yeah, we could multiply the individual users curve with the total one, before then selecting the resulting maximum as the score going into the ML. I think this makes very good sense, actually.\,:) (8:16) .\,.\,Oh, and one could even make several rounds, maybe where you first do the ML process without this adjustment, then you do the adjustment, but where you first filter the total curves according such that.\,. you only take the most.\,. Ah, I was about to say that this would be a $n^2$ runtime, but you only need to compute the distributions for each scale, and each of the most prominent ``correlation vectors''.\,. .\,.\,The `principal components,' rather (no wonder I forgot it.\,.).\,. you need to do that only once.\,. .\,.\,Yeah, so after the first round of ML, one could do another on top that adjusts each user's scores, if they have a significant score\_err, towards where there are most agreement among the top principal-component (PC) groups from the first round that they belong to. (8:30) .\,.\,Or you could, of course, adjust in the first (and perhaps only) round already. (8:31)

.\,.\,Yeah, this does make sense, actually. And we could even tell the users about it: When putting a larger score\_err, your score is pulled more towards the most frequent score, which means that you score matters less in the ML process (in terms of putting you in the right PC groups and not putting you in the wrong ones). And conversely, if you chose a small score\_err, your score will not be pulled as much towards the ``center of gravity,'' and which means that your score will be more effective at setting you apart from the average, which is of course often desired, as not many of us average on all points. (8:38)

.\,.\,Perfect.\,:) (8:42)

%(9:59) Considering storing props and contexts at the DOM node. But what about dispatch(), i.e. if we want reducers to be able to trigger ancestor reductions..? ..I could just send a.. ..I could just let dispatch pass a self-dispatcher. Okay. So I'll store state and context data at the DOM node.:) ..But what about reducers..:\.. ...I could also make an auxStore with nonces as DOM node keys.. ..Yeah.. (10:27)

%(15:01) I think I might use a convention instead for classes where we capitalize every "important" word (nouns and adjectives at least, and also the first word, of course)..

%Ah, you could also pass useRef() refs containing dispatchers as the current property in order to pass the parent/ancestor dispatchers.. (15:45) ..But then we have to wrap the returns in that, so I kinda like my solution better..

%(18:12) I seem to be so close, but for some reason, EntityTitle dispatch event repeats four times, and doesn't bubble to the ancestors.. Now, I should actually also use Map() for the auxDataStore, and use the nodes directly as keys.. So let me do that (tomorrow, probably), and then hopefully I will also catch the bug somehow.. (18:16) ..And then an alternative to using event is to just look up the DOM tree for parent nodes with entries in the auxStore (Map() of nodes), and then call the reducer (using the stored setState) from there.. (18:21)

%(18:23) Oh, I have to remember that tags can also be nouns, so I should implement noun predicates as part of my template as well.. ..Hm, maybe I want lower-case after all for the classes, btw.. (8:24) ..And I should keep in mind that tag predicates can also be e.g. 'funky rythms' or something like that, where there is an implicit 'contains' in the front, rather than an implicit 'is'. (8:26)

%(26.09.24, 11:42) Nå, jeg er stået meget sent op i dag, men jeg lå også vågen en del i nat, hvor jeg så til gengæld udregnede, hvad jeg vil gøre med min useDispatch(). Jeg overvejede at bruge contexts i stedet. Men jeg kan virkeligt godt lide at reducerne får DOM-noden automatisk, og jeg kan endda gøre det på en måde, hvor disse noder ikke fastholdes i hukommelsen. ...Hm, I thought I could easily select all ancestors with plain old JS.. But maybe I want to use event bubbling anyway, and specifically to use keys, now passed to useDipatch() along with reducers.. (12:16)

%(15:03) Hm, mens jeg er på Fyn (og på vej til of fra) kan jeg researche lidt, hvordan jeg tegner kurver i CSS + JS.. ..Jeg behøver forresten ikke et søgefelt i den første prototype (når man ser bort fra den for et år siden), for jeg kan bare bruge 'semantisk søgning' til eget behov.. ..(Og muligheden for at gå til et entID via URL'en.) ..Så Relevant predicates, vægt-filtring a klasseset, Arguments (med derived score og impact som en virtual relation), og dermed også Comments, for der er behov for comments som statements. Og så sevfølgelig login, hvor jeg vil lave en kap til lynhurtigt at lave en anonym bruger, hvor e-mailen kan vente. Og er det så ikke nærmest "bare" det, jeg behøver for at kunne vise idéen frem..? (15:10) ..Hm, jeg behøver heller ikke at implementere 'better duplicate' eller 'better representation' til denne første prototype.. ..Nå ja, og der skal også være nogle specially insert-muligheder i det mindste, nemlig så man kan uploade tags (prædikater) og relations, og selvfølgelig også tekst-statements, samt ikke mindst kommentarer.. (15:13) ..Hm, rart at jeg kun skal lave særlige inserts til at starte med. Så sparer jeg i hvert fald en smule arbejde der (inden den er klar til visning).. ..Og jeg kan også bare forklare om browser-udvidelsen.. ..Nå ja, og jeg skal også lige lave et par bots.. ..Hm, hvordan skal man query'e for de samlede kurver?.. (15:19) ..Det skal jeg lige finde ud af.. Men jeg bør jo overveje, om jeg ikke skal skære nogle hjørner med agreement-kurve systemet---eller er det for centralt for systemet..?.. (15:23) ..Hm, jeg kan godt vente med at vise de totale kurver, således at brugerne kun ser guide-(Gauss-)kurven, og altså ikke ser noget aggregeret data om, hvad andre brugere har scoret før en selv. (15:25) ..For det er stadig en fin pointe, at agreement-kurven kan lokke flere brugere til at afgive scorer, og altså skabe en bedre UX også omkring at afgive scorer. (15:27) ..Ja, fint, så jeg skal altså bare have, i hvert fald, en bot der finder type-scoren, men ellers ikke behøver at agregere data mere end det for nu---ja, borset fra at man godt lave kunne opgive en kurve-bredde (som score_err) for de samlede scorer, også selvom botten altså for nu.. måske.. smider disse kurver ud efter hver aggregering.. (15:29) ..Ja, og her taler vi dog om den samlede kurve for én skala, hvorimod jeg før snakkede om den samlede kuve for ét \emph{predikat}, hvor hvert datapunkt så er en instans type-score af den pågældende klasse, i modsætning til i det andet tilfælde, hvor hvert datapunkt er en brugers score for den specifikke skala. (15:33) ..Og vi behøver altså ikke en bot der udregner den samlede score-kurve for en klasse til at starte med.

%(15:40) Hm, skulle vi lave en konvention om at man må have en parentes forrest i sit @[Predicate] i "tag"-prædikat-skabelonen, hvorved denne parentes så kun printes for den fulde titel?.. Lyder da meget nice, ummidlbart. Så kan man nemlig skirve f.eks. 'is a', 'is an' eller 'has' (eller 'contains') foran sine navneords-tag-prædikater..:).. (15:43) ...(15:59) Så, nu har jeg skrevet: "
%    "@[Predicate] should either be a (compound) adjective or ",
%    "a (compound) verb. However, by writing e.g. 'is a'/'is an', or ",
%    "'has'/'contains' in a parenthesis at the beginning of @[Predicate], ",
%    "the app can cut away this parenthesis when rendering the title of ",
%    "the entity in most cases. For instance, you might write '(is a) ",
%    "sci-fi movie' as @[Predicate], which can then be rendered as ",
%    "'sci-fi movie,' since the 'is a' part will generally be implicitly ",
%    "understood anyway. Or you might write '(has) good acting', which ",
%    "can then be rendered simply as 'good acting.' And as a last ",
%    "example, one could also write '(contains) spoilers' as ",
%    "@[Predicate], which can then be rendered simply as 'spoilers.'"
%"

%... (17:21) Jeg er jo lidt flad i dag. Kunne godt have forsat med EntityDataDisplay, men nu valgte jeg jo lige at tænke lidt i stedet. Og nu her, efter at være kommet hjem fra lille gåtur, er jeg så heldigvis kommet på nogle spirende idéer, så der er altså vistnok nogle idéer på vej... ..Hm, Actions som en tab til alle likelihood scales.. ..Og Actions omkring appen og netværket selv.. ..Og så Actions til Issues.. Hm.. (17:28) ..Hm, Actions kunne altid være under Issues, og så har vi altså bare en.. derived Issue for all liklihood scales, nemlig 'Uncertainty about the question'.. (17:33) ..Og med 'Actions' mener jeg altså rettere sagt 'Action proposals'.. ..Og vi kan godt bare kalde dem 'Proposals' for short.. ..Ja, det er altså godt, det her. Det kom sig i øvrigt af, at jeg lige tænkte kort på, hvordan mon min app-idé.. Jeg kan ingen gang huske, hvad jeg kaldte den, men mener den var fra slut 2022.. ..Ja, klart. Men ja, den der hvor man kunne diskutere og forhandle om løsninger. Og så kom jeg jo hurtigt til at tænke på 'løsninger på at opklare spørgsmål' og også 'issues/action omkring selve appen/netværket'.. Men ja, så lad mig spørge mig selv: hvad mangler vi så ellers for at få min app, der? En stor ting var jo at man kunne angive conditional støtte til et proposal, og altså så dermed forhandle om løsninger/actions.. (17:38) ..Og der kunne jeg jo selvfølgelig have en support score for Actions, og så mangler jeg bare den konditionelle.. (17:39) ..Og det må jo så være 'support conditions,' som så altså er en relation, og hvor scoren til disse conditions så er en 'hvis vi vi tager denne action, så vil jeg godt gå med på den pågældende relations-objekt-action'-score.. Hm.. ..Hm, det må næsten være en gensidig ting, hvor brugere på den ene side op-rater Action A som en condition for Action B, og hvor brugere på den anden side så kan op-rate Action B som en condition for Action A.. Hm, mon dog dette virker uden brugergrupper; mon ikke det skal være klart at betingelsen altid er at brugergruppe x også går med på Proposal A, i.e. så vil brugergruppe y også godt gå med til Proposal B.. (17:46) ..(17:48) Jo, det kan godt være. Og dermed er conditional support altså jo nok noget, som først kan komme op og køre på en meningsfuld måde, når først vi er godt i gang med brugergrupperne. (Og her taler vi altså ikke om ML-PC-brugergrupper, men mere om samfundsgrupper, eller bare grupper med en delt holdning (som så dermed faktisk godt kan fremkomme som en ML-gnereret brugergruppe, i hvert fald i princippet).) Men vi kan jo dog stadig godt bare have support i almindelighed for Proposals. Så det giver stadig god mening at indføre Issues og Proposals selv på et rimeligt tidligt stadie..:) (17:52)
%..Hm, i øvrigt bør man også kunne op-rate del-planer under Proposals (det jeg også har kaldt Actions), således at en altså Proposal kan være formuleret overordnet, og hvor man så kan op-rate (del-)planer til en Proposal, som så både kan precisere den praktiske fortolkning a Proposal'en, om hvad hjemlen er for, hvad der skal opnås, og også ikke mindst planer for \emph{hvordan} man skal opnå målet, og hvor disse planer altså kan deles op i modulære enheder, således at man f.eks. kan sige: 'for at udføre Proposal x, så skal vi udføre Plan A, B, C, D, ..., og muligvis Plan I, J, K,' hvor jeg altså her med 'muligvis' mener, at hver af disse planer jo er scoret med en support-score også, og hvor den ikke-så-højt-ratede planer så ryger under en 'måske' kategori (og hvor de lavt ratede---eller den som har korrelationer med højere ratede planer---så ryger i en 'ikke' kategori). Og ja, som jeg nævnte, så bør Planer jo altså også have en indbyrdes korrelations-score, hvor man så fra-/ned-filterer planer, hvor kommer efter "korrelerede" planer.. Og ja, det er jo pludeselig en masse, men i starten kan vi jo eventuelt bare nøjes med kun at votere om én plan pr. Proposal, hvis det gør det nemmere. (18:01)
%..Nå ja, jeg kaldte den vist min 'e-demokrati-app(-idé)'.. (18:02)

%(18:15) Men ja, Issues og Proposals bør dog bestemt ikke være noget jeg skal have med til den første prototype, det er klart. Til gengæld burde jeg måske have det her med at Folk kan oprate scales som gode til at lave ML på. Så måske skal jeg også bare implenetere ML.. tja, og dog: Jeg kan sagtens lave listerne, som laves ved at op-rate specielt fremstillede prædikater, som beskriver, hvad skalaerne skal være sigende for, og så selvfølgelig også op-rate skalaer (statements, kalder jeg det jo egentligt nu..) for disse prædikater. Og så kan min ML-bot altså tage de højest ratede prædikater, og bruge de højest rated statements for diss prædikater til så at lave ML-PC-brugergrupper heraf..

%(18:25) Lad mig også vente en lille omgang med derived score for Arguments-tab'en, og ikke mindst så også med de indbyrdes korrelationer, og den filtrering/om-scoring der bør følge med på et tidspunkt. ..Jeg vil dog stadig gerne lave min derived 'impact'-score.. (18:27) ..Hm, og jeg kan også godt vente med at filtrere grading scale-relationer ud fra den tilsvarende faktuelle relation. (18:29)

%(18:32) Hm, og er der ikke en måde, hvorpå man kan komme udenom brugergrupperne med det her proposal conditions?.. ..Tjo, men nah, for det giver kun mening at forhandle med nogen, der har en magt, og derfor kommer man altså ikke uden om brugergrupperne. Så det må altså rigtignok vente..

%(27.09.24, 8:26) I think mybe the proposal conditions should only be used as an inspiration for the bargaining, and maybe we don't even need them at all, as maybe simply voting one's support of various proposals will be enough as the input for the subsequent bargaining: then people can see which groups doesn't support a proposal very much, and you can see what proposals they in turn support/want more than your group, which means that you might approach a bargain where you conditionally support their wanted proposal.
%I'm going to Fyn now, so I'll be taking notes on my phone instead.. I just want to mention something about each Proposal having a list of Issues that can be uprated for it, and then you can up-rate proposals for that, such that a combined plan goes Issues -> Proposal list -> Issue list -> Proposal list, etc., all the way down to leaf Proposals (or issues to which no solution has been found yet). (8:33) ..Yeah, and then when you finally vote for a Plan, then that is choosing a the whole tree with Propsals at all leaf nodes.. (8:34) ..And so you actually rate support/want for the Plans, at the end---although you could also vote support for the individual Proposals along the way (which are formulated as texts (that are not necessarily full of all details)). But yeah, it will probably be these Plans that you are voting and bargaining for at the end. And then.. Oh, I got to go.. (8:37)


%Kopieret fra telefon:
%"
%.. and then while the plan is being executed, some new hurdles might arrive, but then you can react by voting for new plans under the same tree, and try to quickly vote for these changes..
%
%Issues skal nok hovedsageligt være udformet som desires. Men Issues skal også have et time stamp, og kan også indeholde nogle beskrivelser af de hurdles, der forventes at være. Men nu tænker jeg også, at hurdles skal være en dynamisk liste også, som man så voterer om. Så Issues -> Hurdles -> Proposals..? (9:24, 27.09.24)
%..Hm, I think that makes sense..
%
%(10:37) Man kunne også lade.. Proposals holde deres egen Issue, så det kunne gå mere som Proposal/Issue -> Hurdles (eller problems/tasks/issues..) -> proposals/issues.. Hm..
%..Hm, Desire -> Hurdles -> Proposals..
%.. -> Hurdles -> Proposals, osv., indtil proposals'ne bliver simple nok.. ..Og Hurdles kan så holde deres Desire, som kan være afledt af Proposals (a desire to undertake this proposal), og Hurdles kan i princippet have flere Proposals som forældre (altså en semantisk relation). (10:56)
%..Og ja, en Plan er så, at man udtager et helt træ af Proposals (men hvor man stadig kan bruge det "levende" træ, både ved at skæve til, om der nu skulle komme bedre løsningsforslag (proposals), imens man arbejder, som man måske kan nå at bruge, og/eller bruge det til at rette planen, hvis nu der opstår uforudsete hurdles. (11:00)
%
%(11:13) Og diskussionerne kommer jo bare så i reglen under Arguments til proposals'nes support-skala/statement. 
%
%(13:27) Desires skal nok be være statement, og så bare udformet som f.eks. 'vi burde gøre sådan og sådan' eller 'det her trænger til at blive gjort' osv. ..Tja, måske, men så skal det nok være en underklasse af statements, det så evt. kan hedde desires alligevel..
%
%(29.09.24, 20:51) Problem -> Proposals -> Problems -> etc.
%"

%(30.09.24, 9:14) Problems -> Proposals -> Problems -> etc. virker nice. Og så skal Problems i øvrigt også bare gerne kunne være dynamiske.. måske.. Men man kunne godt have f.eks. maintenance problems, som så kan være vedværende.. Ja, det må vi have..

%Jeg har nogle vigtigere (alt efter hvordan man ser det) idéer omring entity titles også. Nå ja, og den helt store idé er, at jeg skal gå væk fra at have relationer i almindelighed som en compound relation, og tilbage til at have én samlet rating, som både fortæller om sandheden i den samt om 'usefulness.' Og der er den simple iagttagelse at, jamen, hvis en ting ikke har den pågældende relation til objektet, jamen så er den heller ikke særlig useful at se på listen. Så vi behøver bare usefulness i langt de fleste tilfælde.:) ..Og det gør ikke noget at listerne bliver lange, for vi sørger jo bare for ikke at query mere en til D-ratingen, aller højest, medmindre brugeren beder specifikt om det. Og ja, jeg tror jeg vil beholde.. Nå ja, beholde F--A på aksen, \emph{selvom} jeg altså har overvejet at bruge en anden skala: en relevancy scale.. Men måske går det faktisk fint med grading scale. For selvom jeg gerne \emph{vil} vise den der Gauss-kurve i baggrunden, så skal de siges, at det slet ikke er altid at man forventer, at entiteterne skal følge den kurve nødvendigvis (især ikke når der er nogle store spring i, om en ting har en kvalitet eller ej (f.eks. SSD, eller whatever), som gør meget for værdien). (9:28) ..Men kurven kan godt alligevel være guidene, selv i sådanne tilfælde, hvis man altså bare er klar over, at den kun er en vag guide. ..Og ja, måske behøver jeg så bare grading skalaen, og ikke en separat 'relevancy' skala. ..'relevancy/usefulness/importance'.. (9:32) ..Ja, så prædikatet er altså i virkeligheden, hvor relevant, og lad mig bare bruge det term, entiteten er for relationen subj. + rel. + obj.

%(9:35) Jeg skal også sige, jeg har fundet på at omkapsle prædikater i en rektangel med runde hjørner for grading-prædikater, og med skarpe hjørner for likelihood-skala-prædikater, selvom jeg jo så ikke får brugt dem særlig ofte nu, andet end når vi kommer i gang med diskussionerne. Relationer renderes med klassen i parentes, efterfulgt af en pil, og så navneordet. Og når objektet indsættes, så erstatter dette så selvfølgelig parentesen, og får så pågældende klasse som klasse-kontekst i stedet. I øvrigt vil jeg begynde at bruge plurale ord som klasser.:) ..Først når en relation gives et objekt, og bliver et prædikat, vil jeg så tegne den rektangulære ramme (for det meste med runde hjørner, fordi vi jo oftest taler om grading-skalaen). Og når et prædikat gives et subjekt, så vil jeg bare rendere det som 'subjekt : prædikat', hvor prædikatet så stadig får sin ramme omkring sig. Og subjektet får så også subjektklassen som kontekst-klasse, det er klart. (Men jeg lader altså bare subjektklassen være implicit når prædikater renderes, både for sig selv og i statements. For i det mindste kan man jo indirekte se subjektklassen i statements, fordi denne renderes hvis og kun hvis subjektet har en anden hovedklasse.) (9:44)
%..Og jeg vil generalt bruge lower-case-ord, og kun bruge store begyndelsesbogstaver for proper nouns. ..Egennavn, nå ja.

%(10:03) Alternativt kunne jeg vise subjektklassen, også i en parentes på subjektets plads (i 'subjekt : prædikat'), kun når klassen ikke er den nuværende kontekst-klasse.. ..Ja.:) (10:05) 

%..About the plural words for classes, nah, I need singular words for classes.. (10:08)

%(10:26) Hm, i stedet for at poppe en entity data menu ud, når man trykker på entiteten, så kan det da egentligt være, at jeg hellere bare vil åbne en ny kolonne, og så bare hvor man måske specificerer "info" som et ekstra argument til kolonnen, således at man ved at man skal åbne info-tab'en først.. ..som i øvrigt også kan være default-tab'en..

%Jeg skal i øvrigt finde ud af, hvordan jeg renderer klasser; jeg må gøre noget specialt der også.. (10:30)

%..Og selv for lange entitetsnavne, som man cutter off, så åbner vi stadig bare en ny kolonne, hvor man så kan se hele titlen i overskriften, samt se den resterende data, og den "fulde titel," i (data-)info-fanen.. (10:32)

%..Problemet er så, at så bliver det til gengæld vigtigt med en god funktionalitet omkring søjlerne, hvorvidt de bør erstattes eller ej, når en ny søjle åbnes til ventre for.. ..Hm, kunne man ikke bare have knapper i stedet til at rykke søjlen til ventre eller højre.. Hm.. (10:39) ...(11:00) Jeg behøver bare en lille søjle-header-fane i toppen, hvor der er knapper i, og hvis bare man trykker på denne, så ændres skriften (entity title gentaget i mindre format) fra kursiv til ret, og så overskrives søjlen ikke. ..Eller hvis man klikker på en anden fane, eller bare et eller andet sted i søjlen. Ja, lad mig bare sige det. Så behøver jeg kun en close-knap for nu. (10:03)

%(14:29) Hm, har jeg overtænkt mine defProps.. Jeg plukker jo altid bare værdierne ud alligevel.. ..Tja, jo, måske, for det bærer vist lidt præg af, at jeg ikke altid har en beskrivelse til entiteterne, og at man derfor skal kunne sige meget på lidt plads, men måske er dette altså slet ikke nødvendigt.. Måske skal man bare mere eller mindre have sine hoved-attributter plus en beskrivelse af, hvordan entiteten med disse attributter skal fortolkes..
%..Ja, det er faktisk muligvis overkill, alt det jeg gør.. ..Ikke at jeg tror, jeg skal ændre i databasen.. tja, måske på nær otherProps, som måske ikke vil være særlig gavnlig alligevel, men ellers.. (14:38) ..Tja, og måske er det også overkill med input-teksterne, for disse kan jo også bare indgå i beskrivelsen.. ..Hm, alt data som applikationen skal bruge til titles, tabs, column, scales, osv. er jo bare klasse-id'et og mainProps, og det er også ligesom alt hvad disse er til for.. Og beskrivelsen kan jo ligesom opklare resten.. (14:46)
%..Hm, så et template kunne egentligt bare være en liste af property names, og så en delt instans-beskrivelse, men hvor hver instans i øvrigt også måske burde kunne tilføjes sin egen særlige beskrivelse.. (14:50)
%..(14:52) Hm, og måske skulle jeg bare lave et React-modul for hver klasse til titel-rendering, i stedet for at definere det hele via CSS..
%..Hm, jeg kan bare overloade betydningen af instance description of own description, så det for ikke-templates betyder først den delte description, og den extra description (hvor own description for templates er emph{hele} beskrivelsen)..

%(14:57) Jo, har har overtænkt (men af en god grund i det mindste; inst_desc er hvad virkelig ændrede billedet..) entity data, for brugerne skal nærmest kun se beskrivelsen/erne, og så se attributterne, som allesammen også nok bare skal være \emph{enten} strenge \emph{eller} entiteter, og altså ikke strenge med referencer i.. Ja, og dog, for.. Hm, skal vi så have titel-modulerne til at definere de sammensatte properties? Hvordan skal dette så beskrives i descriptions, hvis det overhovedet skal det?.. ..Tja, det er jo nemt. Man bruger bare mine "eksplicitte referencer" ('@[...]'), og så indleder man bare den sammensatte property, evt. bare ved at sige '<my_property>: <sammensat beskrivelse/definition>', a la hvad man ville gøre i en struct. (15:03)

%..(15:05) Jeg fik i øvrigt aldrig sagt: Jeg bør også gøre '<' og '>' specielle fra starten af, så vi er klar til at implementere noget html/xml markup. Og her vil jeg i øvrigt bare starte med at implementere kursiv og bold skrift som noget det eneste, og bare sige at tags ikke må næstes, altså medmindre man vil have et ikke-endnu-implementeret rendering-resultat..

%(15:08) Lige før vi ikke behøver templates, hvis vi bare lige kan få de delte (instance) descriptions.. ..Ikke at jeg har et stort behov for at fjerne templates, men alligevel.. ..Ja, for der er ingen rigtig grund til at spare de bytes som går til at danne property names; dem kan vi sagtens bare kopiere for hver gang, for det gode kunne så også være at full-tekst-søgning så herved bliver nemmere, og så vil det være pladsen værd.. ..Tja, og en anden idé kunne være at skrive dem.. Nej, never mind.. ..Hm, vi kunne omtænke (muligvis) templates som "prototyper" ligesom i JS.. ..Hvor det så bare gælder, at når man "overskriver" beskrivelsen, så tilføjer man den bare i forlængelse af i stedet.. ..Og så kan Entities altså bare være class, prototype, attributes, description, og binary data, hvor attributes så også bare er et JSON-objekt, selvfølgelig.. (15:18) ..Men kan vi så søge på.. ja, på f.eks. compound predicates and statements, det kan vi, så længe attributes JSON'en bare ikke er super lang, hvad den aldrig bør være.. (15:20) ..Men hvad er idéen med prototypes, hvis man ikke kopierer attributes.. Nå jo, men det kan man også godt, sådan set. Men ikke alle prototypes/templates har så egne attributter; mange har bare en beskrivelse.. (15:22)

%..Hm, man kunne faktisk hashe attributes, og så søge via hashet i stedet. For jeg tror ikke, jeg for brug for searches, hvor man ikke kender alle attrbutterne allgivel, på nær hvor vi så bruger full-text indexes, eller på sigt de bruger-/bot-administrerede indexes. (15:25)

%..(15:26) Hm, kunne man bare droppe temlates/prototypes, og så holde et table over descriptions i stedet, hvor hashet er primær-nøglen? (15:26) ..Det lyder jo faktisk rigtig nice.. ..For jeg behøver vist ikke rigtigt attributter defineret af prototypen/skabelonen.. (15:28) ..Det gør jeg..! Og så kan man i princippet bare copy-paste den delte beskrivelse, når man laver en ny entitet, men man bør også kunne vælge at referere til hashet i stedet, tror jeg. (15:30)

%..Ha, en næsten en smule vemodigt at skulle sige farvel til templates igen.. (15:31) ..Bye templates..

%..Så en shared description og en own description.. (15:33)

%(15:34) Uden templates giver det ikke mening at bruge '@123c456'- eller '...@c123'-syntaks i attributes.. Men derfor kan det jo stadig gøre det i descriptions.. ..Men vi har ikke længere nogen placeholders.. ..Kun referencer.. ..Hm, vi bør jo bruge eksplicitte referencer til at referere til attributes, så det faktisk nok ikke mening nok at have '@123c456'- og '...@c123'-syntakserne.. ..Så lad mig bare droppe dem.. ..I stedet bestemmes class contexts så af Reat-modulet (som kan bruge atrributerne hertil), foruden af titlens kontekst selvfølgelig.. (15:39) ..Men de andre keyword-referencer (såsom '@this' og '@null') kan jeg godt sagtens bare beholde, why not..:) (15:42)

%..Ha, nu hvor entiteters rendering også styres af React-klasser, så kommer får jeg slet ikke brug for min DataFetcher-klasse, for nu skal der bare query'es én gang for alt main data, og én eller to gange mere for at få de 1--2 descriptions. (15:46)

%(17:06) Hm, maybe we should just be able to have as many descriptions as we would like.. ..Ah, or at least the maximum unique index key length divided by 56, and then I could just make one text data column with the hashes of all the descriptions.. ..Ah, which can be handed to the app in a query, along with classID and attributes (the text).

%(17:17) Ah, the "descriptions" can also still have text references, i.e. e.g. '@text3', which then references the 3rd "description," which I then might call simply 'texts' instead.. ..Or should I not do this..? (17:20) ..Ah, I should make a separate column, then, for texts, I guess.. ..Well, for \emph{a} text, let's say.. (17:23) ..Which could just be combined with the binary_data column. I think I should do that. And then we'll just implement a '@text' reference, similar to the '@bin' one, which the descriptions can then use to refer to the text. And the app can then also use this text directly, without having to fit and parse this off of the attribute struct. (17:26)

%(17:43) Hm, eller skal vi hellere sige, at vi bare har en række texts (eller binary strings), hvor eneste konvention så bare er.. Nå nej, vi behøver ingen gang nødvendigvis en konvention om at første text altid hentes som den eneste, for det kan EntityTitles (etc.) godt selv ændre, nu.. Men for ikke-kendte klasser, der henter vi bare den første tekst kun, og antager, at den er et JSON-objekt.. (17:46) ..Så jeg kan udskrifte attr_hash, desc_hashes og data_hash med bare data_hashes i stedet..(?) (17:47) ..Nah, vi tager en konvention om at første tekst altid fetches, for så vil jeg nemlig lade den normale.. Nå nej, der kan jo bare være flere query procs. Ok, hm.. (17:48)

%(17:49) Ah, default-titlerne bør i øvrigt også stadig printe alle properties i text1, hvis dette er et JSON-objekt, og ikke bare 'title,' for så kan brugere namlig stadig i princippet inføre deres egen rendering via bruger-sheets. (17:50)

%..Alle description texts kan i øvrigt bare indledes med.. <h1>Description</h1>.. Hm, på nær at man så får en masse gentagende headers.. (17:52) ..Well, jeg kunne jo bare indsætte tal i alle gentagne headere efterfølgende.. (17:54)

%(18:01) Hov, jeg skal bruge SHA2(_, 256) i stedet, for den har JS også..

%(18:05) Ah, man bør jo egentligt vende det om, så shared descriptions kommer først, og så kommer JSON-objekterne til sidst.. Og disse descriptions kan så referere frem til de andre tekster via '@text<n>'. Og dermed behøver man ikke en header som sådan, fordi konteksten for teksten i princippet altid er defineret på forhånd. ..But then again, we want to query the first text at defualt..
%(18:10) Okay, attributes skal også kunne have '@bin' og '@text<n>' som mulige values, udover entity-referencer of rene strenge.. ..Og jeg beholder attributes som en separat kolonne. ..Jeg bør nok også beholde descriptions. Og så mangler jeg bare texts og binaries, som dog godt kan være én kolonne.. Hm.. (18:13) ..Hm, lad mig bare beholde det som attr_hash, desc_hashes og data_hash.. (18:15) ..Og så har vi bare '@data' keyword'et som det eneste.. (18:16)

%(01.10.24, 9:15) Hvis vi på et tidspunkt lader botter normalisere grading skalaer, så skal det dog være lidt længere ude i fremtiden; ikke noget vi skal bekymre os om i starten.

%(9:59) Hm, vi skal "kun" op på 4 milliarder entities før at vi begynder at få hash kollisioner, så jeg skal enten finde en måde at handle det, eller jeg skal gå op i hash bits.. ..Hov, do'h, det er jo 256 bits, ikke 64, never mind. (10:05)

%(10:12) Hm, burde man kunne referere til noget data via dets SHA2-256-hash i stedet, altså f.eks. ved at skrive '@2a85b7a91afaf6a54e1ae232d32a0d1505987a64b05c7da6f74c94fe8113f8df'.. ..Måske ikke.. ..Nej, man refererer til tekst-entiteten så i stedet, det er klart. (10:25)

%(11:20) Hm, should I keep the c123 syntax, or should I actually drop the ClassClarification..? ..I think I might do the latter.. (11:22) ..Yeah, I think so.. ..Yes, we can just implement it for specific classes instead, if we ever get a need for that. ..And then we might even reintroduce the syntax again, if we really want to. But for most classes, the class of the given entity can probably be implicit in most cases. So let's remove the ClassClarification and the c123 syntax for now.. (11:26)

%(11:38) Okay, det virker egentligt fornuftigt, det jeg har gang i, men der bliver nu alligvel lidt clashes nu imellem classes og descriptions. Kunne man ikke samle disse?.. ..Hm, ikke helt, jo, men mon ikke man kan gøre noget alligevel.. ..Tja, tjo, måske kunne man godt samle det.. (11:41) ..Så skulle man bare have en klasse-liste i stedet for en descHashes-liste, og så stadig beholde attrsHash, hvor man altså har hele attrs structen i én.. (11:42) ..Og klasse-listerne bør så udelukkende være lister, hvor hvert nyt element er en subclass af den forrige klasse.. ..Så slette desc_hashes og erstatte class_id med en class_list.. (11:46) ..Hm, skal klasse descriptions'ne så bare holdes i dataHash?.. (11:47) ..Eller skal jeg genindføre inst_desc..? (11:48) ..Og own_desc bør så også komme med igen.. (11:50)
%..(11:56) Hm, jeg kunne da egentligt også godt bare gøre descriptions til en del af JSON-attributerne.. ..(11:58) Wow, man kunne næsten reducere Entities til bare attrs_hash, then.. ..Jeg vil jo egentligt gerne kunne tilgå f.eks. klasserne i MySQL-procedurerne, men mon ikke man kan komme uden om det, hvis det endelig er.. ..Og jeg har stadig ikke lyst til at hente descriptions, når jeg skal rendere titlerne.. (12:01) ..Men det behøver jeg jo så til gengæld heller ikke, når de er klasser i stedet.. (12:02) ..Ja, så vi \emph{kan} faktisk måske godt bare have attrs som den eneste ting, der definerer entiteter..(!..) (12:07)

%...(12:18) Lad os beholde klasserne som en separat liste (kan i øvrigt bare være en decimal interger-liste..).. Hm, eller skal vi lave det til en liste af CHAR(16)- eller BINARY(8)-strenge?.. ..Eller skal jeg udtrække en hoved-klasse.. ..Vi får jo alligvel hele attrs med, når vi query'er, så jeg skal ikke bekymre mig om appen.. ..Og hvad med bots'ne.. Disse kunne jo bare styres i control-laget.. ..Ah, men MySQL har også REGEXP, så bots'ne kan også godt aflæse, om en entitet har en vis klasse eller ej. (12:30) ..Jamen så bør vi jo bare gøre Entities ækvivalent med attrs-structen..:) (12:31) ..Og så er den underlæggende fortolkning bare, at når en entitet har en klasse, så kigger man på den klasses description, og tilføjer den til fortolkningen af entiteten. (12:33)

%..Og så er det bare lige klasse-kategori-dualiteten, der eventuelt kan komme til at spøge lidt, nemlig hvorvidt en klasse skal bruges som en kategori eller bare et template med andre ord.. (12:34) ..Men jeg tror bare vi må sige, at nogle klasser kan bruges som kategories, imens andre ikke gør.. ..Uh, kunne man lave en form for "anonyme" klasser? (12:36) ..Uh, måske hvor vi laver en superclass placeholder, som så kan bruges i klassens titel, f.eks. til at sige: "%sc with <my_attribute>".. (12:38) ..Nah, for kompliceret, ift. hvad vi får for pengene.. (12:39) ..Måske kan klasser også i langt de fleste tilfælde bruges som kategorier, når vi bare husker, at attributes gerne må være missing/null/undefined. For så kan de meget brugte klasser bare sørge for at dække sig godt ind med mulige attributter til at identificere/definere instanserne.. (12:41)

(01.10.24, 12:52) I've actually written a lot of important notes out in the source comments above this paragraph. I won't copy them, so go read them there instead. A lot of them are in Danish, even the important ones, sorry.\,. .\,.\,They're partly about Problems and Proposals, about not using compound scales for relations, about entity title (and entity data) rendering, and also partly about how I will restructure the database once again. (12:56)
*There are also important notes in the comments below this paragraph.

%(13:02) Oh, do I remove the data_hash? Maybe I should.. ..Well, but then there's the issue that I don't want to fetch the data always. So can't we just make a data_attrs column.. ..Or simply keep data_hash.. ..Okay, let me keep it as is. Yeah, and then we can also use it for the class descriptions, actually! (13:05)

%(13:18) Damn, I already feel like I lack the extra texts.. What to do..? ..(It's for the inst_desc..) (13:19)

%(13:24) Hm, vi behøver jo ikke nødvendigvis at begrænse os til JSON-objekter; vi kan jo også have (JSON-)string-entiteter.. ..Og så kan vi bare bruge entitets-referencer i stedet for '@data'/'@d' osv..:) (13:26) ..Hm, og så kunne jeg faktisk eventuelt have en liste af 'descriptions' i stedet for klasse-listen.. Og så er vi også sikker på at undgå klasse--template-dualitetsproblemet.. Så vil 'classes' og 'descriptions' være to meget hyppige attributter, man ser i starten af JSON-objektet..

%(13:32) Lyder rigtig nice. Og så kan jeg kalde attrs-kolonnen for json_value i stedet.. ..Og så får vi bare json_value samt json_hash som de eneste kolloner (hvor hashet afhænger af json_value, self.).

%(13:41) Så sekundær-nøglen bliver jo bare json_hash, og så er det jo lige før, at jeg bare indføre @<json_hash> som en slags midlertidige referencer, der bliver oversat til primær-nøgle-referencer under indsættelsen.. Og dette kan jeg jo bare implementere i JS.. (13:42) Så lad mig det..:)

%... (15:01) I'll just make pseudo-references instead that references objects in the current batch of insertions.

%... (17:38) The bot(s) that used by default for the relational entity lists shuould first of all cut off (and not score) all entities with a score of F or lower, and should also cut off entities that haven't been scored by enough people. But then the users should of course also be able to change to at least one other bot without these cutoffs.

%..(17:45) Instead of using 'instance descriptions' for classes, let us just instead simply display an entity's class' descriptions underneath its own descriptions. And then these class' descriptions can just speak about what potential attributes that its instances might have, where this is then not needed to be repeated in the instances own specific descriptions (if any).

%(02.10.24, 9:41) My "explicit references" should be called "attribute references" instead, and let us just render them with a monospace font rather than capitalizing.
%..Hm, maybe I should use XML instead of the '@[syntax]'.. ..Then I can just define my own <attr> XML, and I could also btw define a <class> tag, which I will then also render using a monospace font myself (but this can then be changed in the future, and adjusted according to user preference). (9:47)

%(10:08) Maybe I will just have one 'documentation' instead of 'descriptions,' and then I'll use <h1--h6> headers inside the documentation.. ..(yes 'documentation'..) which I will then all make expandable on the info page, including the outer <h1> one.. ..And I will allow tags inside the <h1--h6> tags (but otherwise I'm gonna limit nesting quite severely in the beginning).. (10:13) ..Hm, then I should also include the paragraph tag.. Nah.. Hm.. ..Hm, I guess I ought to, such that my XML will be HTML-like. (If you include a part of HTML, then it's only expected to continue in that direction.) But I don't care to look up if <attr> or <class> matches any HTML tags; we'll never need to go \emph{that} far down the line of making the markup HTML-like. (10:18)

%... (12:02) '@this' in text entities (when def_str is a JSON string) should refer to the entity that references the text.. I think..
%I will make a <flags> tag to use e.g. in the beginning of the 'Special attriutes' section.
%I will make a selectFirstEntity (on an entity list) proc, and give it a nullable defaultEntID input, where it selects this defualt entity instead if no entity is on the list (for the provided query user). I can then use this proc for always selecting the best representation of an entity, instead of just selecting the entity itself.

%...(12:20) Hm, if I really want to be able to link to any entity in texts and attributes, even future ones, then I could introduce a system, where the username of the submitting user is hashed together with a user-chosen key.. Well, maybe never mind about the hashing, actually; we can just use keys of.. Ah, user \emph{IDs}, follwed by a key. Great.. (12:33) ..And do we want this enough to do it, I guess so.. ..Hm, then technically, my EntityInserter don't need to.. ..Hm, we should just make this the standard kinds of entity references, should we?.. I mean, they are not much longer in size than using the primary keys.. Of course, it's a bit slower.. Hm.. (12:27) ..Well, in order for it to make sense, the keys ought to be words/identifiers, so I guess we'd rather still use the primary keys for references as a standard.. ..Except that having this remapping actually also has the benefit that it will be easy then to remap the entity IDs..! (12:31) ..So the 'address' of an entity could be the user.. Hm, the username, then, if we want to be able to remap entities..? (12:33) ..I guess so.. ..Well, I could also just make a user IDs something separate, and something that shouldn't change.. Ah, and they can get a prefix of the specific database once we start making it distributed.. ..Hm.. (12:36) ..So an adress could be: an optional prefix, a userID, which is not the user entity ID necessarily.., and then an identifier that's unique for the user.. ..And then entity (PK) IDs could be completely invisible to the users.. (12:39) ..And selectEntity should then return either the address or the entID, depending on which of these keys you input (not by overloading it but by dividing it into two).. (12:41)

%..(12:42) I should be the way also allow for references where the text is replaced by something else, like the <a> elements of HTML, or the links of MD..

%..And the addresses for users are just the userIDs, potentially prefixed by the database node, of course. (12:45) ..Let me call it an 'entity key' (entKey) instead of an 'entity address' (12:46)

%.. So e.g. '1."class"' or '117."LoTR"', or  '<db node identifier>.1."class"' and '<db node identifier>.117."LoTR"' instead if the prefix is explicitely included.. ..But maybe it should always be included, though.. or what?.. ..Hm, or not, maybe it should actually rarely be included: only when the DB node itself doesn't contain the entity data.. (12:52)

%..Hm, I do think I like this.. (12:54) ..For it also means that users can now quite easily write the entKeys.. well, maybe 'entity addresses' is actually better.. write these themselves when they submit new entities. By the way, maybe texts should just rely more on references where the users write the link texts themselves.. Well, that's not for me to decide..:).. (12:57)

%(13:07) Hm, since all entities now has an adress with a userID in it, I no longer need the creator_id column, nor the associated index. ..Unless I want to separate ent_addr into creator_id and ent_key.. (13:10) ..Yeah, let me do that, that seems pragmatic. (13:10)

%(13:19) Hm, jeg skal så enten tilføje entKey til def_str-hashet, eller jeg skal gøre det muligt for brugere om at stemme om re-mappings af keys.. ..Måske gøre det en del af hashet.. Hm.. (13:21) ..Yeah, that seems like the samrtest thing to do.. (13:22) ..Making the entKey part of the def_hash.. ..Hm, or even better, make addresses that each only refers to another address.. ..So making an AddressRemappings table.. (13:27) ..Hm.. (13:28)

%..(13:31) Hm, maybe we should forget about address remappings for now, and just make the key part of the definition.. ..And since the key ends in '"', we can just concat it with the def_str before making the hash.. (13:34)

%(13:36) Oh wait, this doesn't work for derived entities.. Well unless we could find a way to construct.. 'derived entity addresses'..!!.. (13:37) ..Okay, something to think about... (13:38)

%..(13:40) Oh, that won't actually be too difficult, since all we really need is to implement some entity addresses that basically says: 'ent1(ent2, ent3, ...).' We don't need.. Wait no, this is wrong, it requires more than this, namely since you need to know where the inputs go, now that we don't have the templates any more.. Hm, so should I reintroduce templates \emph{just} for derived entities, or what to do.. Hm, or make a syntax for telling the app where the inputs go.. Well, that means making a template, unless we use a prototype instead, and we don't want to do that.. (13:44) ..Hm, if predicates and statements are the only derived.. nah, never mind: We need the derived templates to get an entID.. (13:46) ..Yeah, so things to think about indeed. (And I do want these 'entity address' changes now...) (13:48)

%(13:55) Let me put the userID last in the address instead, at least in the app. layer, and then I'll also make the app collect all the addresses it encounters in a list. And when a user types a new reference, it can then make autofill suggestions from this list.:D (13:56) How great is that?.:)

%..(14:01) And yeah, I just need to introduce the concept of function entities, which then holds a template for their derived entities.. ..Or we can call them 'templates,' and then there're just like my templates from before.. Hm.. (14:03) ..Ah, and the database should then have a table that maps derived entiti addresses to entIDs, perhaps without actually inserting anything in the Entities table.. ..Yeah, I guess that's the solution.. (14:05) ..Hm, so the def_str could be NULL iff the entity is a derived one.. (14:07) ..Ooh, or I could make def_str a JSON array iff it is a derived entity.. ..Of course, then I can't use that to define lists, but I wasn't planning to do that anyway, as things were.. (14:10) ..And if I do want lists at some point, then I \emph{could} just do the same thing of using '[[...]]' instead of '[..]'.. (14:12) ..On the other hand, if the key holds the same information, why not just make def_str.. well, because of the unique index on def_hash, of course, but.. Hm.. (14:14) ..Well, we could then just hash the entKey instead..:).. (14:14)
%(14:16) Oh, there's a problem: We need to be able to abbreviate.. the.. hm, I could hash the full derived enitity key before it comes into the.. Ah, but then I do need to let def_str store the actual entity addresses. But that's fine.. So the entKey is hashed before it is send to the database layer.. no, wait.. ..Oh, I forgot the entKey is already concatenated to def_str before the hash is taken anyway, so never mind about not letting def_str just be "" for derived entities.. (14:20) ..Ah, and if def_str is "", that means that the hash will alrady just be the hash of the entKey, which means that we can just use this SK when querying for the entID of a derived entity.:).. (14:22) ..So entKey is then combined with both def_str to make one hashed SK, and with creator_id to make another SK?.. (14:23) ..While the Entities table of course also holds all the plaintext columns, i.e. creator_id, ent_key, and def_str.. (14:25) ..I think that's it.. (14:25)

%(14:32) So I'll reintroduce the templates, only where the placeholders can now only come as the sole and whole value of an attribute.. ..Templates may have optional inputs, but not list inputs, where the user can then include an arbitrary number of inputs. Instead I \emph{will} just say that entities defined by JSON arrays constitute 'list entities,' which can then potentially be used as inputs for templates, or as attribute values in general, i.e. instead of writing out the array explicitely as the attribute value. (14:36)

%... (15:49) All classes should hold a template attribute, on top of the other things that they include now. And then I'm gonna merge templates with classes. Now that I don't have 'main classes' anymore, this makes perfect sense to do: all templates has an associated class, and all classes should have an associated template, as things are now, so let's merge them. Also, I now don't need to include def_str in the hash. It is enough to have a hash of the userID and the entKey as the SK, where for derived entities, the userID is then of course just NULL.
%When typing an entity reference, the non-derived entities should be shown at the top.. well, that's natural.. And then if you reach a point where the entKey doesn't match anyone that the app has seen---or is part of a "hard-coded" initial list, which we should btw also have---, then it takes the last subkey that the user is in the process of typing, and restarts the match from there.. ..Yeah.. ..Hm, and if the user erases, what then?.. (15:56) ..Well, then we start from the last template call and redo the search from there, and again, if no mathces are found, then we query for the key that is currently being typed (which might be an input).. ..Nah, or maybe we just always start from the key that the entity is currently editing, and if it is a "function" (template/class) of a "function call," then the whole call is being replaced if the user chooses an option that is a full call.. Hm, this gets complicated.. ..Let's just replace the current key, and then the user must just erase the previously typed inputs if these haven't been erased before then substitution in the middle. Something like this.. (16:01)
%And when showing suggestions.. Ah, wait, we should just never.. Nah.. ..never mind.. ..And when showing suggestions, we should also show the given username, instead of just the user ID. (16:03)
%Note that most entities will now be derived ones once again..:) (16:05) ..Mostly consisting of a class, plus the relevant template inputs, and that's it. And note that this means that we mainly need the userIDs in the entKeys when it comes to classes. (16:07)
%..Oh, and this means that we should also make strings part of entKeys, hm, and couldn't we just.. No, we can't just use the string, not for texts.. So what to do with templates that takes strings as inputs?.. ..Well, we could allow for both: Both to insert a string/text via its entAddr, but also just to insert an anonymous string (a pure string, without an entAddr) directly.. (16:10) ..Yes..

%(16:25) Okay, I'm not completely sure I want templates for entities that are not derived.. hm, nah, I only want it when the semantics is defined functionally, and not just when we have a standard way to write the important attributes of a class instance.. Hm, so let me think a bit..

%(16:45) What about a system where every user can just upload an address--entity pair at any point, as long as they havn't used the key before? And then every entity can just get several addresses: as many as the users upload for it..?

%(17:01) Hm, as an alternative that allows us to insert future-/sibling-referencing entities, we could also just add a proc to request reservation of some entity IDs, where you then also get a code to redeem these reservations.. ..Ah, and then each user can then in principle just define their own entKeys, which the app can then translate into entIDs..
%..(17:09) I mean, this sounds like the better alternative.. And in terms of the distributed database, I've already figured that out, plus you can always remap the entIDs at any point anyway.. ..Hm, so what does that mean, it mean we are back to just having a def_str, which is then hashed for the SK?..

%(17:48) Yes. And we just create an entity with a coded def_hash for each reserved entity, and then then the entity is finally submitted, with the right passcode, we just replace the def_str and the def_hash. ..By the way, I will make a flag for the 'special attributes' section that.. Well, or maybe I will just make a 'discontinued attributes' section instead, e.g. to say that a compound statement no longer needs the scale type, as this should now be defined by the predicate/relation. (17:52)

%(20:26) Hm, I think it's better to request a entKey--entID pair, 'cause then we don't have the problem of accidentally inserting the same entities twice; if the entKey has already been used by the user, then the request fails. Okay, I'll do that. And then I'll implment these @<userID><entKey> references after all, but where other users then generally don't see, that the reference is anything other than a PK entity reference.. ..And we'll instead mainly use the entIDs.. Oh, and at some point we can even implement that the database changes these entKey references with entID (PK) ones.. ..Well, except that again makes searches impossiple, so no, we use entKey references, and keep using them, for, and only for, recursively defined entities. (20:33) ..Oh, and we don't actually need the userID, then in the syntax, 'cause that can just be the creator.. well, but maybe let's include it anyway.. (20:34) ..Or should we just not make recursive definitions, how about that?.. Sounds better for the moment. Okay, let me just put this on the shelf, then, and just for now make no recursively defined entities.. Hm, but what about class?.. Should I just not.. well, I can either write '@this' or 'itself', or just not include the attribute, but define in its description instead. I think I'll do the latter.. ..Yeah, 'cause we also don't want to show class's description twice.. ..So all entities have a class, \emph{except} class itself.. ..Well, I might as well just write "class":"itself" just to make all entities have a class. Fine.. ..Then again, I could also just go back to where I was before "..Or should we just not make recursive definitions[...]".. ..Nah, scrap recursiveness, for now at least.. (20:47) ..Well, no, except the entity and class classes, for now, which I will then insert via SQL. And since we can always edit entities anyway via 'representations,' we can always edit these afterwards---which is the right thing to do, anyway, rather than changing them out, just for a more clear/useful definition. (20:51)

%(03.10.24, 12:41) Jeg gik mig lige en lille motionstur, som så blev til en tænketur i sidste del af den. Jeg tænkte egentligt: 'scrap rekursive entities helt' i formiddags, men nu er det faktisk lige før, at jeg vil genindføre ent-adresserne, bare hvor disse så ikke bliver en del af definitionen, men hvor hver bruger kan uploade sine egne entKeys, hvilket jo egentligt bare svarer til nogle bruger-IndexedEntities, hvor man bare ikke kan slette en indeks-nølge/række igen. Og det der så virkeligt for mig tændt på idéen nu er, at man så vil kunne søge i disse indexes også. Og så kunne man jo eventuelt have brugere/bots, der laver meget komprimerede keys velegnede til brug i definitioner, og brugere/bots der laver entKey, som er nemme at søge i.. ..Nå nej, til definitionerne bruger man jo bare enten entID i stedet, eller sin egen entKey til rekursive entiteter.. ..Hvis man har en god bot, der har gode entKeys, så kunne man jo endda også eventuelt lave et FULLTEXT-indeks for den, som man så kan querye, når man løber tør for direkte matches.. og/eller når brugeren trykker 'søg,' I guess.. (12:51)
%..Hm, skulle jeg så bruge '(<entKey>)'-parenteser til entKey refs, og bruge '"<entKey>"' til inserts der bare venter på det pågældende outID, eller omvendt?.. (12:54) ..Lad mig sige omvent, tror jeg.. ..Hm, og for at gøre det nemmere, kunne jeg også bare bruge "@'<entKey>'" i stedet for "@(<entKey>)" for de midlertidige refs.. (12:56) ..Nah, lad os sige "@(<entKey>)".. ..Tja, eller måske vil jeg hellere bruge '@<userID>[<entKey>]' for entKey-refs'ne.. (12:58)

%(13:01) Hm, der er så potentielt mulighed for at implementere en form for redigering, hvor man bare omdanner sit indeks.. men jeg tror nu ikke, det er en god idé. Så kan jeg bedre lide at bruge 'representations'..

%..Hm, gør jeg det her? Indfører jeg disse ikke-mutérbare bruger-indexer, som så kan bruges i referencer?.. (13:04)
%..Hm, og hvorfor ikke bare bruge dem i teksterne i stedet for ID'er, for så bliver definitionerne jo helt uafhængige af rækkefølgen af indsættelsen..
%..Hm, jeg kunne også skjule entKeys'ne for brugerne, i reglen, og så gøre det frit for brugerne at vælge---eller bare gøre det til tal.. ..nå nej, for så skal databasen generere disse tal, og så bliver det ikke længere uafhængigt af rækkefølgen. Men vi kan jo skjule entKeys'ne i det daglige, og så kan brugerne bare altid søge i et specialt indeks (i.e. for en god bot) efter entKeys, og så sætte dem.. Ja, og så \emph{kan} vi lige netop godt bruge to typer endekser: ét som er godt at søge i, og et som er komprimeret, da sidstnævnte så netop \emph{er} det, der kommer til at stå i definitionen, og ikke entID'et.. (13:12)

%..(13:14) Hm, det virker faktisk rigtigt fornuftigt nu, det her..

%(13:16) Uh, jeg har en idé til FULLTEXT-search-indekserne (og generelt til indekser faktisk). Man kunne bestemme sig for et prædikat og en bot/bruger, med andre ord for en entitetsliste, som så er/giver en liste over brugere og bots med forskellige scorer. Og så er reglen, at hvis en bruger eller bot vil redigere i en nøgle/række i indekset, så må de kun gøre dette, hvis deres score er højere en scoren for den bruger/bot, der sidst redigerede pågældende nøgle/række. (13:20) ..For på den måde kan man meget direkte deligere retten til at rette og tilføje til indekset ud på flere hænder, uden at.. Ja, i stedet for at alle brugere i mængden skal op-votere ændringer gennem en relativt noget længere proces. Her er en rettelse i stedet bare lynhurtigt, og kræver ikke noget op-votering; det er kun brugernes autoritetsscore der skal voteres om (aggregeret af omtalte bot, der administrerer denne brugerliste).

%Man kunne i øvrigt eventuelt bruge et tilsvarende system for representations, bare hvor der så er en bot, der sørger for administrere.. Ah, man kan gøre det samme med indekser: Man kan lade dem være styret af én bot i stedet, men som så automatisk indfører rettelser, når en bruger med højere autoritetsscore en den forhenværende submitter en rettelse.. ..Og hvordan signalerer man en rettelse til botten..? ..Tja, enten lader man den bare løbende tjekke.. Tja, eller også indfører man en procedure til for brugere at signlere bots, det synes jeg, at vi skal gøre. (13:29)

%..Men FULLTEXT-indekserne får vel så ikke noget med entID-indekserne at gøre, gør de?.. (13:30) ..Ja, adskiller man ikke de to ting: de komprimerede entID-indekser og de søgbare..? ..Hm, og skal de søgbare indekser faktisk ikke være indekser \emph{over} entKeys/entAddrs, således at man altså får dette først (og dette kan man jo også bruge til at query'e databasen direkte for entitetens data, uden at behøve at fetche ID'et først)..? (13:34) ..Jo, sådan bør det jo være..

%(13:39) Ah, skal jeg så alligvel indføre compound ent-adresser for at gøre definitionerne helt uafhængige af entID'er, også for compound entities, det må jeg vel næsten.. ..Men brugere er så stadig frie til at uploade entKeys for compound/derived entities; man behøver ikke bruge den afledte adresse.. (13:41) ..Og hvad, siger vi så at definitionen på derived entities består af et JSON-array, og at lister så bare må være nestede arrays?.. (13:43) ..Ja, det gør vi vel. Og templates er så bare vilkårlige entiteter, hvorom det bare kræves, at de holder en 'template'-attribut.. (13:45) ..Ja..:) (13:46)
%(14:08) Ah, alternativt lader vi bare definitionen af derived entities være en reference til dem selv. Nice. Så kan vi nøjes med at bruge arrays til lists.:) (14:09)

%(14:50) Okay, men et spørgsmål er så, for nu har jeg lige lagt op til, at hver bruger kan danne deres egne nøgler, men skulle man mon ikke bare have det ene.. Hm, jeg kunne eventuelt bare sige, at alle adresser skal være integers, og så må brugeren bare lave en mapping i JS.. tja.. men tanken er altså så, at alle entititer bare får én adresse, defineret af creator-brugeren.. Hm.. (14:53) ..(14:56) Hm, jeg har lyst til at sige dette.. Og så er det bare brugeren selv, eller dvs. appen, der sørger for, at brugeren finder en ny heltals nøgle for hvert nyt insert (og som initial user skal jeg sørge for, at dette bliver gjort på en konsistent måde, hvor jeg også mapper brugbare identifiers til hver heltalsngle).. Og så har vi altså bare én adresse pr. entitet, som i øvrigt også indeholder creator-brugeren, så vi behøver så ikke creator-indekset også. (14:58) ..Og så skal jeg bare sørge for, at enhver bruger også lige uploader nogle keywords, som kan søges på, når de uploader en entitet, inkl. "initial user," således at man fra start af kan finde addresserne ved at søge på dem.. Ja, og appen må sådan set også godt vise dem, på samme måde som at jeg ville vise ID'erne førhen. Hm, nu kom jeg lige til at tænke på, dette svarer jo egentligt bare til at gøre entID'et dobbelt så langt.. bortset fra at man næsten kunne nøjes med INT UNSIGNED for bruger-ID'erne og tallene.. ..Ja, man kunne sågar gøre sådan, at brugere skal anmode om at få et insertion-user ID.. (15:02) ..Eller bare et 'insertion ID'.. ..(Og så kan samme bruger bare bede om flere, hvis denne bruger indsætter mere end en \emph{millard} entiteter..) Hm, og man kunne faktisk endda bruge dette som en måde også at begrænse brugeres inserts på, nemlig ved at de først skal anmode om et vist interval, som de kan indsætte for.. ..Okay, hvad er bedst, at vi har både entID og de kun dobbelt så lange ent-adresser, eller at vi bare har ent-adresserne, som vi så bare sørger for ikke overskrider en BIGINT UNSIGNED (long)..? (15:05) ..Hm, måske er første mulighed bedre, fordi man så altid også kan putte et DB-præfiks foran adressen, selvom det ellers er fristene at sammensætte de to, når de jo nu er så tæt på hinanden i størrelse alligevel.. Men jeg er ikke helt sikker... (15:07) ..På den anden side, man kan jo også altid præfikse entID'erne.. Hm.. (15:09) ..Hm, det er dog lidt et problem, hvis vi skal anmode om et interval, for så betyder rækkefølgen jo lige pludselig noget igen.. Tja, på den anden side det gør det jo alligevel i form af bruger-ID'et, som jo afhænger af rækkefølgen af brugere.. ..Hm, alternativt skal brugere bare selv vælge entID'erne, og så sørger man bare for, at det lige reserverer dem inden de sender uploads'ne, som så kan sendes én efter én.. ..Men vent, hvorfor er dette bedre end bare at få genereret et entID automatisk? Nu bliver jeg da helt i tvivl: hvad var idéen egentligt lige med det her? (Det virkede så smart, men nu kan jeg pludselig ikke lige se hvorfor..) Nå, jeg trænger vist til en pause og en gåtur (i lækkert solskin, ser det ud til)... (15:15)

%..(15:22) Okay, det \emph{er} smart at brugerne bare bestiller et interval ad gangen, og at vi bare smider ud, at Entities skal fyldes op fra en ende af, for så kan man nemlig, om ikke andet, indsende en masse entiteter på én gang, uden at vente på ID'er for hver ny gruppe af entiteter, der afhænger af de foregående.. Tja, selvom man nu nok som regel ikke ville behøve at vente længe her, for så mange dependencies vil man heller ikke have normalt, men alligvel: rart hvis man bare kan submitte hele skidtet på én gang, og så også stadig sove trygt, hvis nu der skulle ske en fejl, for så bør man bare kunne indsætte entiteten igen. (Hm, og hvis vi oplever, at der kan ske irriterende felj, så må vi bare sørge for at lave et vindue, hvor brugeren kan redigere sine insert..) Hey, det er da også en nice ting.. At lave et vindue, hvor brugeren kan redigere sine inserts.. måske før brugeren signalerer til botsn'e om at reagere på inserts'ne.. Hm, sure.. Nå, lad mig gå nu... (15:27)

%... (16:29) I \emph{will} let the users reserve entity IDs, and just make it like a malloc, where they reserve a whole interval at the same time. I will then make an index over the intervals. And actually, I think I will use this to save the creator_id for each interval, namely just be keeping the index over which user each interval belongs to. Then we don't need the entAddrs. And a thing that I'm a bit excited about: I will make it possible for users to edit all entities in their own intervals, until either a time expires, or they state that their interval is finalized.. Well, or maybe they can do it for each individual entity on the interval, I'll see about that (but this is something we can always change). And at first I'll just forget about the expiration time and just let users declare themselves when an entity is finalized. Now, I thought I wanted to state, then, that bots should.. Oh, I should say this first: Non-finalized entities should actually still be visible to other users, and scorable, etc. But they should just have a warning at the top of their entity AppColumn that says that the entity is not final yet. And then, as I was about to say, I thought I wanted to say that bots should just not rate non-finalized entities. But I think, let's scrap that, and simply let the only difference for a finalized and a non-finalized entity, from other users' perspective, i.e., be that warning at the top of the column.:) (16:37)
%..Oh, and I should say, I think I'll still use AUTOINCREMENT for Entities' IDs, but the database should then just jump the given interval when a user reserves one, and then return the first ID of the interval. (16:40)

%(16:46) (And editing is just done via the same insertToReservedID procedure.)

%(16:54) Ah, and to guard against repeated inserts, we should let the users choose a code when they reserve the interval, and then we save that code at the interval. And if they make the same request again, we just return the pointer to the same interval again.

%(18:01) Let's make is customary for users to always reserve at least 100 entities at a time.. or not?.. Hm.. ...(18:35) Maybe we should make a bot insert derived entities.. For I don't want these to be editable, and this also mean that we won't get many single-entity-insertions, which means that it makes sense to always just reserve an interval first.. ..Yeah, let's make that a bot, in any case.. ..And yeah, let me just.. ..make do with the procedure for reserving an interval and then one for inserting/editing/finding (where we find only if the entity is finalized, otherwise we replace it) an entities in these reserved intervals. (18:41) And then I can always make a procedure for inserting only one entity at a time, if that becomes useful..

%(18:47) Wait, saving derived entities as their reference means that texts containing nothing but a reference to a derived entity won't exist.. Hm.. ..Hm, but we could make it a noramlly ill-formed reference.. (18:50) ..For we don't care about ill-formed texts.. not existing.. right..? ..Maybe we might.. ..Hm, since numbers.. Nah, we can't use JS numbers for BIGINTs, never mind.. ..I could also just try-catch JSON.parse, or always do a preliminary.. nah.. ..Hm, alternatively we could just use 'null' as the defStr for derived entities, but then hash the.. ..the non-JSON derived-entity reference.. Yeah, that could work.. (19:00) ..And we already have entID = 0 as the null entity, so we won't need an entity with a defStr of null at any point, I'm sure.. (19:02) ..Let's do this..:) (19:02)
%..Well, although that then requires us to always pass along the defHash as well, which is not too pretty.. ..Well, or require us to make two queries when getting a derived entity in a place where you don't know it beforehand.. (19:05) ..Why not just try-catch?.. ..Wait, why do we even need special.. Yeah, no, I'm not using entAddrs anymore, I'm using entIDs, so I can just go back to saving derived entities exactly like I would them if they hadn't been derived..(?) (19:08) ..Yeah, so never mind about all this.. ..So is templates then just something that is only implemented in the app (although the descriptions can of course also specify them)..? (19:10) ..Yeah, they are.. They are only potentially described in a class'es description, or potentially in another type (class) of entity, but never in an attribute (attributes are only for telling the app certain things, and.. well, maybe..).. Maybe there comes a time when custom templates are useful, and where it might then be useful to define a template in an attribute of an entity, but I'm just gonna describe mine in the descriptions instead. (19:14) Okay, so never mind about entKeys of derived entities; the fact that they are dervied just means that we know exactly what hash to search for. (19:15)

%(04.10.24, 9:42) I thought about having the app parse the class descriptions for attribute specifications when constructing the submssion fields for new entities, but no, let's make that an attribute: 'attribute changes' or somethig like that.

%(10:40) Let us just store the lenght of each ID interval in the table row as well, since that also opens up for allocating intervals anywhere in the space. ..Oh, but does then require more work reuse unused/obsolete parts of previously allocated intervals.. (10:43) ..Yeah, but it's not much work, so okay.. (10:44)
%..(10:45) Hm, wouldn't it make sense to divide the space into sections..? ..like hard disk pages, i.e.. ..Maybe, 'cause we can always change implement later a way to make use of the unused IDs.. Hm.. ..(10:54) The app is going to make a choice for the user anyway, when it doesn't need only one entity, and one rarely does, at least if we mainly use bots for inserting derived entities.. So yeah, let us just always reserve 256 IDs at the same time, at least in the beginning. The user will then get a page where they can see all their inserted entities.. (10:57) ..And edit them from there.. ..Well, then it actually \emph{does} make sense for the user to choose their own interval size, 'cause that then works nicely as the app can then make exactly these intervals expandable/collapsible.. ..So instead of having a system where you go to a class to insert an entity, and then send it off into the ether, only to get an entID back, that you are essentially responsible for remembering, now we instead get a system where entities are edited and inserted from a whole page where a user can see all their inserts. (11:02) ..And how does the user then select the class for each insert, and do we also group entities together in classes?.. ..Ah, I have it.. (11:04) Instead of keeping is_finalized in the rows of the Entities table, we instead keep it in EntityIDIntervals instead, but make it possible for users to subdivide their interval, which is done by adding new interval heads inside their own interval. And those subintervals can then each have the is_finalized bool. That way the user not only get a way to batch entities together, and in a way where it is perfectly possible (in principle at least) for the user to regroup these batches/groupings dynamically (even though the IDs should not generally be changed), which is something we very much want, but we then also get the possibility to finilize a whole batch/group at once in a neat way. Okay, I'll do this. (11:09)

%..Hm, and maybe we should make identifiers.. ..mandatory..? ..Mandatory identifiers that then nudges a user not to rearrange the entities.. Well, that's probably overthinking it.. But identifiers would be useful.. (The whole page will essentially work a bit like I imagined using my EntityInserter..) (11:14) ..We can just warn an editing user not to rearrange entities once they or others have begun scoring them..
%..We don't group by classes, btw..
%..Hm, do I even make fields for all attributes, or can the users just be entrusted with writing JS objects (with JS syntax, not JSON systax) instead..? (11:19) ..Hm, that could be an alright start. And then if I'm feeling fancy, an upgrade could be to automatically insert attributes parsed from the classes of the 'classes' attribute.. (11:22)
%(11:33) I actually think that this kind of entity editing is fine, even for a released version of the web app. I mean, we can also at some point implement that the defStr field is transformed into a whole group of form fields, which might actually especially be good for a mobile app, but then this could just be a toggable option.
%I do think that we should parse the attributes from the 'classes,' and we could even parse flags about what values are expected for these attributes. So yeah, let us for all classes keep an 'attributes' attribute (implicitely meaning 'atrributes of instances'), where each element then consists.. well, we might as well make it a struct, where the "property names" are the attribute names, and the.. well, I'll see about that, but we need to pair the attribute names together with an object specifying the attribute, and which also includes the flags.. Hm, or we could make a.. ..Nah, that would work. And if the attribute has the 'remove' falg, it is removed from the list of expected attributes.. are there's our name: 'expected attributes'.. of expected attributes from the previous class in the list. ..The 'expected attributes' is btw an optional attribute, since a subclass might not change the expected attributes from its parent superclass. (11:43) ..It's 'parent class,' let's call it that. (11:43) ..All ancestor classes of a class are superclasses of it (and potentially it has even more superclasses (in existence)), but only one class is a the \emph{parent class} of a class. Great.. (11:45) ..This then means that we could replace 'classes' with just 'class' for entities, since only the parent class is expected to precede a class in these lists, so yeah, let's do that. Entities should all have a 'class' attribute (and also a 'description,' btw), and classes should have a 'parent class' atttribute as well. (11:48)

%..And scrap the identifiers for now. We can instead just use ID integers that are relative to the start_id. (11:50)

%(11:52) Oh, we don't have to change the original interval header if we reuse some of the IDs within the interval. We can just allow other users to have intervals inside a users interval. So when we ask which user an ID belongs to, we just go back to the first start_id <= ID, then see if interval_length >= ID - start_id + 1, and if not, then we technically go back to the next start_id before that.. No, 'casue that is infeasible.. So what to do instead..? (11:55) ..Keep a pointer to the parent interval head in each head, potentially.. (11:56) ..That actually sounds quite nice.. (11:57)
%..Ha, this could also open up for a dynamic in principle, where users can lend out/give/sell space to other users that don't have any.. for some reason.. Well, anyway.. *(Or more realistically, the user can hand the space back to the database node. (12:03))
%..But yeah, I think I will do that, since it also works quite nicely for the distributed databases: Then each database can just keep track of all other dasabases outer intervals, and make sure to confer in public with the whole group before allocating a new interval, in order to prevent collisions, which is in no one's favor, out of any non-malicious node. (12:01)

%... (15:50) Det er bare et enormt skønt vejr i dag.
%Maybe I don't need the interval_key after all.. ..It's only needed for interfaces that don't query for the outer intervals first.. Hm, there's another thing.. ..Hm, we should still just query for all intervals, I guess.. ..Oh, I \emph{could} make a linked list out of all the outer intervals of a user.. Hm.. (15:54) ..(16:01) Wait, by keeping the intervalKey, we might not need.. Well.. ..Hm, we could just indeed make a column that contains the next sibling head, but then still just return a bunch of intervals (or 'interval headers,' in other words..) for each query. And then it is purely up to the app to utilize the sibling pointers.. ..Sure, that sounds reasonable.. (16:04)
%..And about the intervalKeys..? ..Hm, you could also reference the former sibling-to-be, and essentially use that as the intervalKey.. (16:07) ..Or reference the parent, if it is the first interval within an interval---or potentially the first interval in an interval of the database node.. Hm.. (16:08) ..Well, inside your own interval, you just choose exactly the start and the end yourself.. ..And when reserving a new interval, yeah, let's just make sure to reference the previous interval, and thereby essentially use that as the intervalKey, gaurding against accidentally reserving another interval after you have already gotten one.. (16:11)
%..Hm, but I don't like for the procedure to be going through too many loops, though.. (16:14)

%(16:18) Dejligt at føle, at jeg får virkeligt godt hånd om entities nu (7, 9, 13; det er helt utroligt så mange gange, jeg har lavet mit system om nu, efter egentligt at synes, at jeg havde den, det er helt vildt..). Og især at dette også nu inkludere inserts af entities: føler at det bliver virkeligt nice, nu hvor vi får en hel træ-inddelt side fuld af alle brugerens entities, som man endda kan redigere i (indtil man endeligt finalize'er (hvad man gør for at fjerne advarslen i toppen af entitets-appsøjlen)).. Men ja, er lidt træt her til eftermiddag, så resten af arbejdsdagen vil nok bare lige gå med at finde ud af, hvad jeg gør med træ-strukturen helt præcist (ikke at jeg nødvendigvis når at finde helt ud af det).. (16:23)
%..Måske skulle jeg lave to tables: ét over brugernes ydre intervaller, inden i database-nodens interval, og så ét mere over hvert bruger-intervals fin-indeling..? (16:26) ..(Puh, blev \emph{virkeligt} ramt af træthed nu..)

%(20:55) Lad mig lige notere: Jeg nøjes bare med én liste over intervaller, og g(l)emmer det med at lave en træ-struktur for nu. Og så har vi altså bare et user_id--head_id-indeks, som appen kan query'e fra. Og hvad angår is_finalized, så bliver dette så bare et flag for hver enkle entitet. I øvrigt skal alle intervaller kunne navngives, men dette kan ændres til hver en tid af brugeren, og bruges kun til at gøre insert page mere overskuelig, fordi hvert expandable interval så får et navn ud for sig. (Og vi tjekker ingen gang for kollisioner af disse navne.) ..Nå ja, og interval_key bliver så i stedet bare intervallets nummer i listen over brugerens samtlige reserverede/allokerede intervaller (så brugeren skal sende: "giv mig interval nummer $n$, og hvis brugeren allerde har et $n$'et interval, så får vedkomne altså bare det samme interval tilbage igen). (21:01)

%(05.10.24, 9:42) There's also another way of making a tree structure for the insertion page, where we just use a creator--id index, and then another table for the tree structure. And that might be a good idea, 'cause I had the thought in bed last night that we may not need a unique index on the hash alone, as it could be an index of hash--id, or it could also be hash--creator, as I thought of this morning.. So nw I have a bit of thinking to do.. (9:46)
%..Maybe we just want to treat derived entities and other entities (with there own description, usually) differently.. (9:47) ..Well, comments are also technically derived entities at the top level, and we want to be able to edit those (more than anything else).. (9:49)
%..(9:56) Hm, we could indeed devide it up into two tables: searchable and non-searchable entities, and then we also let go of a lot of redundant data thereby, come to thing of it, since most entities wont see a use of their secondary key (SK).. ..Then searchable entities are just constant, whereas non-searchable ones might be editable, at least for a time.. (9:59) ..And non-serachable entities can be repeated (dublicates, in terms of the defStr), and can also be dublicates of serachable entities, but the searchable entities are constant; they cannot be changed.. (10:01) ..They might be deletable, though.. Nah, we just don't record the user, than it should be fine. (10:02) ..Well, we have delete illegal content, if that can happen, but then we just remove the entire entity, of course. (10:03)
%..And instead of is_finalized, we should use is_publicized, which then doesn't allow other users to query the data when it is still false.. (10:06) ..(And then it is also not editable once it is publicized..)
%..Oh, but it is easiest if I can keep it on the same table.. But I guess I can't.. ..Hm, it is only a hash plus a BIGINT for each entity to have them be part of the secondary index.. (10:11) ..And then the constant ones could just be ones with creator_id = 0.. That actually makes sense.. (10:13) ..So a UNIQUE INDEX (def_hash, creator_id)..(?) ..Right.. And then what about the inserts and the insertion page..? (10:15) ..Hm, by making it (creator_id, def_hash) instead, we then already have a list over all the creator's entities.. ..But it's not the best one.. ..No, I would rather want a creator_id--interval_id/group_id--interval/group_key--id index.. Hm.. (10:21)
%...(10:49) Ah, unpublicized entities could actually provide a great way for the app to save user settings!.. Including the tree structure of the insertion page, by the way.:) (10:50) ..Oh, what about the hash for unpublicized entities, should we salt it, or somehow hide it?.. ..I guess we coud hide it, although the straightforward implementation would be vulnerable to timing attacks (or what they're called), but maybe that doesn't matter too much.. well, maybe it will, who knows.. ..Oh, I could just make is_publized part of the index.. Hm.. (10:54) ..Hm, that might also serve other purposes to have that division.. Hm.. (10:55) ..And then you simply cannot search on unpublicized entities via their SK.. (10:57) ..He, nice to all of a sudden have the ability to store user settings privately, something that I had forgotten would also be a need at some point..:) (10:58) ..Let's call it is_private, then, since they might not be meant for publication in the first place.. (11:00)
%..Okay, so do I make the creator_id--id index as well, on top of the hash--user one, and the primary one?.. (11:01) ..Yes..:) And then for the first version, perhaps before the prototype, of the insert page, I can just list all unpublicized entities first, followed by the publicized ones. But that reminds me: where does is_private go in the indexes, it goes in front of the hash--user index, right?.. ..Oh, \emph{and} in front of the creator_id--id one.. (11:05) ..Let me call it is_public, then, since that will make it a bit more natural to have the unpublicized ones come first, and it also sounds better, not least.. (11:07)

%(11:32) Yeah, it seems that this is the way forward.:)

%... (14:35) Jeg laver InitialInserter i PHP i stedet, hvad jeg jo også bør. Og måden den virker på er så bare at indsætte alle entiterne i første runde med relative ID-referencer, hvor disse altså bare hver især er nummeret ud af alle brugerens creations, som den referede entitetet har. Lad mig bare lave en syntaks for sådanne midlertidige relative referencer i øvrigt. Og så i anden runde så rettes alle entiteter så, der indeholder relative referencer.
%Første indsatte entitet bør så være "initial_admin," som så også er den indsættende bruger. Og bemærk at vi ikke vælger userID = 0, for jeg vil faktisk gerne kunne redigere legitimt i entiteterne efter realease, nemlig ved brug af denne "initial_admin"-bruger.
%Now that all entities with creatorID != 0 are editable, always, one way to back up another users entity will then simple be to submit a duplicate with the anonymous creator_ID = 0. Then one can subsequently uprate this as a "better representation," and just choose a very wide rat_err, if you don't want to really say that it is better than any potential future editions of the entity by its creator. (14:43) ..Duplicating an entity and "backing it up" using one's own userID, rather than creator_id = 0, is not constructive; creator_id = 0 should always be used for this purpose, when the user in question don't actually make any changes to the original defStr. (14:45)
%..And let me just underline: isPublic does not mean that the entity is not still editable; it is. (14:46)
%..And the admin(s) can also edit or delete entities at will, in principle (although in practise it has to be done by request, of course, or to prevent legal issues). ..An edited entity might keep the same hash, but the admins might also change the hash with it. Note also that deleted entities can still be in indirect use if their representations are not deleted/missing/undefined, since I'll make that query proc that can be used to get the best representation directly, without ever having to query for the original entity. (14:49)

%..Oh, and I should say: The insertion page/column should just use the same procedure, i.e. inserting entities in two rounds.
%..(Btw, since we allow only one entity of any given defStr for each user, we don't need any keys to prevent inserting the entity twice.) (14:52)

%Oh, when I'm inserting the "wrong" defStrs in the first rounds, then I'll keep replacing and replacing back the entities each time.. ..Well, so the app then has to query for all the creations first, and then suppress all that.. No.. Hm.. ..Should I just not care about this for now..? ..No, the app should just replace the relative IDs before the first round if possible (meaning that it should first query for the creations).. And my PHP Inserter class?.. (15:56) ...I might as well do the same thing: query the creations first..

%(16:34) Oh, PHP doesn' allow for functions in preg_replace.. ..Ah, but it has a '\n' syntax, that I can use instead.. (16:37) ... (18:07) Hov, nej, jeg har jo brug for en funktion til at slå creation-ID'et op.. ..Jeg kan dog bruge preg_match_all i stedet.. (18:14) ..(Det er jo nok noget formiddagsarbejde, kan være jeg holder fyraften i stedet..) (18:17) ..Ja, jeg er for træt nu, og jeg ved det bliver en let sag i morgen..

%(06.10.24, 15:17) It's really nice that user settings are stored as private entities, and ones that are editable. It means that we get the following for free: Users can change there settings at anypoint without having to upload new entities each time, otherwise costing them data space with the database node. But they can also save several settings by making more than one setting entity (for whatever given setting that we are talking about), and then uprate all on the entity list over their prefered settings (which is already how we find a users setting, even if they only have one). And it also automatically mean that users can publicize their settings, although they should of course then prefereable publicize it as an "anonymous entity" such that they forfeit the rights to edit it (as that would be a very bad situation, where a user can all of a sudden edit a bunch of other users current settings). (15:23)

%... (17:14) Jeg \emph{vil} lave (en slags) entKeys, bare som kun er synlige for brugeren selv. Og så vil jeg bruge dette i stedet for id'erne til at ordne creations-listerne. Disse entKeys kan så være ligesom mappe-paths (file system paths), hvilket således giver en måde for brugeren at gruppere sine creations på. Man kan godt ændre en entKey. Men dette skal i så fald gøres uden at ændre defStr samtidigt med. For måden man redigerer defStr er nemlig ved at pege med entKey'en (eller 'creation identifier'en,' kan vi også kalde det), og sige "indsæt denne defStr for min entitet med denne nuværende entKey/identifier (og hvis ikke der allerede er en, så indsæt en ny en)." (17:19)

%..Nå ja, og jeg overvejede, om man godt kunne overskrive identifiers, og så bare tage den sidste i rækken.. ..Nå ja, og noget andet er: Creation-referencer skal jo så også bruge disse creation identifiers (som så erstattes med entID'er, self.), og her kan man så bruge relative stier også (jeg ved godt at dette er at overengineer'e en smule, hvad der egentlgt kun kommer til at blive brugt af superbrugere, men jeg synes alligevel, jeg bør implementere disse relative stier.. ..tja, måske..).. (17:24) ..Ah, og nej, det giver ikke mening at kunne "overskrive" identifiers, for de grupperes jo ud fra disse, da de jo virker ligesom filsystemsstier, så never mind about that. (17:26) ..(Plus det er også vigtigt for input procs at identifier'en er unik.) (17:27)
%..Hm, men skal jeg så bare generere en automatisk identifier for "anonyme entiteter" (hvis jeg gerne vil have en unik-restriktion, ligefrem), eller..? (17:29)
%..Vent, på den anden side behøver vi måske heller ikke at bruge identifieren i input procs; måske queryer vi bare for creaations i starten, og så bruger vi i stedet entID'er til input procs.. (17:34) ..Hm, måske skal backend'en ikke gøre andet en at gemme og returnere identifiers'ne, og så er det bare appen, der kan gøre brug af dem, ikke andet.. (17:37) ..Ja, det virker fornuftigt nok. Men \emph{vil} jeg så gemme dem i Entities, eller hvad..? (17:39) ..Hm, det virker jo som det mest effektive, og også samtidigt det nemmeste, endda, så hvorfor ikke?.. (17:40) ..Og så er det også bare appens ansvar at advare brugeren om, når denne har to creations med samme identifier, således at brugeren ikke kommer til at redigere gamle creations om til nogle med forkert referencer (og derfor bør appen faktisk virkeligt sørge for, at dette ikke sker, og bør kaste og brokke sig, hvis den prøver at lede efter en creation, og så der er to på samme sti). (17:43)

%(07.10.24, 10:18) Another nice feature about editable entities is that now third-party bots will be able to store bot data as well, and update it whenever they want. And this actually makes me want to let the native bots do the same, not that it is better than storing and querying data from a BotData table in principle, but it means that third-party and nativ bots will use the same protocol, which means that the app can treat them the same (and implement queries for bot data in the same way). The bot data entities should then not actually be standard JSON-object entities, but should just be (binary/text-based) JSON-string entities. These entities thus should not "know" that they are bot data, and thus shouldn't have a class or anything; they should just be the data itself. We can then make it such that.. Hm.. ..Do we.. Ah, we need a derived entity, yes, for each bot dataset, which a bot might have for each individual entity that it scores. And that derived entity.. well, no, it should just be a relation.. Well, and then the relation is a derived entity dependent on the bot. And the object is the entity in question that the bot is scoring (and more), and the subject is then the current-data entity. The bot then only uprates one current-data entity for this entity list, and this is then the editable bot data entity. (10:30)

%I will use such bot data entities to e.g. store the curves, both the curve for an indivudal statement and also the curves of all the scores of entities of a certain class w.r.t. a certain predicate. (10:32)

%I need to also introduce RecordedInputs at some point, even though I will not use it in the beginning. But here all scores will be stored with a date. A user might then erase a RecordedInput/Score, but only if a bit of time has passed since it, such that the community can react if a otherwise trusted user/bot quickly changes a score back and forth. (10:35)

%About the rating scales, I think I will let there be a menu of options (buttons) on the left, where the user can swith between different views, e.g. to see the individual curve or the class curve. Underneat is then the F--A (and beyond) intervals. And on the right you then have the zoom buttons at the top, and below that, above the submit button, we can have the score displayed on several lines.(!) So we could first write the star rating (oh and we are of course talking specifically about the Grading scale type here), which is 0 at the lower end of F, I think. And underneath that you could have a grading, which I will actually write as e.g. 'C + 0.5,' which I will by the way be the middle of the scale, i.e. in the middle of the C interval. And as another example, a grade of \emph{just} A will be 'A + 0.0', and a grade of \emph{just} A++ will be 'A + 2.0'. :) (10:42) Underneath that again, we can then show the "score_err", or the chosen curve width in other words. I think I will actually show plus-minus \sigma instead of \pm 2\sigma; instead of trying to create a new convention, let's just get the general public accustomed to what the standard convention is (not that I planned to \emph{break} the existing convention, only to expand it).. (10:45)

%..Oh, and for the guiding Gaussian curve, I will jut make it gray (perhaps light gray) on the inside of the curve, and maybe even not draw its outline.. (10:47) ..And for the "individual score curve" and the "class score curve", I will store these and draw these as histograms, rather than as smooth curves. (Then we can always implement smooth curves at some point if we want.) The rat_err curve will of course still be drawn as a Gaussian curve, on the other hand (and this is drawn in all cases.) (10:50)

%I also just realized something this morning (in the bath or getting out of bed, I don't remember), which is that I should also think about how to implement my e-democracy system as well, now that I'm so close anyway. And here I'm not talking about the conditional votes and such, which I have already considered (and found that maybe this idea will not be so practical after all; there are perhaps more practical alternatives), but rather the thing about being able to hand over one's vote to "representatives" in matters of voting for proposals. And I think this will not be too hard. I'm thinking that the user just uprates users on a list of ranks, and if the user themselves does not appear on that list, then it is assumed that the user is at the top of the rank. And then when a proposal is voted on, we simply go down the list for the given user, and use the vote (i.e. the score, more precisely) of the first user on that list (including the user themself, typically implicitly at the top) that has given a vote/score for the given Proposal. Now, one could also expand this system such that a user can take some kind of weighted mean when several of their representatives have scored the Proposal, but that can wait until there is a desire for this; this rank-choice-vote-like system can work fine on its own in the beginning, I think. But an extension to this system that we might want to implement at the beginning, on the other hand, is the ability to have several lists of representatives for different topics / classes of proposals. The community is then expected to be able to categorize the various proposals in an honest fashion, not trying to cheaat in order to get a desired outcome. (And if users start cheating, then the voting users can just change..).. Oh right, it should ultimately be the voting users that also decides who decides the proposal classes/catagories (i.e. by ultimately choosing the right bot). And once we have proposal categories, the users can then in principle make a list of representatives for each of these categories if they want to. (And that will generally be norm to have, I think, since we generally also want "division of labour" for the representatives, such that they only need to take responsibility of being active and scoring Proposals of a certain category, not all categories at once.) (11:06)
%..And who then counts these votes / sum up the scores?. Well, that depends on the situation. Again, in order for this to make sense, the users also needs to have some voting power in the first place, outside of the Semantic Network; the Semantic Network can only grant voting powers to users about things related to the network (and the app) itself. For any real-world proposals, the voters that are interesting to include are only the real-world people that actually have a say in that matter to begin with. So who counts the votes, and what are they used for?. they are counted by whoever the people with power w.r.t. the given proposal trusts to do so, and the result is then used by those same people to come to a decision about how to proceed.
%Now, in my e-democracy party, the voters should of course be all people of a given country (that can authenticate themselves online (or get someone they trust to do this for them)). And the results are then used to determine the decisions of the "leaders" hired by the party (which are contractually obligated to follow the instructions, or be fired). (11:13)

%..But a good place to start is also the intra-Semantic Network proposals, where each user who has contributed enough to the network gets a vote, e.g. about what bots to implemnt, what app features to develop, and things like that. (11:15)


%(12:14) Hm, er det bare op til brugeren at lave creationIdent-stier, der starter med et tal, hvis man gerne vil ordne kronologisk.. Nej.. Hm, så hvad gør jeg..? ..Ah, men da vi ordner efter public og.. vent.. ..Hm, kunne is_public omdannes til en tinyint (char), der har flere værdier, og som så kan opdele insert page i grupper?.. (12:16) ..Eller tilsvarende hvor vi bare indfører en ekstra kolonne, selvfølgelig..(?) (12:17) ..Nå nej, med et helt filsystem, så er der allerede rigeligt orden på det, never mind.. (12:20)
%..Hm, jeg har både lyst til at have is_public før crIdent i det index, og har lyst til ikke at have det.. ..Hm, men hvor tit søger man på en anden brugers creations.. Hm.. (12:26)
%..Åh vent, creationIdentifiers skal jo slet ikke være synlige for andre brugere.. Så vi ville skulle bruge et andet indeks her, eller bruge en bot.. Hm.. (12:32)
%..Ja, hvis ikke man skulle bruge en bot, så skulle man jo lave et index over creator_id, is_public, id.. (12:35) ..Nah, vi kan jo også bare undlade at vise identifierne.. Hm, eller bare lave identifiersne public..? (12:36) ..Hm, man har jo ikke behov for særligt mange private entiteter ad gangen, så skal jeg ikke bare springe alle private creations over, når man selecter en anden brugers entiteter. Og så kunne jeg bare lave creation_identifiers public, altså kun hvis entiteten selv er offentlig..? (12:38) ..Ja, lad os sige det. (12:39)

%... (14:57) Nå ja, forresten, jeg tænkte på for noget tid siden (forleden engang), kan ikke huske hvornår, at jeg også kunne lade rektanglerne gå hele vejen rundt om statements'ne, og ikke bare om prædikatet. Så det tror jeg, jeg vil gøre. Og så renderes relationer så som en rektangel (måske med rundede hjørner) rundt om et navneord efter et ':' og et '\rightarrow' symbol, prædikater er et ord eller et '<objekt> \rightarrow <navneord>' compound, som så står efter et objegatorisk ':' (også inden i rektanglen), og statements are så enten tekster, eller prædikater med et subjekt foran kolonet, også altsammen inden i en rektangel.

%(17:13) Since I'm using JSON encodings, I think it becomes a bit of a mess when I then also use backslashes to escape '@'. Let me me from now on instead use '@@' to escape '@' as the beginning of a reference.


%(08.10.24, 11.36) Hm, jeg skal lige have overvejet, om det virkelig \emph{er} det her "app column" system, jeg vil bruge, eller hvad.. ..For alternativt så bruger vi jo bare browserens tabs i stedet, og det kan jo være, at de har bedre memory management eller noget, det ved jeg ikke.. (11:39) ..Men hvad så for mobile..? ..Hm, skal jeg satse på mine "columns"..? (11:43) ..Ja, det skal jeg jo nok.. Og kan man ikke bare gribe ind, hvis nu der bliver problemer med memory og hastighed (hvad der jo nok kun vil, når vi får store billeder og videoer ind..)? ..Eventuelt ved at lave en billed-wrapper, der spørger en React context, om billedet skal inkluderes eller ej.. ..Jo, det må være det.. (11:48) ..Men jeg tror jeg vil kalde det app pages, og så vil jeg bare sørge for at alt i appen bliver inden i disse "pages".. (11:49)

%(11:59) Hm, og lad mig gøre colSpec til en URL path igen, sådan at URL-stien kun bare parses af "app page'en"..
%Hm, og hvad med ReactRouter eller lign.?.. ..Ja, men altså hvor man går tilbage til den kolonne/side, der sidst var i fokus, ikke hvor man går én tilbage i listen a kolonner/sider, som jeg ville have det før.. (12:09) ..Men lad mig bare selv pushe URL-tilstandene i stedet for at bruger ReactRouter, det bliver det nemmeste, tror jeg faktisk.. (12:11)

%(16:11) I'll keep track of the previous currInd, and then popState() rather than pushState() if user navigates back to the previous page. ..Oh, there's no popState().. ..Hm, so do I use React Router..? ..I could also just replace state when going back to the prevInd. Then.. Hm.. ..Or maybe I could just only push a state when going.. Wait, none of this makes sense without the things that React Router does, so let me do that.. ..Ah wait, it is just about making a "popstate" event that prevents default.. ..Or maybe not: It's not cancelable, it seems..

%(17:40) Hm, bør jeg ikke bare bruge brwoser tabs, og så altså bare sørge for at åbne en ny tab for hver page.. nå nej, det er jo det med, at man så får tusind tabs på mobil.. Hm.. ..Hm, eller skal jeg virkeligt gå tilbage og prøve at færdiggøre min useSessionState()..(?..) (17:43) ..Nej, jeg vil gerne bevare mit "søjle"-system.. ..Og må brugerne så ikke bare blive nødt til at lære på den hårde måde, at de ikke må trykke 'tilbage,' i hvert fald ikke indtil vi får implementeret session states..?.. (17:45)
%(18:11) Alternativt kunne jeg bare vise URL'en i toppen af "søjlerne" i stedet for at pushe to location/history, således at brugerne ikke for det falske indtryk af at de kan gå tilbage via tilbage-knappen.. ..(Altså i form af et normalt <a>-link..)
%(18:38) Lad mig gøre det. Og så bliver det et fremtidigt problem at finde ud af, om man kan gøre noget mere nice, og hvordan..

%(09.10.24, 9:23) I just had an idea for my useSessionState: I don't actually need to store the states in a tree structure!.. I can just store it in an array.. Hm, at least if we don't use the 'back up and restore' feature, let's see.. ..But that can also just be done with a single boolean, added to the state, that determines whether the content should be removed or not. So I don't need that feature at all, actually. So yeah, we can just store the whole state in an array.:) The idea is then to let the App component have a React context that says.. Well, or we could also just use a global variable, why not; it only needs to be reacted to once, namely at statup, so using a React context is actually a bit overkill, even if it would be the straightforward thing to do. But no, we can use a global variable or function to get whether the page has just loaded, and if there's a saved session state that needs to be restored. Then my idea, so far, from there is to perhaps make all stateful components add an one-time event to them, or perhaps just a class.. No, they should add an event, \emph{and} they can also add a temporary 'session-stateful' class as well, or some data, if it helps the App get them there states. When the App get to its LayoutEffect, it then goes down the DOM tree and hand each stateful component their states from the saved array. ..Maybe I could even add a key to the states such that they can fall back on their initial state if they somehow get the wrong key (to give us a graceful failure, but still detectable (and we can even console-log the fault)). Then it just hand each component their states in order, namely by calling their one-time event waiting for their states, which each call the given components setState() with the stored state as the input. (9:37) ..And I guess I should make a popstate event that stores all the states in the sessionStorage, then.:)
%This will make users able to navigate away from my app and back, and still get the same state. ..And it will allow us to pushState() and replace states, and when the user hits the back button, we can make sure that they always get to where they were.. (9:40) ..(I was planning on testing whether this is also true with the React Router, but now I might go on with something more important instead..) (9:42) ..Yes, great, so we can get it exactly how we want it. And therefore I \emph{will} definitely use my "column" system. So let me continue with that (resting assured), and focus on the content for now.:) (9:46) ..Well, unless I fear that the useSessionState will be difficult to refactor.. Nah. ..Nah it just replaces.. Oh, because I don't even have to conbine it with dispatch, right? now that I'm only saving the session state on the popstate.. or changestate.. event.. Right, and even if I were, it wouldn't cause much refactoring. But yeah, I think it will just be replacing useState with useSessionState instead.. Yes, no need to entangle it with dispatch; useSessionState should only really differ from useState on the initial call to it. (9:52) ..Ah, I can use the beforeunload event.. (9:59) ..(Perhaps together with popstate, who knows.. ..Oh no, it fires when "any type of navigation is initiated," so I should be fine to use just that.. ..Yes..) (10:02)

%(10:10) Hm, I just had another interesting idea. If we stored all the column specs in the URL path, then users would be able to have a whole row of "columns"/pages open on their bookmarked starting page, and they can also share these with others. ..And then I could display that URL it the top of the individual pages/"columns" so that they can also easily share specific links.. ..Plus it will also make my useSessionState() less critical.. (10:15)
%(10:29) Yeah, I think I might do that. But since it is an easy refactoring, let me still focus on the content for now..

%(11:44) Okay, jeg gik lige lidt i stå, fordi jeg overvejer, hvad jeg skal gøre med account manager.. ..Hm, jeg har ikke lyst til at prøve at gemme sesID, for det første, for så ville jeg skulle lave DBRequestManager om, og tilpasse den løbende, og det har jeg ikke lyst til. Og det var egentligt en fin idé at have den separat, på nær at vi så ikke får opdateret appen automatisk, når tilstanden skifter. Så nok bedre at lade App håndtere tilstanden, som jeg også har gang i nu.. ..Og så må vi jo bare gøre userID til en kontekst, enten sammen med sesID.. Hm, eller også kunne vi bare have en global getSesID(), for denne bør skifte hver gang bruger-ID'et gør.. ..Ja, en global getUserSesssionID(), og så bare have userID i en React context på App.. Ok.. (11:49) ..Eller hvis jeg vil blive indenfor React-patterns, så kunne jeg lave en getAccountData() callback (med useCallback), som så ændrer sig hver gang userID (eller andet data) ændrer sig, og hvor man så altså bare angiver account properties i input-strengen.. (11:52) ..Sure, that sounds nicer.. ..(11:55) Hm, I could also add a setAccountData() callback as well.. ..Nå nej, det kan jo bare være en reducer. (11:56)
%..(12:01) Oh, a useCallback callback that accesses state is not so easy to make.. Let's see.. ..Oh, but I don't need the updated state, is the thing. It's fine the my callback just accesses an old copy of state.. ..Hm, and dependencies are value-based, right, so I can spread the accountData?.. ..Well, it must be, so yes, I must be able to spread it. (12:06) ..(12:14) Oh, I can't spread it anyway, so I'll just spread it manually.. ..Or use Object.values().. (12:15)

%(12:27) Hm, since the maximum TEXT sixe is only 65,535 bytes, shouldn't we just make this the constant maximum, and.. oh no, the potential binary data entities.. Hm.. ..Well, but I could just make selectEntity return the 65.535 first bytes, always, and then make other query procs (and/or PHP handlers) in the future to handle larger blobs.. (12:30) ..Sure. And then I can always just JSON.parse() the defStr, without worrying that is will be.. Oh no, that's not true, but if it fails, then it is some data blob (or potentially a large text).. (12:32)
%(12:42) Ah, my php query handler also assumes now that the whole thing is a JSON object, so I have to keep that in mind, if I want to implement larger texts or blobs.. Hm..
%...(12:57) Hm, I could make an is_json flag for all entities, where that then also implicitely says that the length is 65535 or below, but now that I'm writing this, it also makes my think to just add a TINYINT UNSIGNED type identifier, where 0 is then a JSON object at maximum 65535 bytes, 1 is a text at maximum 65535 bytes, and above that is not specified yet.. ..A datatype_identifier column.. I think I will do so.. (13:01)
%..And then the query handler should just return the defStr rather than the def for ent queries.. (13:04)

%... (14:28) I'll make the datatype_identifier a CHAR(1) rather than a TINYINT, and then I will concatenate it in front of def_str before making the def_hash.
%..Hm, what else was there, besides some new useSessionState thoughts..? ..That'll come to me. ..Oh, or maybe it was nothing, actually.. (Or else it will come to me.)
%My idea now for useSessionState() is to make an event that takes all its "session-stateful" descendants that has no "session-stateful" ancestors between it and the given node that had the event, then contructs an array of that length, and then triggers the same event in each of these "stateful" descendants to provide an object of there state and all the states of their stateful descendants to a callback. The given node then inserts these states into the array, and calls its own callback once the array.. or object, I guess, is ready. This then goes all the way up to the body DOM node, which then has a similar event, but one that stores the resulting state in sessionStorage rather than giving it to a callback. I will then run this process with my "lazy delayed promise," where we queue the saving of the new state each time a setState is called (modified and returned be useSessionState()). And what then when a page reloads? Then useSessionState() will note whether there is a saved state being reloaded and set a boolean if the component should wait for that, rather than draw itself in its normal initial state. Then in a use(Layout)Effect() call, we set in motion a process to restore the state. And I need to think a bit more about that, I think. But let me continue working on the new procs and DataFetcher, and such, first.. (14:40) ..Oh, by the way, I did mean object rather than array, and the keys are those defined in each useSessionState() call.. (14:41) ..Oh, and I think that dispatch \emph{should} then be a part of useSessionState again.. Oh no, perhaps not; we don't need to trigger the lazy saving of the state via dispatch, so no, we can keep the two things apart (and then just hand the modified setState from useSessionState() to useDispatch()). (14:43)

%(16:05) Okay, I think that does it for today, perhaps. Let me then think a bit about how I would reload the page with my useSessionState()..

%... (17:43) Okay, I think I've figured it out with regards to useSessionState(). I plan to hand a callback and a key from each stateful parent to a stateful descendant, potentially by drilling it if there are non-stateful intermediaries (or using contexts for that matter). The callback is used to get an object of a class that holds a reference to the same kind of object of the stateful parent (first stateful ancestor, depending on how you look at it). So I'm essentially handing over an object of a certain class together with a key to each stateful "child," let's call them that, through the props, only I might wrap the object in a callback getter. A parent should then declare all the keys of its "children." And when it saves its and its children's states (and so on recursively) at its parent, it then stores them with these keys. And when the page reloads, a state the look in its parent state for its stored state. (This can also work if the parent moves the child to a completely new location in its part of the DOM tree, by the way.) If none is found, it just falls back on the normal initial state, of course. And the way to save the states is just that each state upon change (and notice that now I've started calling the stateful components for just 'states' for simplicity..) stores itself at the parent state/object. And then they also call the "lazy delayed promise" (which I might call "delayed overwritable promise" instead) to cause the app to save the whole session state, perhaps after a small delay (blink of an eye). And what about dispatch()? I actually \emph{will} then make this part of the same system, like I had it before, such that useSessionState() and useDispatch() again both comes from the same session state library. (And I'll just shelf my current implementation of useDispatch().) And finally, instead of making sure that each dispatch call has access to a node (in order to listen to details about the painted DOM tree), I will just implement a getNode() instead and give it that. This function then simply uses the keys, which I will also make sure to store in the stateful component's className, to navigate and get to the given node in the DOM tree. ..And there we have it.. ..Oh, and I thought that maybe dispatch should use a different set of keys, ones that are defined by the components themselves and not their parent. And yes, I'll do that. So the dispatch keys are not the same as the stateful children keys. Oh, by the way, dispatch is of course also handed down via the parent object, i.e. the object that is passed (by the users themselves, just, and not via a passData() function like I did before) together with the stateful children keys to the children. (18:02) I know this isn't too well formulated, but I hope this paragraph is still somewhat understandable. (18:02)
%*(19:14, 10.10.24) Maybe this idea isn't too great after all; I don't like that the.. well.. Hm..

%(11:00, 10.10.24) I should make an XML tag for expandable texts, where the content is of course the (section) title of the text, and the text's entID is passed as an XML attribute to the tag.
%*(19:25) Oh, I should also make XML tags to insert the best section scored w.r.t. a given predicate, as according to my.. ..what did I call that idea?.. ..something documents.. ..Anyway, the idea about having modular documents exactly like that.. (19:27)

%(15:49) I was about to start on refactoring PagesWithTabs, but then I recalled isLoaded, and my whole close button for each tab. Now, this would actually be a good use for my useSessionState, namely to just keep the state of the tab in memory, but remove it from the DOM tree. So shouldn't I actually implement my new useSessionState now?.. Well, tomorrow perhaps. And then I should just continue with the EntityDataDisplay / InfoPage now.. (15:52)
%(19:16) I'm not completely sure about my recent useSessionState() implementation idea, but in any case, I'm just gonna not remove the opened tabs for now, but still forget about the close button, and thus just let them be open for as long as the column/page is open..

%(19:19) Oh, we could actually get everything to load on the second rerender at once, even without passing session state props, namely if we just make sure to load all potential children on the first 'waiting to rerender with saved state restored' render.. ..That is not such a bad idea (I think).. (19:22) ..No, that still requires us to pass props, so it doesn't really change things much..
%(19:34) Hm, and what about if the child determines its own sKey..? ..Hm, and what about just taking one render at a time for every level of the stateful tree..? ..We could do that for sure.. Hm.. ..And it doesn't matter if we render the wrong.. nah, we should probably just wait for the right state before rendering each stateful node down the tree.. ..And we could just use the first class of any session-stateful node as the key.. (19:41) ..No.. ..No let's let the components set their own sKey on their DOM node, automatically in the hook.. ..Ah, but I still have the problem, then, so no, I would need at least the sKey prop. And a better solution would be to integrate it with React, so let me just let all this be a future problem. Then I'll for now just make sure not to change the URL to give the impression that the users can safely navigate back, and I will also make all external links open in new tabs. That'll be the temporary solution.. (19:47) And then I'll keep my current useDispatch() for now.

%(20:11) Oh, but continuing on the session state thoughts, I could also just use every node in the DOM tree as nodes in the session state object.. ..And then we don't need any sKeys (those will just be the indexes), and then we can still restore the whole webpage from a saved state, if we just wait one render for each level of stateful components down the DOM tree.. But maybe I can't use it to backup and restore part of the tree, let's see.. ..Oh, you \emph{can} back up and remove any component, as long as it is just restored in exactly the same spot in the DOM tree again.. ..Hm, and I would by the way need garbage collecting, since I couldn't rely on the components to clean up after themselves.. (20:17) ..Hm, I could also just back up the individual components in an auxillary structure, and then I wouldn't need them to be restored at the exact same spot, as long as they save the key for their backups in a state.. Yeah, that could work.. (20:19) ..Ah, and instead of having garbage collection, we just construct each new full backup of the app from scratch, and then remove the previous backup once it's done (giving it to JS's garbage collector).. (20:23) ..Oh, and let me then just not implement a backup-and-remove functionality, but let that be a job for the user, which can then just implement this by having a boolean in the state, that if true makes the components remove its content (but keeping the rest of its state as is). (20:25) ..With this implementation, I should then not alter my useDispatch() implementation; useDispatch() and useSessionState() will then be two independent hooks (where you just give the modified setState from the latter to the former if you use them both). (20:27)

%..Oh, and for the PagesWithTabs/TabbedSubpages, I should then also just use that boolean to remove the contents, while keeping the state of the subpage.. (20:29) So I still don't need to implement this new useSessionState() for now, even if it does seem pretty neat with these latest ideas.. (20:30) ..(But yeah, I'll postpone implementing it.)

%(11.10.24, 10:30) Jeg har sovet helt vildt meget i nat.. På trods af at jeg vist gik i seng ved en elleve-tiden, og lå ikke vågen særligt længe. Men jeg fik en rigtig god idé i denne ikke-lange tid jeg lå vågen, og det er, at man fpr grading scale entity lists skal så grading-skalaen ude til ventre.. For grading scale entity lists, we should be able to see the scale to the left, as a second scroll bar, but with gradings on it. And more importantly, the user should be able to drag an element out there and go to the point on the scale, then drop the element where appropratie. It then.. Oh, we could even make something nice where the scroll bar extends to a two-dimensional scrol bar, where the more in to the right the cursor is, the slower the scroll speed.. Anyway, just an idea.. (10:36) Okay, but the point is to hereby make any non-compound (but possibly filtered) entity list a way to score the entities directly as well. It builds upon my past idea of moving the entities around in the list to rate/score them, but where.. Yeah, but where the can now drag to a scroll bar on the right (if I didn't think of this before). I could say some more things, but this idea has gotten me thinking a bit, so let me think for a bit.. (10:40) ..Since grading scales depend on the entities that are scored themselves, and the predicate thus depends on the given subclass, essentially, it would make sense to merge scales of subclasses together for their parent class.. Hm.. ..And I'm thinking that this could potentially be done by scoing where some or everyone of the five common intervals' midpoints lie in the parent scale.. ..It would essentially alleviate us of some ML.. (10:46) ..(Instead of comparing all apples and pears together, we can compare apples to apples and pears to pears, and then compare apples and pears more broadly afterwards to get the full list..)

%(10:49) Oh, I also forgot to mention some ideas from my walk yesterday: User/third-party bots should not be recorded as Users, bot should be categorized as bots; we should make a distinction. ..And what was the other thing that I just thought about?.. ..Oh, that my "conditional votes" could actually be made to.. well, not work, but her's a different thing we could do. Any voter could give special points to proposals that they would be willing to support conditionally, and give another kind of special points to proposal that they would like to bargain in order to get more support for. Then all users can thereby get an overview of which proposals might be able to be bargained through.. (10:54) *(14:25) It might then make sense to say that the points you put conditionally into other proposals also have to match the points you put in proposals that you want to bargain through, but we'll see.. Or maybe it \emph{should} just be conditional votes, where each conditional point then has a partner point set in a proposal that the user wants to bargain through.. But yeah, we'll see about that.. (14:28)

%..(11:00) Hm, when a user has then scored an apple on both the full apple-and-pear scale and the apple scale, then we could say that only one of them counts for the given aggregation bot, or the bot could use both scores (if the user has also scored how the two subclasses compare).. By the way, there are of course also other alternatives to comparing two subclasses; we could also just rate the middle point and the std. deviation of the curve, rather than the midpoints of the interval. Several possibilities.. But now I'm having a few second thoughts about the sorta circular nature of the grading scales, let's see.. (11:03) ..Hm, maybe we should just refrain from letting the semantics assume a Gaussian distrubution, and more to the point, not let the sematics talk about percentages of entities in the list, e.g. by saying "A is 'among the top something percent'".. (11:06) ..Or alternatively we could just add a parent/superclass to the predicate denoting what the context of the grades are.. (11:09) ..Well, we are already essentially doing that as things were, i.e. by choosing the given subject class of the predicate.. (11:12) ..But yeah, maybe we should still try to eliminate the circularity of the predicates.. ..But then again, maybe that's impossible in pratice, since we always compare to other things mentally, when we rate something anyway.. (11:13)

%..(11:18) I think it's good to be able to choose which class that you are rating a given entity in relation to, and then just have automatic merging of the scales. And I think I should keep my grading scales as they were..

%..(11:19) Oh, about the "two-dimentional scroll bar," I can instead just say that when dragging an element up or down, the more to the left the cursor is, the faster the scrolling..

%..(11:22) Oh, I forgot to mention an idea: Whenever we drag and drop an entity, there should appear a bookmark pin on the left scroll bar that when pressed returns the user back to that original position (after which this bookmark is then automatically removed).

%(11:26) Ah, we could make a slider above every entity list where the user has gone into drag-and-drop scoring mode, where the entities are then filtered on the list according to popularity, and where the slider then determines the cutoff..(!..):) (11:28) ..That would actually be awesome..

%(11:30) Okay, and I also had the idea to show, when scrolling, a sideways histogram over the entities on the list, and potentially also a sideways Gaussian curve in the background. Ah, and then the histogram should just always be for the whole, unfiltered list.:) (11:32)

%(11:33) With this, I of course don't need to implement that thing about showing entity examples of the intervals when the user scoring an entity the "regular way," at least not before there is a demand for it, if there ever will be. 'Cause now the user should just be able to click a link to open a column to drag-and-drop-rate the entity instead.

%I should by the way also say that the score_err should still be determined each time the user drops an entitity. And this can perhaps be adjusted on the scale. While adjusting this, we might also color each entity according to the intervals of agreement they have for the score that is currently being submitted, such that the user can also scroll up and down to see what entities that the score "agrees with." (11:38)

%(11:48) Instead of scoring how two subclass scales compare to each other, or rather scoring how a clild scale compares to its parent, the user can just rate samo entities on both the child and the parent scale, that should also do the trick.:)

%... (14:01) I couldn't help but take a midday thinking walk, and I'm glad I did.! The most important idea was this: ..Since all grading scales take root in a class, then we can of course filter all such scales with a predicate of belonging to the given subject class. And this then means that for the grading scale relations, we can just make a class for each such relation, and then get the filtering that way.! So for instance, we can make a class of 'actors of <movie>', and then we can make a relation that has this (also derived) class as its subject class. There we are! Automatic filtering of the relations.. ..Ah, and you can say the same thing about comments; we can have a class of 'comments for <entity>,' and then filter the (graded) comments w.r.t. the predicate of belonging to that class.
%Another thing is that I won't implement the drag-and-drop ratings for the first prototype. (That was by the way the main theme of the thoughts: what to implement and what to postpone.) But instead I will make it so that when you have rated an entity, you get a button to move that entity to its new place in the entity list, or if you are not currently viewing the entity in an entity list of the relevant class for the given score, then the button just open a new column with that entity list. And now about the entity list header: I might not need to implement compound entity lists for the first prototype.. Well, no, never mind, I'm probably going to. And by the way, about the button to move the entity after scoring it, this actually also makes sense even for compound lists, since you can just calculate the new compound score and move it down to that. ..But then we would probably \emph{also} like to have the button that opens up a new.. Oh never mind, 'cause I'm also planning that the.. wait.. ..Nah, let's add the button to open the new column.. About the entity list header, I think I can just make do with a list of predicates for the given class (the most popular one for the class), and for its ancestor classes. And then the user can just press these to add them as part of the compound list, also potentially adding a wight to each one. The pressed predicates will then just be hightlighted, and then the user can just un-select them again to remove then from the list generator again.. (14:15) ..The queried user should then just always be the user themselves, with first priority, and then the standard most-frequent-score bot. That should do for the first prototype. And that's why we only need the weight and nothing more.. Well, no, we also need to select whether we are talking about a filter or a sorting-boost.. Well, but I could just make two lists: one of sorting predicates and one of filter predicates. And all the filter predicates could have that slider that I mentioned before.. Oh, and the weight could also be determined by a slider for the sorting predicates. Great.
%Now, for compound lists, I am not going to show the grading scale for the right scroll bar, of course, but just a plain scroll bar. Oh, but I'm not implementing that for the first prototype anyway, so never mind.
%Now, does it make sense to view likelihood-scored entities in an entity list together?. I actually think it might, and it might make sense to group such factual statement entities according to their 'topic.' ..So maybe I'll just use the same functionality for the likelihood scale as well, as well as the value/quantity scale.. Hm, so do I also make all these predicates have specific subject classes as well?.. ..Yeah, I think I might.. (14:24)

%..Another thing, I have to implement that most-frequent-score bot, and for that I do then need the auxallary data entity for each statement. Oh, but an idea from this walk was: I can just make a cutoff for the grading scale for the first prototye, such that the bot just always uses the same bin sizes for the histograms. And let's just postpone implementing the value/quantity scales, I guess.. Now, whenever a new entity as scored, or whenever its most frequent bin changes, the bot then also have to update the histogram of the whole subject class (such that we get to show that histogram as well to the users). (14:32)

%I will not implement any actual ML, though, for the first prototype, but I \emph{can} implement the lists of relevant statements for a given topic (i.e. a very broad topic, e.g. 'science' or 'music' or 'movies and tv,' and such). And then I can just talk about the future purpose of these. (14:34)

%This leads me to another group of thought from my walk just now: First of all, as an alternative to just uprating statements for a given topic, the users might also have a specific user quality in mind, and then try to score beforehand of how each statement correlated with the given quality. And thereby we could make a very low-cost, calculation-wise, ML, where it is up to the users to try to find user qualities that are not too correlated, and then we just use those as the axes instead of the principal whatevers (I always forget..) 'principal components' I think.. ..Yeah, that was it (the PCs).. And the users are the ones who determines (together) the estimated correlation between each statement and the given "PC" (each user quality axis, rather) that they are under. And then with that we just try to make statistics of how each given statement that we are interested in (not from any of the groups necessarily, but out of all the relevant statement in the database) correlated to these user-defined PC-like axes. And that then gets us a low-cost ML alternative.
%Now, these thoughts also made me think: Maybe it \emph{is} worth experimenting with using the user--user-agreement scores and make ML (regular PC ML) based on that, and then each agreement group PC can just work like the user quality axis for the further calculation, namely in that we then just do statics for each relevant statement in the DB for how they correlate with each of these, let's call them 'aggrement PC groups' for now. I mean, that could be worth trying out, if nothing else then for the experiment of it..:) (14:45)

%..But back to the question, I guess: Should likelihood (factual) statemnts also have specific subject classes as well?.. ..Let me take a small break and "hum" over it (in my head).. (14:48)
%(14:55) Oh, I should absolutely make them have a subject class. 'Cause we might want to use the predicate as a sorting predicate for the class, and not least also as a filter predicate.. Wait, but that only applies to compound statements.. (14:57) ..Well, but most statements ought to be formulated as compound statements I guess; there is almost always a subject, so might as well make that subject explicit as an entity.. (14:58) ..Oh, but I want to group factual (and atomic) statements according to Topics.. (14:59) ..Hm, should we add a 'topic' attribute to all statements, and thereby also to all predicates and relations?.. (15:01) ..We might as well, don't we.. 'Cause that can also help once we start getting more than one bot: Then the users can each in principle choose a bot for each topic.. ..(Of course using mainly the very broad topics, like the ones I mentioned before, and not specific topics (Topics are my new "Categories" in a sense, or rather my old Categories are now changed and divided up into Classes and Topics)..:)) (15:04)

%..But topic might also be something that is upvoted semantically, though, and maybe that's best.. (15:06) ..Hm, I'll take another thinking walk soon.. ..Well, on the other hand, we can both edit now, and we also have the 'better representations' in the near future.. (15:09)

%... (17:31) Ah, that was nice with another long thinking walk. Nice relaxing thinking day, today.. Okay, about whether factual statements should have subject classes, no, they should not. That would quickly get us some infinite recursion. So no, but both grading scale and value scale statements should always have a subject class. ..Or rather the predicates and relations should, but these scales also don't really make sense for atomic statements.
%About whether statements, and therefore also predicates and relations, should have topics, yes, why not?. Let's give then all an optional topic attribute, and that is countinng all the scale types (factual, grading and value/quantity). 'Cause this also means that users are forced to think about topics, even if they don't really use the 'semantic search' part of the app.
%About the 'popular' predicate that we might often adjust the slider for as a filter predicate, that should indeed just be a regular predicate (with the given subject class in question, or the parent class), and not the other popularity/freshness score that I've thought about before. And let's just always make the first filter predicate in the list, and always have the slider out and ready to be used by default. About my freshness-popularity score, I'm thinking that this could be implemented via user-owned entities, where the app can edit these for the user, and add new entities that the user boosts.. Oh, and it might even be private entities, if we allow native bots to access those (and why not?.). I'm then thinking that the format could be a bounded list of entity IDs, preceded by a timestamp *(or rather a date). The fact that the list is bounded is what makes the user unable to uprate more than a certain amount of entities each week. Now the user can however keep list for several weeks at the same time, namely since the bot might still let older weeks scores count to some degree for the aggregated freshness-popularity score. So that's my plan now.:) (17:44)
%About something else: I think that classes should be able to have templ.. well.. Well, it doens't matter too much, since for the first couple of prototypes, the app is ganna make the all derived entities for the users, such that they don't have to construct these themselves. I should by the way have special submission pages for each kind of entity that I'm using for the app, such that only superusers should ever have a need for the insert page, at least in the early days.. ..Oh, and for regular entities, we should parse the expected and optional attributes from the.. Oh no, that should be part of the class'es attributes, as I have already figured out, so never mind.
%About whether we should transport scores between parent and subclasses by having the users rate the comparison, or by deducing this automatically, like I suggested earlier, I think that it is much better to make the user rate the comparison. 'Cause thereby the users also get to essentially give permission to the bot to copy their ratings from a child class to a parent class, and the other way around. So yeah, let me do that. And important to note, as I just mentioned: It can go both ways. So a user never needs to rate the same predicate twice for the same entity in principle: Just choose one class w.r.t. which to rate the predicate, and then let the rating be automatically transported to all ancestor and descendant classes that the user has submitted the comparison ratings for (and perhaps limited to a maximal number of ancestors and descendants per predicate). (17:53)

%So this is great. I should then just focus first of all to get a protoype where the user can navigate to a given class.. Oh, and I should also mention: The fact that classes are rated as a predicate for each entity is also nice as it helps us give as that semantic tree structure of classes and their subclasses/parent classes and their instances.. But back to the point: Users should be able to navigate semantically to a given subclass and see the list of all the entities in it. And then from there they should be able to rate them according to all their relevant (sorting and filter) predicates (also including the factual relation of belonging to that class in the first place). Oh, and by the way, I will not implement any moving around of the entities, I think, but just open a new column where we go directly to the given entity, and where the list is sorted according to the given predicate as well. So that's the only button out of the two I talked about that I will implement for the first prototype. And once I have that, and have the most-frequent-score bot, and potentially the ability to sort the list according to several predicates at once, and the ability to create and uprate new predicates for a class of course, then I already have something.. And from there I should of course start making the Arguments tab, and then also the comments and discussions. (The correlation system might be a little hard, so I could just forget about the intra-correlations to begin with, and certainly the derived score.) And then I could also make the freshness-popularity bot. And the general insert page. Ah, but I should also have a page for inserting standard entities in the first prototype. And when I have all that, then I think I can just about safely say that I have a prototype.. Oh, and of course I should implement user login again (but just make it so that they can create a user with one click..).. (18:04) ..And I should also take the first steps towards making the ML, namely by making it so that users can uprate statements for a given topic, that says a lot about the tastes/opinions of the user in regards to that topic. (And maybe I could also look into making the statement group where the users themselves declare the 'user quality' and also score what they think the correlation is for each of the chosen statements themselves.) But I should not try to start to implement the actual ML (before I go on to try to show off the prototype); that should be taken as a more advanced step. (18:08)

%..And Proposlas and all that could be the next thing after that, but I should focus on the Arguments first, and all the rest that I just mentioned (apart from implementing the ML, of course, which is a later thing).. (18:10)

%..Okay, this seems pretty doable..:) I'm probably forgetting something, but it seems pretty doable..:) I'm taking tomorrow and overmorrow off, by the way. But when I get back and start again monday, I'll hopefully be more motivated by this fact that I don't think it will require \emph{so} much work, 7, 9, 13, to get a showable prototype.. ..I should by the way remember to \emph{not} shy away from talking about the SRC part of the idea, as this is a very important part that helps ensure that whole thing can indeed take off in a big way. (18:14)

%(14.10.24, 9:41) A few things: First of all, I should not make a (derived) class for all relations, but instead make a standard factual relation (for "is a" likelihood scores), and then I should actually make a relation function that takes a predicate (often a string rather than a whole entity) and outputs a relation.. well, actually it is just a format for creating relations, and not actually a function that is an entity itself. But the idea is that you should be able to create relations of the form ': -> Noun + Predicate', which for full statements will turn into 'Subject : Object -> Noun + Predicate'. Then the scale of these statement is strictly of how well the entities fit the predicate, but it is assumed that such list are always filtered w.r.t. the corresponding 'Subject : Object -> Noun' statement. (9:48)
%Another thing: I've probably talked about this before, but I think we should make it a habbit for users who score a product to provide a receipt number, or whatever we call such thing, when they give their reviews and scores. Then the company will get to say, 'hold on a minute, that is not a valid reciept number'. And if they are lying, there should be great repercussions.
%Another thing that I just thought of, I think an important tab should be Topics, at least for all standard entities (including all Topics, of course, we the tab then is semantically equivalent of 'Subtopics,' but we just write 'Topics').
%And this idea is related to another point, which is that a big part of the app, I should remember, is also still the fact that you can go to an entity, and then see all kinds of relevant links. I should then group links into subcategories, that users can provide themselves, by uprating them for the given class. And, oh I forgot to talk about that, but each tab, which is governed by a relation, should also have a list of predicated that it can be sorted w.r.t. ..Let my change line..
%About the relations again, when viewing an entity list from a relation (which is typically done in tabs of an entity page), then it should be similar to viewing a class'es instance list, in that at the top we should also have sorting and filter predicates, and among the score bars of each entity element should be the score for whehter the given entity even belongs to this list in the first place, which for relations will be the standard 'Subject : Object -> Noun' statement, whereas for classes, it's the '<Subject> : <Class> -> instances' statement. But another difference is that the predicates come in two versions for the relational entity lists: There's first of all the same stand-alone predicates, but there also predicates that are then used for 'Subject : Object -> Noun + Predicate' statements.. Hm, yeah, I think so, but I should actually think some more on this.. (10:02)
%And that then explains what I wanted to say for the previous paragraph, namely that the link lists can also even be sorted w.r.t. various predicates, apart from also being divided into several categories.
%..And yeah, this is a very important part of the idea, that I absolutely shouldn't forget.. ..It's really what makes the thing into a 'semantic web', i.e. a "web" of useful semantic links from wherever you stand..:) (10:06)
%..Oh and another important idea, the section tags in my XML texts should also have attributes that explains what the function of the section is. Essentially, if we take my idea of just having section headers that defines the function of the text, and then the text is fetched elsewhere, we can take the idea and just add an initial text for those sections. Thereby the initial section texts can be shown, until somebody finds a better version, and it is then uprated as such, or of course if the user adds a predicate to get e.g. a 'shorter' or 'funnier' version, etc., of the text. (10:11)

%..Now, is it enough to have 'Subject : Object -> Noun + Predicate' statements, to e.g. have 'Subject : Object -> Noun + useful' statements, where the point is that useful is then implicite followed by.. Or we could also say that "Subject is Predicate as a Object -> Noun".. (10:14) ..And we could also write it like 'Subject : Predicate (Object -> Noun)', such that the predicate becomes ': Predicate (Object -> Noun)'.. (10:17) ..I think this will indeed be enough, hopefully so.. (10:17)

%... (14:36) I'm thinking that maybe we should implement subtabs by expanding Noun of relations such that they can be whole paths. For instance 'Object -> Comments/Reactions'. Then you would uprate '-> Comments/Reactions' as a subtab/subrelation for '-> Comments'. And apart from this we are also still using filter predicates, which almost has the same functionality.. Hm.. Let me think some more.. But then when you rate e.g. a reaction comment, then you also see the scales of the "parent class" (which is actually a virtual class) '-> Comments', such that you can also uprate it as a comment in general, while uprateing it as a reaction.. (14:40)
%...(15:05) Okay, rather than continuing coding, I'm just going to dedicate the rest of the day to figuring out more about what to do about relations and tabs, and such..

%(15.10.24, 10:06) I have come up with some new nomenclature. I will remove the AtomicStatements table, and I will call Statements 'Scales' instead, and call Predicates 'Tags' instead. Normal :->Noun relations, which are all scored on the Likelihood scale (type), will still be called relations, and then the ones ordered by another Tag (formerly 'Predicate') I will now call tagged relations (as if you "tag" the predicate to the relation making it part of the relation). ..SO there we are about that, and then I will implement Tabs, which are relations plus a tab title, and a specification of wether the tab is one of the main tabs of a class, or if it is a subtab to another tab. The Tabs furthermore potentially include data about which element components should be used for the EntityList. But I actually think that we can get a very long way with just a standard component that just depends on the class of the entity in question, as well as perhaps some context parameters. Oh, let me mention a good idea from yesterday now: All tab menus should have an 'All tabs' tab, which extends to show all the uprated tabs for the given entity class. And when you press one (in the right place, probably a button), then it goes up and becomes part of a list of temporary tabs (perhaps as an extension of the normal list) for the remainder of the session (and other wise the user will have to uprate the tab instead). The tab is also immediately opened when pressing this button, i.e. when the Tab element component is viewed in this context. And a big point is that the user can then rate the various tabs in this 'All tabs' tab, which means that we don't need another button to go the the EntityList itself, or whatever, to uprate tabs (or add tabs, btw). We can just do it under the 'All tabs' tab. (10:20)
%..Tags can of course also be factual, and in particular the 'is true' tag, which we can use for Statements (which are now just statements like we know then; they don't have any scale types attached to them). (10:22)
%..Oh, and another thing, I will call the unordered entity lists from the "pure", non-tagged relations 'sets.' ..And call them 'entity sets' when this clarification is in order, which probably means for the the most part.. (10:25)
%..When we uprate tags for relations, we can both uprate regular tags thenselves, which is often useful for filtering, but not always for sorting, and then we can uprate tagged relations, of course. If the "pure" relation inside the tagged relation is the same as the relation in question, this is optimal. But in principle, you could also use a "parent relation", or whatever to call it, which forms supersets of the child relation, or in other words, use relations from supertabs of the given subtab's relation. But then again, I do have this process where the users can transport their scores between supersets and subsets, as I've talked about, so why not use it? My idea was, otherwise, that the relation could be shown as './' in the list of tags for the given set, and a "parent relations" could be shortened as '../' (and a parents parent as '../../', etc.). But maybe we just want to always copy the scores to the subsets..(?) (10:32) ..Note that I am here talking about a list of tags, shown in the top of a given tab, which can then be used to order and filter the given entity set, shown in the main body of the tab.. (10:34)
%..Hm, that's a bit hard to say; maybe it \emph{would} be a good idea to use '../' etc. in front of tagged supersets, but I don't know.. (10:37)
%..Oh, by the way, if a Tab ever specifies an element components other then the broad standard one, it has to specify the component for a given class. And it can potentially specify special components for more then one class (although sets of very mixed classes will probably not be too common..). Then for any entity element that has doesn't have any of the classes that uses special components (including when you look at its class's superclasses), then the standard element components is just once again used. (10:42)
%..I think, let me try to implement the '../' (and '../../', etc.) thing. And then it should just be noted, that the app will be slow if the users use very large supersets to sort even a short subset, since the time complexity will be O(n \times m).. (10:45) ..Og, but the are fine for filtering, so I \emph{will} need then anyway, I think, so yeah, let me just try to implement them. Then I imaginge that reguler tags will be rendered as just the Tag entity reference, and tagged relations will be rendered with this path in front, like e.g. './', which is then followed by the tag that "tags" the given relation (which is the relation that underlies the given Tab). (And note that the Tab "knows" its parent Tab, so any given Tab will know all the "parent relations" of its underlying relation.) (10:49)
%..(10:52) We could also render to "regular tags" with '/' in front, of course..
%..Oh, I forgot to elaborate about tags, that these should actually basically always include an entity set in their definition. And it should also be noted that even the instances of a class with be an entity set generated from a ("pure") relation, namely the 'instances' relation.. And I was thinking that the tag should also denote the class of the entities, which is then where we can look up our "regular tags" for the filter and sorting tag menu.. Hm, and is that it: The Tags all have an entity set (which is formed by a relation plus an object) and a class as two attributes?.. (10:58) ...(11:21) Hm, no, this is just for the "regular" tags. The tags that you add to relations don't need to specify anything more about the set then the relation already does.. ..And so you can pull the "regular tags" for filtering from the relation of the Tab itself, via its member class.. ..Yeah, it'll work.. (11:26)

%(13:03) Hm, about useSessionState, you could attach the setState *(and the reducers) to each node in the DOM tree copy, and then you could use dispatch by getting the DOM node, calculating the path, and then look up the setState and the reducers, then execute.. Also, for several nodes in a row with a single child, you can just skip all the 0-indexes in the path. And when a node inside such a single-child line is a stateful component, you just let the last "index" represent the vertical index in this line, instead of the horizontal index between actual node siblings. (13:07) ..Now, dispatch then still needs the ref(Callback) to hook on the n.. Oh, no, not when the event is triggered by a.. well, an event.. Nice enough.. (10:09) ..Okay, this is quite nice.. ..So I don't need ref(Callback) for this implementation: If the user needs it for some reason to trigger a dispatch, then the user can just import and use useRef(). Hm, should I actually try to implement this idea now..?.. (13:12) ..Hm, the state is still only restored one level down the stateful DOM tree at a time, but that's fine; we only don't intend to restore that often (so actually, it's perfectly fine).. (13:14) ..So useSessionState() should just return a bool as well called isReady.. ..I almost feel like I ought to do this (now), I don't know why.. (13:18)
%..(13:20) Oh, a problem is that we need the component to rerender in order to store its state and its setState anew.. ..So this doesn't work well with the React key system.. ..Well, unless the current state path is just stored as part of the state.. ..Hm.. ..Hm, what to do..? (13:26) ..We could store a state ID/key at the stateful node, and then make sure to.. either store a separate tree where the leaf is now the state ID, and where the states are then themselves stored in another structure (that sounds good), or you could copy the states to this tree as well, but no, let's do the former, then.. (13:29) ..Oh, and I could even actually allow myself to make a minimal tree of only the stateful components here, since we update it each time from scratch, i.e. when we save the full state to sessionStorage (at frequent intervals (but only when there are any updates to it)).. (13:31) ..Hm, so each stateful component just knows its state ID, and makes sure to store this on its node, I guess.. No.. No, it should preferably not be needed to be stored on the node.. But then I might run into the same problem with updating the tree, let's see.. (13:33) ..Yeah, I think I need to store it on the node.. which means I need the refCallback.. ..Wait, couldn't I also just return a custom event to put on the node manually, rather than putting the ref there manually?.. (13:38) ...(13:51) Hm, react hooks doesn't work that well with custom hooks, so let me see/think.. ..Well, something else, I could just use a more standard event, and then trigger it automatically; it shouldn't matter that the user can also trigger the event, if we are just talking about saving the state.. ..Ah, it could even be the onChange or onPop.. well, React probably don't have those, but still.. ..Well, then again, I could also just return the refCallback.. ..Or I could use passData() once again, which could then for instance pass/modify the ref prop.. (13:58)
%...(14:17) Ah, I could insert a template (or similar) element \emph{above} the stateful node, i.e. via a passData() like function.. ..And since it would have zero size, I'm free to just use the click event if that helps.. ..Yeah, this would be a decent way to do it, I think.. (14:23) ..(I mean with the template element above the stateful node ..(and when there are several nodes, we just get several templates above the node)..) (14:24) ..The states' IDs can then just be nonces.. ..And you replace node with getNode().. well, perhaps.. ..Hm, I like just using the onClick event, I think I will do that..:) (14:27) ..Well, except that it should have inputs.. ..Well, I can also just use custom events, I guess, but then we need to wait for use(LayoutEffect).. ..Wait, unless the.. Hm, does these nodes even need anything other than the state ID?.. (14:31) ..And perhaps the dispatch key..? ..Hm, the session state tree can be created from just having the template nodes displaying their respective state IDs.. But dispatch()?.. (14:34) ..Well, if we just give dispatch() the caller node, then it can find the state from there, and it can find each of its ancestors states, reducers and dispatch key, as long as these are also stored at the state ID in that auxillary store.:) So therefore we don't need for the templates to show anything other than the ID. This also means that I could just implement the whole thing via the data attributes, or the classes, as long as I just this time also make sure that a single node can hold several stateful components at once (if a stateful component has a stateful component as its only child).. (14:38)

%... (15:41) When a state is mounted, it can add its new state ID to the first node, then look through all its stateful descendants who will have done the same, and then figure out the relative paths in the old tree. Then it can signal to a global struct that it needs its old state ID, providing its new state ID.. ..And maybe it could set an event on itself, or a template next to it, such that it can be.. Oh no, it should just provide the private setIsReady().. (15:45) ..Then when this setIsReady() is called, setting the private isReady state to true, the component can then rerender itself, with a new useEffect (or the same with isReady as a dependency) to make sure to set the data-state-id attribute on the outer/first element again.. (15:47) ..And then it can go through each of its stateful children and give them all the right old state IDs, and call all their setIsReady() functions when this is done, et voila!.. ..This way I don't need to add any template elements to the DOM tree, potentially causing bugs for users trying to traverse it. (15:50) ..(Or when styling it.) ..There should then be a boolean that is set to true immediately when the page is loaded, if their is an old state stored. And this boolean is turned false again when the page is fully re-loaded, and only then is the old state tree removed, i.e. when the new state tree is completely ready, and all the states are restored in the auxillary struct (with the new state IDs). (15:54) ..By the way, dispatch() doesn't need the node as an input with this implementation. If it is missing, then it can just search the tree for the node with the given state-id. And let us therefore turn node into getNode(), such that dispatch only finds the node if it is needed. (15:56) ..I will use a passData() function rather than a refCallback(), though, and when the user uses another ref prop, I will then wrap this when I pass the underlying refCallback, which I intend to pass through passData() (or what I will call it).. (15:58) ..So dispatch() will just be a wrapper around a constant dispatchKnowingTheStateID() function, where the wrapper then also just stores the state ID.. (15:59) ..I think I will call it passStateID() instead..
%..(16:18) Wait, I can get around the isReady state if I just set the actual state to the stored state.. ..well, maybe not, since React will then skip the rerender.. (16:22) ..Wait, no, it won't.. (16:22) ..No, as long as state is a new object, then.. wait.. ..Well, what if the user wants a primitive type for their state..? Hm.. ..Ah, I could just make setState take an anonymous empty object as the initial state when the session state is not ready yet.. (16:30)
%..(16:38) When exactly should we make backups of the state..? ..(I just had a fun idea about something else that I will write about later..) (16:39) ..Ah, I could make a useEffect that queues a backup whenever state or props or contexts is changed.. ..Well, or just on every rerender, then.. (16:42) ..(16:46) Hm, this is work much better suited for mornings/middays..

%... (18:46) Jeg fik lige en idé her sidst på en lang aftengåtur, som er at det måske nogen gange kan give mening at bruge en slags tagged relation, men hvor skalaen så er ækvivalent med skalaen fra et større sæt (altså mængde), hvor objektet så indgår i entiteterne definition i det større sæt. Den specifikke ting jeg har i tankerne er performances, hvilket består af en skuespiller samt en film eller serie, eller episode, som denne skuespiller har spillet i. Når vi så ser på mængden af skuespillere til en spicifik film/serie/episode, så kunne det måske være gavnligt at ordne disse efter performances, men hvor skalaen så er identisk med den samlede skala over all performances.. Jeg skal lige tænke lidt mere over denne tanke, nu nævnte jeg den bare lige.. (18:51)

%..Btw, maybe I should call the scales 'scalars' instead, such that it is easier to distinguish bewteen specific 'scalars' and scale types.. (18:53) ..Oh, no, a scale should be be the one for the entire set, and then the scalar should be each individual scale for each individual entity of this set.. I think.. (18:54) ..So a 'scale' is without the specific subject, and a 'scalar' is with a specific subject.. (18:55) Maybe..:)

%(16.10.24, 11:53) I lay awake for a couple of hours this night. Of coure I thought some more about useSessionState() (which I might call something like useManagedState(), at least for now..). The important ideas was to use refCallbacks to set a data attribute for both the beginning and the end of components that returns an array, or a fragment with several children.. Oh, and what do we do for a.. Hm, it probably only works when returing an element, not something like a modular text.. Can you do that in React?.. ..But to finish the sentence, the point is that you use it set the attribute both for the first and the last element. And for a single-element return, you just set a single data attribute (different one than the two other kinds) on the outer tag.. ..Well, I know that react can return just strings, 'cause I have tried that.. (12:00) ..Okay, so let us just add templates at the beginning and end of JSX that has strings at.. well, at either the beginning and end, and then we add a template there.. ..Or add the attributes to the first and last elements that are not strings, that's actually better. And for components that are pure strings.. Hm, we don't need their state-id for dispatch(), I think, but we'll still need it for backups.. ..Hm, we could wrap any first and last strings in spans.. ..Sure, let me do that, but what about context providers..? (12:07) ..We can just skip those and put the attribute(s) on their inside.. Oh, never mind, we are doing this to the node, so we don't see the providers.. ..I can also just insert the templates when the.. Oh wait, I still need to handle the JSX element to put the refCallback(s). Btw, as I thought of here in the bath *(or shower, I should say) just then, when the elements are not components, we should just set the data attributes directly.. Well, we don't \emph{need} to, I guess, but.. (12:17) ..Okay, let me wrap all beginning and trailing strings in spans, and for empty components, I'll add a template element inside the React fragment.. (12:22) ..Nah, let me also add templates in the beginning or end when we start with a string. Then when styiling we can always just filter out any templates in the selectors to get the normal behavior..
%..Another idea from this night is that I want to add an includeScroll boolean, which then automatically makes sure to store the scroll position (x and y) in the backup as well. I thought of some other things like automatically dispatching ON_SCROLL and/or ON_SCROLL_END actions as well, but that's overengineering it, I think.. (12:28)
%..Oh, and let us put the scroll restoration as part of the refCallback (the first one, in case of lists), which means that I should just always use refCallbacks rather than changing the attributes directly in non-component JSX elements.. (12:31)
%..By the way, as I thought about in the shower (or coming out of it), maybe I should just limit the backing up of the state at the popstate event, rather than doing it continuously.. (12:33)
%..(12:36) Wait a minute, the scrollable nodes are not always the outside node.. Should I just automatically record all scroll positions..? Hm.. ..Nah, maybe the user should just record the scroll positions themselves.. (12:39) ..Well, or we could make at data-attribute that says 'record-scroll'.. (12:40) ..Or rather 'restore-scroll'.. Yeah, I think I will implement that at some point as well..
%(13:12) The sroll positions can be restored when the whole document is fully ready (after having been restored).. ..Hm, will that cause flickering..? No, 'cause we don't render.. Well.. If I can just hide all statefull elements until they are completely ready, then we don't get flickering, and we can indeed just restore all the scroll positions at the very end. Let's say that..
%(13:43) Hm, I want to run the restore processes in a useLayoutEffect, such that I can run them only once..
%(13:47) Let's store a bool in the aux. struct to denote whether a state has been changed from the initial one at all..
%...(14:31) Oh, I need to know (for sure) the order that the ref callbacks are called with this implementation.. ..(I'll get tired early today I can feel; probably didn't sleep very well after having lain awake either..) ..Ah, I can't be sure of the order, so I need to do something else.. (14:36) ..Wait, I know the order, 'cause I'm creating every callback on top of the existing one, so never mind.. ..No, not never mind, let's see.. (14:39) ..Oh, I shouldn't even add refs to React components.. (14:40) ..Hm, there's a forwardRef() function that one could technically use, I guess.. (14:45) ..Oh, the useId() IDs are increasig, so to speak, so we can just sort.. well, we still have the problem of handling React components, though.. (14:47) ..Hm, I could also just wrap the thing in a template, then remove the template on the ref callback.. (14:49) ..Yeah, Ill do that, and just use replaceWith().. (14:53) ..(15:02) Hm, I don't like this too much.. ..(15:05) Ah, couldn't I store it in the auxillary struct when a stateful component is the parent of.. another.. Hm..
%...(15:17) Hm, we could also just render an empty template first for such components.. Hm.. ..Damn, this not such a simple solution to make work after all.. (15:25)
%...(15:36) Hm, I should just return a refCallback for the user to set, right..? ..And the user should then just make sure to call the refCallbacks in the right order, parent through children, if the set several ref callbacks on the same DOM node.. (15:38) ..And can that be made to work with dispatch()..? ..Well, you could just set the same refCallack on all elements that have children that might dispatch.. ..Or just return a second refCallback to put on the last element if you have to return a list rather than a single element.. ..Well, I kinda like the other idea better..
%..Hm, alternatively, I could make a passRefsFromParent() hook.. (15:46) ..(And use a reserved prop to pass the refs automatically..)
%..Or maybe I could do something where I automatically wrap a context provider around it.. (15:50) ..Hm, I could basiclly provide the full "sKey" of the stateful parent to each useManagedState().. (15:52) ..Well, that'll make things easier, so let's do that.. (15:54) ..useDispatch() then also becomes very easy to make, no DOM tree traversing needed.. (15:55) ..(And by "full sKey" I mean a list of the parents' useId() IDs..) ..(We then still only restore one stateful level at a time when the page is reloaded, btw..) (15:57) ..And how do we then construct the backup tree (of state IDs)..? (15:58) ..Hm, constructing the tree should be simple, and restoring an old one..? ..Should also be pretty simple; you just construct it one layer at a time, then.. Oh, but how do you then \emph{find} your stateful children, is the question..? (16:00) ..We could render each component as a template first, with the data-state-id attribute, and once the parent knows its order of the children, it can signal the children to render.. ..Hm, or we could render a shadow, or whatever, of each component, where the components then figures out the order of its children.. ..Where a boolean is set for the "shadow tree", or whatever to call it, such that all stateful components render as empty templates. And since every steful component will have such a shadow subtree tree, the whole tree can then be constructed from these.. Hm, doesn't sound too ridiculous, I think.. (16:05) ..And we can trim each "shadow tree" of all non-React-component nodes as well, yielding us trees of only template nodes.. (16:08) ..No, we can't do that.. Hm.. (16:08) ..Well, I could also just return the ref callback..
%..(16:14) Hm, the refCallback would then only be needed if we back up the state.. ..Which also means that they can be set anywhere within the component, and in any order, funnily enough..!.. (16:19) ..Well, no the have to be set before the next "statefull level".. (16:20) ..And so the order matters, expect that useId() should be increasing when going down the tree, I think.. (16:21) ..Hm, but it still almost sounds like we can skip some components.. Hm.. (16:23) ..No, the ref needs to be called before the next "stateful level," like I said.. (16:23) ..But okay, the user should be able to do this..:).. (16:24)
%..(16:32) Wait, is it too much to ask the users not to create restorable states for parts of lists, but just to move the state to the parent instead?.. ..I shouldn't think so.. And we can also ask them to wrap any single-React-component return in either a div or a span, can we not?.. (16:34) ..And I only need to require this when restore == true..
%..Then I also don't need all those contexts.. well, yes, I do need them, unless I just always ask the users to not return lists or non-wrapped React components for stateful components.. (16:37) ..I think I want to do this (regardless of restore == true or not).. (16:38)

%(17.10.24, 9:21) I've thought about some additions to useManagedState()/useStateManager(). The big thing is to add query() and post() functions, where only the reqStrings (doesn't have to be URLs; can also e.g. be JSON objects) are added to the backed-up state, together with a status. Then you have some other functions to get the results I guess.. But the main thing is that when restoring the state, all query requests (which \emph{can} be made by the POST method, as the user is the one providing the requestManager) are sent again and waited for if their saved status is "successful." Then once all these have returned once again successfully (hopefully, otherwise we need to handle that..), \emph{then} the state is marked as isReady, and the scroll position is restored, for any.. ..well, this is done for the whole document at once, like I've talked about. And for "posts," the succesfull requests are not resend, but rather the returned data \emph{is} stored in the backed-up state. All pending post request are marked as failed, on the other hand, immediately after (or potentially just before, if that is much easier) the document is marked as ready. (9:32)
%..Another thing is that I will store the parent state in the state store, such that dispatch never needs the node after all. (9:33) ...(9:53) But I will still provide a getNode() function to the reducers/methods..
%Btw, I will also implement backUpAndRemove again, 'cause it \emph{will} be useful (for being able to restore any child states of the given state).. ..I'm just calling it 'closed' now.. (10:04) ..(10:08) Hm, but how do you figure out whether "isRestoring" for the children of a reopened state..? ..Hm, should I add React context such that the state just knows their parent immediately, and then the can instantly look up whether the parent isRestoring..? (10:11) ..Yeah, let me just do that, and call it StateIDContext.. or ManagedStateContext, perhaps, for more clarity of where it comes from. And then dispatch never has to look through the DOM tree.. well, except whenever we get the node.. ..Oh, I should still let dispatch have an optional node input, btw.. (10:22) ..Oh no, I of course still need the data-state-id, so forget about the ManagedStateContext.Provider.. (10:23) ..But how on earth do we get isRestoring without it, if I want to be able to implement "closed" (backUpAndRemove)..? ..Maybe I should do both.. ..Or I could also just set the outer isRestoring, but then have each state check up the.. Well.. Oh, we can set a ref callback only when isRestoring.. (10:30) ..That's smart..

%(12:00) Hm, I could just let a stateful state update an array of childStateIDs on every render, except the first one when restoring, before the right state has been found.. ..That saves me from having to compute the structure of the stateful DOM tree in all other instances, i.e. on the popstate event and on backUpAndRemove/closed.. ..On the first render, we can also get the parent.. ..Hm, I don't know.. (12:06) ..Nah, I don't want it to be done on every render, no.. (12:07) ..Oh, if I instead record the react keys.. Hm.. ..can I do that..? (12:10) ..Then I could also render all levels at the same time.. ..Well, then I'm back to passing a reserved prop to all child.. All component children.. ..Hm, back to my old solution, except.. Hm, nah, I'd don't like to have all components call some hook, and wrap their returns.. ..But if I just wrap all child components of a stateful state.. in a monad or a context provider.. Hm.. ..No.. (12:16) ..No, I'll stick to restoring one level at a time, and using data-state-id attributes.. ..Hm, but the keys only work on the same level of the React DOM tree, so I could still pass those via a private prop.. ..Oh, and aren't they always put on HTML elements, not on components?!.. (12:20) ..No, we can put them on components.. ..But I could just disallow that for stateful components, it that helps.. ..Hm, but I guess the whole thing is just to use a tree of child state IDs, rather than an array.. (12:25) ..But nah, we have to search through the DOM tree for all the stateful children.. ..Hm, CSS style sheet need for the DOM tree to be searched several times on each render, or at least each render where there are significant changes, so it doesn't even matter, I can easily search for the stateful children, even on each render.. Hm.. (12:30) ..(12:36) Wait, the stateful children can be the ones who finds their parent state ID and their position relative to the parent, right..? ..Hm, only when restoring.. ..Wait, I only need for the child indexes to be set when restoring and on popstate or closed.. (12:41) ..So I should not at all worry about speed here, that's good to know. Okay, so how to do it? When restoring, the child state makes an initial empty template node with a ref callback that finds it child index, then calls its own setState() with the saved state?.. (12:44) ..(12:50) Hm, I could make a function to set all descendant indexes / node keys of a given node, and then also in the same process see if the given descendent state is waiting for its state.. Or should I make it two similar but different functions..? I will definitiely use a function called once on a node to store all the descendant indexes/keys when backing-up-and-closing a state, and for popstate event, just before saving to sessionStorage. And for backing-up-and-closing a state, I will do this on an initial rerender, then rerender once more afterwards with the emptying passData(). This also means that the user gets to update the state in the same render as "closed" is set to true (i.e. when closing in two (re)renders in total).. But yeah, maybe I want to set the index/key from the child instead when restoring?.. (12:56) ..Sure.. ..No, I don't want that. I have to do it from the parent as well, and then just call the setStates() of each children from the stateStore.. ..So yeah, two similar functions.. (12:58) ..Hm, what when a non-stateful child component waits for something to happen, either a user input or some data fetching, before it adds a stateful component as a child?.. Well, in that case we don't need to restore that state, so it shouldn't be a problem.. ..Yep, no.. ..If the user makes a component that rerenders for no good reason.. well, or for the reason of getting.. no.. for no good reason.. then aren't we in trouble, or rather that user is.. Sure.. In order for the backup to work, non-stateful linking components should not.. Oh, never mind, they can't change their children if the state or the props or contexts don't change (or else they are messing up).. Hm, and is there a danger of a context being.. Nah, if a contexst isn't determined when restoring, the user must know that they cannot expect the state to be preserved in the right way.. ..Restorable children should always only depend on restorable parents, either stateless or with a restorable state, that's just part of the usage.. ..And when fetching fails unexpectedly..? ..Then restoration should stop there, I guess.. (13:09) ..Yeah, let us just then set the state as the initial set and console.log/error a warning.. (13:11)
%... (14:38) Since some stateful components can have statefull children passed to them.. Wait no, we always render stateful components as empty on first render when restoring, never mind.. ..I can just use querySelectorAll() to get all the stateful descendants of a node, and then go through each one and simply push its own stateID to an array held by the parent, where the index in that array then ends up determining the child state index (since the returned NodeList is always ordered in the same "document order").. ..And how do I then structure the tree en sessionStorage.. well, I guess I just save part of stateStore, right?.. (14:43) ..Specifically the state, parentStateID and childStateIDs.. ..Finding a states old stateID is then simply a matter of, when knowing the parent's old stateID, looking in the old (saved) stateStore for the childStateID at the same index, as where the current parent state has the child's new stateID.. ..Easy peasy.. ..And a similar thing applies when restoring a closed state, right.. Where do we store the old sta.. well, we have to save the whole subtree to another place, otherwise it will be cleaned up after being removed.. So I guess I should make a stateBackupStore as well.. ..And then the useEffect that runs just before the emptying render should save the state there, and schedule a cleanup of it.. ..Hm, and how do I correctly trigger the emptying in this last render, do I make.. Ah, I just store a boolean for readyToBeClosed, or something of that effect.. (14:54)
%..I hope I get some more sleep tonight.. ..Oh, it's actually really hitting now, the tiredness (I woke early this morning (forgot to open the bathroom door, or rather wrongfully convinced myself that I had.. (so it got too warm)))..

%(18.10.24, 12:34) Lay awake again for 3--4 hours, but luckily I got some more sleep after that. I just thought, we should actually store a hash and only restore if the data hasn't changed.. ..Hm, or maybe we could hash, but also keep some of the data.. And another thing, maybe we keep the data when it is just a tiny (TINY) string or less.. ..Hm, a quick Google search says that sessionStorage is between 5--10 MB, so it seems that I could just store 5 MB of the data, and then hash all larger query (and post) results after.. well, maybe after the 4 MB mark.. (12:41) ..Yes, let us do that.. ..And for backUpAndClose, we always hash.. (12:47)

%(15:03) Hm, can I set the parentStateID from the parent? How would I then initialize.. hm, the first state.. Hm, should I make the state roots know that they are roots..?

%... (17:39) Hm, overbeviste egentligt mig selv om igen, at det var okay at returnere isReady = false og state = null på første render, når den restores, nu kommer jeg lige til at tænke på, at det jo ikke er så nice hvis.. well, hvis nu man vil putte en useEffect-effekt på en component, hvor effekten bruger state-fulde børn.. Hm, men dette er jo ikke vildt React-agtigt at gøre.. Men.. ..Jeg kunne måske også.. vent.. *Hm, we can just pass isReady along to the linking children..
%..Hm, we \emph{can} set ref callbacks on React components.. (17:50) ..But only when they are declared as classes..
%..How about we just render with the initial state, then set a useEffect to look up the DOM tree to get the index/"sKey", and call setState if there's an old state that should be resotered..? ..No, I should render the empty template first, like I'm doing.. Hm, shouldn't I just do what I'm doing?.. (18:02)
%..(18:07) How about letting the user pass the relative "sKeys" to all its stateful descendants.. Hm, including descendants with stateful descendants themselves.. ..Hm, which would not be dissimilar to using Redux, I guess.. (18:09) ..Or requiring one stateful child per JSX node with a relative key in the returned JSX.. ..And then we could just use the key prop, and pass it.. Well, or wey could just say: One stateful child per component in the return JSX, and then also make sure to.. well, either to pass the keys via contexts, but even better: to set the data-key attribute.. (18:14) ..Yeah, and then we can restore the whole tree in one render, with only that small restriction on the returned JSX..:).. (18:15) ..Oh, and with the restriction as well to always put keys on HTML elements, and always wrap any component in a HTML element, I guess.. ..Nah, but we can require that.. Oh, wait.. No, we can require that I component must not return a list of elements (only a single element) unless it is wrapped in a parent with a single.. Well, or just say that all components must.. well, they must not have siblings before them of variable number, is that enough? Or should we restrict more?..
%..And I can make a hook such that stateless components can also pass relative sKeys.. (18:22)
%..Hm, I have to make a context if I want the tree to be rendered in one go (which would be done only to save the isReady handling..).. ..So let us just do that.. (18:24) ..Hm, I don't think that will actually be very easy, nor very elegant.. ..(18:30) Hm, would it be insane to just require that all components of the "child states" are defined in the component of the parent state?.. ..Or pseudo child states which doesn't actually hold a state of their own, but which has one or several stateful children that does..? ..Which is essentially just another way of saying that the user is responsible for providing the child states with their relative key.. Hm, but we \emph{can} still make a function that automates this, and which then also passes the parentStateID.. (18:35) ..And then we should make this function callable several times, where it will pass different relative keys each time, but will pass the same ones for each render.. ..So a fresh function that does this is returned by the hook each time.. (18:38)
%..And I can do the same thing as I did before, where I just give the whole props to the hook and then parse the keys from that inside it.. (18:40) ..Hm, and I guess we should still set the data-state-id on the nodes to make dispatch work to the same degree..? (18:42) ..Well, on the other hand, if the stateID is now deterministic, we are free to expose it to the users, and therefore free to also just make them pass it along to any children that wants to dispatch actions.. ..And the user is also free to put set it as the id of the node, which means that the node can be gotten.. Hm.. ..Wait, this is exactly my old solution, isn't it?.. ..Almost.. ..Oh, and it doesn't work too well when data needs to be refetched, although one might then just say that the component is just rendered in its initial state in that case.. (18:48) ..Hm, but it seems then that there is not too much reason for combining queries into the hook then.. (18:53) ..No, and that's actually a good thing; then I don't need to implement it all before I can continue.. ..Hm, but it is nice that those actions/reducers/methods are added automatically, but I guess I could just.. ..well, those actions are easy to add, anyway, either as part the hook, or just by importing them and writing them to a reducers/actions object.. ..Oh, or making a class to then extend, of course.. ..Yeah, let's do that..:).. (19:04) ..I could also implement the scroll restoration and the input restoration (which I haven't talked about, but the idea is to do the same as for the scroll, which is to implement that the user can just give a class to the inputs that should be restorable, and then the rest is done automatically from there) via an extension of this reducers/actions class, where there are then an action to back up and an action to restore all inputs.. ..Hm, and I could make the hook fire a standard "ON_RESTORE" action when the component is ready.. (18:09) ..And I could also make "ON_POPSTATE", I guess, which I then just make sure is run on the popstate event..
%(19:20) Yes to the last ideas about ON_RESTORE and ON_POPSTATE. And the class should just be a StateManager class, I think, rather than reducers.. ..Now, let me also mention this: One could just have a global passKeys() function that passes the parentStateID and relative keys to the children, taking props as the input, just thinking out load.. (19:23) ..(19:30) Ah, or a global function to get a (fresh) passKeys() function..
%(19:35) Oh, I've forgotten that you can forward props with the spread syntax. So we can also pass the keys more neatly that way.. ..Simply writing '{...keys[ind]}' or something.. ..well, easier to just use the key prop, and then wrap the whole return, I would say.. ..Except.. Ah, we could use variables as keys.. ..So key={varN}, or key={key[n]} or something.. (19:39) ..Hm, I could make an array/object of all stateful children, like I've thought about before as well, and then pass their.. wait.. Hm, I could map a component to its initial state, but then pass the state directly from the parent if there is another.. ..I'm not sure how that helps, but it's an option.. (I'm not actually completely sure what are the big obstacles to overcome right now, but I guess it's also getting late, although I don't feel very tired yet, luckily enough..) (19:47) ..I should be able to find a way to pass deterministic keys to all stateful children (with some requirements to the users), and I also don't need to set the node id attributes / the data-state-id attributes automatically, the users can just do that themselves, I'd say.. (19:50) ..So I can let the user worry about the key length, I guess.. ..Or I can use indexes, but allow the user to use their own keys instead.. ..Hm, and then we only change the relative  "_sKey" prop to the index, but keep the actual React key as the user chooses it (but require that it should correspond to one in the mentioned array/object when it is a stateful component..).. ..Yeah.. ..Well, no, let us say that the user should just commonly make sure to declare all stateful child components either in variables or in arrays, and then wrap their initialization in a function that passes the "sKey"/"_sKey" prop.. Well, no it should actually not be hidden, since the user can use it for attributes; it should be exposed to the user. So let us just make it a public prop that needs to be provided to any stateful component. And we'll then just make a function to generate each sKey from the parent, and the user then just has to call that function in a deterministic order (which is generally always the case).. (20:00) ..And we can just call the prop the id, why not... ..Well, or the stateID, we'll see, but I do like id.. Yeah, I think: 'id'.. (20:02) ..Oh, but the user is actually free to name it themselves, since we just pass it to the useManagedState hook. Hm, but aren't restoration and dispatch() not two different things now? And can they therefore not be separated?.. (20:03) ..Hm, yeah, I could just make each hook receive the deterministic ID.. ..Hm, I should allow for relative IDs/keys that are strings for sure.. (20:06) ..Well, unless I use a hidden stateID for aux. data storage, and then just use an updateable restoration ID.. ..I'll see about that.. (20:07) ..Hm, I guess I like to have deterministic IDs that are also constant through the lifetime of the child (and which is determined by the parent as well, as opposed to my current stateIDs).. (20:11) ..Sure, why not, so therefore the user gets to fully determine the IDs.. (20:12) ..Okay, so that's the plan. And nice that I can factor out state restoration, dispatch, scroll restoration and queries and implement it separately instead of having to have these things dependent on each other..:) I think I might as well hold *(in Danish we say "hold free," etc., so that's what I mean..) for now, then, and call it a day.. (20:15)
%..Hm, whenever a not-so-stateful component delays in rendering a stateful child passed through props, we might as well just.. ..well, I was about to say render it from scratch, but maybe we'd rather throw an error when that happens, namely to guard against flawed restorations.. ..In any case, if I want to mark throw out the old state tree upon successful restoration, I can always do that, no matter how we define "successful" (i.e. if we allow for states to not be rendered again on the first render, even though they ought to).. well, we should allow this, I guess, do to.. well, no: If we through away states due to the size limitation, then those states are just not expected to render again.. well, their children might. Let me just allow for saved states to not be rendered, I guess.. (20:28)

%(19.10.24, 10:31) Ah, rart endeligt at få sovet godt. Jeg vågnede stadig midt om natten og havde det for varmt, på trods af fuldt gennemtræk denne gang, men faldt hurtigt nok i søvn igen *(efter at have skiftet dyne).

%I guess sets also need a threshold for the score.. So I have to think about what to do there.. Do the users vote on the thresholds for individual sets?.. (10:33) ..Hm, I guess one could also just choose a pretty low threshold, and then filter the sets afterwards.. ..Yes..:) (10:36)
%..Hm, and maybe it would be fine to put a maximum on the sets for the bots, which then forces the users to make divisions of large sets.. (10:37) ..Sure, I like that.. ..One can then always group by realease dates (or submission date, etc.), and one can also always query for more time-divided sets at once, if the app can handle larger sets fine one ones device (including on future devices).. (10:41)

%(10:48) Hm, let's just have the users save the scroll positions and the.. input states in state, although as I just thought about, they could also store them in refs instead, especially if a also let useRestorableState take refs.. Okay, I'll do that..:)

%..Hm, I'm thinking of adding a restoreAfterUnmount boolean, but that then requires the user to set this for each child state, or we need to build a tree also for useRestorableState.. Hm, if they pass it via a context, can we be sure that the children rerenders before the parent state is unmounted?.. (10:56) ..Hm, there is no reason to build a tree; then it is better to just require that descendant ids all begin with the id of any of their ancestors (we actually don't need to specify the seperators here).. ..Hm, but we could also just specify the seperators.. (11:00) ..Let's do that. That then saves both a little time and also some session storage space.. ..(11:04) Nah, let's not implement backUpAndRemove.. ..No, it's better for the users to just hide the thing, if it is important that it gets restored, and otherwise just not restore it.. Well, but do I then want to hide tabs after they are loaded, or do I want to reset them? Should I make the tabs have that close button after all?.. (11:07) ..I think I ought to.. ..But maybe we could extend it to have the same height as the tab..? ..(11:14) Or just hide the subpage.. ..Let me just do that..

%... (15:24) I should give RestorableDataStore to the user, and then also give it a method to store the state down to some level, or rather return the state.. well, I want to use it for pushState().. ..But I guess I should just make a pushState method, then.. ..(15:32) Oh, I thought sessionStorage worked more like the pushState() sessions, but the same state is used for all pages loaded/reloaded within the same tab and the same domian. So you can't go back to an earlier state when clicking the back button, e.g.. ..Hm, I guess I should just put all query-related parts of the state of a component in a normal, non-restorable state.. ..And never use indices for the relative keys, I guess.. ..And and maybe use unique keys for each component, to make sure to prevent a component from getting another components state.. ..Nah, the user should just make sure of this be choosing distinct keys.. ..Well, we could also provide a key.. ..Hm, and I could make useManagedState() which is then just a combination of useResorableState() and useDispatch(), only turning two hook calls with repeated inputs into one.. (15:45) ..And when doing it this way, it's okay to allow all data to be queried anew (although it will probably be cached), and then just let the document be restored in several renders. And then I can use the (smaller) pushState sessions instead of sessionStorage.. (15:48) ..Oh, and I'll just assume, then, that the state always fits the size limit, as long as I just make sure not to save states that have not been changed.. (15:51)
%..Oh, I shouldn't combine the two hooks, since we might also want to use.. Well, the fact that we might need to use two states actually means.. ..Yeah, that useDispatch() should take two states as well?.. Nah, we can also just make it take a combined state object.. But then we also have to give it two setStates, though.. ..Should it just take an array of states and setStates..? That does seem like the best---that will also make it work better with any other potential libary that has hooks that returns their own setStates.. Hm, and let us then just give all these setStates as input to the actions.. well, we have to do this. So yeah, let us that.. ..And let me indeed all them "actions," btw.. (15:59)
%(16:03) Oh, but maybe I shouldn't let dispatch() use keys; isn't it better to let the parent handle the child, and not the other way around, and therefore say that the child should generally not "know" its parent/ancestors?. I think so.. ..I should therefore just make the same concept where the action bubbles up until it is handled by an ancestor.. (16:06) ..Hm, but then one could just handle dispatches via custom events instead, using ref callbacks to set and trigger the events.. ..Hm, instead of using unique keys for useRestorableState(), I \emph{can} just say that one should never let a component be able to give the same key to two different components (on different renders and with different props and states); any key given should only ever be given to the same kind of component.. (16:11) ..And by "key" I mean the relative id part.. ..And if a component has some illegal states when they have a certain set of props, also make the key be dependent on those props (or reimplement to make all states legal).. (16:13)
%..Hm, so should I actually just use my current version of useDispatch(), only without the keys?.. (16:15) ..And with a state and setState as part of the inputs, and where these can also be whole arrays instead.. ..Well, if I'm already making these ids, I could also use.. Well, but it's nice to not let useDispatch be dependent on that if it doesn't really need to be, though.. ..Yeah, so let me just change my current useDispatch() slightly.. (16:21)
%..Huh, I shouldn't start to feel tired now..:\.. (16:26)
%(16:58) Ah, the page will not reload, and the components will not rerender, I'm pretty sure, when going to a state that has been pushed or replaced, otherwise it would not make sense to give the state of the new page to the popstate event. And that also explain how the React Router works, of course.. ...So I guess I should tell the App to rerender on the popstate event?.. (17:10) ..Wait, I \emph{could} also just use sessionStorage.. Ah, no, the problem is that users.. wait.. If users then hit back on their mobile devices, then they can't go forward, but if they go to the site again, they will still have their data restored.. Hm.. (17:13) ..Oh, I guess I could save to sessionStorage whenever the popstate event.state contains nothing, meaning that the user leaves the site.. (17:15) ..Ah, no, preferable only when going back away from the site, and then I can replaceState() instead when going forward..
%(17:25) Ah, the popstate event is fired on the new page, not on the old, just before going to the new one.
%..Maybe I should just use DelayedOverwriteblePromise to update the session state on each setState call for any restorable state.. (17:27) ..Okay, I can initially push a state that when popped.. No.. ..Hm, I guess I just want to use sessionStorage always, actually, and then just keep the App on one state. And when you go to a specific URL, the app just opens a new column  with the given path.. ..Great, and that mean I only have to dust off my DelayedOverwriteblePromise (currently called something else), and use that instead of the popstate event to update the session storage.. Wait.. What was that thing about preventing popups.. Well, okay, so maybe I don't understand the popstate event yet, after all, but maybe I won't need it.. (17:33) ..Ah, as I think I've discovered before, I can just use the beforeunload event instead. (17:38) Great, so almost no changes needed, huh?..:)
%..Okay. I think I will make the changes to useDispatch() tomorrow, though.. Something else to do this evening instead?..

%(18:01) Hm, should't I make a special Table for sets, actually, such that I can also limit their sizes?.. And \emph{should} I actually use a simpler score for these, maybe just a true/false score?.. ..Hm, nah, maybe the likelihood scale is still the best.. (18:03) ..Well, or I could make a relevancy scale type.. (18:04) ..Hm, or maybe just use the grading scale.. ..Well, except that we never expect a normal distribution here.. (18:05) ..Hm, I could use a likelihood specifically of "how likely is is that a user, on average, will find the entity relevant to the given set".. (18:08) ..Sure, I like that.. (18:08) ..And maybe I should just make limited Tables, and then make an unlimited Score table as well, where we only have the secondary index, promoted to the primary index, and where the users thus can't query it for entity lists/sets.. (18:11) ..I do like that.. (18:11) ..Well, no, we should instead just put a limit on the entity lists that can be queried, and just remove the offset. This way.. Well, it does mean that we get some index entries that will be unused, but we can always refactor the index as another table, I guess. But we then don't need to store entities redundantly.. Wait, if I split the table in two, then I also don't need the secondary index on the table for querying entity lists. That's actually nice.. ..And we'll just maintain the EntityLists table automatically, meaning that users just need to input their score once (for the Scores table, and then if their are room, or the score is sufficiently high, it is then also copied to EntityLists). I like this.. (18:17) ..Great..

%..And again, a 'Scale' is defined by either just a Predicate, including Compound Predicates each defined by a Relation plus an Object, with the semantics of being "likely to seem relevant for that predicate for a user on average, or it can befined by that \emph{and} and another scale-defining predicate gives the scale any other kind of semantics that we might want, but where the semantics still only applies to the set of entities that are "in" (or "relevant to") the given 'Entity Set' which is defined by aforementioned Predicate. (I should have defined an 'Entity Set' first, and then used it to define the 'Scale' saying that this is defined by a Set and potentially a scale-defining predicate as well that changes the semantics of the scale.) A  'Scalar' is then a Scale plus a Subject, which then gives you a specific thing to be 'scored.' A score is provided by the individual user/bot to a Scalar, giving the user's opinion, or the bot's aggregate of opinions, of the matter in question. (18:31) Oh, and an Entity List is when the Set is ordered as well, potentially by a specific Scale, or by several Scales, taking some aggregate as the final score. The Entity Lists might also be filtered as well. (And we will for instance often filter by the relevancy scores, such that irrelevant entities below a certain user-specified/-modifiable threshold are removed.) (18:34) ..Hm, and shouldn't I call the special "scale-defining predicates" something else rather than 'predicates'..? ..I could call them Tags..? (18:35) ..Yes, I think so.. ..Okay, let me call it a day.. (18:37)

%(21:26) Hm, maybe we shouldn't put a limit on the entity lists.. ..Yeah..
%..Well, maybe sets should declare their own limits, somehow, or users/bots should declare these limits.. (21:37) ..Well, then it's better for the bots to just make a cutoff (a lower threshold) instead.. ..And we can just store the length of any entity list if we want that, which  we might..

%(20.10.24, 11:09) Okay, I think there might be a good way to have more absolute scales instead of my kinda circular grading scales.. I think rather than voting on the.. Well, I think that whenever you rate a predicate for something, you just always see the sliders for all the most relevant classes (super- and sub-).. And I think we can use a semantics of something like "how much do you.. does it improve your life," or something slightly along those lines.. These idea are quite fresh so let me think.. (11:13) ...(11:29) Hm, it's hard, but a very important thing to figure out.. I think the grading scale might be useful, still.. ..(Ha, funny thought: You could measure everything in money, or work time..) ..Hm, or recommendation points, or.. ..Wait, recommendation points, that might actually not be a bad idea..!..(?) (11:37) ..No, not bad at all..!.. (11:38) ..Where one always "recommend to like-minded people" (or "to one self," in principle).. (11:40)

%(12:39) Ha, I forgot how 'this' works in JS again.x) ..Might not be the last time..x) ..Hm, one could refactor and put SCROLL_INTO_VIEW on the AppPages instead.. (12:48)
%...(13:00) Tror jeg tager en gåtur nu. Når jeg kommer tilbage debugger jeg OPEN_PAGE fra EntityReference, og begynder så at prøve at bruge useRestorableState.. Men på turen vil jeg så tænke over de her recommendation points, etc. (og måske når jeg kommer tilbage)..

%Btw, I never mentioned this, but I intend to use purely XML for the XML texts, and not my @-syntax. So EntityRefs will be implemented as an <a>-like tag.. (13:04)

%... (15:14) Okay, some important ideas: First of all, just like there are now private entities, there should also be private entity lists and ratings. This can in particular be used to note what e.g. videos, and such, the user has seen (although one could actually also use private entities for this..). And in general it can be used when you want your scores to contribute, but you don't want them to be public.. It sorta goes against the open-but-with-anonymity idea, but maybe the private scores \emph{will} be useful. It also goes against the prospect of making it a distributed database. But on the other hand, with my SRC idea, maybe it doesn't need to be a distributed database; maybe the openness can just lie in the SRC concept alone.. (15:19)

%But that was not the most important idea. Second of all.. Hm, let me take a small break, actually.. (15:20) ...(15:41) Hold da op, blev virkelig ramt der.. Får sikkert energien igen lige om lidt.. ..(15:46) Jeg har bl.a. tænkt på, at man kunne have rent relative skalaer, og at brugeren så bagefter ligger en absolut skala over, altså ved at rate correlation og offset. Og så har jeg også tænkt i, at man kan rate'e sub-klasser for sig, og så transportere dette (lidt som det jeg har snakket om før). Men en virkeligt stor idé er så også, at korrelationer bør være på tværs af en hel klasse ad gangen. Til gengæld bør vi så ikke have nogen derived score. Og i stedet bør der være en 'other'-score, hvor man så kan samle de sidste ting, der har impact, men som er for specifikke til at gå i dybden med (ift. den energi man har lyst til at bruge på det).. ..Men disse tanker får mig jo så netop til at tænke (eller faktisk var det den anden vej rundt, men whatever), at der så er disse, og ikke underklasserne man bør dele ind i. På nær at da jeg så kom hjem nu her for lidt siden (eller også var det lidt før, nu bliver jeg lige lidt usikker) kom jeg i tanke om, at vi jo skal indele i underklasser, for korrelationerne kan jo være afhængige af underklassen. Så, lad mig lige holde lidt mere pause, så jeg bedre kan samle tankerne.. (15:54)

%(16:27) The correlations should be from absolute scales to absolute scales, right?.. ..(16:33) Hm, maybe the idea about subclasses divisions alone is more importants on its own; maybe it doesn't need to be altered to include.. well, "Arguments".. ..Well.. (16:36) ..It would also make sense to extend this to not just to subclasses but also arbitrary predicates.. ..Hm, subsets and predicates.. Is there a way I can include both things in this system that I'm trying to come up with..? ..Hm, but maybe the score shouldn't depend on the "Arguments," but instead users should just check how well their correlations fit their actual (overall) scores.. (16:41) ..Hm, one thing is about realtive scores, and the other (Arguments and Correlations) is about absolute scores.. (16:45) ..Ah, so the rating of how relative scales relate to others is really just saying something about the otherwise non-specified offset and correlations of the relative scale, rather than saying anything directly about how the user deems that subset of entities in relation to other subsets. It then only inderectly says something about that.. Hm.. (16:49) ..Yeah, I think I got it. Basically, we just turn every scale into two: A scale with only relative semantics, whose actual, absolute semantics then only depends on.. Well, not on another \emph{scale} but on another \emph{scalar}, just, or rather two scalars if we include the potential offset. And that's it, really; then the user just decides what relative scale to rate a given enitity on, and then makes sure to score the scalars as well that yields the absolute scale. And then Arguments and their correlations can then be a thing concerning the absolute scales only.. (16:54)
%..Oh, by the way, we could let the 'other' Agument be a derived one.. (16:55)

%..Well, the Argument correlations shift the entities on the relative scales as well, though, so the two systems still ought to be dependent, I think.. (16:57) ..Yeah, the thing is quite complicated to figure out.. (16:59) ..Maybe if I first try to pretend that the non-absolute scales aren't necessary.. (17:02) ..Wait, or do we want the possibility to have Argument correlations be about the relative scales..? (17:04) ..No.. (17:06) ..So if we only used absolute scales, we would for each subclass have a group of predicates that gives us the Arguments. The user can then rate all relevant predicates across the whole class, I guess.. Well, maybe some aren't relevant for the whole class, and maybe their correlaton might depend on the subclass.. ..But the user can maybe see a list of all the Argument lists of each relevant subclass.. Hm.. (17:09) ..Including the ones shared for the whole class, not least.. ..Well, maybe none are shared for the whole class, actually, if we think of e.g. movies.. (17:10) ..And I guess I should allow for more complicated correlation functions, and then we should also let the "belongs to <subclass>" predicates be part of it.. (17:12) ..The user can then score all these, and when their score differs from the average of the given user group in question (i.e. governed by the bot in question), the entities can then be shifted on the list.. ..(That requires the app to fetch all relevant entity lists, but that's also perfectly fine. Especially since they only need to be fetched after the user has scored a given Argument correlation..) (17:15) ..Oh, and maybe the "belongs to <class>" predicates can just work as the alternative to introducing the relative scales, such that we might not need these after all..!.. (17:17) ..Hm, and when you then score an individual.. Well, let me actually not count out the relative scales just yet.. But when you then score an individual entity, you then overwrite all their Argument scores, I guess, essentially (not the Argument correlations, though).. (17:20) ..But you are then of course prompted about the Arguments, that you might want to try to judge where your opinions for the individual entity differs from others, resulting in the difference in your score to the one derived from the Arguments, with your chosen Argument correlations, \emph{if} your score differs, of course.. (17:22) ..That's really great.. 
%..And what about the relative scales, then?.. (17:23) ..Hm, could the user not just instead be prompted if they want to change their scores of a given class, whenever they change the score of the "belongs to" predicate for the given class..?:).. (17:24) ..That seems like a really good solution..

%(18:25) We could call them Standard Arguments, those that are shared between the whole class, and derived from the given entity (similar to how a compound predicate is derived from the object).. And then there are the Other Arguments, I guess..

%(20:59) I don't think I need more complicated functions after all. When we use the likelihood scale type for the "belongs to" predicates, we should be fine using linear correlation functions..

%(22:24) Oh, the "belongs to" predicates are actually a way to make more complicated correlations.. second order correlations if you will.. where the correlations depend on the "belongs to" predicates.. And these are then always the header, so to speak, of a group of predicates.. ..Well, more precisely, the correlations/impacts are all multiplied by the "belongs to" correlation times the likelihood, in order to give the full contribution from the given predicate group.. (22:27)
%(22:29) Hm, does it make sense to have the correlations be between relative scales instead?.. ..Wait aren't all "absolute scales" just another relative scale one level higher, i.e. on a superclass?.. (22:34)

%(23:26) Modes..

%(21.10.24, 10:16) These ideas are really great (at least once I finish them), since it will mean that user get an immediate incentive to use and undertand correlations---and will give them an easy understanding of why intercorrelations are unwanted---and it will actually mean that we almost get ML already from the beginning, without any actual ML.:)
%..Now, I've just had the idea that maybe we should just normalize the relative scales for the users, which then actually means normalizing the correlations to any relative scales. That might work, but it also seems a little bit unstable.. ..Alternatively we should just make sure to always superimpose the scores of the given user group.. (10:21) ..Over the users own changes, that is.. ..Hm, but normalizing seems easier.. ..Ah, but it's not necessarily true: I can have the exact same feelings towards most movies, e.g., than another, but then just have more positive opinions of one specific subclass of movies then them.. ..But then one could just impose the correlation to the absolute scale.. Hm.. (10:25) ..Hm, that actually does make sense.. ..Hm, but so does just comparing to the average distribution.. ..Hm, maybe the user could select their own fix points.. (10:29) ..Hm, maybe I like normalizing better.. (10:35) ..Then we technically need to renormalize for each individual scoring of an entity, but I guess that is fine.. ..Well, the user could be responsible themselves to hit 'normalize'.. ..Ah, or the bot could just do it for them, whenever it saves their correlations.. (10:38) ..That's pretty good.. ..Well, the app can also do it as well, such that the point system is understood the same accross users (and you can talk about the relative scores with others).. (10:42) ..I think it's a good thing to let the bots normalize, to essentially make the "votes" more fair. Of course, this system then.. well, it isn't very dependant on the score_err's, the 'agreement curves' in other words, and maybe we don't even need them.. Well, if the whole thing is normalized, then.. ..In any case, it doesn't seem as helpful in relation to this system, where you compare to others.. Hm.. ..And exactly when we normalize all scores for the whole set (even if the user has only scored part of the set; then we just use the average scores for all other relevant scores of the given user group / bot), we \emph{can} safely use the average on a limitless scale without risking that.. Well, no, people can still boost one particular thing unfairly.. But we could also just bin the.. Nah.. ..Yeah, choosing approprate bin sizes, we could bin the scores and choose the most frequent ones. Or better yet, we could just choose a constant score_err for all scores and then use that (in order to achive almost the same thing, only with more precision (and without discretizing the scale)).. (10:56) ..Or we could use the median, that should bassically do the same trick, and it's a simpler way.. (10:59)
%..Hm, am I about to say goodbye to my score_err then?.. 'Cause I'm also thinking that most scorings should be done in this way, viewing the whole scale with other examples around the entity being rated.. But then again, we might need it for absolute scales.. (11:01)

%(11:02) I think that whenever we score an entity, we should actually open a new column/page (and I might just make columns smaller on PC screens.. hm..), and here we have the horizontal rating display at the top, and below is an entity list containing the most popular entities of a large enough superclass, as well as of course the entity veing rated, which you always have in focus. The user can then drag and drop that entity to score it, or they can adjust the slider at the top to jump to a different place in the list.
%Then when it comes to scoring Correlations, it gets a little bit more involved, and also a little bit more exciting..:).. (11:06) Two seconds.. ...(11:35) Okay.. ..In this regard, I'm imagining scoring them for one subclass at a time, although you might also have a list of all the subclasses, and then just change the entity set whenever you select aother one.. Well, let me say one subclass at a time for now. Here it's a bit of the same where you have a score display at the top, but now we plot in all the most popular entities (or rather representative ones) of the given subclass, imposed on the histogram of the superclass. You can then click on the display/curve and go to that point in the entity list as well, seeing the representative entities of the subclass in on color / border color / etc. and seeing the representative entities of the superclass in another color/..., or more likely in a neutral color/... You can then go back to the correlation tab to adjust the carrelation scores, and while you do, you can see the plotted points shift at the top. When you have shifted the points, you can then hover over them to see the entities that they represent.. Ah so these should actually be the "representatives of the representatives," and they should include both those of the superclass and the subclass (in different colors (probably with white or black for the superclass, which shouldn't generally shift, unless there are dublicates..)). And in the entity list, when you look at that, you see all the representatives of both, and not just the representatives of the representatives.. (11:45) ..And yeah, at any point, you can also click on the display to go directly to the tab with the entity list and jump to that point.
%..And note that the user can also score themselves what are the representatives, and also the representatives of the representatives, to taylor the list more to the user themselves..

%..As I talked about yesterday, the correlations are all multiplied by the likelihood of being of a certain subclass, and we also make sure to normalize these factors so that they always sum to 1, i.e. the factors coming from the likelihood scalars of the "belongs to" predicates.. (11:50)

%..Let me also talk about the "modes" that I mentioned: At some point we might also add "modes" to an entity, meaning that it essentially get several copies that can be scored differently. So you could for instance divide a movie into "that movie when watched on a TV screen" and "that movie when watched in a theater." And you could divide a video game into playing it in different difficulty modes, and/or with different challanges (home-made or developer-made (e.g. Steam acheivement challanges)). Then you can give different scores to the entity depending on the mode. (11:55) ..Oh, and this could also account for having e.g. several editions of an entity, e.g. a movie. Then you don't need an entity for every edition of a movie or a game, you can just have different "modes" (or we could also call it something else like "versions"..).. different "versions" of the entity. (11:57)
%..A user group can then select the standard version as the most positively scored one, that is also still widely obtainable. (If there's a better version, but it's not nearly as obtainable as the common version, we could show that.. Well, or you might just make "obtainable" (or price, depending on the given class of entities) part of the overall score, and then always choose the best scored one with the user's correlation settings..) (12:01) ..Yeah, the version shown can just always be the best one for the given user and their preferences, at least at an advanced point of the app.
%..Hm, maybe we should talk about \emph{both} "versions" and "modes" as separate (but similar) things, where the first one is about the "product" that you get, and the latter is about the way in which you enjoy that "product".. (12:04) ..Now, we might group them together as 'versions/modes,' but we might also take them apart and treat them not exactly the same in the app (but they should always have different classes, I guess: Versions and.. wait, not classes.. Wait, how to implement them..?).. ..Well, as entities, I guess.. Well, we'll cross that bridge once we get to it.. (12:08)

%..Then there's a rule of thumb that you ought to adjust your correlations for the more fundamental things first, like e.g. humor, before you go on to adjust the things that rely on those predicates. So you ought to make sure, e.g., that you scale over funny things looks alright (adjusting the relevant Correlations) before you go on to adjust the Correlations of e.g. Movies, at least before you adjust any subclasses where humor is a factor (which it is for most genres, if not all, to some small degree at least).. (12:12)

%..Imagine if we can really get this far so early with the app..!.. This idea of adjusting the correlations while viewing the whole scale at once, and adjusting things on it \emph{via} the correlations, and not just try to make the correlations fit your scores (as a sort of check) after the fact, is really such a good idea, it seems..!.. (12:15)

%..I still have the thoughts about the role of the absolute scales to get back to, let me not forget.. But yeah, this really does seem like such a good set of ideas.. (12:17)

%(12:36) Oh, if we say that all the scales are relative, including those of any given subclass (which can be said about any class except 'entities'), then we should not weigh the the correaltions according to the likelihood of the "belongs to" predicates.. Well, we should do that, but we should also impose a correlation, which is essentially a correlation between two different relative scales (before the final normalization of the scale of the superclass).. (12:40) ..And then like I said yesterday, "what are absolute scales other than relative scales on a higher level".. Or more precisely, I don't think we need absolute scales, 'cause ultimately the relative scale of all entities is a better alternative to an absolute scale anyway, for a given predicate. ..(For the difference between those two things is precisely just a subjective matter, which isn't relevant, really, to anyone other than you. And if it is relevant for someone, then they can just do a quick survey sample to get the a good estmate of the absolute scale, namely if they want they conversion to the actual feelings that people feel.) So yeah, no absolute scales are needed for the core system, I think. But then the big question is, does this really mean that my score_err, my "agreement curves," will nt be needed anymore, with this new approach of scoring things in mind?.. (12:46) ..It does actually seem that way..:) ..It was a very good idea, but it's of course still very nice if it isn't needed after all..:) (12:48) ..Oh wait, the likelihood scales, and all the value scales, are all absolute. And there is does make sense to use the score_err.. Hm.. (12:50) ..So maybe it just isn't needed for the "grading scales" (although I might actually remove the F--A-intervals, since I think the'll actually be more confusing than helpful now), and I should therefore make special tables for these, also such that the median bot knows which scores to normalize.. But should I keep the score_err exactly like is was for the likelihood and value scales?.. (12:52) ..Yes, I think so.. ..:) (12:53)

%..Yeah, these ideas are really awesome.. Imagine how far we are getting at such an early stage.. And how good the system will be for obtaining.. well, essentially the thing that I wanted to achieve with my.. what did I call them?.. questionaires?.. (13:01) The system where you rathered a group of scalars that was representative of ones opnions and tastes and then made ML through that.. Now we might even need all that, and actually jump straight to something that's probably even better... (13:03)

%... (15:15) I think I will make likelihood scales for classes only, and then once again implement relations via, not "categories" this time, but classes. I will then just make sure to implement a query proc where you give the hash of the class/predicate/function instead of the ID for all such tables. So I think there will be a likelihood scales table where classes are always the "predicate," and then a relative scales table, where the "predicate" is always what we might or might not call a 'Tag' (where the Tag then also knows its own subject class (just like a class knows its own superclass)), and then a table for the value scales, where the "predicates" are '(scalar) functions,' which of course also knows their subject/input class. For the likelihood table, we store small-sized score_vals and score_err (they can be small since the scale always only ranges from 0 % to 100 %). For the relative scale, we may just use floats, even though we don't need to go very high or low, and we don't need the score_err. And for the value scales we use a float (preferably a double) for both the score_val and the score_err. Then there's a fourth score table for correlation scores as well. Here we can also use single-precision floats, and I also think we can drop the score_err, just like for the relative scales.. Now, about the correlations, each score should be seen in relation to a group of other correlations. And the user is actually free to choose their groups themselves. But if they choose groups with intercorrelations in them, they'll get weird and misleading results. And if they choose unpopular groups, they also can't rely much on other users scores to create a foundation, since their won't be as many scores to derive the averages from. So it's preferable to choose groups (first of all that makes sense for you, and) that are both popular and without ay significant intercorrelations.. (15:34) ..And yeah, I think the selected group should actually be input as.. yeah, for sure: the group should be part of the correlation score table, namely such that each correlation scale don't have to "know" what group it belongs to. Okay, and that justifies making it a seperate table from the relative scales one.. (15:36)
%..Hm, unless we \emph{want} the scale to "know" the group to which it belongs.. ..We could then just make it derive from the underlying scale that "doesn't know" its group.. (15:39) ..Nah, it makes more sense to add the group as part of the input when scoring the given correlation scales.. ..Ah, but there is the 'Other' correlation whose semantics is dependent on the group chosen.. (15:41) ..And in a sense I think the semantics of the others can also depend on the group. So no, let's wrap each correlation scale, making it a scale that knows the group to which it belongs.. Oh no, since we provide the group to the table, the semantics of the provided score can just depend on that group. So the same 'Other' correlation might be rated several times for the same entity but in relation to different groups, where the semantics of this 'Other' correlation will then (potentially) be different for each time. And the same can be said for any other correlation scale that might appear in more than one group. Okay.:) (15:45)

%...(15:58) Nå jo, der var også lige det med, at ens correlation score.. Nå nej, "afhænger af botten/brugergruppen," det gør den vel ikke.. ..Hm, men det er måske lidt funky med, hvordan man lige normaliserer korrelations-scorerne..

%By the way, kom på en sjov of fin tanke her på starten af min middags/eftermiddags-gåtur her, hvilket var at man jo kunne have en scala af 'hvad der giver lykke,' eller måske endnu mere relevant "happines per time spent." Og måske kunne der faktisk blive noget positivt i, hvis folk begynder at gamify'e at opnå mest muligt lykke i en uge.xD^^ Og selvfølgelig holder man selv sin egen score, men det betyder ikke, at folk ikke godt kan influence omkring dette "game," ligesom man gør det med alt muligt andet, og skabe lidt en "trend" om at prøve at "maximere" (eller rettere forbedre; maximere kan man nok aldrig) sin lykke. He.:)^^ (16:05)

%Og noget andet var.. Hm, hvad var det nu.. ..Nå jo, at jeg måske skulle implementere alle brugergrupper igennem mine "simple brugergrupper" (som så bare ville hedde "brugergrupper," altså hvis alle brugergrupper er sådan). Og så kan nogen brugergrupper have én central moderator, der står for at fordele pointene, og andre grupper kan altså være mindre centraliseret og have flere moderatorer.. (16:07)

%..Nå, normalisering af korrelationsscorer i hver korrelations-skala-gruppe.. (16:08) ..Nå ja, der er jo forresten også korrelationerne mellem en super- og sub-klasses relative skalaer for et givent prædikat.. ..Hm, der må jo gælde lidt samme regler som for prædikat-grupperne.. Hm.. (16:10) ..Hm, måske kan man blande de to ting ved bare at have korrelationsskalaer, som både består af en likelihood skala plus en korrelationsskalar ovenpå.. (16:12) ..Åh, hvilket bringer en anden ting på bane, man skal også i princippet tage højde for skjulte korrelationer, hvor.. Tja, eller vi kan bare kalde dem interkorrelationer, men ja, man skal jo bare passe på ikke f.eks. både at bruge klasse-korrelationer og prædikater som er inter-korrelerede med disse, når man f.eks. laver sine skala-til-skala korrelationsgrupper.. Okay, dt bliver jo lige en anelse kompliceret, så lad mig lige prøve at finde ud af, hvordan man kan simplificere disse ting.. (16:15) ..Hm, korrelationer er vel generelt mellem to relative skalaer?.. Hm, men vel ikke altid for en under- og en overklasse?.. (16:17) ..Tja, nej, nogen gange er mængderne de samme, og måske kan man også bruge prædikater som har en overmængde som deres subjekt-mængder, altså medmindre vi bevidst skal forhindre dette..?.. (16:20) ..(16:28) Hm, man kunne også gøre mere for at bruge at score inter-korrelationerne, således at man bare frit kan score en vilkårlig samling af dem.. ..But maybe it does also make sense to keep the sets apart and only start to go to the predicates once you have a small enough class.. well, but that might be subjective: A person that knows a class well might want to use that relative scale, whereas a person that only knows one entity from a given subclass would want to rate that on the/a superclass's (relative) scale.. (16:32) ..So ideally a user should be able to do both things (although not necessarily at the same time; we can potentially allow ourselves to make them choose, if that helps..).. (16:34) ..To choose how they divide the class before rating each subscale, if they divide it at all.. (16:35) ..Yeah, and then we must be able to derive the results if they had chosen differently.. unless some predicates are dependant on the subclass that you see them in.. Hm, it is a bit complicated for this time of the day.. (16:37) ..I think I can figure it out, though,:) if not today, then tomorrow perhaps.. (16:38)
%...(16:58) Hm, there might not be any point in trying to derive e.g. the correlation ratings at the superclass from correlation ratings at a subclass. It's just like how it is when choosing between predicate groups within a given class: If you choose an unpopular group, then you can't rely so much on the scores of those predicates within the class, since they might not be so thoroughly scored by other users. And in the same way, if you choose an unpopular division, then the same thing can be said.. Well, except that users might also rate the correlation between a predicate for a subclass and the same predicate for the superclass, in which case the scores can be transported.. (17:02) ..Ah, and maybe you could "import" a predicate from the parent class to your correlation group, such that the score does not need to be transported between the two classes..!.. (17:04) ..I like that.. ..Yeah, this is taking form.. (17:05) ..And then for each subclass division, the correlation scores of the chosen group is then normalized such that the relative scale of the given subclass is normalized (to have a certain variance (I probably havn't mentioned this this time around, but that is what I mean when I talk about normalizing a whole scale)). And for the superclass, the user then also scores the correlation for each subdivision (which are divided according to the likelihood scales of belonging to the given subclasses, where those likelihood score are also normalized (to get a sum of 1) for each entity) to then yield the scores for the parent class, which is then also normalized once more (also normlizing the correlation scores of the user in the same process). (17:10) Okay, it's still complicated, but it makes sense!:D.. (I think..:)) (17:11)

%(18:20) Oh, there's actually no need to "import"/use a predicate from the superclass, 'cause.. well, since the users rate the correlations, we can always transport the scores between the relative scales, at least when they \emph{have} scored the interscale correlation.. But it user are gen.. wait no, we can just say that whennever they haven't scored the correlation, possible because they are using a different division of the parent class, then we can just use the average (actually the median) of the whole user group instead for the interscale correlation score, and then transport the score of the given predicate between the sets with that.. (18:24)

%Ooh, now there will actually also be a more immediate benefit (as soon as we implement the following) of scoring the intercorrelations, i.e. the correlations between various predicates used in the correlation groups (which I need to find better namings for, btw), since it can then allow us to better transport correlation scores between different (competing) correlation (predicate) groups.. (18:34)

%(22.10.24, 10:49) I went to bed an hour or so earlier than what's usual at the moment and had some pretty good ideas on top of all this. I of course then didn't sleep for another three hours, but that was definitely worth the time..

%The grand thing is that I believe that we can basically do all things related to both searching and scoring in one type of page/column. Let me try to paint it. When viewing an entity list, then at the top, just below the header of whatever page/column we're in a tab of, we first have the rating display, like I talked about yesterday, with points ("representatives of representatives") from the current subset and from the outer class plottet in. Then we have an expandable menu consisting of several tab bars. First we have the overall filters. (The filter for the class's likelihood cutoff will then be used for any class/subclass that we are currently viewing, below this menu in the entity list.) Yeah, let me also say first that below this menu is the entity list, which can change completely depending on what tabs you have selected. After the filters comes the subclasses tab. ..Well, it's a group of bars concerning classes. At the top you have the current outer class, but you can actually also click an arrow up to view all its ancestor classes in the same group. Below the current outer class you have a list of subclasses. But.. Wait, let me think.. (11:00) ..The thing is, we want to be able to both see the full list of subclasses (ordered according to usefulness, as we almost always do), but we also want to be able to select a given group of subclasses used for scoring.. (11:01) ..Well, maybe we don't need to select a group.. Maybe the correlation, or rather the overlap, isn't much of an issue for subclasses, let's see.. (11:03) ..Well, only if we implement a way to aggregate several scores for a given entity, but here we could just take the mean, then.. Well, or a weighted mean, potentially.. (11:05) ..Hm, that would then mean.. Hm, it isn't too pretty.. ..Oh, unless we simply weigh by the likelihood of an entity being a certain subclass.. Well, I still think the user needs to select a group of sublasses.. ..So let me also think, how about if there's a bar just below the bar where you have selected a given class where you select, not the subclass, but the goup of subclasses to appear in the next bar (more like what I had mind (somewhat vaguely))..? (11:10) ..Hm, if we do the other thing, and if we then average the user weights for various subclasses, we should take the arithmetic mean rather than the median, by the way.. (11:12) ..Hm, maybe this \emph{is} actually the right way to go, scoring weights for each subclass.. (11:15) ..Where we also just normalize the weights to sum to one, of course (such that they are more compatible between users).. ..Now, could this weight double as the effective "usefulness" score for this tab?.. (11:17) ..Oh, and the parent class is by the way also part of the classes that are given these weights.. (11:19) ..Hm, I don't dislike this, and it doesn't have to take very much "space;" we can just show it whenever a user has made changes to two subsets with overlabs, and prompt the user to score the relative weights of which scales should count for more when combining them all into one.. (11:23)
%..(11:32) Hm, this won't be very easy to aggregate.. I might have to do something else with this particular issue.. ..Oh wait, unless we just weigh the correlation scores when we aggregate those..!.. Hm.. (11:34) ..(When summing over each user's correlation score..) ..Hm, maybe (yes).. ..Let me just interject: It's really a beautiful thing that now we automatically get that the underlying scores beneath any combined score (such as 'good') will be more important to score for the users.. ...(11:50) Maybe we should just implement a priority system, such that.. Nah, it's still hard.. ..Maybe we should just choose a specific division, or rather each user should, and then it's just a matter of in which of the subclasses the entity belongs most, with the highest likelihood score, i.e.. (11:53) ..'Cause it's all about determining what correlation predicate group to use for each individual entity in the end, right?.. (11:55) ..Yeah, let's definitely view it in that light.. (11:56)
%..Hm, so to get a bit back to the menu in question, maybe we just also have an arrow down to view the subclasses/subcategories of an entity, but then the group of subclasses that are relevant to scoring firt comes in the next section of the menu. I think so. So note that with this classes part of the menu, we essentially get my tab-based class/category-oriented semantic search that I have wanted before. Oh, and let me just also mention already: Each tab should have a down arrow to the right which opens a list of all the relevant tabs (in the the entity list container below, on top of what was there beforehand (until you go back)). And that is also how you can then score the tabs themselves (apart from picking one to add to the tab bar). Okay, so the next section of the menu, then, will be concerned with scoring, well, unless we should have the order predicate menu next.. (12:03) ..Well, the two are kind of mutually exclusive, aren't they, so maybe we start by having a tab bar to select which of the two?.. (12:04) ..Let's say that for now.. And when it comes to the scoring menu, we then first have a tab concerning the division, we you can see each of the subdivisions (which might just be one, i.e. the full class (meaning no further subdivision)), and you can select each one of them. The down arrow on the right then in this case gives you a list of all possible subdivisions (which are groups of subclasses). But of course, it is a good thing to select a popular one, like I talked about yesterday. (12:08) ..And yeah, as I just concluded, let's indeed make these divisions disjoint, meaning that an entity goes where the likelihood is greatest.. Oh, and entities that does not meet a likelihood threshold for any of the subdivision then goes in a 'rest' subdivision, where you then simply use the correlation predicates of the parent class. (12:10) ..Hm, maybe the ordering/searching menu section should just be the last one.. (12:11) ..Let me take a small pause.. ...(12:27) Okay, let me continue. (Yes, let's say that for now about the ordering section. ..Well, no let it be open for now, actually..) ..When having selected a subdivision, we know see the correlation predicates.. Wait, where is the outer predicate.. Ah! So let us put the ordering section \emph{before} the scoring section, such that we can select the outer predicate to even rate first. And then if you select.. Well, let me first describe this menu section.. We have the list of relevant predicates, and then below that, we also have a list, or rather a tab bar, with what we could "views," potentially, which are constant compound lists that have been upvoted enough be the users for the bot to save them in storage as a sort of virtual entity lists. This is of course only really relevant for sets that are so large that ordering them w.r.t. more than one predicate takes significant computer power.. And below that, or in the same bar, you also have a initially empty list of "views" that the user can adjust themselves on the fly in order to create a more specific order of the set that they want.. (12:35) ..Okay, that does it for that. But when the user has selected a single predicate, and not a view, we also get the scoring menu below that, starting with the aforementioned subdivision bar.. ..(By the way, I might make these "bars" be able to have multiple stacks of tabs on top of each other..) Then once a subdivision is selected, we also get a bar with the correlation predicates, which can be dependend on the subdivision, but which are all w.r.t. the selected single predicate in the ordering section. This is by the way the part that I'm most excited about, the fact that now we'll also get a tab tree of these correlation predicates: Each predicate here is then shown with their correlation scalar, which the user can adjust. And when they do, the whole entity list shifts. (And it is important to always choose a small enough subclass/subdivision before rating such that the entity list.. Oh no, for the larger ones, we just show the representatives instead, never mind..) The enitites on this list is the respresentatives of the parent class, as well as those of the subdivision class, the latter one colored/styled in a way that sets them apart in the list. Also, the plotted points in the display at the top will be the "representatives of the representatives" for both these classes, also colored differently. Both the points in this display and the entity list below shifts when adjusting any of the correlation scores.. (12:43) ..(Time flies..) ..And so with a mixture of feeling inside to predict/feel what correlation is the right one for the user in question, and also be seeing the shifts in the representatives list, the user can then figure out how they think the deem the predicate correlations (which will then be important for all subsequent queries for the given class, inclduing all entities that are not part of the few representatives). Now, before we talk about actually scoring individual entities, whithout which the correlation scores would mean nothing, let us also talk about opening a tab of the correlated predicates.. that's a better name for them: the 'correlated predicates'.. well, and I can probably find an even better name for these predicate groups at some point.. ..opening a tab of the correlated predicates's correlated predicates.. (12:50) For instance, if you have selected the predicate 'funny' as a correlated predicate of 'good' (e.g. for movies, why not), then 'funny' is another predicate that is complicated enough, and more importantly subjective enough, that it might be worth "subdividing" this predicate further into other correlated predicates.. (12:52) ..So below the bar of the correlated predicates pops out a new bar with the potential subdivisions of the current one. Now, the score that you give to the correlation of the 'funny' predicate to the 'good' predicate, in our example here, is independant on this "subdivision" of the 'funny' predicate. But when you score these subdivisions, you can nonetheless still see shifts in the entity list.. well, unless we want to postpone implementing this, and perhaps just make it a link to a open the given "subdivision" predicate in a new column/page with the same class as the currently chosen one, but with this other predicate selected.. Well, maybe that's actually even harder if we want to combine all these things.. Let me think about what it will take to aggregate all these correlation scores, and then hopefully there will be a way to implement both things, i.e. having the list shift immediately, and also to go to a different column to rate the new predicate, and then upon return, the list is then shifted all the same.. (12:59)
%..Well, the app always needs to query for the prefered subdivisions, both of the classes and the predicates (where the user's own scores them overwrites the average/median of the given user group).. So I guess we can make it work.. For then the app just always need to query all these relevant entity lists, i.e. for each subclass and each of that subclass's correlated predicates.. Hm, so do the app then not query the overall funny score?.. I guess not, but then we might not want to subdivide 'funny'.. Or we could mix the scores somehow (back to the same problem, then, though..).. (13:06) ..Hm, one usually do want to subdivide 'funny'.. (13:07) ..Hm, aggregating these redundancies is only a problem for the not-yet-scored correlations of the user, where the app then users the average/median of the given user group.. (13:09) ..But it doesn't really need to be a problem here either, 'cause you can just use the average weights until the user overwrites these.. Or use the highest rated option.. Hm.. (13:10) ..Hm, or we could perhaps try to calculate the uncertainty, and then use the least uncertain score for each individual entity.. (13:12) ..Well no, when we choose to subdivide, it is always better to use the more fundemantal scalars, even if these are more uncertain for some entities: If the user(s) has/have chosen to use that subdivision, then there's a reason for it, and users then just need to dive into these subdivision predicates and make an effort of rating these.. (13:15) ..Oh, and you can always rate the 'Other' option to get the full score that you are after anyways.. ..Of course, "cheating" with this will then make it less usable for other users with different correlation scores, so you should try to make an effort into.. well, also for your own sake: putting the right correlation scores helps you for all subsequent searches in the given class (and not just for the representatives).. Okay, so no weighting; either a class or predicate is subdivided, or it is not.. (13:18)

%(13:45) Hm, what else to say about the scoring.. well, there's the scoring of an individual entity. And here I had the idea that scoring by moving it around in the entity list should actually be the standard, i.e. for these relative scales. When you drop between two entities, the place where you drop it then also matters, meaning that you can see the score inside the entity element change for every pixel that you move it. When place you can of course also adjust the score inside the entity element. And you can select an entity to be moved and then scroll in order to change its position. And you can do the same but click in the scale display on the top to jump to that position. There we are.

%I should also say that each entity you select gets added to some list, perhaps a bar as well, along the tab bars, such that you can always reselect a given entity from there. (The app then remembers where you placed it.) The same applies for selcted tabs; they also get a list sp that you can easily go to them again. This could then by the way also act as an alternative to having the close buttons on the tabs in the tab bars. Instead you have those close buttons in this special list, such that you can close opened (and remembered) tabs.

%And I should say that all class selections should also have a button to insert a new entity, where you then go to a new column to do this.

%Oh, and antoher thing: ..Huh, that slipped my mind.. Well, let me then talk about the fact that when going to the column of an entity, the tabs are then actually all classes, where you immediately get all these same menu sections. And whats more, the subtabs, these are then just the subdivisions of the class! The only difference is that the entity page tabs are of course derived classes, which means that all the given tabs should be pulled from a list of.. well, we could say 'templates' that takes the Onject as input before giving us the actual class/predicate/whatever, and then also from a list concerning the specific derived class as well. (So it requires the double amount of queries to get the list tabs in each bar, generally (but of course we will generally mostly be seeing the derived tabs)).. ..Of course the Info subpage is an exception.

%Oh, now I remember what I was about to explain: Then diciding whether an entity belongs to a class, the users should also always make sure not to add duplicates. And this includes different 'versions' and/or 'modes' of the same entity. And yes, let us thus implement 'versions' and 'modes' as full entities indeed. But yeah, these are then not genereally part of the same class as the more abstract entity covering these versions and modes. Therefore you ought to only find versions and mode \emph{under} a given entity (meaning after you have opened that entity's column/page). Hm, and this then means that when it comes to choosing the score of the best of the versions, let us put a pin in that for now, and just rest assured that we can with all likelihood implement this at some point.. (14:03) ..Sure..

%I should also still use the "more popular duplicate" and the "better representation" relations (derived classes). Oh, and let me reintroduce the thing where a user is able to make an entity constant. But then the user just reserves the right to scrample their creator_id for that entity, potentially in an unrecoverable way. (14:05)

%..Was there anything else?.. It really was a lot to unpack from this night xD^^, although of course some of the ideas here were from today (but mostly only the things that I have brain stromed over; most ideas have been from last night otherwise..).. ..Maybe I will even get some coding done today..;)x) ..But yeah, in all seriousness, let me also take a walk in a minute, and then "hum" over all this some more.. (14:08)

%..Oh, and a natural question is then also: Will this really mean that when it comes to statement Arguments, the correlated statements/scales will then also be scored for a whole class to which the statement belongs at once?. And the anwser is actually: yes. The users should alwasy try to classify a statement before Arguing about it. And if it is a very special statement that doesn't really fit in any exisitng statement classes, well, then you just add a special class just for that (trying to make it as general as possible, still, though), and then you can go on to uprate its correlated scales (right, 'correlated scales,' not (just) 'correlated predicates'..), and then rate the correlations, as well as scoring the resulting scalars.. (14:14)

%... (15:36) The subtabs should be the subdivisions, as these are only a way to make it so that the set of correlated scales, which I might call 'underlying scales' instead, can differ depending on subclass. Rather the subtabs should simply be the (most useful) subclasses, as simple as that.

%Apart from that, I don't have too many other things to add. One thing is that I might keep the term, 'subdivision,' though, both for the subclasses used for rating, and for the groups of 'underlying scales.' Another thing is that if value scales use doubles for their score_val, the correlation scales should also use doubles, in principle. But let us in actual fact instead just limit ourselves to single-precision floats, and just say that the users just need to define an aprropriate unit if we ever need to go above what a float can manage (with the most standard unit of the given value). So yeah, single-precision floats for the correlation scores it is.. (15:42)
%..And about the value scales, here I will also show the representatives and all that in an entity list, 'cause it might be helpful in special cases, who knows, and we have already implemented the whole thing do to the relative scales. And then I will also show the (adjustable) agreement curve in the scale/scalar display at the top. (15:44)
%And a more important thing, it actually does matter what bot / user group the user is using when adjusting the correlations, namely since their choices might of course depend on the resulting entity lists that they create. But let us just say, in order to account for this, we'll just prompt the user and suggest to them that they make sure to rate all the relevant underlying scales if they can, partly since it might help then, partly because, hell, they can always readjust these correlations later, but also because it can help others, and not least others who use a different bot / user group. (15:48)
%Btw, when correlation scores are normalized, it's always to shift and normalize the parent scale.. Well, that goes without saying.. (15:49)

%Oh and that makes me think of, I believe, the last thing, which is that I also need to consider and write about how and when the user rates the "belongs to" predicates, and also including the "belongs to the representatives of ..." predicates.. (15:50) ..Hm, we should put these scoring fields/sliders in the entity elements, actually, and then perhaps the user just adjusts the filter(s) in order to see more of the set..? (15:53) ..So three class/"belongs to" predicates: "belongs to the class," "... to the repr. of that class," and "... the repr. of the repr. of that class".. (15:54) ..Wait, if we score.. Hm, I mean, that will then require three separate filters, right?.. (15:55) ..Sure, and then maybe this then also answers how the user gets to see more of and/or the whole class.. I think so.. (15:56) ..Oh, and it also does make sense to diplay the other relevant scales in the entity element (i.e. the thing you can move around to rate (or click on a button to go to the entity column/page)), and especially also all the underlying predicates, as it can giv a quick glimse of why if the entity is in and odd/wrong place in the list, even when the user has adjusted all the underlying scales. Now, all these scales might then still not be immediately scorable; the user might have to click them to go to a new column (or just open a different tab) in order to score them. I think so, at least for all relative scales. But they should be there anyway (once the user has clicked the element to expend this list of scales). (16:00)

%..Otherwise I don't have anything else at the moment. So happy programming, I guess..:) (16:01)

%(16:24) Well, one thing, though, we can't query for all the different entity lists for all the underlying predicates when the set is too large.. So what do we do when the set is too large..? ..For the representatives we are fine, but when we get to actually making searches in the whole class of a big enough class?.. ..Hm, we could of course use the averages/medians, but then the point of scoring all the correlations doesn't really help the user as much.. (16:28) ..Or we could try to utilize subsets of the entities that appears in the top n for the given predicate for most users.. (16:30) For example, the top n 'best' entities in the class, when trying to get as many entities from the individual user's top m lists as possible.. ..So yeah, a subclass of entities where the likelihood of being (considered) part of the class grows with how many users have the given entity amoung the top m for that prediate.. Hm.. (16:33) ..I think this might be the solution; then your correlation scores still very much matter for your search results, but these searches will not be too heavy for you app (or the servers, btw) even for large classes.. (16:34)
%..Yes, and then the thing about having "three filters" will not be true at all. There will stall be three relevant "belongs to" scales to show when expending the entity element, but there will only be one of the corresponding three filters active for the list at any given time (including the list of point plotted in the scale display at the top). And the user then needs some kind of switch to go bewteen the representatives and the whole class (but really the top n entities of that class, i.e. with relation to the given predicate). And I guess this switch is whether you are currently in "scoring mode" or "searching mode".. (16:41) ..Yep..

%..(16:43) Oh, wait, there hould also be some kind of "switch," or whatever, which determines whether we see.. well, whenever in "scoring mode," we see the same kind of entity element, with the same React components, i.e. And when searching, well, there might be several options.. And each tab should.. Ah, so each tab on the entity page just also defines the initial type of EntityElement component, but then you can also shift to the default component, which is also automatically switched to if you go into "scoring mode".. (16:46)

%... (19:29) The bots don't have to make the n cutoff for all sets, and n can also vary. For very popular sets, we might have no cutoff, and for less popular sets, there might be a not so large n. And here I'm talking about these specially ranked sets/lists where the higher scores count have more weight than the lower scores, used for making the class more searchable w.r.t. to its popular ordering predicates.

%When it comes to normalizing the correlations (and shifting them to also give them the same mean always (not just the same variance)), the app should keep track of all the user's chosen subdivisions, such that whenever a scale is shifted, the scales that are correlated with the given scale, above or below, will also all be shifted. The app might store this data in an entity, or it might simply query for it one relation at a time.. (19:35)


%(23.10.24, 11:12) Hm, rather than my SubpagesUnderTabs, maybe I now need to.. Hm, I need several racks of "tab bars" (although the "tab bar" is just one potential styling of the menu).. Should I make them signal their relative id/key to the AppPage/Column?.. ..Well, maybe the AppPage/Column should be the one to hold the state of the menu.. ..Sure, let me do that, yes.. (11:16) ..But yeah, it's gonna be a complicated state.. ..But I can manage.. It's basically just that there's a currently selected tab/choice for each submenu, which gives us the entity list and the scoring display, and then there's also some more data to the state internally, which the outer AppPage doesn't have to know about.. Well, Should I modularize it then.. Nah.. ..Well, if we do, that then means syncing the states, which is not a good thing, I think (don't sync states with each other in React, I'd say..).. ..So no, we can just modularize by dividing the state into several parts, and keep the currently selected choices closer to the surface of the state object.. (11:23) ..And let me try to start simple and then build from there.. ..Also, do I call them Columns or Pages now?.. (11:25)

%...Let's move the scoring display down to just above the entity list (and out of the menu, actually).. ..Well, maybe not..

%... (14:11) I think I will remove the agreement curve for now altogether.. It does so little now, especially when we're also using the median for likelihood and value scales. So yeah, let me remove it, at least for now.
%Another thing: Yes, the app should save the user's scores in a private entity as well. This will then also even allow the user to try different settings without having to create and/or change to different accounts. They can then experiment and use different settings, while still keeping the same scorings for the outside world.
%Now, we might even make it at some point such that the bot just reads the scores off of the user's puplic score entity, instead of the user uprating the normal way. And this then also relates to the first thing about the agreement curve, 'cause when doing it this way, there's also no limit on how the bots can reimplement the users' inputs. (14:17) ..For instance, we might implement the agreement curve (or another kind of "uncertainty") this way, or we might also implement a system where a user also proves that they have the right to score a given entity (maybe by attaching en encrypted receipt of a product, for example).

%About where to start.. Oh, one thing first: The class's likelihood scale should also be an option (the first one) in the ordering predicate menu.
%Let me start by not implementing the bar at the top where the user can select the class to get the right Main Tabs. Let's also ignore the scoring display for now. Oh, about that, this \emph{should} actually be above the menu, in a way, an take the place of the top column/page header when this is minimized (which it is automatically after having scrolled down). Let me then start by simply showing the class entity list ordered by likelihood. Then I can add the subclasses tabs. Then I can start adding the Ordering Predicates. And then I'll see what to add next from there.. (14:26)
%..Hm, and let me just keep calling them 'pages,' actually..
%..Oh, I should also use my useRestorableState() now, so that I don't have to refactor a lot later.. ...I'll do that tomorrow instead..

%(15:37) Hvor kunne det være svedigt, hvis jeg kunne få lov at sove en hel nat her en af dagene, uden at vågne pga. varme (?? på trods af nu fuld udluftning og sommerdyne!..)..!.. ..Fatter ikke, hvor den varme kommer fra, men den må jo komme fra mig (kan ikke mærke nogen "varmebroer" fra naboerne, og min radiator er slukket.. (har tjekket flere gange.x)))..

%(15:51) Oh, I haven't mentioned: I'm thinking of just calling the "relative scales" 'rating scales' instead..

%..Hm, if I'm using single-precision for all scores, and not using score_err anywhere, should I just combine the three/four into one table?.. (15:53) ..We have Likelihood, Rating, Correlation, and..? ..Oh, Value, of course.. ..And all these indeed use the same datatype, single-precision, at least unless I want to be more elaborate about the likelihood scores, which I probably won't.. ..Hm, I'm almost treating them the same rn as well, except for the normalizations, but I'm using the median for all, right?.. ..Yes, but apart from the normalization, I also intended to include the subdivision as a field in the CorrelationScores table.. (16:05) ..I think, let me make four different tables..
%..Oh, we could also, then, add the class to the RatingScores table, which means that we don't have to make a predicate for each class.. I think that'll be quite a good thing.. (16:08)

%By the way, I'm thinking of using the same mean and variance as the IQ scale, meaning that the mean will be 100 (and the std. dev. will be 15 or 20, or whatever it is)..

%(16:29) Hm, couldn't bots now also just use entities instead to implement "RecordedInputs" if they want?.. ..I'd say so.. ..And recording their inputs in a non-editable way (but where the bot/user ID is kept), the bot/user could then also make sure that people don't expect it of making temporary wrong scores.. well, or we can surely find some way to implement this, anyway, by using entities.. (16:33) ..Oh, and entities can also be used as a way to quickly get a lot of commonly gotten data, without having to make several different queries, and importantly, having to make several \emph{subsequent} queries. (16:35) ..Great, yes to that, and also yes to removing RecordedInputs.. (16:35)
%..And what about IndexedEntities?.. I guess we do want to keep that table (or rather to implement it at some point).. Well, I don't need to figure that out now anyway.. (16:36) *(And we can also always implement "RecordedInputs" later as a table, if it becomes useful.)

%(16:40) Hm, about the (Rating)CorrelationScores, I should also make sure to include the offset as well.. ..Hm, and the offset should actually only be for the class (sub)divisions, right?.. (16:43) ..Or maybe not.. (16:44)

%(16:57) Oh, the Main Tabs / MainMenu should really just be the useful derived classes, right?.. Well, yeah, I think I already wanted that, actually.. (So we don't have to switch-case the class for now, nor add the "class tabs" to the MainMenu, since all entity pages will now work the same, in the prototype at least..)

%... (18:28) Hm, kan offsettes ikke bare være korrelationen med klasse-likelihood-predikatet..?.. ..Og så kunne man bare gruppere ud fra dette.. Tja.. ..Hm, mere centralt: kan korrelations-scoren ikke altid bare være i forhold til den ydre klasse (hvilket jo så er 'entities'..), og hvor man så scorer.. Hm.. ..Hm, hvis bare man sørger for at bygge hele træet top down fra 'entities' så er det vel det samme..?.. (18:34)
%(20:01) Jeg tror, jeg skal gøre, som jeg havde tænkt mig..

%(24.10.24, 9:03) It would technically be more elegant and more right to use a system where you instead let predicates enhance or inhibit other predicates linearly (or higher complexity), rather than having this binary system where the predicate group (or rather scale group) kicks in once a certain likelihood of belonging to a class is reached. But this latter system is nonetheless exactly what we should implement in practice, though.:) 'Cause I think it's actually much preferable in practice, at least at the early stage anyway.:) It's simply much easier to think about, and therefore to use (and to build and expand).
%I should also be serious about users saving their correlation scores as data structres in entities. This will be so important for searching as well, namely that you thereby can easily change your preferences on the fly (and without changing your submitted scores), and easily change back and forth. (9:10)
%Now, I'm thinking of how to make it.. well, perhaps a tad less complicated still, and to maybe make the subclasses and subclass divisions wok more together in a sense... ..Hm, maybe just be making it so that the app can automatically select some "representatives" from any given (sub)class, by filtering by 'popularity' (as a predicate) and selecting entities from each significant interval (with a preference of the 'popular' ones).. Then the app can add that to the entity list when going into rating mode.. And here you go to the first superclass that is part of your structure, I guess.. Unless you also use the most popular structure, btw.. Hm.. ..Maybe automatically selecting representatives should actually be a more gneral thing, such that users just score 'popularity,' rather then assigning entities to the 'representatives' (derived) class, or the "representatives of representatives".. Yeah, could be nice to make this more automatic, and just use a single 'popularity' scale instead, let's try to achieve that.. (9:20) ..We could also use a 'known' predicate, where it is then (implicitly or explicitly) understood as known \emph{by} the scoring user. For then the app would also be able to use this predicate to taylor the representatives specifically to the user. And we can still use the average (or rather median) score for users in general, then. So yeah, let's say 'known' instead.. (9:25)

%(9:28) Hm, maybe we could make it easier for the bot(s) if we just assume normal distribution of all the fundamental predicates (the leaves of the tree) when normalizing.. ..They'd still have to know the count, though.. oh wait, no, not when the class is constant.. Yeah, so only for the class divisions.. (9:31) ..And then we could just, after having normalized all individual correlation scores thus, aggregate the collective simply by taking the median, and then normalizing the whole thing at last.. ..Wait, there's something that I havn't thought much about: when dividing a predicate into underlying predicates, we might go to a different class.. Well, or maybe not..(?) (9:35) ..We could make the class division structure (tree) independant on the predicate, and in fact, that would be way easier, so let's do that.. (9:36) ..Wait, about making it "a tad less complicated" like I thought I might be able to, why not just use the 'useful subclasses' for the divisions, and then for each entity you just take the first class where the likelihood threshold for "belonging to the class" is met!.. (9:39) ..And of course, you then in practice also make a cutoff of how many subclasses yu check before simply assigning the entity to the 'Rest' category.. (9:40) ..Hm, the only problem is that more subclasses in total might mean more work for the bot, let's see.. (9:41) ..Well, we can just make a cutoff.. yeah, in principle there's no difference.. ..But maybe we do want for the bot to just use one structure, i.e. the most popular one.. Hm.. ..And maybe it's dangerous as well to make 'useful subclasses' do double duty and be useful for both searching and scoring at the same time.. ..Is there and alternative to aggregating the correlation scores, and maybe instead normalize each individual set of scores..? ..Hm, the user should be able to rate correlation scales for any given (sub)class.. ..And of course it helps to rate the most popular ones.. And maybe 'useful subclasses' \emph{can} do doulbe duty, since the usefulness might not change much at all from when you are searching or scoring, at least for the subclasses that are the most popular (scored as the most 'useful').. (9:54) ..And whenever we are in a class that we want to sort, we could just go up to the first ancestor class that is deemed 'useful' enough, and then look for the correlation scales there, maybe.. (9:56) ..Ah, but how to then determine if we from there go down to \emph{its} most useful subclasses, or not?.. ..Hm, so maybe we should divide 'useful' into two.. (9:59)
%(10:10) Maybe there's a better way.. Maybe I'm getting distracted by.. Nah, the inter-class correlations do matter.. Hm.. ..Yeah.. ..But it might be fine to have less diversity in the class divisions that user use for scoring than, say, the useful subclasses for searching.. Although even here there might not be too much diversity in how to divide a class into its main categories.. ..Wait, as long as the user rates.. Nah.. It's best not to compare apples with pears, and not to try to aggregate the rating scores on an more or less absolute scale.. (10:15) ..But I should remember that 'useful for rating' then more precisely just means 'useful for comparing entities entities in this class among themselves'.. (10:17) ..Since the user can select any given subclass thereof, and then still rate just that (on the parent/ancestor scale).. Hm, actually not just 'useful for comparing entities entities in this class among themselves;' it's also about selecting the right underlying predicates/scales.. (10:19)

%(10:43) Wait, the aggregated scores for the non-fundamental (divisible) predicates aren't really very important, are they?.. So maybe we don't need to sweat so much over normalizing.. Let's see.. ..Hm, ideally, a user just rates a bunch of relevant predicates for a given individual entity. Then other users can catagorize that entity and select what predicates are important (in a specific search and/or in general searches..) for that category, and..(?) (10:47) ..Well, yeah, maybe we don't really need to aggregate correlations much at all..!.. (10:48) ..Oh, we might even make these scores purely entity-based, and then not aggregate them at all, except in the shape of (maybe a selection of) default entities that the bot can provide to the user as a starting-off point for their settings, their 'default search settings' in other words.. (10:51) ..Hm, and then we only normalize the fundamental predicates?.. Or can we actually make those absolute.. Nah, again: absolute means a high likelihood of being very subjective.. ..But yeah, now we're down to just selecting appropriate classes for each predicate, or at least each 'relevant predicate,' which might consist of \emph{both} a predicate and a class.. (10:54) ..And then I can still use the same tree of class divisions and predicate/scale divisions, but now where these are purely entity-based structures, and maybe with no aggregation at all: Maybe user can just uprate the most useful 'default settings' instead. (10:57) ..This makes a lot of sense..

%..Hm, so when scoring a predicate, the class is always chosen by the users, i.e. before rating it as a 'relevant predicate'.. ..And then that is the entity list we show when we rate that predicate.. And we can then still do the thing about creating 'representatives' automatically from said 'known' predicate.. (11:01) ..Great..

%..Well, the user also still needs to score the inter-class correlations and offsets, so we also still need all that. But we then just look at the user's settings to find the relevant parent class, or the relevant subclasses, for the given class..

%..Oh, and the sorting menu can now just be basically a subsection of the whole settings menu, only where your changes aren't saved by default (well, maybe they aren't in the full settings menu, if we even implement that for the prototype). So you just pull the relevant predicates from the settings menu, well, and also potentially query for other relevant predicates on top of those, and give the user the opportunity to add those to their settings. Oh, and now *(inter-)correlation will just be a problem for the individual user, and one that they can likely just fiddle thier way out of pretty easily!.. ..Cool.

%And something else: The database then don't have to know anything about the users' settings. You could even let the app store these settings in localStorage if you really want to be sure, and then the app can just promise (which is easy registerable) not to send these settings to the DB at all (except when deliberately submitting a set of settings).. (11:14) This then gives the system this extremely cool quality of 'not needing the app to gosip/tell about the user in order for the user to get personalized settings'..! (11:16) ..(Of course, if the DB is controlled by the wrong party, the can still figure out stuff about you from your queries, but another part is that the DB shouldn't do this: We should not log anything about our users that they don't submit themselves, other than perhaps some connection-related data in order to prevent DoS attacks.) (11:19) ..(But for non-guilty/not-highly-suspicious users, we should also even delete these logs after only a short while.) (11:21)

%(11:37) Oh, but the user \emph{can} also rate the "compound"/"divisible" predicates themselves, so how to deal with that..? ..Ah, whether you ever query for that score, or if you only query the underlying predicates, that is then just up to the user with their settings.. (11:39)

%..Hm, now I do have three similar-looking score tables. Do I gather them?.. (11:41) ..Hm, especially if we just make predicates refer to their class, perhaps.. Oh, we could call it a Tag, a predicate and class pair.. Hm.. (11:42) ..Or a 'rating'.. (11:43) ..A rating scale, rather.. ..And then we can combine the tables by writing scale_id.. (11:45)

%(12:56) Oh, in a way, a specific search (using predicates) is like creating temporary outer "compound"/"divisible"/whatever predicate that essentially says 'relevant for me now'.. ..And let us actually implement searches this way, such that users can save various searches, only where the users then don't have to submit these predicates publically (and they don't even have to submit them privately)..:) (12:59) ..(And they also don't have to name them..)

%(13:03) These changes (from today) means that we have to implement Arguments in their own way, but that is also perfectly fine.:)

%... (15:13) I should actually make/derive scale entities from classes to make the likelihood scales, even though it's just a one-input function / monad. But it pays to be semantically precise. And luckily, we can always use the secondary key, i.e. the hash, of these class likelihood scales.

%About "absolute scales," in a way, when you define a predicate in relation to a class (or rahter define a scale from both), this \emph{is} an absolute scale, at least with the simplifying assumption that the class is constant. And even though it isn't, that's still as close as we are ever gonna get, anyway, on an absolute scale.:)

%Like I've mentioned before, users should be able to rate paremeters that allows the app and/or the bots to transport ratings of the same predicate from a subclass to its parent class (and perhaps the other way as well, if we implement it). I should implement this pretty early on, but perhaps I'll then just let it be completely governed by the app at first. Then we can always implement it for the bots later, in order to save storage space, as well as upload data.

%When it comes to scoring entities, I still think I will make the drag-and-drop/move-entities-around functionality. And I've also had the idea that users should then be able to click back and forth between levels of "knownness," but all the while keeping the top level out, which are the "representatives." The user can then move these around to rate, after which their coloring/style will shift to mark it as rated. They can also rate the 'knowness' scalar for each, which is also a way to skip rating the entities, as they will then move down many levels, if you give them a low 'known' score. So when all the entities on the list are now either rated or marked as not-very-known (by the user), the user can then submit an go one level lower. And when the user in the future rates a different predicate (or the same) for the same class, the 'known' scorings will then be kept such that the user will see their known entities closer to the top. (But the app still always selects a representative list, such that the representatives are approximately equally spaced.) I think this will be a really good way to get the users to rate a bunch of entities at once---and one with good UX..:)

%About normalizing, all that should be done is that a bot ought to keep two lists for each rating-scored entity lists: one unnormilized and one normalized one (perhaps where the non-normalized one is not queryable, I don't know..) And that's it.:) (15:29)

%(18:34) I should make settings entities modular..
%(18:45) For the thing about the entity lists aggregated with more weight (perhaps even growing quadratically, or more) to positive scores for each user, we just need to implement a bot that does this.
%About transporting ratings, we should also make the app (at some point, but in the early stage) transport ratings fro the "compound"/"divisible"/or-what-to-call-them predicates as well, by the way.:)
%...(19:00) Wait, but maybe the aforementioned type of aggregation isn't really useful.. ..No, I think we just need the median bot for the early version of the app/backend..

%(20:23) I should make a new entity datatype of just the class ID followed by the "template inputs" / attribute list, I think.. ..And I should make a query proc to get the defStrings of a whole list of entities at once, or rather the defStrings until some max length.. (20:25) ..At least at some point, we should do these things.. (20:26)
%(20:32) Ooh, could we not just reserve the rights to edit all entities such that they are transformed to this other.. Wait.. Another idea: Couldn't the aforementioned query proc not just query an entity with this data instead?.. (20:33) ..Yeah, that's what we will do instead.. (20:34)

%(25.10.24, 9:44) When we "scramble"/overwrite the creator_id for an entity, we should actually make a new entity containing an encrypted key to reclaim the entity, with a maximum deadline as well, and perhaps the user can also choose a closer dealine if they want. And the creator_id is then set to the ID of that entity. *(Never mind about the deadline..)
%I should also at some point make a proc to get the (public) entity with the lowest ID above 0 for a given defStr hash..
%..I should also make a UTF-8 (perhaps with datatype = '8') datatype, used for bot and settings data, ect. *Let us actually say datatype = 'u' instead, 'cause then we can potentially use datatype = 'U' for UFT-16.. or unicode..? ..Well, that's just a union of UFT-8 and UFT-16 in practise, as far as I can see. So yeah, let's plan to do this for now..

%..Maybe I'll keep the '@...' syntax for the XML texts, actually, meaning that we should write '@@' to get '@'.. well, \emph{or} we could say '@\n'.. I like that better.. ..Well, no, 'cause when users.. No, I like it better.. (9:57)

%Let's make 'description' an optional attribute, since an entity can also just rely on the description of their class. And let us do this for our derived entities, i.e. not include the description.. (10:12)

%(10:20) Hm, when we implement a feed at some point, I could let the app save a number in localStorage of how far down the user has reached, and then make it possible for them to return to that. And a more important thing: We could let localStorage hold a least recently used list of.. wait no, of "seen" entities.. we don't need that before we get to a point of having more user-taylored feeds, and here it might still work to just save that number.. oh, if we're talking about videos, or something, where the user has seen the thumbnail, but not necessarily the video, it might also make sense to keep a list over "seen" videos/movies etc., and perhaps a least-recently-used list indeed. But all this is only at an advanced stage of the app. ..Well, except that a user would want to keep such lists across platforms, so one culd save it in a privite entity instead, if not in a private entity list.

%(10:57) Let's make a convention of always ordering the attributes from first to last added, when going from the root class, 'Entities,' (always an implicit parent class of 'Classes') and all the way down to the class itself. But let us also always put the optional attributes below the mandatory ones.. Hm, and always put 'description' last as well?.. ..Hm, should 'Enitites' actually be the outer class, or.. No, yeah, it is the outer class.. ..And then the 'special attributes' attribute than only counts for JSON entity instances of the class.. ..Well, we shouldn't divide them into data entities, and general/abstract/standard/object entities?.. ..Ah, or I could make a Data Entities subclass of Entities, and just remove the attributes.. ..Or simply write.. ..A boolean attribute that removes all atrributes.. Wait, in order to even define the class, we need to use a JSON entity, so no, it just goes without saying that the 'special attributes' are for JSON entities only. ..Well, we should say it, of course.. (11:10) ..And yeah, let's just always have the description at the bottom. This convention is then both used when writing the entities, and also used for the submission forms. (11:11)

%..(11:13) Hm, to use 'Scale types,' or to divide scales into subclasses..? ..Hm, and to capitalize all first letters of classes, or just the very first..? ..Hm, to only capitalize the very first.. ..Hm, and I think I should subdivide into subclasses.. Then the Scales class should just not be used other than a parent class.. Hm, and if so, can we denote this.. ..Well, no, let us simply just not have the 'special attributes' attribute for it (such that you would technically need to define instances of it from just the description, if not for the subclasses). (10:20)

%And for the 'Special attributes' attribute, let's also list them in order of mandatoy and optional.. hm, and maybe 'removed' first?.. Sure.. And let's also list the removed attributes in the order that they would be written.. (11:32)
%(11:37) Oh, should we also denote the expected type and/or class for the special attributes?.. ..I think so, yes.. ..So either a strings denoting the type, or an entity ID (a string starting with '@') denoting the class.. (11:39)

%(12:06) Hm, 'Scalar predicates' or 'Tags'..? ..'Tag'.. (Used to define Rating scales..)

%Hm, for the order of the Special attributes, let us actually just put the removed attributes where they would otherwise have been in the order.. (12:19) ..And the same when overwriting their option with either 'mandatory' or 'optional'..

%(12:45) If we ever want default values for attributes, let us implement it by using a '= <default value>' syntax right after the type/class declaration in the Type row of the Special attributes table.

%(13:04) "Or should we use a Scale type attribute intead after all?.." No, we shouldn't..

%(13:17) Oh, relational classes should also inherit from a list of useful relational classes.. And one might even extend that to (derived) subclasses of (derived) subclasses, on so on.. Hm.. ..Ah, but this is just done by uprating Relations for Relations, as the Object will be constant. So we just need a relation of 'Useful sub-relations'.. (13:25)

%By the way, I'll call.. Wait, if I call general class members 'members,' and only call members with the class as their 'class' attribute's value 'instances,' then I don't need to say '\emph{child} instances'.. (13:53) ..Sure..

%(14:03) Hm, I guess def_str should be a BINARY instead, or what?.. ..Hm, SVG is a text file format, not a binary one (of course).. ..But binary files might be needed, e.g. JPGs.. ..But why not make a separate database for binary files like images and videos, and then just refer to them via an URL?.. ..For if the files are still stored by the same DB / DB node, this node can then still make the exact same promises about when to keep these binary files, and when to throw them out. So yeah, let's say that binary files are refered to by URLs (and some nontext file formats, such as SVG, might also be referenced by URLs as well).. ..So let us use utf8mb4 for def_str.. (14:14) ..Well, one can set that option for the whole database instead, so we'll just keep the simple 'TEXT' type..

%... ...(16:19) I'm gonna call it 'Rejects submissions' rather than 'Rejects instances' such that I can use it for Users and Comments as well.. ..Oh, or I could use a self "type".. But then I also need to remember implement checks in the backend.. But.. Oh no, the attributes is simply not set when it has that type. Rather the creator will be the input. And so I could call it 'creator' instead.. (16:24) ..Well, no let's instead just make the app check that the creator attribute matches the creator.. ..Hm, it makes more sense to make a boolean, then, that says 'is claimed by user'.. (16:29)

%(17:31) Hm, could be nice if we could make it part of a class's attributes, whether or not the creator should always be 0, a bit similar to the 'Rejects submissions'.. well, shall we not just include that attribute?. That should be all there is to it (and then the app can just handle it from there)..

%(19:11) Regarding Arguments: I should still, however, do the thing of requiring the the correlations are determined/adjusted for whole classes at once (if if they're small classes).. ..Well, except that a lot of statements has specific arguments for them, so actually no.. But maybe we could do both: allow for Arguments to be uprated for the individual statement, while also allowing (derived) Arguments to be uprated for the class. Yes.. (19:16)
%..Hm, but shouldn't we try to make Arguments the same as predicate subdivisions after all, since they share a lot of similarities..? (19:19) ...No, I don't think so..

%(20:04) Maybe I should make a relevancy scale type after all, and use it for classes as well..(?) ...The point is that we're already excluding dublicates and.. well, come to thing about it, why \emph{not} make a relevancy scale?.. ..Well, because we might use the rating scale instead, but no, that's not better. It's better to.. well, or maybe the rating scale \emph{is} better.. ..No.. ..Well, yeah, maybe.. ..Hm, what about 'likelihood of being found useful/relevant'?.. ..Yeah, that's it.. Then we just need to.. make the likelihood be understood subjectively.. Hm so should we give likelihood scales a boolean of whether it's subjective or objective.. well, yes..!.. (10:24)

%(20:31) Hm, I might not want to use the "belongs to class" likelihood scale much at all. So maybe we should go back to having Statements as the subjects of likelihood scales.. ..Well, the objective ones at least.. ..Yeah, maybe that'll be the difference.. But I might as well call it a relevancy scale, then, i.e. when it comes to the "subjective" likelihood that a user finds an entity relevant to a certain class.. Hm.. (20:34) ..Yeah, so "relevancy" is also measured as a likelihood.. But maybe we should.. No, maybe we should keep classes as the input.. Well, or 'Predicates'.. Hm.. (20:37) ..Nah, maybe there are just always to scales for the members of.. Well, or maybe the members of a class is always measured according to relevancy. And yeah, maybe I should then make the obective likelihood scales take Statements instead. I think so, yes.. (20:41) ..Yes..:) (20:42)

%(23:13) Maybe Arguments \emph{should} be more closely related to the "division of predicates" and the settings..

%(26.10.24, 9:30) When the user rates / has rated the representatives, the should also still be rendered at their original position, just with a different color/style, such that the user can always keep in touch with the average (median-wise) scale..

%(9:55) Let us always say 'predicate' and 'relation' about things that can be either true or false (or unknown), and then say 'tag' when it is a continuous scale..

%..(9:57) Hm, if we use Statements, then there will only be one likelihood scale.. *(I'm making just the one likelihood scale.)

%..Hm, let us instead of parameters like the IQ scale use 5 as the mean, and then make the spread something where only a few entities exceeds 10 or goes below.. Hm, well, maybe something like (100, 15) is more reasonble 'cause it won't have prior associations.. That's the thing: do we want some prior associations, or not so much..? ..I think not so much, so maybe yes, let's use something like (100, 15) (or whatever it is)..

%..Hm, we could use classes of statements to divide the likelihood scales up.. ..Or I could just let it be one scale..

%(10:33) Nu kunne jeg ikke lade være med lige at tænke lidt på brugergrupper, trust, og recorded inputs.. ..Det kunne være rigtig godt, hvis vi også kan få brugergrupperne godt i gang og hurtigt fra start.. ..Og så tænker jeg lidt på at lave én liste over recorded inputs.. Ah, eller det kunne være for en klasse af brugere for hver individuelle liste.. Hm, interesssant, lad os se.. (10:36) ..Ah, det skal være en faktisk 'brugergruppe' og ikke en brugerklasse.. ..For hver skala skal man så kunne vælge en vilkårlig brugergruppe, og så se alle recorded scores fra den brugergruppe, måske ordnet efter vægt først, og så måske tid.. Hm.. ..Vægt først, ja.. (10:40) ..Åh, eller rettere for hver skalar, ikke skala.. ..Yes, det kan virkeligt blive nice, så kan vi måske ret hurtigt få en effektiv form for fact checking, eller hvad man skal kalde det, m.m.. (10:42)
%..Folk skal selv melde en deres profiler til en brugergruppe, hvis de vil deltage, og så hører det altså med at deres scores for den profil så bliver recorded for en hvis gruppe af skalaer.. ..Tja, i hvert fald for den type, jeg har i tankerne her, men generelt kommer det an på typen af brugergruppen, om scorer bliver recorded (gemt) eller ej.. (10:48)
%..Hm, og behøver jeg egentligt selv at implementere noget med trust? Eller kan vi ikke bare nøjes med at implementere pointene/vægtene, og hvordan de fordeles, og så kan brugerne selv lave deres trust-prædikater..? (10:51)
%..Hm, og vi har moderator-point, som er fordelt i udgangspunktet, og som kun kan deles over og gives videre, men ikke uden at moderatoren så mister tilsvarende point (så mængden af samlede moderator-point er konstant).. Og så har vi de faktiske vægte i systemet (hvor moderatorerne igen gang behøver at have nogen selv).. ..Og der må man jo bare sige, at det samme princip gælder, bortset fra at moderatoren bliver ved med at have kontrol over, hvordan vægtene deles ud. Hvis en moderator giver nogle moderator-point væk til en anden, jamen så falder vægtene som vedkommende har delt ud bare med en tilsvarende faktor (og i stedet stiger vægtene for motager-moderatorens fordeling).. (10:56) Og det er pænt meget det. Så har en hver brugergruppe selvfølgelig en beskrivelse og et slags regelsæt, om man vil, som moderatorene bør holde sig efter.. Hov, skal vi ogå tilføje en process hvorpå moderatorer kan stemme om at fratrække point fra en anden moderator?.. ..Hm, hvis der også kan gives negative vægte, så.. Hm, men det skal der vil ikke kunne gives, nej.. Nej.. ..Men jo, man kunne godt lave et system til at veto'e en moderator.. ..Eller bare trække en hvis mængde point fra dem, hvis altså ikke dem alle.. Og så kunne man sige, at det kræver 60 % af stemmerne, eller noget. Måske meget fint.. Nå, og så er der altså også gruppen af skalaer, hvor deltagernes scorer bliver gemt. Og denne skalagruppe kan så bare være tom for nogen brugergrupper. ..Og er det så det, eller mangler jeg noget..? (11:03) ..Måske et system til at flagge ting til moderatorene.. (11:04) ..Og hvor ikke-deltagere godt kan flagge, men selvfølgelig får disse flaggings så lavere prioritet end intra-brugergruppe flags, når det kommer til at blive set på. Og jo mere vægt, desto højere prioritet generelt, det er klart.. ..Og man skal kunne flagge anonymt.. (11:06)
%..Ok, og jeg kan vel godt implementere dette via entiteter, right?.. Selv flags..? ..Det kunne være en slags direct messages.. ..Hm, hvordan implementerer jeg det..? (11:09) ..Med et table over DirectMessages, vel?.. (11:10) ..Og så bliver flags bare implementeret som et format for direct messages. Og disse sendes til en profil, og så har en moderator selvfølgelig bare en profil, der er beregnet.. Tja, eller også skal man bare kunne dele messages op i forskellige kategorier/klasser.. (11:12) ..Det er bedst, ja. Og dette gøres bare ved at tilføje en sti emne-feltet.. ..Ja, så jeg skal implementere Direct Messages således, og herfra kan jeg vel så bare implementere brugergrupper via entiteter og entity lists..? Hm, specifikationerne bør bare være en (JSON) entitet, ja.. Og hvad med pointene og vægtende helt præcist..? ..Moderator-pointene bør implementeres vie brugergrupe-entitieten, som er en redigerbar entitiet styret af en grundlæggende bot. Man signalerer så til denne bot, når man vil give point væk, eventuelt ved at have en entitiet over transaktioner. ..Hm, og vi kunne endda tillade at botten redigerer disse entiteter for brugeren og fjerner/markerer udført transaktioner, bare for nemhedens skyld.. (11:23) ..Og vægtene kan så i stedet godt implementeres via scores, hvis man vil.. ..Ja, lad os bare gøre det, ikke?.. ..For resten, vi kan altid tilføje det med at stemme om at trække point væk fra en moderator (og fordele på resten med en kontant forøgelsesfaktor for alle..) på et senere tidspunkt.. ..Hm, måske er det faktisk nemmest, hvis vi implementerer vægtene via entiteter. For så skal man ikke lave alt muligt med at sige til andre brugere, at de ikke skal give scorer til det prædikat.. Ah, og hvis vi i stedet brugte et fælles prædikat, jamen så kan en moderator ikke være det for flere brugergrupper på én gang, og det vil vi ikke forhindre muligheden for. Så jo, det gør vi også med entiteter (offentlige, men redigerbare).. (11:30)

%(12:02) Hm, let us use doulbes for weights and then make sure that each weight is unique such that.. hm, that each weight maps to a user (/bot) one-to-one.. Or is this too elaborate..? ..Hm, maybe mapping with doubles is just a bad idea in general (I don't know..).. (12:04)
%..Alternatively, we could rank each user in a user group by an integer number.. ..With the highest ranks having small integers (such that 0 or 1 is the highest rank).. Hm.. ..No, cause then you have to shift whenever there are changes.. (12:08)

%(12:19) Hm, should we stick to calling them 'user groups,' or should we find another name..? ..Hm, members can also be bots, so yeah, I think I ought to look for a better name.. ..Scoring groups?.. (12:21) ..Or something like 'opinion groups'.. (11:22) ..'Score groups'.. no.. ..Hm, 'assessment,' 'judgement'.. ..'view'.. ..'taste'.. ..Of these, I think I like 'judgement' and (then) 'opinion' best.. (12:26) ..'estimation'.. ..Hm, 'estimation' is also an option, but I think I still like 'judgement group' best out of these.. (12:29) ..'inference'.. ..Hm, and there's also 'assessment group' that doesn't sound too bad either.. ..Okay, now I think I'm leaning towards 'assessment groups' or 'inference groups'.. Nah, the former, mostly.. (12:33) ..Hm, 'valuation'.. ..Nah, that has a too specific meaning.. ..Hm, maybe I'm leaning back towards 'jugdement groups'.. (12:36) ..'decision'.. ..Hm, or 'panel'.. ..Hm, now I'm leaning back towards 'opinion groups'.. (12:40) ..Hm, what about something like a 'collective'..? (12:42) ..Nah, well, then 'user groups' is almost better, I think.. ..'user pool'.. (12:43) Okay, let me just keep thinking. And let me then just call them 'user groups' for now, like I've done until now.. ..Hm, 'congregation' or something close to that.. ..'segment'.. ..Hm, but maybe I'm wrong that it should be able to include bots. So maybe 'user groups' is just what we should call it.. (12:53) ..Yeah..

%... (14:17) I think I will call the generel thing 'user groups,' and then I will call the specific subclass of user groups where you have to apply for and/or accept the invitation for the group, and where your scores might be recorded for certain scales, 'panels' instead. Also, I think I might start to call scalars 'parameters' instead. *(Or maybe 'scalars' is better..)

%And something even more important: I now intend to make an 'aggreement scale' type as well, intended for all subjective statements, and then let the likelihood scale be intended only for objective statements.:) ..Two minutes.. ...(14:34) So the argreement scale works like the other scales, where you have 'underlying parameters,' or 'sub-parameters' for short. But for the likelihood scale, we should actually try to make more like a probability table, I think. Now, before I continue, let me think a bit about whether to include the "unknown"/"missing" middle value.. ..Well, and also about using correlations / prob. tables across a whole statement class at once.. ..Well, I think that we simply should indeed include this middle value (between true and false).. (14:40) ..Hm, no, maybe we should not import correlations/probabilities.. (14:46) ..Oh, we probably should, but then we should just make quite specific statement classes.. (14:48) ..And maybe the users should uncorrelate.. No, 'cause that will require agreement..
%..Oh, I should call it the 'Probability scale' instead, since we might need 'likelihood' to refer to the probability when assuming a 50/50 prior.. (14:53)
%..Okay, we should actually always put the probability tables at the classes, but these can then just be very specific.. ..(14:59) Hm, and we can still use a kind of "inter-correlations," rather than using a whole table.. ..And then we can also disregard missing informations.. ..And there we are..:) (15:00) ..Well, maybe we should have outcomes that are more than just binary.. (15:02) ..Then the inter-correlations just becomes all the more numerous, but then again, if we go to the extend of making more-than-binary outcomes for the "pieces of evidence" of a statement class, then we can also go to the length of estimating these correlations.. (15:04) ..When they are there.. (The default is 0 correlation..)
%..(15:06) One could perhaps instead also implement it via entities, and then just uprate whole prob. tables at once.. ..(15:13) Hm, maybe that's actually prefereable.. ..But maybe we could do both, then.. ..And make it so that these entities refer to the actual scalars/parameters.. Hm, maybe 'scalar' was actually better.. that they determine.. (15:16) ..Well, or just have the necessary information such that you can find the right scalars.. ..Hm, let me at least use entities for the whole outcome / pieces of evidence group.. ..(15:25) Well, if we uprate whole outcome group entities, then I think we might as well put probabilities and correlations on top of that. And then you generally just look at the most popular ones from various user groups.. ..But still, the users can also rate these things themselves individually, and a user group might choose based on the medians of the group.. (15:28)

%(15:45) Hm, I think I've come up with a good ranked choice voting system for voting on user groups and bots.. The DB / service providers determine a number of bots that they will provide for the coming period, perhaps grouped into different categories. Then for each category, yeah, they select a number for that period. A user can then make a list of bots (or keep their old one). (If the list is longer than the number, then it is just automatically cut off at the beginning of the vote.) The most popular bot is then selected from across all lists. Every user then has that bot crossed out for the remainder of the process, voting on the n - 1 remaining bots to be provided. And importantly, each user who didn't have the winner bot on their list also has a bot crossed out as well, which will then be the last one on their (cut-off) list, i.e. with the "lowest rank." Then the process repeats but where every voters' list is now just one bot shorter. And after the second winner is found, that is then again crossed out on all lists, and lists that didn't include it will have their currently lowest ranking bot crossed out instead. And so it goes until all the n bots have been selected this way. (15:42) ..And if you want to introduce weights for the users, e.g. proportional to have much they "pay" (directly or indirectly) for the service, then the process is just exactly similar, only where each of these votes in the n iterations are now weighted. (15:54)

%..(16:00) Oh wait, there's a problem here of creating a majority rule to some extend, in some circumstances.. ..Hm, it is a bit of a tug of war between eliminating the potential for overstrategizing voters and then majority rules.. (16:03) ..Hm, you could potentially penalize overstrategizing voters by reducing download speed when querying for popular bots that they didn't vote for.. ..But no, that's to complicated, and also hard to make such that users can't get around it.. ..Hm, you could also do something in the middle, where you do cross out the lowest-ranked bot from lists that don't include it, but then also increase the weight for these lists slightly for the remainder of the process.. (16:11) ..(16:13) Oh, another thing you could do is to make the resources at the bots disposal increase with its popularity to some extend.. ..That seems like the ultimate solution, almost (but not one we will implement for the early stages of the system, though..)..:) (16:15)

%... (17:25) We probably shouldn't try to group statements after all and try to make prob. tables across the whole group/class.. I'm not completely sure what else to do, 'cause it also seems like too much work to do it for each individual statement.. Hm, could we do something about using "theories" instead..?.. (17:27) ..'Theories' to guide the determining of the correlations..?.. ..And the "inter-correlations".. Yeah, that sounds like it could be a good approach.. (17:31) ..A 'theory' is then just a more or less formal description for how to compute/estimate the relevant correlations, and also what kind of Arguments to generally include, and such, and then you just query for the top theories of a given statement class and display them above or below the correlation scoring sliders.. (17:34)


%(27.10.24, 11:10) About the EntityPageMenu, I want to have it signal the EntityPage whenever a new change is made. ..Should we just let it signal the JSX element to put in the body?.. Hm.. ...(No, that's a bad idea..) ..Hm, many of the options is just about determining a class or a relation and then also potentially a tag.. (11:37)

%..I must admit, I'm also a bit "disturbed" *(distracted, rather) by some other interesting thoughts, mainly about the selling points of the system. So let me just make a short list of points..
	%- Being able to change parameters to get the "algorithm" results that you want, and even without needing to "feed the algorithm" first.
	%- Having a lot of informative predicates when you view a given entity.
	%- Having a abitrarily complex semantic tree that go can explore for each entity. (Discussions->Argument->Argument->... or Propblems->Proposals->Problems->Proposals->..., or Useful links->Links for learning more etc..) *Man kan også komme med den lidt mere precise pointe, at brugerne selv bestemmer tabs'ne, og tabs'nes under-tabs, og under-tabs'nes under-tabs, osv.
	%- The ability for links to go outside of the website (a bit simple, but still a worthy point, I think)..
	%- Fact checking! (with user groups, and especially 'panels').
	%- Finding groups that share you opinions and tastes.
	%- Being able to use these groups for anything you want, since the website isn't limited to a specific kind of thing, but is basically a whole Web within the Web (or it can be, anyway).
	%- I should also mention the browser extension here, I think.. *Oh, this is quite important, 'cause it also means that the small initial "club" of users can then utilize each other across the web. ..And it also ties in well with the 'fact checking' point. ..And the point about being able to make use of ones user groups across the Web (e.g. to see specifically what users of that group has commented)..
	%- A democratic site where the revenue of the creators and open source developers (which there are) is determined by the users (more or less, and more and more over time). *(Payments can even be determined by having users score the user groups (including panels).)
	%- Ads where the user types in a code, perhaps some positive (or neutral, but brand-defining/-related) phrase that includes the name of the brand, in order to continue.. (This is a new idea that I haven't written about before, by the way, at least not this exact version of it.) And by adding a code at the end of this phrase (if not the code is the whole thing), we can check that the user has done this (at least if we use a bit of capcha technology). This then counts as payment by the user. Now, the point is, I think that this will get much more users to pay to skip ads, and get more democratic say in the app/system/network, since they'll then get a much better understanding of what their ad-viewing time is worth. And yeah, the fact that it also gives you a bigger voice in the (semi-)democratic decisions might also be a big plus for a lot of people.
	%- The fact that the first users who gets customer shares before the whole thing takes off will get quite rich from these customer shares. *(Which will then also likely make them recommend the network to other people, and also motivate them to contribute more (content, scores, source code, etc.) to the website/system/network.)
%(11:57)
	%- Oh and some 'panels' will be political ones, which means that we could great some great political discussions between panels. And with the thing of having 'theories' for how to argue and answer for various types of statements also means that these discussions might be very constructive, and much more than we have ever seen on the Web before..
%(12:01)
	%- Another thing: Rating a whole list by draging-dropping, and on a relative scale, might also attract a lot more data (and very sophisticated data at that).
%(20:57)


%..Now back to the EntityPageMenu.. (12:03) ..The menu could also just signal a Scale (ID).. (12:12) ..Yes, and optionally also a Subject.. ..Then there's also some tabs that signales something else, but we can just wrap the scale signals in an object that identifies them as scale signals.. (12:14) ..Hm, most tabs are also represented by an entity.. ..But let's still signal the scales directly, it's just that this might help us open tabs from the outside.. (12:17) ..Let's make all tabs be represented by entities, yes, and then just have most of them be represented by relations (and also tags for some, I guess).. ..(They don't have to be a class of 'Tab' entities, or anything..) ..Hm, maybe it's actually best not to signal the scales directly, and thereby take too much control; maybe the menu should signal the tab entities instead.. (12:23) ..Or just general objects.. Yeah, forget about tabs having to represented by an entity; they can be represented by any JS object in principle.. ..Or JS value in general, maybe even.. ..And then the tab just signals itself to the EntityPage?.. (12:25) ..Or a itself wrapped in a 'tab'-type signal.. ..Or maybe we should just make different 'open entity page body' actions that the menu can call, simply providing the necessary input data for that action. Yes, let's do that.. (12:28)
%..And for the tabs, don't I then make a TabBar/MenuSection component, which is then parhaps also the whole menu.. or at least the outer parts of it..? (12:34) ..Yeah, and then.. ..Hm, or we could just have a list of outer MenuSections depending on the state of the EntityPageMenu.. ..(12:41) Okay, this is a bit hard. I need to figure out about when to hide/collapse menu sections, and all such things, which is a lot.. ..Well, we just divide it into sections, and then at some point we add actions to collapse/hide sections, I guess.. ..(14:49) Hm, I guess the outer menu is just a general component that can open/close and collapse/expand menu sections.. And then the menu sections keep their own state..? Or..? ..Yeah.. ..Hm, and the MenuSections can actually also just be Menu components, right?.. (12:53) ..Unless they are instead menu points (leaves)..
%...Maybe it is TabBars I want instead.. (13:07) ..Yeah, I think so. So, at least for now, maybe we should make the Menu a list of TabBars, and then implement these. Then we can always make actions to collapse tab bars, etc., at a later point if/when needed.. (13:14) ..Hm, but then let us make it EntityTabBars instead, where the tabs of each EntityTabBars follow the same.. well, they share the same space of possible tabs, using a single (combined) way of defining each tab.. ..Hm, but let us then generalize it to AppPageTabBar.. (13:17) ..Oh tab \emph{list} to be less restrictive.. (13:18)

%(13:37) Ah, panels and user groups can also be our way to define the flow of revenue to creators, perhaps at an early stage (as well).. ..Similarly to how you vote for bots to be maintained, you might vote for usergroups, and this time where there's a very clear way to divide make the resources be dependent on the number of votes, as the resources are here simply just the total payments sent to that group.. ..Hm, and in the beginning, let's pay out shares rather than money.:) (13:41)

%... (15:28) The SettingsMenu should be below the main tabs/classes/relations menu. And I think we should display a list of all the ancestor classes, and expand the one that's the closest class to the current one going up the parent class route..

%Maybe I should make the query handler accept consecutive queries that uses the output of the previous ones, potentially, and then sends the whole array of output in the end.. ..Oh, maybe we could even do it by including the query_handler.php in a loop.. (15:34) ..*I mean 'require'.. ..Well, that's the same.. ..But it's more effecient to refactor it as a function, then.. ..Hm, yeah, maybe I ought to do this indeed.. (15:36) ..Or I could implement it in MySQL, using the REPLACE() function.. (15:39) ..Oh, and SUBSTRING_INDEX().. ..Yes, it can be done in MySQL. Now, do I then also make a proc to get an entity list in the end..? ..Or maybe that could be a special req in the handler.. (15:44) ..Or I could just prepend the outIDs in the subjID column of the entList, and then let scoreVal be NULL for these prepended entries.. Okay, let me do that..

%(16:24) Hm, der er nok at rive i. Men kan godt mærke, at vi er sprunget til vintertid.. ..Kunne være nice lige at arbejde en time mere, men er ikke sikker på, at jeg kan få så meget ud af det..
%(16:27) Hm, jeg kan også tænke lidt mere over, at Arguments jo nu igen svarer til sub-scalars.. Hm.. ..Men jeg kan jo bare også kalde det 'Arguments,' når det kommer til Settings også..
%Hm, skal tags ikke altid bare dømmes ift. til den første ancestor-klasse i rækken med settings tilknyttet sig? Og hvad gør man, når en entity har en underklasse, hm, så sørger man vel bare for.. Nej, det er ikke bare.. Hm.. ..Hm, ser man på, hvor i underklasserne, den hører til.. Ah, eller rater man bare aldrig i en klasse, som har en underopdeleing/division? Det er nok det, vi skal sige.. Og når man så rater for en klasse, der har en under-division, jamen så er det altså kun omhandlende at få korrelations- of offset-parametrene indstillet.. (16:40)
%..Men skal der så være samme opdelinger for, når man rater, og når man søger?.. (16:41) ..Hm, det skal der faktisk måske.. (16:42) ..(16:49) For hvis brugeren har en mening om, hvilke argumenter, der spiller ind, og deres korrelationer, jamen så må man også kunne rate entiteterne imellem sig selv.. Well, dette er ikke nødvendigvis givet.. ..Og hvad med den anden vej: Hvis brugeren kan rate klassens entiteter mellem sig selv..? ..Og foretrækker dette frem for i overklassen, jo, jamen så må der nok være nogle specialle forhold, der gælder for den liste.. ..Og jo, jeg tror også godt vi kan sige, at en bruger.. Hm, well, man kan jo bruge forskellige settings, både til at søge og til at rate.. Hov, vent, settings'ne er da egentligt slet ikke relevante, særligt i hvert fald, når man rater.. Nej, det er de vist ikke, ups.. (16:55) ..Nå jo, på nær at man gerne skal vide.. Ah, men ethvert "relevant tag" kender jo sit eget domæne, var det ikke det, jeg kom frem til?.. (16:57) ..Og jeg lod bare appen transformere alle ratings til ancestors og så submitte dem automatisk, når brugeren har rated de påkrævede parametre, ikke?.. ..Hm, its better to let the bot do this.. (17:01) Men okay, her kan vi så i hvert fald bruge settings, når det kommer til at bestemme disse parametre, om ikke andet.. ..Men måske jeg skal vente med at implementere dette, og så bare regne med at alle brugerne bruger de samme inddelinger lige til at starte med.. (17:04)

%For 'relevant tags' skal appen også kigge i de nuværende settings.. (17:05)

%..Ok. Nok at rive i.. Jeg holder nok fri nu.. (17:10)

%(28.10.24, 8:30) I've been a computer scientist for too long when I feel like I kinda forgot about the fact that 'inputs' are called 'arguments' in mathematics. I feel like I haven't recalled that for a while. But the point is then how well that works for my Arguments here, since we can then also interpret the Arguments of a scalar to be the "inputs" to a function producing that scalar. How neat is that?.

%Okay, I do think that the settings structure will also be integral to searching.. Well, of course, but to scoring as well.. well, of course.. Hm, okay.. (8:34) ..Yeah, regardless of which mode you are in, the tags are always taken in relation to the first ancestor class from your settings structure.. ..And if you are in a class that is divided, they are taken from the.. Subclasses.. Hm, not necesarrily when it comes to.. Oh, never mind, I decided that all tags should just the same class structure. So for a divided class you always construct it from the subclasses, and interpret the tags in relation to the subclasses.. Except something like popularity and 'known'-ness?.. (8:39) ..Instead of 'tags,' I could also say 'scales,' btw.. *(No, 'tags' are domain-independent, whereas 'scales' have a domain..) ..Hm, no, 'knownness' is also for the specific domain. And popularity, could also be that as well, although I think we'll do with 'knowness,' btw.. (8:43) ..So it's all either going to the closest ancestor in the structure (going up the 'parent class' attributes of the current class), or dividing a class into several if you are on a non-leaf node in the structure.. ..(Or staying where you are, if you are on a leaf already..) ..And the 'dividing' part seems the hardest.. (8:47) ..You then have to query all the subclasses in principle.. Hm, when rating, we can just query the most 'known' ones, and potentially a level of lesser known ones, if the user is going down the levels while rating the whole class.. ..But when the classes are big, what do you do more precisely..? ..If even the subclasses are too big to.. Oh, 'knowness' is also determined on the domain of the subclasses, so that's a relief at least: then we don't have to hope that the 'known' entities of the parent class matches those from the subclasses.. (8:50) ..And this also means that when querying the most 'known' entities on a non-leaf node, you actually go down and query from all it's leafs (of itself and of its descendants).. And you of course just query a limited amount from each leaf.. Hm, in a way that means that when you stand on 'Entities,' you would have to query from all leaf-subclasses.. That will probably not be advisable in the end.. ..So what do we actually do..? (8:55) ..Hm, we might copy scores all the way up such that the app can also query from linking nodes directly.. ..That sounds like the best solution.. Then the system is also not reliant on the fact that user choose the same structures.. (9:00) ..But then you shouldn't necessarily expect.. well, there's a limit on how far down any search goes, when you adjust the settings structure. So even though your whole structure is very fine-tuned, unless you have rated the entities yourself (which you can only do for one specific structure), then you won't get \emph{that} level of fidelity if you are standing on an inner node of the tree.. (9:04) ..But we rarely want such a big mix, so maybe we could just not allow searches when the.. Nah, no reason for that. They'll just not have as good fidelity when you mix a lot of different entity groups together.. (9:07) ..Well, sometimes, like in feeds, you do want a big mix, potentially.. ..But a feed could just sample from various subclasses, though, maybe make it a bit random which subclasses are selected each time.. (9:09) ..And it is not a must, in any case, to have an outer search.. ..well, or at least a quick one; you could make a fine search on all entities, and then just be prepared to wait a bit.. ..Well, and at a later stage, we can prepare entity lists for all the major types of user tastes, if you will.. ..Hm, one could also.. Ah, no.. ..But yeah, that feed with a mix of topics, chosen with some randomness, is not a bad idea.. (But that is for a later stage of the app..) (9:17)
%(9:27) How about scoring the subclasses instead in terms of how well they do with certain tags..? Then when you choose which tags to boost, then that gives more points to some subcategories than others.. Hm.. ..(9:35) Yeah, a simple concept, if you don't know which category/ies you have an interest to look in, then you search for that first.. And then we just need users to rate categories/classes as well, not just w.r.t. the single 'useful subclass' parameter, but according to several parameters..
%(9:38) But in principle, you could also score the tags for a whole class at once, and then also make sure to score the correlations between tags. Then that could be a way to get the right classes for a certain set of tags.. ..Well, or you could just score the tags themselves, and then.. Well.. Hm, I guess we expect the correlation to be trivial overall: If a group has a lot of 'funny' entities and a lot of 'educational' entities as well, then the chances are that you can find many 'funny' \emph{and} 'educational' entities in that group/class. So yeah, maybe we shouldn't score these 'correlations' but just score the classes according to the predicates.. ..Hm, which actually is the same as scoring the offset and the correlation parameters, or rather it follows from that!.. (9:43) ..Well, that's great. So with a certain settings structure, you can also get the subclasses that you are interested in when searching for given predicates..
%..Hm, so maybe I just shouldn't show the instances for inner nodes of the structure, but instead show the curves of the subdivision classes on a display.. (9:50) ..Well, showing some of the 'known' entities from each curve/subclass as well.. ...(10:01) Yeah, that's it, so when you are standing on a class with divisions, you only see the most 'known' entities from each subclass, and see the curves of each subclass w.r.t. the given tag.. Hm, and when there are more than one tag?.. Do you then just see some automatically shifted curves..? (10:03) ..I mean, yeah, probably, also again since we might as well assume trivial correlation within each subclass (in order to calculate the shifted curves).. (10:04) ..And when you get to a leaf, that's when you then start seeing the whole entity list.. ..Right?.. Hm, do we then only want leaves of limited size, then..? I guess so.. (10:05) ..Hm, to the extend that the user might just divide into something like creation time, in order to chop down larger classes..? (10:07) ..Yeah, maybe, now that we'll introduce this technology to shift the curves. And hey, if non-trivial correlations becomes a problem, then we can just make sure to include those correlations as well, i.e. when rating where the subcurves fit in the parent curve over the space of relevant tags.. (10:10)
%..Okay, yeah, this is pretty great, actually.. (10:11)

%(10:23) Hm, so this search becomes an alternative to searching through subclasses tabs, at least when searching for entities that fulfills one or several tags.. ..Hm, we could also show a list of subclasses, and then when hovering over them, we show their curve on the scale display at the top. And then we also sort the subclasses according to the chosen tag, if any.. ..That sounds good.. ..Hm, and do we then remove the Instances tab for non-leaf classes (that are not below the leaves either), or more precisely nodes that have subdivisions?.. I guess so. 'Cause then we also just plot some of the known entities in the display when having selected a given subclass..
%..Hm, should there still be a limit to the number of subclasses in the divisions, trying to eliminate intersections, or?.. ..Yeah, maybe you can only choose from the division.. (10:37)
%..You only have the subdivisions, yes, but then you also have tags.. ..So for the outer structure of all the entities, you are actually searching via these (modular!) Settings entities..? (10:45) ..If this is so, then we don't need the 'more tabs' button for the first couple of class-realted tabs..
%..So I guess it would then be natural to have the tags first, and then.. ..the subdivision tab bars.. although you could also add new tag tab bars in the middle.. (10:48) ..Yeah, you can add a tag bar whenever, changing the tags that you search after. This then also allows for some structuring of the tags, since you can go to a subdivision first, and then choose a tag that is especially relevant for that.. (10:50) Should we further structure these tags?.. ..Nah, there won't be too many per class, so we can just show them all in a list, I think.. (10:51) ..Well, you also have the natural structure via the Arguments.. (10:52) ..And even though Arguments are only truly relevant when we get down to a leaf, we can still use them as a way of structuring tags for the upper nodes, can't we?.. (10:53) ..Yeah, especially if we use inheritance, which might be a good idea.. (10:54) ..Making each subclass of a division alter the parents group/structure of relevant tags, much in the same way that I'm altering the so-called "special attributes" now for the classes' instances.. (10:56) ..Okay.. (10:56)

%(11:05) Hm, when Settings are modular, let us perhaps make them a lot more Score-based.. Maybe every lookup of subdivisions and their parameters should be done by querying scores.. ..Sure.. ..Wait, we don't need the subdivisions to not intersect..! 'Cause we are just never plotting the 'known' entities from more than one curve at the time.. Well, that actually also doesn't matter: it's okay to plot the same entity twice on the scale, but with different colors. (Just like we also do when the user themselves has rated a given known entity, where we then show it in both its old (pre-user's-own-rating) and its new place at the same time..) So no, maybe we don't need to worry about intersections at all, at least until we get down to Arguments that tries to answer a certain question.. ..But here we also have the inter-correlations to adjust, so even here the list doesn't need to be a list of disjoint/uncorrelated things.. (11:12) ..And if this is so, then we can just query for the useful subclasses again.. ..Then we also just query for the offset and the correlation/scale factor, and then Bob's your uncle: we have an effective way, it seems, of searching on tags for even the very large classes.. (11:14)

%..Then I should have the 'more tabs' button for the subclasses tab bar, always.. ..And I should still let the tag tab bar be something that you can extend after each subclass tab bar, I think.. (11:16)
%..When when having just selected a subclass, you first see, if you go down to the PageBody, or what we call it (the entity list), the most 'known' entities of the subclass imposed over the most known ones from the parant class.. (11:19) ..And when ariving at a leaf, I guess you can then select a different tab to get only the instances of the leaf class itself, and not its parent class, and where you show a lot more entities as well.. Yeah.. (11:21) ..And before you get to that point, it is all about showing the known entities imposed over the ones from the parent class, like I just talked about.. (11:22) ..And here you can also score the offset and the relative scaling..
%..Hm, but what happens to having private settings then?.. Should we just rate them privately.. Oh, there's also the thing about the usefulness of a subdivision changing from user to user.. (11:26) ..So we should include the Settings entities, I guess.. Well, or the user can in principle just score whether a node is a leaf or not.. ..Okay, but how do we make it so that the user can have several, inculding temporary, settings?.. (11:29) ..We could let the app construct a Settings structure in localStorage automatically, and then the user can just save these structures as entities, if they want to keep them.. Yeah.. (11:30) ..Yes..

%..Oh, and I \emph{will} try to measure the correlations between tags, by the way, at least at a slightly later stage.. Hm, but how?.. (11:32) ..Nah, maybe not.. ..Well, or yes, maybe.. (11:33) ..Well, if you can calculate the correlations for each leaf, then you can just propagate these upwards.. ..Okay, but yes, I will only implement all this at a slightly later stage, I think.. (11:36) ..Just to give an example, if one class of recepies has some.. Nah, bad example.. Hm.. ..Hm, all the examples that I'm thinking of have trivial correlations, I think, when you think about it.. Hm.. ..Oh, and these correlations also typically show up simply in the fact that this might allow the tag curves to be wider than they would if the two tags conflicted more with each other.. Okay, so let's forget about these measured tag correlations.:) (11:42)
%..Great..

%..Let us also extend the Arguments tab bars from the tags, just like we extend the tags tab bars from the (sub)class tab bars..

%Ah, and we can put the scoring/adjustment of the offset and the relative scaling on the scale display, where this is done by dragging and adjusting the center of the subclass curve, as well as its "error bar".. (11:49)

%..Oh, and instead of computing a combines curve when selecting more than one tag, we can also just show both curves.. We can at least just do this for the first implementations of the app, and then we can always see if we want to compute the transformed (estimated) curves as well.. (11:52)

%(11:56) Wait. We still have a problem of scoring the most 'known' entities in large classes according to the tags when tags are only scored in relation to the leaf classes.. ..But I guess we just propagate the scores of all well-known entities upwards, and then take the average score, perhaps, when there are intersections.. ..Or just take the score from the subclass that is the most popular when there are intersections.. (12:01) ..Yeah, maybe the 'known' entities are all propagated from below, together with all their predicate scores from their (best) leaf node, and where these scores are then transformed each time it moves up.. ..And the 'knowness' itself is also part of this, meaning that not all subclasses will propagate their entities all the way up, even if they are consistently among some of the most 'useful' subclasses all the way up.. (12:04)

%..Hm, do we just make the bot record the most 'known' entities for each (usefu enough) class, together with their tag scores, via entities?.. (12:08) ..I could also make a special Scores table where the score, or a package long with the score, can be an arbitrarily large (almost) string of data.. (12:09) ..Let me do that in any case.. (12:10) (..:))

%..And we only need to propagate tag scores whenever the parent also has the given tag among its most 'useful'/'relevant'.. (12:15)

%Wait, we can also rate for each entity which class is best for it for scoring.. (12:17) ..Then app can just query the tag scores from there.. Well, no, 'cause we still ned propagation in order to transform these scores. So yeah, never mind, I think I was on the right track..

%(12:19) Now, what do we do with relations that has large domains?.. Do we try to make these sortable as well?.. ..Maybe not; maybe we just make do with the fact that these can have "sub-tabs," implemented via the "sub-relations" relation.. (12:21) ..Yeah, for sure.. (12:21) ..Hm, so we won't really implement tag searches for relation subject lists?.. (12:24) ..We make do here with subtabs and user groups?.. (12:25) ..Maybe I should implement a kind of "tagged" tabs, then at least.. (12:26) ..Well, on the other hand, you can just add the adjective in front of the relation in most cases, and then the 'relevancy' will automatically include the degree to which that adjective fits the subject.. (12:28) ..Well, but let's indeed make tabs that are derived from a relation and a tag.. or rather: derived relations each made from a (non-tagged) relation and a tag.. (12:29)
%..So yeah, maybe tag searches will otherwise only be a thing for the outer entity structure. And then once you have selected the first entity and search via relations from there, you only get tags in terms of "tagged relations," but not in a way where you can (re-)sort the given tab; all relational tabs will only have one order per user group.. (So you can at least inlfuence the scores and the ordering this way, at least, namely by selecting a user group that shares your tastes..) ..I think this might be so.. (12:32)

%(9:26, 29.10.24) I have several points. First of all, let me clarify that when I talked about the prob. tables (probability tables), I was talking about scoring the probability that the "Piece of evidence" is true given, 1, that the relevant statement is false, and 2, that the statement is true. But since you can also calculate a correlation from this, let us just make it the task of the panels to realize that this is the best way to estimate your statement correlations, and then include this in their 'theories.'
%Another small point: There should also be a Questions tab, next to Discussions and Comments.

%A more important point: Entities should also have a tab about "Classes of similar things", or maybe we could call it "relevant classes" for short. A good way to find new interesting stuff is to look at what you know and like, and then go and see what else there is that is similar to that thing. And the idea is then to.. follow/afford/honor.. ..this concept by introducing this tab and suggest that the users try to find groups of things that are similar in terms of whether you like them or not. The idea is then to both uprate this class for the entity, and also uprate the entity as relevant for that class.

%Another thing: For each panel, there should be a bot that sorts there comments to any given entity in chronological order.. ..Well, and perhaps where the commenters with the most weight in the group stays longer at the top of the list when showing this "thread".. ..This could then basically work as our feed for the early app, perhaps through the comment thread of the 'Entities' class entity.. (9:39)

%I also thought of an idea a little earlier about smaller panels/user groups being the clients of a moderator, or just a member, of a larger and/or more popular group. These patrons could then promise to look at what the smaller panel/group offers contributes, and report it further up if it is worthy of having the intensity boosted. Then depedning on the smaller panels size, it could also be overall responibility of this group to evaluate each others' contributions, before submitting it to their patron(s).. (9:44)

%Another quite important thing: While this idea of using curves to make it possible to (indirectly) make tag searches even for the largest of groups, and even where you own opinions might be taken into account, e.g. in case it is a tag that is divided into Arguments.. ..Yeah, in principle.. But in any case, despite all this, I will not implement this at first. Instead I will just make it so that you rate the classes that are suited for sorting (not too big, but also not too small either), and then you always sort and rate tags w.r.t. the class that is the ancestor furthest up that still is rated highly enough as being suited for sorting/rating, also including the class itself. And then all rating and sorting will just be done w.r.t. to these classes. ..So tag sortings only makes sense for classes that has parent classes, that are suited for rating and sorting.. And for all other classes, all we do is score the entities' relevancy to those classes (which can also include a tag), at least for the early version of the app..

%About tagged relations/classes, I should make both these things: There should be 'tagged relations,' producing 'tagged classes,' but you can also produce a 'tagged class' by tagging a class directly. (This then means that any class can become a 'tagged class,' and not just relational classes.) (9:56)

%..Hm, let me briefly mention that I woke at between 5 and half past 5, because I thought about that physics idea of being able, perhaps, to derive the same path intragral from the n-dependently cut-off Hamiltonian, and I didn't fall asleep again after that. But luckily I also thought of many of the things that I'm in the process of writing about now..:) ..(And the physics idea might also be grand, potentially..) (9:59)

%Maybe the last things I need to talk about, is that I've thought about what the early users will be doing that will keep them hooked to the app. But before I get to that, let me just underline that I really ought to make those 'panels' and other user groups for the early app. 'Cause we can also use this as the way of determining where the customer shares flow to. (Recall that we're not determining where a promised share goes immediately; we can instead determine how the shares are distributed for a given period over a longer time afterwards, namely by only giving out a small amount of shares at frquent intervals for that period. ..This wasn't too well formulated, but I've talked about it before..) And the deciders of where the shares should go might actually be the members of the group, and the moderators could be the actual shareholders (also meaning that the moderators change when the shares change hands). This way, the shareholders can ask the contributing users to decide more than there shares would otherwise allow. (This might be a god thing early on to create a better sense of user participation, plus it might also limit some of the work for the shareholders in this decision process.) (10:07) ..Hm, and maybe the members are the moderators of another group.. Hm, so I guess I should make the kind of groups as well, where the moderator power can change.. I actually also need that for the shareholder-moderated group, then; that should be controlled by a bot/admin that adjust for whose has the shares currently.. (10:10)

%Again, let us recall that users should also upload entities that references their open source code work (also including CSS styles).. ..And when a user is part of a panel, the scores are also recorded, and the user can then also reference these and say 'here's some work done scoring entities on these lists'.. (10:12) ..Oh, and there should also be something about being able to dedicate the reason why you joined the network to some user. Then the user can reference the class of users that joined because of them..

%And then, when it comes to the things that will hook some of the early users, I think the will primarily be:
	%- Going to a list of entities and sorting them w.r.t. various predicates. And an important point is that most lists other places is what that pariticular site has to offer. But here we can have the list of \emph{all} relevant entities, in the world / across the web..
	%- Veiwing particular entities to see their scores.
	%- Giving your scores, both to contribute this for the sake of other users, but also to make yourself think about what makes an entity good.
	%- Go to an entity to see about what there are of similar things, i.e. by viewing that 'classes of similar things'/'relevant classes' tab..
	%-Go to an entity to find relevant links for that enitity.
	%-And a very big thing: Discussions. I think a lot of users will be hooked by joining a panel and discuss various things. Especially since we'll make sure to get a feed of discussions, maybe via that comment way that I just described.. ..And the whole thing that you can build the whole discussion tree, and actually score the correlation and/or impact of the Arguments, and see what the various panels think of these Arguments, that might interest a lot of people (mostly somewhat nerdy people, but that's perfectly fine, i.e. to target a specific group more than others at first)..
	%-..And also doing this via the browser extension, 'cause then you might feel that your contributions can soon be used by a lot of people, e.g. for fact checking and such..
	%-And then, once enough users are hooked with adding entities, scoring them, (providing links, also), and not least by perticipaing in discussions, then some users will also get busy rating which contributions are important to the project, and participate in the whole politics of this democratic endeavor.
	%-..Hm, and the particular Proposals--Problems kind of discussion might also attract some interests: "Let's discuss how to solve some problems! (about the world or about the app/network itself)..
%(10:29)

%(10:34) I should just call those 'classes of similar things' something special.. Maybe something like 'classes of shared likeness,' well, I know what I mean.. ..Hm, maybe I could call them 'Interest classes'.. (10:37) ..Hm, let me mention that I had a different version of the idea last evening, which was just to make predicates defined loosely via a group of entities.. Hm.. (10:40)
%..Well, I could also just make the tab 'relevant classes' (in general), and then just have a special type of classes that is only (sometimes) vaguely defined with some entities as references.. Hm.. ..Hm, 'Related-to classes'.. (10:44) ..Hm, but this is so close to the 'relevant entities' relational classes, only where the object can here be a group of entities.. Hm.. ..And there might also be \emph{some} specification of what the relationship is here.. (10:46) ..Hm, could I call it something like just a 'family' of entities?.. (10:47) ..Hm, or a 'similarity class,' simply.. (10:48) ..Yeah, 'similarity classes.' And then these are defined by an initial group of entities, or perhaps just one, as well as a potentially vague description of the qualities that the entities of this class should share with the ones from the group.. (10:49)


%(12:20) Okay, jeg undskylder altså lige mig selv, hvis jeg ikke får nået så meget i dag. Jeg tænkte lige: lad mig gå i gang med noget simpelt, såsom at lave en entity list over instances, bare. Men nu tænker jeg lidt, hvordan skal EntityPageBody egentligt overhovedet fungere..? ..Well, jeg kan jo godt lave en EntityList-komponent, uanset hvad.. ..Ah, og vi querier jo altid bare hele listen.. eller så meget vi kan fra toppen af.. ..Hm, og smider vi bare listerne fra tabsne ud.. Nej, vi beholder dem vel i tab-komponentens state..? ..Okay, jeg kommer ikke til at nå så meget i dag, kodningsmæssigt.. (12:27) ..Åh, det fysik, der, til gengæld.. Det er altså spændende..

%(14:19) Hm, I \emph{could} also say that a scale is always defined by a relation, potentially a "tagged" one, and an object.. ..And then 'Instances' will just be a relation (and one which we can give different "tags" to (if it is suited for rating/sorting)).. ..The relation, perhaps via/together with the tag, would also has to denote its scale type, but that is also fine.. And the "tag" can also be a function.. Hm.. (14:23) ..Hm, I think I might like this.. (14:24)

%... (16:35) Yes, I think I will change the scale classes for relation classes, suh that each class of relation just has to define the given scale type (and if other attributes are needed as well, suh as a tag or a function). Let me then actually also call them 'scalar relations' instead, although I'll also just shorten this to 'relations' for short as well. These then produce scales, when given an object, and they produce 'scalar \emph{parameters},' let's call them that, when further given a subject as well, although let us also abbreviate this as simply 'scalars' as well. (16:39)
%(21:11) Wait, you then still need the scales, unless make the Scores table hold both the relID and the objID. But maybe its much better to do one hash and lookup, rather than doing that extra uncompression, is my thought..

%(30.10.24, 10:26) I'll just do the same thing as before, but just make a Relational scales class that can take a relation and an object directly, instead of having to take a class derived from those two things.
%(11:11) Hm, but this makes some weird splits in the Scales class..
%(11:28) Hm, I think I might want the relational classes, after all.. I'm not sure, though.. But I just had the idea: A solution could be to just ("just") make the 'special attributes' syntax more complicated, allowing for attributes that are mandatory only if another attribute is present, or attributes that are removed if another is present, and things like that.. ..Yes, it sounds like that might be a good idea in any case.:) And we can just make sure to let the syntax be open for extension, introducing only most necessary syntax for now.. Okay, so how to make this?.. (11:33) ..Well, but do I even need any of it now, if I'm going to keep the relational classes after all?.. ..Hm, I could make a 'if-not-<attribute name>' syntax to make attributes that should be there if and only if the attribute of <attribute name> is there (and let's say that that always has to come before in the list).. ..Well, it should then be 'iff-not-<attribute name>' instead, and then we can also make 'if-not-<attribute name>', 'iff-<attribute name>', and 'if-not-<attribute name>', where 'if' means 'optional' if, and removed otherwise.. Well, it sounds like it should be 'mandatory, and optional otherwise'.. Hm.. (11:39) ..Hm, it could also be a 'if-<attribute name>-(mandatory|optional|removed)' syntax.. (11:40) ..Combined with 'if-not-...' as well.. (11:41) ..But I should wrap <attribute name> in brackets, since it can include spaces.. ..Yes, so don't use brackets in your attribute names, folks.. (11:44) ..And let's just turn the '-'s into '\s*', i.e. some optional whitespace.. (11:45)

%Great, let me do that, but then: to keep relational classes or not to keep relational classes..? (11:47) ..Hm, I kinda like having them, and I also kinda like the idea of a scale just always having a class as its so-called domain.. (11:48)

%(12:09) Hm, now I think I might be leaning back towards making the Scores table take a relID and an objID, always, such that the Scale class gets removed (unless someone else wants to introduce it), and we let the relations define the scale type instead.. ..I mean, 'cause we're compressing the table anyway.. ..Hm, I think I'll do this, yes.. (12:12)

%(12:16) Okay, now, if my physics idea (see the Nelson renorm. setion below) holds (and I will likely find out if I think so today), then it'll probably be a little while until I return to this project (less than a month, though, I think (7, 9, 13)). So this might be where I'll have to resume when I get back: Making the Scores table take a rel_id and an obj_id instead of its scale_id, and then remake the initial inserts to accommodate for this fact (and also remake the query procs, especially the new one), and then get going with those tab bars from there.. This is of course unless I will change my mind when I get back (if I'll indeed start doing physics now)..:) (12:21)

%(20:03) Yeah, or maybe I'll keep the relational classes, and continue the way I was going, I'm not sure. I'll figure that out when I get back to the project..


%(04.11.24, 9:25) I will continue exactly like I was, always defining the scales such that they have a class as a domain, and then making derived 'relational classes' from relations.

%Something else that I realized here in the shower: Even though my unbounded relative scales are a good idea, in a way, I will use bounded relative scales instead, at least for the early app. I'm thinking of a scale from 0 to 10. Then we can always convert to the unbounded versions of these relative scales at some point if/when we want.

%This then makes the rating part of the app a lot simpler for the early stage. And I even think.. these two other things: We should just postpone implementing any automatic transporting of the ratings between parent and child-/subclasses. So for the earliest stage, it is just the users responsible to maintain all these, if they want.. Well, or put in other words: Let's not care about the ratings of superclasses that are too big and too varied for the (relative) ratings to make sense. It's not that I don't want this whole thing where users get to essentially sort even the very large classes (see what I have written above), but let me just focus on some other things first (for the first prototype). And then the other thing: This means that we can just uprate all relevant scales, other than the obligatory relevancy scale for any given class, and for that class alone, meaning that we don't include any automatically derived scales. The relevant scales are made and uprated for a particular class individually. (And when you uprate relevant scales for an Entity, each scale of course knows it own domain, and it is therefore easy to figure out if you also want to uprate that scale as a relevant scale for its given domain (you useally do when you have created the scale, after all).) So all sorting for the earliest stage of the app will just be to see, when you stand at any given class (including relational classes), if there are any other uprated scales for that class (apart from the standard relevancy scale), and if there are, you just click on them and then see---and potentially then give your own ratings for---the relevant entity list.

%By the way, having a scale from 0 to 10 also means that the users can also just be free to score an entity on the scale without seeing the entity list (even just the representatives (or the reps of the reps)), much like it was in my previous little prototype. And since I will then focus much more on the semantic structure part of the app, including the discussions and Arguments and Proposals and Problems, and also very much including the user groups, I might also postpone the drag-and-drop rating, even, at least for a while. (9:43)

%Something else: I should actually also implement representative voting for user groups. Oh, and I also have a point about that that I need to make in the E-democracy section.. Maybe I'll do that first.. (9:45) ..(9:48) Oh no, it's maybe fine that I just write it here, at least for now. 'Cause it is a bit technical and specific anyway. ..Oh, and it's also somewhat trivial, maybe I have even suggested something like it before. It is just that when calculating the votes, you start by looking at the whole user group, with initial weights for their voting power, potentially. Then you extract all users who has given a vote themselves and aggregate that. Then you take all the votes given to other users and derive a new distribution of voting power based on this. Then you can repeat the process in exactly the same way, where you extract and aggregate all users who have given a vote themselves. And then you just repeat this process for a finite number of iterations (or until done). If there are any voting power left in the distribution after these n iterations, you just simply cut that out and renormalize the already given votes so that they sum to 1, if you want that. (This process means that loops will not only not cause problems to the process, but they can even make sense to have.)

%Now, should I try to implement this for the app at a pretty early stage as well?. Yes, I think so.:) (9:57)

%(11:18) The "sorting menu" should then be displayed just above the entity list, as part of the "body" of the page, and thus below the whole subclasses and realtions tab menu. Oh, and something else: I might pull the Info tab out to get more consistency. Hm, and maybe I'll pull out instances as well, or make it a stand-alone tab bar for class entities, well, but why not put Info next to it, then?. So we have a Main tabs bar at the top, which then.. Hm, should there then be a Relations tabs in that, or something to that effect, or should we union it with the 'relevant relations' entity list (yielding us more tabs)..? (11:22)

%(11:24) Oh, something else: Don't we want the current (selected or just opened) tab bar to be expanded vertically. Well, I guess that also relates to the previous question, 'cause if so, then it would make sense to have a 'Relations' tab at the top..
%..(11:27) Oh, we do want this, and we even want these list to be expanded out into the "body" of the page. The difference will then just be that when selecting an entity on this list, where you would normaly go to another column when clicking, here the entity is just transported up into a tab bar, where the top tabs are still shown, but where the currently selected tab is now among this top. And if you then click on the same parent tab again, you get the same entity list as before, but where the previously selected entity, along with all other that has been selected before or after it, is also at the top. (This is only saved in the state for the gavin app page/column; when going to antoher page/column, the list will be (as if) reset there..) If you then select another tab, that will now be among the top as well, but (as I've alluded to), the previously selected tab will also still be there among the top (in the tab bar, after the selction, i.e.). (11:32) ..Let us make it so that the left part of any entity element (in an entity list) is generally always a button to either open a new column/page or select the entity as a new tab, depending on the context.. (11:34) ..Hm, when selection a subclass, do we then show the instances or the sub-subclasses.. I think the instances, and for relations you also show the subjects. Then there should just be a button at the top to switch to the subclasses list or the sub-relations list, depending on the context.. (11:36) ..Oh, you just do this by extending the tab bar in its collapsed version. And when the user clicks on it, that's when the body is replaced with that list of tabs.. Well, except of you click directly on a tab..(?) Hm. (11:38) ..Maybe we make a big button to the left as well here to extend the bar?.. ..Nah, to the right, like I thought before (i.e. to "show more tabs").. (11:39) ..But when.. Oh, but then we also scratch that whole thing of showing the extended list vertically as well; that will then only be once the "show more" button is pressed.. (11:41)

%(12:20) Hm, maybe it could be expandable menus rather than tab bars.. ..Hm, ones to show basically inside the page body?.. ..Perhaps making it more like a PagesWithTabs-within-PagesWithTabs situation, or something similar to that..? (12:22) ..That might make it easier to implement as well, since then you can spread out the state, rather than requiring the whole menu state to sit out at the app page component..

%...(12:56) Yes, I can implement it all with a PagesWithTabs component that just knows its own scale(s) to find more tabs, and knows how to turn a selected entity (ID) from the list into an actual tab (which means deriving a class from it (not a scale; we always open the relevancy scale first)). And then I will indeed just have a main tab at the top with Info, Relations, and (in the case of class entities) Instances, which is then implemented in a (slightly) different way, probably with a different compnent..

%... (15:06) I was going to say that we should also add Comments to the list of basic (main) tabs, since we need different subtabs for different user groups. (I'm thinking of dividing it into user group tabs, where each user group tab then has a sub-tab with comments in chronological order \emph{from} the user group's members, and a tab of the comments sorted by ratings weighted by the given user group (but perhaps of all comments).) But now I'm thinking: The app should just be able to look at a given relation entID, and then if it matches a switch-case, then the standard page is interupted, and another component is used instead. We can also use this for the Instances tab, and we might even use it for Info as well, where the meta info might just be under a tab, potentially the default one---or potentially only the default one for some classes of entities (i.e.\ the class of the app page's/column's entity), and not for others.. Anyway, if I do this, then I should have a longer main tab, I guess, also with a button to expand it..(?)

%(15:15) I will by the way not record ratings of any of the user groups in the early stage..

%(15:20) Yeah, the main tab bar should then be the relations bar, essentially, implemented with the same component, which I might call PagesWithDerivedTabs, or something like that, and then when we select e.g. the Instances relation.. (the app can just change the page component).. Hm, but Instances is not a relation the way I'm implementing it now..?.. ..So do we make all Scales be derived from a relation and an object after all?.. (15:23) ..Yeah, a mandatory relation and object, and then an optional predicate/function that produces a different interpretation of the score, other than relevancy. And these predicates/functions can then also use the object (and relation) directly in their semantics. For instance we might have a scale of Actors of a Movie, where a the score might represent the actors' performance in that particular movie, as an example (of the semantics being dependent on the object specifically). Okay, let's do this!:) (15:31)

%(15:34) Oh, let us indeed just make the chronologically ordered comment section for each user group (but still of course also let the user group bot rate comments, meaning that they can also be sorted when you go to a different tab. But in that case, I think you just choose the user group for an outer context.. Well, but the tab menu could also be present above each entity list if we want.. (but where the defualt user group is determined by an outer context..)), 'cause then we can always just re-sort or filter by rating.. Well, but that'll only be for a later stage in that case..

%But yeah, let me repeat the important news: Scales should now always have both a relation and an object, instead of their class-valued 'domain' from before.

%(15:32) Let me add a second bar when choosing a tab from the "more tabs" list, so that the normal, topmost tabs can stay in the same place..

%..Hm, I think the PagesWithTabs should also have the potential for initial tabs, and also for not including the 'more tabs' button.. ..We can then use this e.g. to the subtabs of the Info page..

%..Hm, I could also have a class of virtual relations, not meant to be used for scales directly, but only meant to be used for tabs.. ..This could be used specially for the meta info tab.. I think this is a good enough idea.. (15:57)
%..With the initial tabs, we do however save a query if the topmost tabs are all initial tabs.. ..But a query that will quickly be cashed by the browser; I think we might just make all the tab bars query fo their tabs.. Hm.. (16:04) ..Hm, sure, let me just do that for now, and then we can always either insert some constant tabs in places that we want, and/or make another PagesWithTabs component to accomodate for whatever needs we might have.. (16:08)

%(16:11) Oh, in general, whenever one would consider combining two (or more) relations for a single tab bar, we should just divide this into two (or more) tabs instead.! For it doesn't usually matter that the two tabs will then (often) consist of a lot of the same elements, that's fine.:)

%(16:21) Oh, maybe I \emph{am} gonna make a user index for the Scores table.. ..Ah, yes, I will, 'cause we can always remove it later if it becomes not worth it. I could then also make it the user group bots' responsibility to make their own user indexes for the various salars, either before or as part of removing the all-inclusive user index. But at first I will just implement the bots such that they might use this all-inclusive index for their processes. In particular I'm thinking about the process of determining user group weights, as well as voting power distributions (for each iteration of the process that I described earlier today). Here the bots might go through said index (one user at a time out of all the users (which is fine at the early stage)) in order to calculate these things. And this thus means that I might implement weight distributions (and voting power distributions) via scores. (16:30) ..Yes, alright.:)

%(17:56) Ah, the task of making the first prototype seems so surmountable now..!:) Let me also just give another shout out to my Arguments idea (thank you very much): Using scales for discussion trees is just such a good idea..

%(19:05) Ah, the app page body component should also generally read the subject class of the given relation, and then it can also branch according to that. So if the subject class is e.g. Comments, it can load itself as a comment page.

%(19:52) Hm, maybe I should give the conditional votes a comeback.. ..(A vote conditioned on another thing passing..) ..Hm, and maybe also one that can't be retracted after a certain deadline.. and which remains valid until a certain deadline as well, importantly.. ..Yeah, we should implement these as well..

%(20:39) Oh, I don't think there's any need, really, to try to make the conditional votes binding. Not for out digital system. So I don't think I need to do that. And then all we have to do, is to count all the different types of votes for each Proposal, i.e. the 'yes' votes, the 'no' votes, the 'yes on condition Condition A' votes, the 'yes on condition Condition B' votes, etc. Let us also make a cutoff for any condition that didn't get enough votes with it as the antecedent. And that's it, then you show the results, and people can then decide and/or renegotiate from there. (Of course we should also automate deriving when two Proposals is voted through conditioned on each other, and such, but that might all depend on the rules of the given democratic system, so that can just be implemented later, sorta on demand..) (20:45)

%(05.11.24, 10:18) Hm, I'm not sure optional attributes should always come after the mandatory. Maybe we just remove that rule and treat them equally..? ..Yeah, so except the description, which is always last, the order of the attributes is just determined by the order in which they were added.. (10:20)

%(10:31) Wait, so do I implement all scales via 'Tags' instead, letting the optional 'Tag' be a function to reinterpret the score as something else, other than the standard relevancy score?.. ..(This is instead of introducing a class for each individual type of scale..) I mean, this actually does sound like a much better solution, right?.. (10:34) ..Hm, but then the tag would just need to specify the type of scale.. ..But we can do that.. Hm.. (10:37)

%..Hm, side note: When we render 'Instances of <Class>', let's just render this as '<Class>' for the scales, I guess.. (10:41) *Hm, let's actually not do this at the beginning.. (11:08)

%..Hm, I like this idea, but I think we need a word that's better (more precise) than 'tags,' then.. (Then we can use tags for the specific subset of the given class..) (10:43) ..Well, 'tag' is actually not too bad.. ..Hm, 'function' might also work.. ..Hm, but then it is in principle a function of the score, returning a (real-world) semantical meaning, rather than being a function of the subject.. ..Hm, something like 'label' could also be a good idea.. ..(Which is why 'tag' is also not too bad in principle..) ..Something like 'interpretation' might also be worth considering.. (10:49) ..Well, or something like 'definition'..

%..By the way, let's actually just make relevancy a 'tag'/'function'/whatever as well (but still have it as the standard one, the one to most often query for first)..

%..Hm, simply 'scale type' could also work.. (10:54) ..Hm, and then we could have different subclasses of 'Scale types'.. (10:57) ..(11:02) Hm, 'quality'..? ..Oh, that seems almost perfect.. ..Yes! And the idea itself of changing Scales this way is also very good, since this means that when looking at different sortings for an entity list, then you just have to look up the relevant Qualities. (11:05) ..Okay, yeah, this is great..!

%(12:07) Hm, maybe we don't want to mix simple tags with e.g. 'relevancy,' which is very much dependent on the "domain".. Well, so are tags.. Hm..
%..Hm, I could also make tags be able to have formats, maybe with a syntax using '<Domain>', '<Object>', and (if ever needed) '<Relation>'.. (12:14)
%..Hm, if I do that, maybe I \emph{should} then render 'Instances of $class$' as simply '$class$' instead..
%..(12:20) Well, we could also just always show Domain in parenthesis to the left side of the tag.. ..Let's do that.. ..And we also don't need the formats even for something like 'performance in <Movie>', as we can just let the tag name be 'performance' still for this, and then elaborate in the Description. (12:23)

%(12:36) Hm, or is it a good idea to let the scale type sit on the quality.. Well, yes.. Ah, but maybe scale type could be an attribute, rather than being defined by the subclass of qualities.. (12:37) ..Yep..

%..There's another thing: I do like having the 0--10-star scale for the relative scales at first, but how do we prepare a neat transition to the unbounded relative scales, without having to change all our Qualities for others? Can we make a scale type that transforms into the unbounded one at some point?.. (12:42)

%And I also need to anwser, by the way: Lowercase for all tags, or just adjectives and verbs?.. ..Hm, if I'm going to use lowercase for verb tags, then I should also do so for attributes.. (12:45)
%..I might just say: Think of it this way: A tag (or an attribute) is never defined just be itself. Thus, it's its own thing. Two tags (or attributes) with the same names are not necessarily equal. They depend on their Description. And users should in principle always read these before scoring (or assigning) them. Therefore, we should always use upper cases (for the first letters), since they are never understood from just the name, but is something more.
%Hm, I could say this.. ..Yeah, I think I will.. (12:53)

%Hm, we could also then use this as a signal when a tag has no further elaboration. When the thing \emph{should} be understood just from context alone, then we can use lower cases, and, as I said, use this very deliberately to say: 'Don't go looking for an elaboration, 'cause it isn't further elaborated. I think I like this convention.. (12:56) ..(Hm, and then I should make the Description optional for Qualities..) ..Yes, let's do this!.. (12:58)

%..Oh, but back to my other question.. ..About the relative scale types.. ..(13:04) Hm, one potential solution is to just use a 'better representation'.. But this is a little bit hacky, though. Maybe we could just describe the 'relative' scale type in a way that anticiates this change.. ..Yeah, I think I might do this.. (13:06) ..(13:15) (But we can still change the Description after the change via a better representation (removing the now redundant part about the 0--10-star scale type).)

%(13:21) Hm, or is it better to make different scale classes instead, and.. Ah, no, 'cause it's a very good thing that the qualities are treated equally, and that the scale type only derives from them. But it does mean that users have to input an entID for the scale type in principle, for every quality that they make.. Ah well, this isn't really a big problem, we can handle that.. (13:24) ..Oh, but that's exactly why we have the attribute classes for entity attributes. Then the app can go look there. Okay, no problem at all, then. (13:25)

%(13:41) Whether or not to use capital first letter for relations with the same convention, I'm not completely sure. I do like to have capital first letter always, at least for the standard noun-based realtions (child instances of the outer 'Relations' class). But on the other hand, I don't really have to decide this now, since I intend to provide descriptions anyway..

%(14:00) Hm, to perhaps divide tab bars into (up to) three parts: Tabs from class, tabs from entity self, and then tabs recently selected?.. ..Hm, yeah, we could do this, i.e. for the PagesWithDerivedTabs component.. (14:04) ..And then it just doesn't have to have both the class-and-class-realtion pair \emph{and} the entity-self relation, and if it ony have one of those, then we just get only one part, plus the always optional 'recently selected tabs' part.. (14:06) ..Great, yes..

%..Oh, but there's also other times, like with 'useful qualities,' where we want to look in both places. Now, I said yesterday (I believe), that we could then just always split the tab in two. Will it be this easy?.. (14:09) ..Hm, not when I'm naming the tabs after the Relation's Noun.. (14:13) ..Hm, I could make a special kind of relations (a subclass, i.e.) for which the app knows to look at both the class and the entity itself.. Hm, this might be a good idea.. (14:14) ..Well, these relations should then have two versions.. But we can maybe just implement this via a Boolean attribute for the relation (i.e. to disambiguate whether the realtion is about a given entity when this is a class entity, or if it is about the instances of that class).. Hm, so yeah, essentially having a formal way of defining such relation pairs.. (14:17) ..Basically where the noun is followed by 'for instances' in one of the two versions.. ..Maybe we could even make this attribute just a standard part of relations.. Ah, sorta like how I at one point considered having a standard 'is meta' attribute.. which come to think of it, I might as well introduce that as well now as an optional attribute, with the attribute having the default value of 'false'.. ..But anyway, shouldn't I do this other thing, i.e. having a 'for instances' Boolean attribute?.. (14:20) ..Yeah, let me do that, indeed.. (14:21)

%..Hm, and then I could indeed split the tabs.. or rather the entity lists.. into two.. ..(And I could also make combined lists, though..) Hm, let me think.. (14:23) ..Hm, maybe we can just postpone implementing any combinations of entity lists, and then, yes, indeed split up each such entity list into two. And for the tab bars, we can do this the way that I just described above.. (14:25)

%... (15:03) Well, it doesn't really make sense to introduce a boolean 'for instances' attr., when the relations have descriptions. One could then instead introduce a subclass of (monadically) derived relations. But I only need it for very specific things, and one of the things, sub-relations, is even different. So no, let me just make the 'for instances' relations the normal way..

%(16:40) Ah, I have a better idea about the tab bars: Rather than splitting them up (apart from the recently selected tabs, which might still get their own line, potentially), let us just query a few from both entity lists. And then when you click on the more tabs button, \emph{then} we just divide the two entity lists up under two different tabs.
%..Oh, and for something like the 'useful qualities' we can also do a similar thing, if we want, just by having three tabs, with the mixed entity list (which only queries a few from each list to begin with.. well, that is up to us; we query for as many as we want) as the default tab, and then the separate lists as two alternative tabs. I think this is a good idea..:) (16:45)

%(06.11.24, 9:22) The ideas from yesterday about changing the Scale class was really great. But I've actually gotten some pretty good ideas here in (and coming out of) the shower. The last one of them, and probably the most important one, is to gather everything before the subj_id and score_val in the Scores table into one thing, introducing the Entity List class. This will do a couple of good things, I think (although I need to brainstorm about it first, here, now), but one particular thing is also to make selecting the bot/user more close to the endusers, and less something that has to be controlled by developers. But anyway, let me see, another important thing is to then also make Instances a virtual relation.. Oh, and an even earlier idea, by the way, is to reintroduce Property Relations (and then use them almost exclusively, instead of using its parent class, Relations).. But yeah, and then let's see.. (9:29) ..So most Entity Lists will consist of a Domain, I think, and a Quality.. Hm, and a user/bot, and then maybe that's actually all of them. And then we just reintroduce those relational classes..(?) (9:33) ..That was the idea, yes.. ..Oh, but then I can't just make that user index.. ..Hm, but Maybe I don't need it..? ..It could just be the bot's job.. ..Sure, it's better that way.. (9:37) ..I think this'll work, but let me think some more if I really want to introduce those relational classes. Of course, now there's not the problem of.. Well, you \emph{can} still derive the 'Instances of <Class>' class, but instances is now a virtual (property) relation, so the app can just (potentially) warn about this. No problem there.. ..Hm, but we can still make the entity lists that are defined from relational classes just take the relation and the object instead, i.e. by letting the app know when to do this.. ..Hm, the trouble of wanting each bracket of things to be its own thing, its own class of entities, while also don't wanting to gather everything in pairs when querying (which is also not possible; whenever you have more than two entities, there are several groupings).. Hm, so let me think for just a bit.. (9:44) ...(9:56) Hm, maybe it was better without using the relational classes.. ..Hm, and maybe I should even keep user_id and scale_id apart.. Okay, now I'm a bit torn.. (9:58) ..(10:05) Okay, I actually don't really want (to use) relational classes very much, save for prefering to use them for uprating.. Well, and when we uprate useful qualities, we (will) even usually do it for the relation, not the class.. Hm.. ..Hm, and that's actually probably all we want.. Okay, so I will probably not use relational classes at all, then. So we keep the Domains spit in their parts, both for scales and for entity lists, potentially. But now I still need to figure out what to do in regards to Entity Lists.. ..Maybe I keep it exactly like it was (no Entity List class).. ..Hm, Scale entities won't actually have much use, outside of when using them for queries.. ..(Scalars can also have their Scale attribute split into parts..) ..(10:20) Hm, is Scalars then the only compounds that I really want to use (for Arguments), or what..? ..I could also use e.g. Entity Lists, if I want, but then just keep them (mostly) under the hood.. ..Yeah, I think so. So '(Scale) Domains'/'Sets', 'Scales,' will not really be used directly as classes (not much, at least), and Entity Lists might also mostly be used under the hood to query for entity lists.. ..Well, let's make that Scales, instead, and drop the Entity Lists class altogether (for now), and split it up into user_id and scale_id again for the Scores table.. (10:30) ..So the only change I might make, then, is to turn Relations -> Property relations, and split up the Scale attribute of Scalars.. (10:32) ...(10:44) Ah, no, I'll keep Scales very much above the hood, and also let Scalars be as they were.. ..Hm, and maybe forget about 'Property relations' as well. That's just implicitly understood. (And if we want to make other kinds, we can of course just make a new subclass with different attributes.) (10:47)

%(11:36) Hm, isn't the "more tabs button" just another, final tab..? ..Well, with a special page component.. ..Sure, let me implement it as a tab. I'v already included JSON strings that can encode for any special tab that one might want (after implementing it). (11:39)
%..Oh, but the tabs shouldn't always be interpreted as relations either. Sometimes they are subclasses, for instance.. ..Ah, but then the 'More tabs' tab just "knows" how to interpret the selected entIDs.. (11:41) ..Hm, and we can always make the moreTabsTab into a button floating to the left via a CSS grid if we want.. (11:47)
%..Hm, should I give PagesWithTabs a function to get a page from a tab..? (Then I could just make it in a modular way, reusing other functions..) (11:55) ..Come on, what is best..? ..Hm, I guess just making the tab JSON strings self-contained.. (12:01) ..Even if it means repetition.. Well, the PagesWithTabs component can still hold the entID and the classID.. Hm.. ..Right, okay, so we can cut out the repititions, fine. Let's do it that way. ..Well, maybe the component should get the page component.. ..Come on, find this out.. (12:18) ..Hm, maybe I should do it more like before and provide a component for each initial tab, and then just also provide a component for all the extra tabs (Including those from 'more tabs')..? (12:21) ..Sure, let me do that instead.. ..(12:28) Hm, I will let that be a pageKey instead (of a pageComponent).. ..Should we then define the page components as sub components, or pass a list as props.. Well, let me do the former.. (12:30) ..(12:38) I need scaleKaysForMoreTabs rather than moreTabsTab if I'm gonna load more almost-initial tabs.. ..And that's fine since we now have the extra tabs component (key)..

%(07.11.24) ... (12:56) I have thought about implementing the "similarity classes" via a IYLTT ("If you like this, try") relation, for which each element in the resulting entity list is expandable, showing the given element's own IYLTT class. (I'm thinking of only make this expandable on the first level, such that if you want the third level, you have to click on one of the second-level elements and go to their page.. Well, or maybe we just make it infinitely expandable). But the point is that then for lesser popular entities, you can make sure to list the popular entities that is very defining for a given abstract quality that is hard to define (and different people might define differently, and therefore it's also hard to search on, otherwise), and then the user can immediately click to see the similarity class of thos popular entities. So in short: a special IYLTT relation with a special page component that makes you access the IYLTT list of any given the subject (in the first-level IYLTT list shown at the top level of this special page. (13:04)

%(19:10) Now that I really intend to pursue the SRC idea fully, the isn't an absolute need for any NFTs, at least not if it won't be too hard to make the SRC contracts. But we might still use them at the early stage, potentially. And then it's a probably very good bonus , as I've talked about, that these NFTs likely will turn into collectibles afterwards.. ..(for the contributors to keep, and trade at will).

%(19:22) Maybe 'members' is better to use than 'instances'.. ..Hm, if I do this, then this frees up 'instances' to mean what I have until now been calling 'child instances'.. ..I think this is the right thing to do; 'instances' is not a common word; 'members' is probably better.. (19:27)

%(21:18) It could also be 'IYLTS': "If you like this, see ...".. *(One could also skip either the 'I' or the 'Y' in this abbreviation, btw.. ..As in e.g. "(If) you like this, try ..." ..'YLTT'.. ..Yeah, that could work..)

%(08.11.24, 10:45) Hm, looking at synomymns for usefulness (and relevancy).. ..Hm 'applicable'..? ..That has almost the perfect double meaning..(!..) (10:48) ..'Applicability' is only a little bit of a mouthful, though.. ..And it is also in too much need of explaining, I think, as opposed to e.g. 'usefulness'.. ..Yeah, 'Usefulness' is better.. (10:54)
%..Hm, and do I really once again want to gether the two "tags" of 'usefulness' and 'fitness,' something that I earlier decided not to do..? (10:56) ...(11:07) Ah, one can always just filter using another user group, but with the same quality. So yeah, no need to split the two things up, I guess.. ..'Usefulness' it is..:) (11:10)

%(12:17) Oh, for classes, I need both relation tabs \emph{and} subclasses tabs.. How do I do this..? ..I should remember to filter the initTabs away from the fetched tabs, btw.. ..By comparing the entID.. ..Oh wait, I've already figured this out: The subclasses tabs are always just the tabs of the ClassPage. So they are always just sub-tabs to the Members tabs.. ..Hm, and then there should be a defualt 'All' tab for the ClassSubge..

%(12:39) Let us make three levels of the tab bars/tab lists: A collapsed single-line one, where we only show the very top most and the currently opened tab, a expended on showing all topmost tabs, as well as all recently selected tabs, and then a third level which is just changing the whole tab bar to an entity list, filling out the "page body".. Oh wait, in this state we actually also want to show the collapsed, first-level version of the tab bar over this full entity list. Okay, and then to expand the tab bar to the second level, let's put an expand button to the left (not right), and then when expanded, let's just put the button to go to the third level right underneath it (making the expanded bar always take up the space of two tab lines, even if there aren't enough tabs to overflow the first line). (12:44)

%(09.11.24, 10:27) Hm, I think I also want a relations tab bar for all class subpages, but preferably a completely collapsed one.. But this means up to at least two tabScaleKeys.. ..Wait, what am I talking about, I already have that.. ..Well, in any case, I could continue like I was, making sure that Subclasses.. Hm..? ..Hm, one option is to make an.. a 'Self' tab as the first amoung the subclasses, with itself as the subclass.. And then you don't go to the ClassSuppage, but to a ClassSubpage minus the subclasses tabs.. Hm, but I think I can do better, though.. ..Hm, or maybe this is actually a good idea.. (10:40) ..Well, I'd prefer to have relations as a first, yet possibly completely collapsed, tab bar at the top.. ..Oh, how dumb; I already figured this out yesterday. I'm putting the subclasses tab under the Members tab..

%(10:52) I should remove pageComponent from the tabKey, and in fact, I think I should let the entID alone be the tabKey. That will make it easier to filter out fetched tabs that are already there. And we can just call it tabID, or pageID, or just id, as to not require it to be an entity ID.. ..Yes.. (10:54) ..Oh, but that doesn't work to well if we want to provide a whole array of initial tabs.. ..Hm, should the.. Oh, we could also provide a getPageComponent callback which.. ..well, "getTabKeyFromEntID()".. Or rather getTab\emph{Component} (Comp).. and tab title.. Hm, and that could be the way to do it.. (10:59) ..Hm, and what about talling how to get the tab titles?.. ..A get tabtitle() with a callback input?.. (11:07) ..Yes, that's a good idea.. ..And let's make the getPageCompFromID() function return an optional pageProps as well, to complete the job. (11:10)

%(11:42) How to mix recently selected tabs, which I might just equate with loadedTabIDs, btw, with the other tabs in the single-line stage of the tab bar?.. ..I could of course just move them all to the front. I think I'll do that..

%(12:43) I think it's better to let SpWT keep a store over tab titles, and then provide the initial titles together with the initTabIDs..

%(10.11.24, 12:39) I have a somewhat chill day today. I'm thinking of removing creation_ident, and implementing it via entities instead. Then I'm thinking JSON objects, where if a key starts with '@', its supposed to be of the form '@<entID>'.. No wait.. ..No.. I guess, we could either say that when a string starts, or ends, with '@,' an entID value is expected, and otherwise a nested JSON object is expected. Or we could make each object property be a dyadic array of an optional entID and an optional JSON object representing "subfolders".. ..I guess I like the latter better.. ..And an advantage of all this is that users can then import other users' entities, and, not least, they can include entities with anonymous creators.. (12:46) ..Let me take a walk, and think some more on how to implement insertions, then... (12:53) ... (13:43) It's just gonna be a PHP program that queries this creations entity, then inputs new entities according to an input object, and then finally updates the creations entity. I might even be able to use this very same program for app's implementation..
%(14:09) Wait, should I make it in JS instead..? ..Hm, why not.. ..Yeah, 'cause then I'm sure to be ale to use the same class for the insert page..

%..Hm, something else: I need to allow for post methods for querying.. ..Unless.. Hm.. (14:15) ..Well, I'll figure that out at a later time..

%(14.32) I'm also gonna make some syntax to reference derived entities as well, btw..

%(15:24) I'm weirdly tired today, even though there's no real good reason for it; I slept well, as far as I know.. ..But oh well, it is Sunday.. I'll just make sure to work harder tomorrow..

%(11.11.24, 11:54) I added is_editable since it could have a future use. But I need to handle it now: should I keep it or not? Should I not just make it possible to turn creator_id to 0?.. ..Yes, I think so.. And what about users who want some entities to be searchable and/or non-editable, yet want to be recorded as the creator?.. ..Hm, we could also exchange this whole principle of using creator_id = 0 as a special thing, as it is a bit hacky, and then use is_editable instead. And users can then just always anonymize entities, even after they have been turned non-editable (so only the creator_id is still editable, and only in terms of being able to turn it irreversibly to 0).. (12:00) ..Hm, it's not quite so easy; maybe I need a second creator_id column, or maybe I can figure out something better.. (12:03) ..Ah, or I could make the creator_id nullable, and then give the option to turn creator_id to null rather than 0 if a colliding anonymous entity already exists.. (12:05)
%..Hm, private entities are always editable, by the way.. ..Hm, if turning creator_id to null because another entity exists, then the user might as well delete the entity.. Well, no, not if it is in use.. Hm..
%..Ah, couldn't we implement the entity SK index as a separate table, and then just require that they are non-editable.. well, or maybe not even that, let's see.. (12:10) ..Hm, and creating the index entry could even be on the user's own initiative.. (12:11) ..(12:17) So we would have a creator_id that can be turned irreversibly to 0, but only for the sake of actual anonymity, then we have is_private and is_editable. The latter is always 1 when is_private is also 1, and it can be turned to 0 for the sake of making other users feel safe in using your entity id.. ..And as additionally, non-editable entities can also have index entries created for them.. ..Yes, I think this makes a lot of sense.. (12:22)

%(12:32) Hm, we'll have only a few discrete stages, then: isPrivate or (!isPrivate and isAnonymous) or (!isPrivate and !isAnonymous, and isEditable) or (!isPrivate and !isAnonymous, and !isEditable).. Hm, so four states.. ..But I don't think I can gather them in any better way, no, so let's keep creator_id, is_private, and is_editable..

%(19:02) I have a few small comments to write at some point. But let me start with something else: I think the correlation score for the arguments should be more free, and thus not generally always be additive, but instead just be dependant on the given user group's/panel's 'Theory,' which are supposed to be these descriptions of how the user group calculates their scores for certain types of scales from their Arguments. Of course, the user group can also score a scalar directly, and just take the (weighted) median or mean of that. But the can also make a promise to also look at the given Arguments, calculate a derived score from that, and if the there's enough work and enough scorings behind those Arguments, they can then adust their score of the parent scalar from this calculation.. Hm, but we could also just have two different scalars for the same thing: one for which the bot just takes the weighted median, and one where the score is based on a calculation. Oh, and we can by the way also implement uncertainties on all these scores: Just make a scalars on the form (Object=Scalars (class), Relation=Members, Quality=Uncertainty).. And the bot can then in principle also calculate these uncertainties automatically. Hm, yeah, let's do that, actually.. (19:12)
%(19:15) But does this mean that the Theories should actually be instructions to the user group bot..? ..Hm, they should be more than that. They should also be able to include all kinds of instructions *(i.e. to the members of the user group concerning what to do and what not to do, and how to behave, etc.) that the user group decides. But it could contain such instructions.. ..Now, one particular case where one might not want additive correaltions is w.r.t. probabilities. Here you often rather want to multiply the scores of various Arguments (if these are probabilities) together. And say that we actually keep the relative score bounded (from 0 to 10), then we would also want.. Well, that depends on the parent scale type. But if the parent scale type is something unbounded, such as e.g. 'Happiness per time spent,' then you would probably want a divergent correlation towards the end points of the bounded scale (at 0 and at 10, i.e.). (19:22) ..(19:27) Okay, I think I will do this at some point, but I will postpone implementing any such parsing of the Theoris, and initially just use my previous system of additive correlations (or additive impacts, rather) as the only option in the beginning. (19:29)

%(19:30) Another thing that I had to mention was that instead of looking at URLs, the browser extension should actually especially also search for strings like '@123' or 'entID=123,' and then use these. The idea is then that users should post such references in the comments of the given site. This has the added benefit that other users will want to see what all this is about. It could thus help draw users to the site. And of course, posting links directly to the site whenever possible is also a very good idea. And with this, I don't think we actually really need the browser extension. I think users will just copy an entity reference posted by another user, or copy the site's URL, then open a new tab with the Semantic Network website, and then paste this into the search field. This is then possible both on desktops/labtops as well as mobile devices. And for the former, at doesn't require a browser extension. So thare, I think that's better than a browser extension.

%This also leads me to another thing, 'cause one benefit of using NFTs, or centralized tokens altarnatively *(but NFTs of course has the previously mentioned benefit that the can get an additional value as a collectible after they have been used), is that it can also help to distribute the work in verifying claims. So users might for instance have claimed to posting links to the website on various other websites, providing some written description in some form that points other users to those posts. And then the other users can rate the veracity of those claims, in a distributed way. And the implicit thing in all this is: Yes, it might be a good idea to reward users for posting links to the site, \emph{if} done in a way that is helpful to the users of those other websites, of course (the links has to be relevant, at least if posted in specific places with a specific context). (19:41)

%(12.11.24, 9:28) I'm going back to 'Relevant' (I think we should use adjectives whenever it suits), and then the description should just quickly say 'relevant and useful to the list.'

%(13:34) Oh right, I had to mention: Theories should just be able to declare the bot, user, or user group that should be responsible for calculating the derived scores. Of course, one might also implement more advanced bots at some point, where there can be an additional specification input (and for users and user groups, this specifications can just be human-readable text), but until then, the Theories just denotes the given bot/user/user group. (13:36)

%(14:20) Hm, I could just store the workspaceEntID in localStorage for the time being: until I implement a scale where the app can uprate the user's workspace(s)..

%(17:02) I can't be sure of the order when inserting in parallel like I do now, but then again, I can just make sure to store a hard-coded version of the initial workspace, and then all these will get.. Wait, no.. Hm, but I could make a program that just queries for all the entities that we want to know the IDs of, and then outputs a JS program that becomes the module that I want to import from.. ..Hm, that's actually nice to do in the first place, so yeah.. (17:06) ..Then there's just a little bit of circularity that I need to deal with, since the workspace entities will be dependent on these IDs. But maybe I can just run initial inserts twice, then run the program that outputs the module to export IDs, then change and save that module, and then restart and run initial inserts once more.. well, or this might be the second time, actually; maybe we don't need to run it twice at first. Nah, why would we?. Okay, so that could be my solution. And then the constant IDs are taken care of as well in the same sweep.. (17:11)

%(13.11.24, 10:31) Gik for tidligt i seng, viste det sig, og kunne så ikke falde i søvn før sent. Så nu er jeg også stået lidt sent op. Ikke så sent her, dog. Men jeg kan ikke rigtigt få hul på at udføre min løsning her fra i går. Så lad mig taste-tænke lidt.. ..Jeg vil gerne have at.. Ah vent, er løsningen ikke bare, at jeg kun kører initial inserts, når jeg har slettet databasen og startet forfra? Og så er denne funktion så altid bare opfulgt af kopiering af omtalte modul. Og derefter må jeg køre nogle andre programmer, som kan.. Tja, men disse programmer kan jo også ende med at definere konstanter, som jeg gerne vil bruge.. ..Hm, er løsningen så ikke bare at køre alle inserts i dette ene program, og så gemme.. workspace til sidst.. Men hvad så, når jeg gerne vil redigere (små ting)..? Hm.. ..Alternativt laver jeg en initial inserts kun til bruger og workspace, og.. Hm.. Ah, jeg behøver ikke skalaen til at oprate den. Jeg behøver bare, at workspacet for et konstant ID.. ..Hm, jeg kunne også lave en.. Nej.. ..Det ville være farligt (at lave en metode til at ombytte ID'er).. ..Hm, skal jeg ikke lave en metode til at danne ID-modulet, som så kan køres når som helst, jo, det bør jeg.. ..Og så er alle ID-værdier ligegyldige i praksis.. ..Hm, jeg kunne måske gemme workspacet efter den første kørsel af initial inserts.. ..Hov, var det ikke også det, jeg tænkte i går?.. ..Tjo tja, ikke det med at gemme workspacet først i anden omgang. Men det behøver jeg heller ikke.. ..Men lad mig gre det, ja. ..Ah, der er altså en svær ting i det, for måske er det bare nærmest umuligt (slet ikke umuligt, men alligevel..) at få programmet til både at redigere og ikke redigere.. Tja, nej, jeg kan bare query'e for workspacet, via en skala muligvis, og hvis der ikke kommer noget, så kører vi bare inserts'ne med et nyt workspace-objekt.. ..Så programmet importerer fra det modul, det selv genererer.. ..Ok.. ..(10:58) Oh, I can also just make sure to insert the initial_admin and their workspace in sequence at first. Let me also do that..

%(14.11.24, 10:36) Subclasses should generally reject submissions, I think, if they don't add to the Special attributes. 'Cause as a rule of thumb, we should just the parent class instead, if it doesn't change the semantics (to reduce duplicates). I should also make a 'Useful for specifying new entities' Quality, and then uprate it for the 'Subclasses' relation..

%... (14:18) I think maybe the 'Relevant' Quality ought to have a Relative scale type, instead of a Probability one.. ..Well, except that we perhaps want a fixed offset, always, and therefore don't want a completely non-absolute scale type.. ..Hm, but maybe the bots could just.. well.. ...Hm, now I'm considering using a grading scale again, which could just be the relative scale but where the star score is also converted and displayed as a grade.. Well, then again, this is not too great for scales that are supposed to measure something linearly, like happiness over time, and such.. (14:37) ..I by the way got the plusses and minusses wrong last time around: They mean plus/minus 1/3 of a grade, not plus/minus one whole grade..
%..Hm, let me indeed hange it to a more arbitrary scale such as the relative scale (i.e. the scale type of the 'Relevant' quality).. ..And I could also just change the 'Relative' scale type to 'Arbitrary' scale type.. (14:41) ..Hm, and maybe we could even keep it bounded, and just implement e.g. 'happiness over time' via an.. well an 'Arbitrary, unbounded' scale, and then the other could be the 'Arbitrary, bounded' scale.. Hm.. ..Or just a 10-star scale, and then we could call the unbounded one just the 'Arbitrary' scale type..? (14:45) ..Hm, maybe a 5-star is better?.. ..At the end of the day, this is all up to the users, except that I start the whole thing off, and my conventions have a good change of sticking (not least because they will be the only ones implemented at first).. ..(14:51) Oh, the cutoff point for the Relevant scales should actually be in the midle of the scale.. ..So all I would need is to fix the offset for the relative scales (meaning that at some point, the bots will start shifting the scores to meet the required mean.. oh no, we can't do that for the Relevant scales..).. (14:54) ..Hm, let me by the way implement both the 5-star and the 10-star scale. And maybe the Relevant scale should then just be one of these.. I think so.. ..And let me keep calling it the 'Relative' scale type, as well, and maybe I should stick to the idea of making it bounded at first, but then open up for it to be unbounded ones the bots have become advanced enough to handle this.. (14:57) ..Yes.. (14:57)

%(15:24) Oh, haven't heard of the 'await' operator in JS until now. Nice. Then one can serialize a lot of actions that each wait for callbacks much easier. Also, JS \emph{does} have a built-in SHA-256 algorithm. I don't know why I couldn't find it again the second time I looked, nor why I forgot about finding it the first time I looked..

%(17:06) In terms of selling points. Let me try to go through some things again:
	%- You can rate all kinds of things according to all kinds of predicates. I think I can find something for everyone that they can relate to, for instance: movies/series, music, products, services, events *(books, recepies, excercise activities, activities, sport clubs, clubs, organizations, activity groups, podcasts, institutions, moral values/codes, life tips, advice, political opninions, parties, performances, theater plays, etc.). For all these things, you can go and look at what other people has uprated as they important qualities about the given thing, and look how they rate those qualities, and how much those qualities count towards the 'goodness' of the thing.
 	%- And when you are there, you can give your own opnions through rating these scalars, and you can add your own points about the thing. Maybe others will then start to rate the quality that you pointet to as well, both in terms of how much the quality applies, and also in terms of how much this quality contributes to the 'goodness' of the entity. I should formulate this in a much better way, but I know that I can do so, and this is just such an important point, together with the previous one: Being able to argue about why things are good and bad, and see what others think, and see how they respond to your points. This both gives good UX in terms of being able to reflect on why something is good, etc., and good UX in terms of being able to express one's opinion (and not least in a way the can be aggregated well, instead of just knowning that your point will most likely be buried under a gaint heap of other comments/points (which I often the same point repeated many times)), and last but not least good UX in terms of seeing what other people think.
	%- Now, for a lot of qualities, it also makes sense to use these for searches, namely when they are general enough to apply for the whole or a large part of the given class at once. And then you can even adjust your search such that some qualities are boosted over others. And as a very important point, you can even do this in an anonymous way, without having to give your data first to an algorithm, and in the process give that data to the website that can then sell it onwards as it pleases. Maybe I should separate these points. But yeah, the simple point is, a lot of the qualities can also be used for searching on.
	%- I mentioned 'events' above. Let me elaborate that there's no limit on what types of activities, events, clubs, organizations, etc., that can be discussed and rated on the site. So users will even be able to search among all kinds of clubs, events, and activities in their local area (if ordering these into subclasses based on location as well, although could also just search on general types of events/activities/clubs and hope that they can then also find an instance of such in their local area afterwards). So adding this to the list, I really think that I can find something for everyone.
	%-(17:29) Apart from discussing and rating specific things like products, etc., one can also discuss statements, and proposals. We all have something that we might wish to discuss, and see what other people thing, about the specific statement, and about the underlying argument, and how they impact the given statment, and perhaps about the underlying arguments of those arguments, etc. Having a discussion tree where the most important Arguments are shown first, and each one can be rated both in terms of its truth probability / agreement, and in terms of its correlation to the parent Statement, will be useful for almost all people (even if they don't plan to add to the discussions themselves). This might in fact completely change how we as individuals do and interact with politics.
	%- Then we have the very important user groups, which means that you can see the opinions of specific groups specifically. This can be used both for discussing statments and proposals, but also for any other things, like movies, products, etc. Here you might prefer to see the opinions of the groups that you share opinions and/or tastes with mostly. And in terms of statements and facts, you probably want to se the opinions of experts, and in general users that are trusted, and specifically trusted in that area of knowledge. I give you: Fact checking! And in a decentralized way! (And don't worry about echo chambers: These will be far less prone to happening with this system where the user is responsible for selecting the user groups themselves. That is the problem with current social media sites (one of the problems): The users aren't in control of the algorithm, and don't generally know when, and to what degree, they are being placed inside echo chambers. But when the user controls this themselves, they are bound to also see what other groups thinks about certain things, out of curiosity: Who \emph{wants} to close their ears to what other layers/segments of society thinks about things. Maybe some will at some times, but almost none will at all times: People are almost bound to step out of their bubble at some point, even if they have found their way into one.)
	%..Hm, and what else..? (17:43) ..Oh, and then there's also the whole thing about having a democratic Web 2.0 (or maybe "3.0," if we decide to call it that) site, where creators are rewarded fairly, and the users aren't expoited. Also: no "enshitification"! And on top of that, there's my idea which might result in more user payments, and not least also the whole idea of the positive feedback loop that the SRC part of the idea will create in terms of getting users to join (and keep being an active part of) the network. But these things don't have to be part of the initial pitch. (There are more than enough points without it, for an initial pitch.) :) (17:48)

%(17:49) I should also rewrite these points now and again, more and more concisely each time, hopefully, to prepare myself for pitching the idea. But it's just so nice that I feel (again) that I can sell this idea well. I've had a time recently where I felt like I still needed something. But no, these things should be enough, and this conclusion has made it easier to just focus on the task at hand: I don't need to get distracted now by thoughts of: 'Hm, is there any ideas popping into my brain about how to make my break through.' (When you are an inventor/idea getter, during a time of needing ideas, you need to check in with your brain at pretty frequent intervals to see if something else pops up.) Now, however, I feel, for the first time in a very long time, that I don't need any more inventions/ideas (save for the smaller ones that I'll get in the process, such as the ideas about the 'relevant' Quality from today, and all that) to make my idea possible, and to get my break through with it. I just need to complete the work, then spread/sell the idea. Of course, there's the physics idea as well, but I don't think I will go any further with that until a future point when my site/network is taken off. It's just so nice to feel like I know how QED works now, and also that there will likely come something beneficial out of all my work, all those years, at some point. So yeah, bottom line: I now feel like I just need to do the work, no more big ideas needed. And that's a bit strange, actually. Funny thing is, you would think that this would make me more relaxed. But I actually blame this new and somewhat unusual state for why I haven't slept too well.. Well, specifically the night before the last one (so the one from fore-yesterday to yesterday), and then last night (the one to today) I just slept a lot, probably because I needed it. However, I think that this is just a very temporary circumstance, and that soon enough, I will find myself more relaxed and focused on completing the job because of this. I've already found a much bigger focus recently, to begin with. This was actually after the physics detour that I had a here a little while ago. So yeah, I already have a better focus, it seems, than I've had for a large part of this fall. And now I just need to sleep a bit more efficiently as well, so that I don't have to get up so late so many days. (By the way, when I do sleep badly, it's still much better to stay in bed and try to make up for as much sleep as possible, rather than getting up early and then just burn out pretty quickly. That's what I've found.) That would be very nice. Then I might be able to get back to working those long days, like I was last year at this time (although I don't want to work myself quite as hard this time, even if I become able to---only possible when I'm finishing the prototype at some point, hopefully soon enough).. :) (18:10) ..Oh, and by the way, I found it fitting to make this little diary-like note today, since it is exactly 11 years today since I moved in here. Cheers for some highly productive years. :) ..I hope they will come to bear fruit soon. (18:11)

%(15.11.24, 13:32) I think I should hold on to the relative scale type, and not use star scale types in its place.. ..Well, except that I want a clear F(ailure) cutoff for relevancy scales.. ..Well, all (unbounded) scales have a 0 point. Maybe we could make a convention that for adjective Qualities, 0 and below means that the adjective does not apply (at all).. Hm.. (13:36) ..(13:38) Well, or an even more general convention is just to say: 0 is generally considered the cutoff line. And then, for adjective tags, 0 then means not really <adjective> at all, and less than 0 means less than <adjective>.. ..That sounds good.. *(Yeah, this is a really good idea. (13:58))

%..Hm, and maybe I actually do want my whole Settings system for the first prototype..(?) (13:42)

%Also, I think I might rename 'scalar( parameters)s' as simply 'parameters' instead..

%(14:26) I'm thinking of getting rid of 'Scale types' and just make each Quality define the unit, the bounds, and potentially also the "cutoff/tipping point"..

%(14:33) I was thinking that: 'I can always just make subclasses of Qualities instead, e.g. for future star rating scales, or grading scales,' and then I thought: 'I should also turn implement '=<some constant>' instead of 'removed' (for the Special attributes), but then I realized that this would not be optimal: It's better to have all the relevant data sit on the entity itself, rather than.. Oh, but we could have the best of both worlds. We could implement the '=<some constant>' option, but then make it so that the app still submits this constant attribute for the given entity.. (14:36) ..Yeah, either that, or we query the class and ancestor classes for constant attributes.. ..Which would be better..? ..(14:40) Well, I think for a long time, we should just use the "root classes" (when not counting entities as a parent class, even though it is) to define entities, and thus not define entities via subclasses.. Hm, of course, we will probably need.. Hm.. No, maybe it's better to query the parent class.. Hm.. (14:42) ..Well, the class.. Oh, it would only require one extra query if we just make a convention of only using level-three classes at most (counting 'Entities' as the root this time, at level 1) for defining entities.. ..Hm, and alternatively we just branch and implement each class seperately (and thus keep using just the 'removed' option).. ..That actually sounds more practical, at least at first.. (14:47) ..Yeah, and this solution is also backwards compatible with when at some point we do implement the '=<some constant>' option as well, and then query the class and the parent class (and potentally even more ancestor classes), when the given branch is not yet specifically implemented.. (14:48)

%Yeah, so let me postpone implementing this '=<some constant>' option, and let me then indeed just make subclasses for 'Qualities' if I/we ever want to implement special scales like star scales or grading scales. And when I/we do this, we can then either choose to expand the hardcoded branching (which is probably what we will do anyway), or we can in principle also use the occasion to then also implment this '=<some constant>' option (which we might also want to do at some point in any case as well). (14:52)

%(15:04) Okay, so it seems that I now want to start off with the unbounded relative scale, unlike what I thought until now. So should I stll make rating bars, are should you then always go to the list and rate there, potentially by dragging and dropping (I think I might implement a version of this, actually, for the first prototype, only where we simply cut and paste instead, which might actually be better in the end as well, come to think of it..)..? (15:06) ..Hm, then I should in principle also have representatives.. Ah, but for small entity lists, the app can just query the full list itself instead.. (15:07)
%..Hm, now I kinda do want the star scale(s) instead, after all.. (15:09) ..Well, I do want a bounded scale, I think.. ..The metric of each relative scale is then just not defined (to begin with).. ..Hm, and maybe I do want star scales exactly (also over grading scales, since the latter is perhaps too specific in terms of the associated semantics, wheras stars are just loose enough..).. And then maybe I'll just reserve a special word for having stars as the units of the scale.. (15:18) ..Yeah, I'll do that, and indeed I might use star scales for most things, then (other than probability and other value scales).. (15:19)

%(15:27) Oh, the syntax should not be '=<some constant>' instead of 'removed,' but should be 'constant' instead of 'removed' (or 'optional' or 'mandatory'), and then the Type should just be '<some type>=<some constant>'.

%(17:20) Hm, lt's remove that Cutoff point attribute.. And I think that I'll let the tab bar size for each stage be determined by the scores, including perhaps up to 20 from the top part of the score range. Let me also just add any other picked tabs at the end of the list of tabs, if they are not in the list already. And do I then remove the "second stage" of the tab bars, or what..? ..Yeah, and then the button on the left just always goes to the "third" stage, i.e. replacing the page temporarily with the More tabs page.. (17:26)

%(18:52) Funny idea: I could also make a special 'thumbs up' unit, where 1 is a thumbs up, 0 is a sideways thumb, -1 is a thumbs down, 2 is *thumbs up* \times 2, 3 is *thumbs up* \times 3, etc. And in a really advanced version, we even rotate the thumb from \pi/2 to 3\pi/2 when the score runs between 1 and -1. (18:55)

%(16.11.24, 10:47) For the tab bars, I'll of course just make them horizontally scrollable. And then we just stick the 'more tabs' button to the right (containing just an arrowhead down), above the other tabs that scrools under it.

%(11:18) Hm, but maybe each tab bar should have a small header of some kind as well..

%(17.11.24, 10:29) I'll give EntityList an optional elementComp prop. And let me just think about selecting Qualities for a moment. We should of course query the relation for qualities. And is that it? I don't want to try to query the Set, i.e.\ the object--relation compound, for relevant Qualities, do I?.. ..Hm, we do want qualities for class members.. But here we also even want to query the ancestor classes.. ..Hm, if we decide to query them all, then luckily the data will be cached for the ancestors a lot of the time.. (10:34) ..But for relations, I think I can just query the relation, indeed, at least in the beginning. ..And then do I indeed query the.. Oh, the ancestor row will often be shorter than the subpage nesting depth. So yeah, let me query all ancestor classes up to some maximal number (maybe 3, or thereabout). And we don't need to use Sets either for this: We just uprate the Qualities for the given classes. (10:39)

%(11:13) Hm, for entity reference links, we could just display the ID on the left of the title, and then make that, and only that, a link.. *Well, no. Inside texts, I want the whole entity reference to be the link. And in other places, I can just insert the entID link manually next to the entity reference, somewhere. (11:38)

%(11:31) I'm taking it slow today (apparently)..

%(16:04) I think I'm gonna add context.. well.. Qualities, right?.. ..Context qualities, which are then added to the.. Hm, qualities tab. Oh right, Qualities are indeed like my former Tag now, only an extended version of them. And yeah, let me give context Qualities to EntityPage, which can be passed as a prop. ..So when we click the Qualities button on an entity element, we go to the EntityPage with the Qualities tab open as the initial tab, and where the extra "context Qualities," or what to call them, are also added. These are then taken from the given tab where the enity list with the given entity element is shown.

%(16:10) Now, where to put the insert new entity to the list button, well, or to the class.. The problem with inserting under relations is that the Members relation has Entities as the Subject class.. So what to do..? ..Well, we can certainly put an insert button or tab in the AllMembersSubpage.. ..Yeah, and in the RelationSubpage body as well. And then we just need to make sure to provide the right Set in both cases, as well as.. well, as well as the right class not least, of course. And then we can also even provide a list of qualities, starting with the currently selected one, which the user can rate immediately after insertion. And when the entity already exists, and the user just need to type in the ID?.. (16:18) ..We could make two tabs for the insertion page: 'add member' and 'create new member.' ..And is it okay to also call Set members 'members'?.. I guess so, right?.. (16:21) ..Sure. (16:21)

%(16:23) If I'm really cool, I also implement a 'show in list' button at the rating bars, where we then go directly to the score that is currently the value for the rating bar. And then we insert the given entity in that spot, as if the user had just moved it there. We can get the local part of the list be making two queries on either side of the given score value, one with a=0 and one with a=1.. (16:25) ..Hm, I actually think I should do this.. (16:26)

%(16:29) By the way, the Qualities tab should be split up into the '<class> -> qualities for members' part, which is the standard one, and then the '<entity> -> qualities' part, which is where one can find reviews and such as well, which is specific (and might potentially be a spoiler for) the given entity. (16:31)

%(17:27) Hm, maybe Reviews will be under the Arguments for the Good quality instead.. ..And maybe we should make an expandable Arguments list under each Quality element.. ...(17:44) Well, it could also be a button that opens the Argument list in a new "column," I think that's better.. *(This means going to the page of the given scalar, or rather 'parameter,' as I call it now.. with the Arguments tab open as the initial one.. ..Hm, maybe I should call them 'scalar parameters' in general, and thus not abbreviate this to just 'parameters'.. (18:56))

%..(17:45) Hm, that made me think: As an alternative the the "app pages," we \emph{could} also make the whole site be one (Sub)pagesWithTabs component.. Just I thought, but maybe worth thinking about.. ..Nah, no reason.. (17:52)

%(18:59) I think I'm going to denote a non-trivially qualified scale by 'Object->Relation[Quality]'.. ..And then the Scalar parameters? Maybe 'Object->Relation(\[Quality\])? : Subject'..? (19:01) ..Or maybe 'Subject \in 'Object->Relation(\[Quality\])?'..? ..Hm, I like that first one better (but without a space before the ':'..).. (19:03) ...(19:53) We could also use angle brackets instead.. *So 'Object->Relation(<Quality>)?'..

%(11:10) Lå vågen mellem 4 og halv otte-otte. Igen var det den der uforklarlige varme, der er vendt tilbage, selvom jeg har åben badeværelsesdør (og vidue), og åbent vindfang. Fik så sovet lidt igen, heldigvis, og fik i det mindste tænkt nogle gode tanker. Og nu er der så nogle ting, der skal tænkes lidt mere over:
%I'm thinking of implementing 'useful as a representative' as a Quality. But then it needs both the class, or Set, \emph{and} also the given other quality.. Ah, but one could perhaps used derived, compound qualitites.. (11:14) ..'Alternatively one should make a compound.. oh, that's just the whole scale. So yeah, it could just be Scale->Members<Useful as a representative>. Then we're only making the compound that is already made to begin with. Okay, so I can do that.. (11:17)
%..Then there's choosing the right Class/Set for the Qualities under the Qualities tab..(?) ..Should this be done by actually giving each Quality a Subject class attribute?.. (11:19) ..That might actually make sense, even though it might appear redundant at first sight.. (11:21) ..Oh, but for e.g. Comments, we need the Set as the.. Hm, no.. ..No, the Subject class here can also be the Comments class. This will then mean that you can get a score semantics for comment qualities that are less relative and more "absolute" in pratice, as you can then compare with representatives among all comments. (And you don't need to do this; you are also just free to score a quality directly using just the bare rating bar.) (11:27) ..Okay, so let's introduce Subject classes for Qualities.
%I btw also have to make a Members relation for scales, then, and I'm also btw considering given classes a singular noun attribute.. ..I'm even thinging of letting it be a regex-like formula, such that the standard value is: 'ves->f,ies->y,es->"",s->""'.. ..'Cause then it only need to be corrected (unless I'm forgetting some regular endings) for the irregular nouns (or for compound nouns that behaves in a more complicated way).. ..But let me just postpone this for now, as I can then always just add this 'string="ves->f;ies->y;es->;s->;' attribute to Classes if needed/desired.. (11:35)
%..Well, but I might use these singular nouns both for easier Quality disambiguation, now that they have a Subject class, as well as also the app page header. So maybe I will introduce it now.. (11:38) ..Oh, the words ending in 'fe' messes this up a bit.. ..Hm, can't I just make do with the plural words?.. (11:44) ..Hm, I could use the \in symbol. Then plural words will be more fitting.. ..Or I could start out with a convention of providing the singular version of the class name as well, I think that might be better.. (11:51)

%..Good, I think that's it. And then I'm by the way also planning to add the Reviews tab under the Qualities tab, even though Reviews are implemented as Comments.. Well, or maybe not, then.. ..And I'm planning to make use of a 'Sub-qualities ' relation, just like how I use 'Sub-relations' for Relations. (11:57)

%(12:20) Hm, I could also just omit 'f->ves' and 'fe->ves' (inreverse) from the default formula.. ..Nah, better to just include the singular word..

%(12:37) The "Cutoff point" is just always (High end - Low end)/2 by default..

%(13:09) I'm gonna make a SetSubpage and a ScaleSubpage component. And I also need a more robust impl. for the "ADD_AND_OPEN_TAB" action. *(I'll just catch the action in the MoreTabsSubpage component, and send it up to the parent.. (14:30)) This then got me thinking a little bit about my (future) useRestorableState().. And now I had the idea: What would happen if we use useId(), and then store this Id, the parent Id, and the relative key at each component's HTML node?.. ..And for root nodes, we can exchange Id for a root identifier.. ..I could perhaps even generate the IDs myself.. (13:16) ...(13:32) Yep, you could implement it with a passKeys() function, that passes the (parent's) Id, as well as the key attribute, which could be the first input to passKeys(), and which is passed both as the 'key' and the '_key' prop.. ..I could then make a usePassKeys() hook, which I can then also merge into the useRestorableState() hook.. (13:35) ..Wait, why not pass the parent Id via a context?.. (13:38) ..Oh, that doesn't help much.. ..But I could also.. Hm.. ..(13:46) Nah, this passKeys(key, JSX) idea, which also passes the parent Id, is the best approach, I think, out of the ones I've had so far.. ..(13:54) Okay, since I'm distracted now by this topic anyway, let me find out: Do I really want to implement resorable states for the prototype, or shortly thereafter?.. ..Oh, I just realized, passing JSX as props is a bad practice, which means that you have to pass components.. Ah, but this is also fine; you can still pass the keys in the component that receives the comp prop.. (13:58) ..Thus, I can just wrap the whole returned JSX in passKeys() always, which makes it a lot better/easier.. ..Hm, I don't \emph{have} to include usePassKeys() in useRestorableState().. ..Well, but I will, probably.. (14:01) ..Oh, I forgot that I actually have an implementation of useRestorableState() that I just haven't debugged yet.. ..Ah, that's the implementation where the hook requires a full path to the entity.. ..But in my new version here, I could just pass it props instead of id.. ..Hm, I could also just restore by replacing the init state with the stored one after the first render.. ..Hm, but that's not easier than this passKeys(fullJSX) solution, so no.. ..Well, except for the fact that.. Ah, no, there is an important reason, why passKeys(fullJSX) is a bad idea, and that is when you import wrapper components from other libraries. And on the other hand, if I did this restore-on-second-render approach, then I could implement it with just a refCallback for each component that uses useResorableState() only.. (14:11) ..And I could use a React context to pass the parent ID, as well as a boolean if the parent state is ready or not. ..And then the user also gets the option to just render nothing when the parent state is unready.. (14:13) ..When ready, this refCallback then searches the DOM tree to get the relative key.. ..(14:21) Oh shit, then there's the problem of finding the React keys of any intermediary component.. ..Then again, if you have keys, then you likely have a state as well as a component.. ..Oh wait, that doesn't matter when we just save the state when leaving the page!.. (14:25) ..Then we can just use indexes for the relative keys!.. ..Great..! (14:28)

%(19.11.24, 11:06) Sov da nogenlunde, da jeg endeligt faldt i søvn, men det var jeg til gengæld vildt lang tid om..

%I've had some thoughts tonight and this morning about the "representatives," and now also about dividing the 10-star scale into intervals with qualifiers attached to them. I think this latter thing will be a really important thing to do. And I've almost figured the qualifiers out, I think.. ..In terms of the representatives, one can say that the relation could be attached to both the class and the particular quality (i.e. with these as the Object), and then I can scrap the idea of using the Scale as the Object. But I'm also toying with the idea of just letting the bot select the representatives, with priority for the entities that has received the most scorings.. (11:11)
%..(11:15) Now, with good qualifiers on the intervals.. ('somewhat so,' 'pretty much so'--'truly so,' 'very much so,' 'extremely so (among the best--among the best of the best)').. we might not need as much scoring while viewing the entity in the given list.. But thinking back on my Settings-related thoughts, having the representatives and such might still be quite important.. So this is what I'm thinking about.. (11:19)
%..(11:27) Hm, slightly so, somewhat so, pretty much so, .. Oh wait, I have five full intervals, so it could be: 'somewhat so,' 'pretty much so,' 'truly so,' 'very much so,' 'extremely so (among the best--among the best of the best)'.. ..Yeah.. And then: somewhat not so, pretty much not so, not so, very much not so, etremely not so (among the worst--among the worst of the worst).. (11:31)
%(11:37) Hm, this essentially gets as as close to absolute scales as we can come. And it means that we might not even need to compare apples and pears separately.. ..Of course this means more disagreement on the scales, since you don't have the overall degree of freedom of how pears in general compares to apples, but on the other hand we also have ML to make up for this fact. I did really like this idea of users being able to adjust their preferences/Settings \emph{before} we get to ML.. Hm, difficult decision.. (11:41) ..(11:47) Ah, but even though there will be.. Oh, first of all, the Qualities still have a Subject class. So we can still very much compare apples and pears seperately, if we want.. And then there's also the point that despite the added disagreement when using broad Subject classes, with enough votes/scorings, the scores might still have the same relation to each other overall, in the end. And then users can still in principle boost entities of different classes. So yeah, maybe I don't need.. Hm, what don't I need, 'cause we \emph{will} still have those subclass-specific Qualities..(?) (11:52) ..(12:01) Hm, yeah, it kinda seems like we get the best of all worlds: Not really any need to see other scores (of represetatives) when scoring, and we also still get the possibility to compare apples and pears separately as much as we want (or compare them together, if we want).. ..Great so I don't really \emph{need} "representatives" now, and I also don't really \emph{need} drag-and-drop/copy-and-paste scoring, since we can just score via rating/scoring bars alone for the prototype. ..This is really nice, not so much because it saves me work, but moreso because it saves the users some work with learning how the system works. (12:06)

%(16:13) Oh, about useRestore(), I can require the users to return something trivial (and stateless, not least) when the data is restoring (and not ready yet). Then the components will always only be mounted once parent is already ready..

%(16:26) Forget about "among the best (of the best)." Maybe I should just say something like 'extraordinarily so' and 'extremely so,' diving the last interval into two halfs. (And one can also divide the intervals next to the middle into 'slightly so' and 'somewhat so,' if we want. And we can keep the exact middle as 'neither nor'..)

%(19:26) Hm, I should consider making Comment classes for all individual entity (that has comments).. ..Well, no, 'cause it actually makes sense to be able to import a Comment from another entity to.. Hm.. ..Would it make sense, then, to just let the Object of the Comment be determined by the context..? (19:30) ..Not if we're also using 'Claimed by user' at the same time, but maybe we don't want that at all anyway.. Maybe the creator of the comment just has to rate their own agreement with it, which can then be queried by the app and shown, if the browsing user clicks to see this score.. I think this is better..:).. (19:33) ..(Although I don't know yet if we'll let the Object be a placeholder.. Well, I don't think so, actually..) ..Hm, I think, let's just use the one class of comments, and then the bot can just be implemented to automatically remove comments abut different things from the commet setion, if that is desired.. (19:35) ..Oh, but the app can also just notify, when there's a mismatch, and then it can, at least at an early stage, just be up to the users, whether such "imported" (not necessarily relevant, but perhaps) comments are welcomed or not. (19:40)

%(11:48, 21.11.24) Ah, damn. I can't return something trivial at first, and then also expect the parent node to have the have the attributes set at the time when it is created. ..Wait, what am I talking about.. Hm.. ..Okay, my brain is a bit slow now, even though I don't feel tired.. So: Will the parent node always get the attributes first? ..It will get it if it loads and then the child modules loads as trivial things on the first render. But then they just can't look to their parent on the first render.. Which is.. Ah, which they won't if we only give then the refCallback once they are ready. Okay..
%..Hm, I'm don't feel in the mood for debugging now, however.. (11:55)

%(12:31) I'm distracted by physics thoughts...

%(21:10) Having 'Informative comments' isn't enough: I should highlight make 'Clarifications' a central relation to statements, almost as important as Arguments (or at least often found right besides the Arguments tab).. ..Hm, and at some point, we can introduce a syntax/formalism to make Clarifications define exactly the part of the given (parent) Statement that it clarifies, making us possible to overlay clarifications as annotations if/when we want that.. (21:13)

%..Oh, and another thing: Let us perhaps just call the 'YLTT' relation 'Liked for similar reasons' instead, i.e. rather than 'If you like this, try'.. I don't know if we want to abbreveate it, though.. 'LSR'..? Or TLSR,' where T stands for 'Things'..? ..Well, maybe.. (21:16)

%..(21:19) Hm, and my modular documents, when we implement those, should be made up of Statements, such that user can immediately go and clarify, argue about, and show agreement and/or belief in each indivifual statement that makes up one of these modular texts. (21:21)

%(22:25) Hm, one could also maybe hash paragraphs to automatically get the Statement ID of that paragraph (or insert it as an individual entity).. ..Yeah, let's do that..:) (22:29) ..And that also means that people can search for posts and/or paragrafs from other places on the web, just by copy-pasting that text.

%(22:33) Hm, even though my useRestore() is pretty cool, once debugged anyway, maybe I don't actually want to use it. Maybe I just want to push a state each time the user goes to a new column, and then if they go to one that has been closed (which can happen if you move away, then back to a column, then closes it, and then goes back again from there), we just immediately go back one more page. ..And we can perhaps also pop a page when going back to the one that was.. Nah, that get too complicated. Just push a page for each click, left or right.. ..(Also just open external links in new tabs always (why not..)).. (22:38)

%(22:39) ..Oh, (T)LSR \emph{could} actually also be a Quality instead, meant for the 'Related things/entities' relation.. (And then it would like be 'Liked for similar reasons'..) ..I actually like that a lot, 'cause it's a good thing to make (T)LSR a subtab of 'Related entities'..:) (22:42) *(Well, but it could just be a sub-relation instead, maybe that's better.. ..Yeah, 'TLSR'..)

%(22.11.24, 8:30) There should be a Truthfulness Quality, which is supposed to be the result of taking an average (perhaps somewhat weighted after importance, although the weights are up to the individual user to figure out in their head, and not something more systematic than that) of the probabilities of the statements in a text. Basically: How much of this text is truthful? (where probabilities between 0 and 1 just contributes with that fraction the the average calculation). ..Well, 'Truthful' is preferred.. (8:35)

%(10:06) I should be programming, but I can't help thinking a bit about it all instead. Just had this idea: Statements should also have a Quality of 'Urgent' or 'Important'.. yeah, the latter, perhaps..  which is a measure of how much the community wants the statement to be analyzed and/or answered.. ..Hm, the question here is then if the median suffices for this. Also, if we use some 'freshness' system, this could help provide a feed of important questions/statements as well.. Hm.. (10:11) ..Hm, maybe we should use that up-rating system that I've come up with before, where users have a private entity over all their recent votes, which is then compiled by a bot.. ..Which is then something I should implement later on, I think. Certainly at least if we want several different types of upvotes.. But maybe we just want the one: 'Importance'.. ..Well.. ..Yeah, nah, at some point we might want several voting-qualities like that, but only at some point.. (10:16)

%(10:20) Hm, let me drill a JSON array of super-relations (from the tabs) to be used to fetch scales to score the entity on, e.g. a newly inserted one, as well as fetching relevant Qualities..

%(10:25) Jeg ved hvordan jeg vil lave insert subpage og insert button, og sådant, mener jeg så. Og også scoring.. Wait.. Should Qualities not know their Relation is well..? ..No, think of 'Relevant'.. ..Hm, so my former 'Relevant tags' page, what will this be now? The 'Scalar parameters' page?.. ..I guess so, yeah.. (10:29) ..But really it's two things (at least): a relation of relations and a relation of qualities.. Hm.. ..Well, but we can still use 'Scalar parameters' as essentially a "virtual" relation.. ..Well, then again, I think I want a Qualities tab specifically, anyway. So what about relational scalar parameters?.. (10:32) ..Well, we don't want one, actually, since these scalars should be scored on the page/subpage of the Object instead, rather than that of the subject.. ..Yeah, okay, so just a Qualities tab. And where do we fetch those from?.. ..Ah, from the class of the page's entity, but not only that, as we also want to be able to provide "context Qualities" to the entity page, like I've talked about above. Okay, so that's the point for the Qualities subpage: That it also takes some context as well. Then if we go back to the insert subpage, this also takes the context of super-relations, on top of the most relevant one, and then it might as well also use the same "context Qualities" as the Qualities tab. And of course, below the field where the user can score relational scalars with the new entity as the Sbject, we have a Qualities field, where all the context Qualities are shown, and not least also Qualities fetched from the list of relations from the former field, as well as from all relevant classes. And what are the relevant classes again?.. (10:40) Obviously it's the class that we use in the insertion. But what about when.. Ah, and when a class is chosen that does not accept submissions, and you go to the parent class instead, or an ancestor further up, then you gather all those classes to become the "context classes" for fetching relevant Qualities (some of which might btw be alost-duplicates but just with different 'Subject class' attributes).. (10:43)

%(10:45) That was nice to go over, actually. And I also want to then go over Quality-based searching, 'cause I do want this to be part of the prototype as well. (And it's not hard, since the entity lists will be short anyway in the early stage, so we can just fetch a lot of them and let the app combine them however we want..) But what do I do here, do I give the Quality tabs som buttons to add them to other tabs..? Or what..? (10:47)

%By the way, I'll put the insert button on the scale references. And then I'll just control in CSS when to show it. ..Hm, I should actually also just control whenever entity references are clickable in CSS, 'casue I can easily control pointer events. Okay, I'll do that as well. (10:49)

%..Hm, the quality-searching could also be a part/responsibility of the EntityListSubpage.. (10:51) ..Maybe another button next to the insert button that adds a search menu above the entity list.. ..Yeah, I like that. Okay, I'll do that.. (10:53) ..And this menu is just about pressing the Qualities to use, then adjust their weights, perhaps just with descrete values, pressing up and down. Then it should just be a descrete scale the is approximately logarithmic (like 1, 2, 4, ..).. wait, or it could also just go from 1 to 0.9 if you press down from there, and then go from 0.9 through 0.1, and then go to 0.09 from there (etc., except maybe we don't want a weight lower than 0.01 anyway..).. (10:57) ..Well, nah to the last part: The user can choose any value they want, also by typing the number in. But yeah, we should also make the up--down buttons that behave as I just described.. (10:59)

%..Hm, and let me actually forget about the various options for the entity elements for the prototype, and just go to the EntityPage when the user clicks on an entity. (But I can of course still provide the Quality and Relations and Classes contexts to the EntityPage as well..) Hm, but wait, what contexts should I implement here exactly?.. (11:03) ...(11:30) I want to include them all. And in fact, Qualities is the lesser important one, since this will just be the.. Well, I guess it can be more than the last tab chosen, if you have selected a sub-quality. But yeah, so we generate all three of these contexts, and pass it along to the next entity page. We don't, however, combine the contexts from there. So each supage can only use the current page context(s), and the "parent context" that was provided when the given page was opened. Okay, nice.. (11:32) ..And yeah, let's pass all this as JSON arrays..

%(11:37) Hm, I actually do want that users should be able to rate entities in entity elements in place, without having to open a new column, then go back again. ..So let's put a Qualities subpage in a drop-down box as well, actually.. (11:39) ..Hm, and let's put the button to expand this qualities/scoring box to the right, next to the score display.. (11:41)

%(11:42) Hm, for Statement entity elements, maybe I also want a button to go to the representations page / editing page, where any user can propose a new edition of the Statement (which includes Comments, etc.), and perhaps one where the various parts of the Statement is gathered into spans, which then consitute all the substatments of the text. When pressing an 'inspect' button, the users can then see all these parts and click on them to see their individual scores and comments, etc. Hm, but where do we then display this, do we open a new "column"/page for all this, or only to see comments, etc..? (11:46)

%(11:54) Hm, Agreement is an example of something where we might want the mean, rather than the median.. ..The reason why I'm thinking of Agreement is that I got the idea that the Agreement score could just be shown as a bar.. Nah, that's too complicated.. ..Ah, let's instead make a dropdown box for each sub.. Nah, then we can just go to the column.. Well, maybe not, but the point is: We also want to be able to see scores like agreement for each individual part at a glance.. (11:57) ..Hm, we could make a dropdown box, expanded at the same time when pressing 'inspect,' where all the parts are listed, with a few of the most important scores shown directly, as colored horizontal bars below the (copied) span text (where the color matches the score), such that you can get a very quick overview of where the controversies/uncertainties might lie. I like this idea..:) (12:00) ..And when a 'part' is a whole section, you just copy and show the section header, rather than the full text. And when clicking on the section header, you then go to the column of that given Text/Statement entity, where you can then see its further subdivision, if you want. (12:02)

%(12:05) Hm, about the mean, maybe I can just generally always query both the median \emph{and} the mean when querying for specific scores, and then I can generally query for the median when querying for entity lists.. ..(I think I'll say this for now.)

%(12:12) Hm, I probably want to basically rename 'Statements' to 'Texts,' now.. ..Yeah, I should do that. And then Comments is a subclass of Texts.. Wait.. Hm, or do I?.. Don't I just want to keep calling it Statements?.. ..Nah, I'll call them 'Texts,' I think. And then I'll just delete the 'Statements' relation, btw, and only use more specific Text relations like 'Comments' and 'Discussions,' and such.. (12:16)

%..Hm, I feel like I really have a good grasp on all these things now.. And for Arguments, we use Scalars, which for Text Subjects can be rendered showing the text above the scoring bar. And for Proposals?.. ..Well, that is essentially just a specific subclass of Texts that then has specific relations and qualities associated with them.. Of course, implementing the voting system will be more complicated, but I don't need to implement this right away. I feel like a have a good grasp on how to make a big part of the app, then. I still, however, feel like there could be some important holes to go over, and I also want to "hum" a bit more over the selling points, especially now that I've considered modular and annotated texts in more detail.. But I'll take a walk soon anyway..:) (12:24)

%(12:27) Oh, I could also introduce my "wish ratings," or rather Wish qualities. These are then just Quality monads, that wraps around a given quality. I can then make a seperate field for these on the Qualities subpage. And let us, instead of making the score denote where you would want the quality to be, make the score denote the degree of how much you would have wanted the quality score to have been something different. For instance: 'I would have wanted a different 'Funny' score for this entity, "very much so",' or 'I would have wanted a different 'Durability' score for this entity, "somewhat so," etc. ..:) (12:32) ..Yeah, this is awesome; it will mean a lot to the app.:) (12:33)
%..(12:38) Oh wait, or even better: 'Wishes' could be a relation instead, i.e. with Qualities as the Subject class (and with the given entity in question as the Object, of course)..:)

%..Hm, I don't think the term 'relevant' really cuts it, come to think of it.. Maybe I should reconsider 'Useful' or 'Important,' or something else.. (12:41)

%... (14:08) I think that we should just use Arguments instead of these text spans (and divs). W.r.t. 'Wishes,' however, this is not something that can so easily be fulfilled by Arguments. 'Cause even if you compile and analyze all the most relevant Arguments for why an entity is Good, there could still be some underlying reason why it could do a lot better, e.g., that isn't expressed in the Arguments alone. Plus, having a Wishes tab also gives a good overview. And again, one that is different from viewing all the Arguments, as these don't tell as much about the users' expectations and wishes for them.
%But back to Text Arguments, maybe we could, however, make Texts that just refer to a section/part of the parent Text *(as their definition), but I'm not sure that we want this, 'cause it is also nice enough that these Text parts are searchable (on their hash).. (14:13)

%(14:18) By the way, I should attach an onmouseover event to entity references to fetch and insert the description (which can then be shown in a dropdown box if :hover, once it is fetched). And this onmouseover event can then also be controlled by CSS, which can thus prevent descriptions from being fetched in the first place, if they are not needed. ..And let me also include a standard EntityPage link, which can then also be controlled by CSS (if I haven't already talked about this..). (14:23)

%..On a completely unrelated note: I didn't know that so-called container queries existed in CSS, but luckily they do.

%...(14:39) I kinda also want to make something more out of using the Correlations for the prototype, if I can.. ..Maybe by being able to uprate a structure of Arguments, divided into subgroups of correlated Arguments, preferably with as many single-entity groups, such that there are as few correlations as possible (since there is no correlation (taken into account) between Arguments in different subgroups).. ..Hm, it would also be nice if I could get Arguments based on (potential) future knowledge to be a bigger (more common) part of it as well.. Hm.. (14:45) ..(14:50) Hm, about the latter thing, we could make a subclass of such Arguments, or rather a sub-relation. That'll be a good thing to do. But how to then combine these things.. Hm.. ..(We could perhaps call it 'Future arguments,' btw..) ..Well, but the future becomes the present, and then what?. Let me think of something else.. ..Well, it could still work; then they would just become regular Arguments once they have become immediately useful.. Or something a bit similar: We could add 'Proposals' as a relation to Scalar( parameter)s, which are then implicit Proposals to get a better foundation for scoring the Scalar (and in particular to get better knowledge).. (14:56) ..Yeah, let me do that, actually.:) (14:57)

%(15:09) Hm, Correlations are generally for when you don't know the answer to a Scalar, either becaue it is not known yet, or because you haven't studied it personally, but you can still contribute by scoring how impactful that Scalar would be, relative to its ultimate score..
%..Hm, and since we can't prevent users from rating the probability even when they haven't studied, I guess this mostly makes sense for user groups/panels.. Hm.. (15:14) ..Oh, and it makes particularly good sense when it comes to (kowledge-search) Proposals, i.e. to score how much impact that knowledge would have on the parent Scalar. Okay, this seems important.. (15:16) ..Yeah, I think \emph{this} is actually where I/we really want to Correlations: in regards to knowledge-search Proposals..!.. (15:18) ..Hm, and one type of such Proposals could just be "discuss and analyze Argument @<ID>". And then we might also make this so that this is, well, a Quality of the Argument, such that we can also potentially display that under an Arguement. Let s do that, indeed.. ..An 'Impactful' Quality, then.. (15:20) ..Ah, well, we more precisely want to score the '<Scalar>->Proposals<Impactful>' scale..:).. (15:22)
%*(Oh, that's a bit confusing.. I mean '*some Scalar* -> Proposals <Impactful>'.. ..(Or I could also write '<Scalar> -> Proposals \langle Impactful \rangle' instead..))

%(15:25) Ah, there's actually also something really nice about the fact that this means that the users will hereby be able to vote for what discussions to focus on (if they want to do so). And a scientist user group might also use the system to then vote for what research they really like to see done, for that matter..:) (15:27)

%..Nice. This is really awesome.. ..(Good thing I made this into a thinking day..:)^^)

%...(And note: The vote for the Proposal is not equivalent with the 'Impact' score. A proposal can be very impactful, but perhaps expensive to carry out, or something, which might make people vote for something else instead.)

%..(15:48) Hm, about Text parts, maybe I should make it conventional to always insert sections via reference. That way, it'll be way easier to import the section as an Argument for the full text, and even in a way where the section can be partly collapsed, showing only the header and perhaps the first few lines, both inside the parent Text, and also when rendered in the Arguments subpage. (15:51) ..(And of course, sometimes you insert section via ID refences, and sometimes you instead insert them via a Scale, namely such that the text can then subsequently be edited and adjusted (after individual preferences, even) in a modular way.) (15:53) ...(16:05) And users should be able to do the same thing with paragraphs: They should be able to insert them via reference, or via a Scale. ..I'll by the way try to use a mark-up where the both the section header and content is two children of the same div, which will mean that we can always just determine the header level (as in: h1, h2, ..., h6 (and more, potentially)) from context, rather than having it such that the section needs to know its context (which is true for MLs such as HTML and MD (although one can also implement my version in HTML via CSS, of course, which is what I intend to do)). (16:10)

%..And let me underline:
	%- Wish ratings/scores will be a very nice selling point as well, I think.:)

%(16:23) Oh! A Relation should be able to define its own standard Quality! (So that this can also be ones other than 'Relevant.') ..What an awesome and exciting idea..:) ..I mean, I could have gotten it before, but then again, you can say that about all ideas (and will often). It's an awesome idea regardless.:)^^ (16:26)

%(16:48) (Maybe I didn't mention: I by the way also want to introduce Quality subclasses again, not least make a subclass of all the 10-star Qualities.)

%..Oh, so 'Wishes' should actually be implemented as a Quality under the 'Qualities' Relation instead.. (16:52) ..We could call it 'Wished for'.. (16:53) ..Well, 'Wish had been different,' I guess..

%..Hm, unless I'd actually rather make a monadic wrapper to create Wish qualities, which can then be scored under a 'Wishes' Relation, with a default Quality for.. No, that becomes cyclic.. Then we should rather just make a class of 'Wishes,' I guess.. (16:58) ..Ah, but then the monadic wrapper could just turn the Quality into a wish instead. That might be a good idea instead.. (16:59) ..Yeah, this is much better. Then people can also be much more precise of what they'd wish for. Okay, that's what I will do.. (17:01) ..Well, what I'm actually saying here is that I \emph{shouldn't} use Qualities to make wishes, at least not as monadic compounds anyway. It's better that the users just formulate the wishes instead. Yeah, \emph{that's} what I will do.. (17:02)

%(23.11.24, 11:45) Åh, jeg har sovet så godt. Det var tiltrængt..

%I have some things to think about. First of all, let me say, I don't think I should be afraid to use somewhat long Quality labels. And therefore I also think that I should rename 'Relevant' to 'Relevant and useful.' This can then be the default.. the default 'Default Quality' for the Relations class..

%I also found a very cool slogan yesterday, by the way. I hope it's not in use..

%But the important thing now is: I should really introduce something, that I might call 'Purposes,' but then again, it might also just be something I implement via the existing Qualities, as they are.. But the point is, it will be very important to group things (into subclasses) depending on what purpose(s) they are trying to fulfill. And it is also important to then have Qualities that says 'Good in terms of fulfilling P'.. Okay, let me think a little.. (11:52) ..(11:58) Hm, we could introduce the concept of "Purpose subclasses".. Hm.. ..Ah, and maybe with a default Quality of.. well, this would mean a special 'Members' relation, then, namely with a default Quality of being good at fulfilling that purpose.. ..Well, not necessarily, since we can also use 'Relevant and useful' for that, at least in principle.. although I'd prefer if we try to make a convention of treating Relations like mathematical relations, meaning that they are just functions that returns a set, essentially (or actually a class in our case (since we don't necessarily care about Russel's paradox here)), and then to let the semantics of the scores being purely defined by the Quality, in a sense.. ..And if using that convention, we then ought to make a special 'Members' relation for.. Well, unless we can find a good way to upvote default Qualities for relations depending on the context, but that sounds a bit complicated, though.. (12:06) ..(12:11) I could also just forget the "Default Qualities" for now, and just let it be the job of the app to find the default one.. ..Nah..
%..Hm, but a purpose will generally be translatable, I think, into 'fulfilling some Quality'.. (12:15) ..Yeah, I think I should focus on that way of looking at it.. So we need to.. ..give a spotlight to Qualities that defines a purpose, essentially.. Hm.. (12:17)
%(12:22) I think we should try to utilize the long adjective form, the '-ing' form, if we can. And now I had the thought: Couldn't we make a subclass of Qualities, of 'Purpose qualities,' and where we make it a convention to use said form..? ..(17:27) Hm, and then one \emph{could} also make subclasses out of these Purpose qualities, namely by combining '<Class> trying at <Purpose quality>'.. Hm.. ..I think I like that a lot, actually.. (12:29) ..And when you rate the Quality, you then rate how succusful they are at that. And if you go to the "Purpose class," the default Quality will then just be the one that is part of the definition.. (12:31) ..Oh, except for the fact that I let Qualities specify the Subject class, as things are.. ..But why do that? The standard Scales will be of the form '<Subject class> -> Members [Quality]' (using square brackets here as a placeholder for angular brackets) anyway..(?) (12:34) ..Oh, I chose that, I think *(yes), because it then allows us to fetch relevant Qualities, and then get the Subject class from these alone.. (12:38) ..Hm, but we could also just look for relevant Scales, instead of relevant Qualities, couldn't we..? (12:41) ..Yeah, we absolutely could.. (12:42)

%(12:48) So let's do that, and then remove the Subject class attribute from Qualities. And yeah, I think we should make these Pupose qualities, then, using the '-ing' form, which can then also be used to make Purpose classes, combining a Class and a Purpose quality. Being a member of the class then means that they try to fulfull that purpose, the relevancy score determining how much they do so, naturally, but the default Quality of Pupose classes will be the Purpose Quality itself, which denotes \emph{how well} the entities does at fulfilling the "purpose." A purpose quality can, however, also be used by the parent class: Just uprate '<parent class> -> Members [<purpose quality>]' as a relevant Scale for members of the given parent class. (12:55)

%..And let me underline: I think these Purpose qualities and Purpose classes will be massively used; I really think they will be important..(!..) (12:56)

%..(13:04) I should try to explain why.. I think that these will first of all help create much more nuance in the subclasses of things, in a very neutral way, meaning that when exploring the Purpose subclasses, you will be able to get a much, much better overview of the different kinds of things that are out there (for you to try), than if you just search by the common genres, which is what will probably most likely happen if you just let the users define regular subclasses, without nudging them towards trying to define and use Purposes, and grouping things in terms of that. And that will also be a major selling point of my idea:
 	%- Get much, much more nuanced subclasses, which makes you able to get a much better overview of what's out there (to try). *And since 'tags' are already a common thing, I should also focus on the fact, in this selling point, that in this system, "tags" can have "sub-tags," etc., and the whole thing can have a tree structure like that, such that users can really dive deep into different types of things, and thereby get a very good overview of what's out there. ..('And maybe you can only think of the first tag that you want to search for, but others that know the "sub-genres" better will then know what "sub-tags" are relevant for further subdivision, and then you can therefore see what subdivisions others have uprated, and go explore those'..)
	%*And actually, the 'Purposes' idea doesn't have much to do with this. And it's a very good point: A semantic structure, not *(just) for finding what you are looking for, but to get a hierarchical overview of all the possibilities!. :) (15:49)

%..And then of course there's also the simple fact on its own that purposes are very important in terms of describing the qualities of a thing, and not least also to make 'Good' more nuanced. 'Cause even if you disagree on the overall Goodness score of a thing, it might very often just because you have different tastes in terms of genres/purposes. And one can often recognize that a thing is good at doing what it is trying to do, without liking that thing it is trying to achieve very much. So having a 'Good at trying to <some purpose>' Quality will help users agree much more on thing they would otherwise disagree on, which of course helps the system a lot in terms of searching and algorithms and such (since it removes some paramaters from the measurements, so to speak, that would otherwise add a lot of uncertainty to them).

%Now, this make me ask the question: Is it enough to formulate the Purpose qualities as '<some long-form adjective>' instead of 'Good at fulfilling <some long-form adjective>'..? (13:17) ..Yeah, it's enough, I believe. The 'Good at fulfilling' part can just be implicitely understood, whenever you see a Quality formulated with the '-ing' form. (Hm, and let us just use Capital first letter for these Qualities, and let us keep those in the Pupose class's references, i.e. in '<Class> trying at <Some long-form adjective>'..) (13:20)

%(13:27) Oh, unless I want redundency in terms of uprating both 'Scales for members' and 'Qualities for members,' it messes up my current system a bit, doesn't it?.. ..Hm, should I actually keep searching for Qualities, and then just use the "context Classes" to get the Scales from them?.. (13:30) ..I think that that might be better, actually.. (13:31)

%... (15:49) I think I should scrap the 'Default quality' again: 'Relevant and useful' is just \emph{always} the default Quality. And if it happens to overlap a lot with another Quality for a given Set, only where that other quality is perhaps a bit more semantcally precise, well, then so be it. That isn't really much of a problem. So no, 'Relevant and useful' is always the default Quality. (15:52)

%..A different topic: I think users should be able to uprate searh Settings, will are then meant primarily as alteratives to the average Good score, at least at the early stage before the ML user groups.. ..Basically, user define and uprate useful Settings for any given class, which can then be used, as I said, as an alternative to searching on just 'Good.' And the individual user can then make their own changes to these Settings, if they want to adjust them even more precisely to their own tastes (in the given moment). (15:55)

%(16:07) How did it become 16:08 already..
%Hm, I'm not so sure about these Pupose qualities after all. The thing is, you can also just formulate them yourself as 'Good at ...'.. But I do think that having different Sets of subclasses might be a good idea (so having and using e.g. a 'Genres' relation or a 'Purpose subclasses' relation, and such).. ..(16:16) Well, 'Purpose classes,' we can just say. Hm, but we could perhaps also just use "Quality classes" instead. And then we could just copy the syntax from the Scales, and render these as '<Class> [<Quality>]' (but probably with angle brackets instead of square ones).. ..Then you can get scales on the form '<Class> [<Quality>] -> <Relation> [<Quality>],' but this is also totally fine.. (16:18) ..(16:27) And then '<Class> [<Quality>]' more precisely denotes a class of entities that \emph{aims} to fulfill that Quality. I think this could be a better solution.. ..Ah, and maybe I can then \emph{actually} use square brackets for those, and use the angular ones only at the end of the Scale reference. I like that.. (16:30) ..Yeah, I like this a lot, and I also think that it is definitely worth introducing the 'Quality classes' relation (and corresponding Class used as the Subject class of this Relation), 'cause I think that this might be useful in inspiring good and useful subdivisions of various classes. (16:36)

%..(16:46) And let me underline: The good thing about uprating Settings for a class, is that it also makes up a crude but useful way to essentially make user preference groups/classes in the early stage, even before we get to utilizing ML.

%..Oh, this is an example where it might be a good thing to use something more like votes, rather than the median or mean, and this then got me thinking: why don't we just make another kind of bot, along with the mean bot (or rather the 'a' for 'average' or 'arithmetic mean' bot) and the median bot, which just counts the positive votes.. Well, that would require a Quality attached to Scalars, if we want to be semantically precise, bt maybe we don't care about that, when it's just an underlying bot..(?) (16:53) ..Or I could make a special Scores table for bots that has several value columns..(?) (16:54) ..Yeah, and then we can also call it 'bot_id' rather than 'user_id,' and 'user_id' can then always refer to users (or third-party bots..).. ..And we could rename 'Scores' as 'UserScores'.. Hm, I think I might as well do this.. It also opens up for getting e.g. histogram score data along with a query.. ..Yeah, if you are querying for a specific score, then you might as well get all the data you desire along with it. ..And this BotScores table could also double in terms of storing data that the bots need to update their score values.. ..'Score estimators' to be more precise.. (16:59) ..Okay, I'll do that..:).. (17:00)

%..Oh, but 'votes' is more complicated, as it also generally means an exclusion from voting for otehr things.. ..Ah, but since every score is given in relation to a Set, it is actually not hard to just calculate a combined number of point, where each scoring user has their points divided according to which entities they have scored, and how much, such that each user's combined point always sum up to 1. ..This seems pretty nice.. (17:03) ..Yeah, this is very nice, actually..! This can also be used directly, as is, for e.g. voting for Proposals. How neat is that?.!..:) (17:06)

%..And then when we consider popular Settings under any given class, and use this voting point score, then we immediately get an initial, somewhat crude, but very good way of getting a good overview of different kinds of user preferences. Hm, but maybe I should also combine this with a negative vote, where you also get.. Nah, we don't need to combine these two, i.e. the total positive an the total negative votes. But we do, however, need to never multiply by a factor greater than 1 when we normalize a user's votes: If a user has given exactly one score of e.g. 6/10, as an example, which means a 1/5 positive score, we shouldn't divide this by 1/5. We should only divide once the sum of positive scores given (when subtracted with 5, and not counting any scores below 5) starts to be greater than 1. (And the same thing applies for the negative votes (only where look at 5 minus the score instead).) ..But once this is taken into account, I think the system works.:) (Of course it then only works for the standard 10-star scale types, but this is also enough, at least as a start, but perhaps even for a long time, since I think, now, my 10-star scales will actually be very long-lived..) (17:14)

%..How \emph{awesome}, honestly.. :) (17:15)

%(17:20) Oh, but we still need an index for every esimator that we might want to see the subjects sorted by. So maybe 'BotEsimators' will be a better kind of table. And then what about.. ..Hm, I could just make a seperate table for bat data that we don't want to sort by, such as e.g. histogram data. (Or I could store this data in entities, alternativly, but we might as well use a specific table for this instead.)

%(19:09) And by saving the DATE of all scorings, we can also een make a bot that reduces votes/points the more the longer time since they were given. That by that we also get our freshness scores, which we can use for feeds!:)

%By the way, let us both compute "votes" and "points," where the latter is just the votes but without any normalization.. Well, on the other hand, no.. That's what.. Hm, I was about to say: 'what we have the mean for,' but no, maybe these points \emph{could} be useful. So yeah, it's a possibility.
%So notice that my idea of using entities for the voting system is no longer needed now. We can just use my new idea here instead.:)

%Another thing: I will make the 'Things liked for similar reasons' Relation, but I should \emph{also} make a 'Liked for similar reasons as <Entity>' Quality template(/class).:) So if you have some very charactiristic and/or iconic things, and it's hard to specify what qualities makes it special, then you can "cheat" (or rather the users can) by just making a 'LSR' Quality and uuse that instead.:) (19:17)

%..And let me mention: A lot of (sub)classes might be reverse-engineered from users who look to put a specific thing, or a specific type of things "on the map," as it were, making it more likely that other users find it. A user can thus take their beloved/liked thing and then, from it, come up with some useful categories and supercategories, that would be useful for other users in genereal, and which can then lead them, potentially, to the given thing as well. :) (19:20)

%..Let me just repeat the nice point about the fact that votes and freshness/trending points will now both be relatively easy to implement. :) I really think I'm starting to get a very good grasp on how to implement the rest of the first (or actually second) prototype, and not least also what my most important selling points will be. :) (19:22)

%..Oh, I should also have 'Subclasses' as an initially uprated relevant Relation to Classes (i.e. members of the Class Class), since it can be interesting to look at the various Sub-relations to that Relation, such as e.g. 'Genres' or 'Quality classes'.. Hm, well, this is just a new idea. I should think some more about it.. (19:27) ..Hm, the point is this. When you look at a set, there are often several ways in which to cut out the divisions of that set. So there are several useful sets of subsets, so to speak. And you can't just mix them up: There are more useful if viewed in a list without to much overlap between the different subsets. Making subsets of 'Genres' and 'Qualities,' and particularly adjective qualities, is a good example. So yeah, I do think that it might be useful to not only have a list of all useful Subclasses available, but also have various different subsets of subclasses available (with a good 'synergy,' so to speak). Now, whether to get this by using Sub-relations, or Qualities (of Classes), I don't know for sure, but either way, they both involves having access to the 'Subclasses' tab, with my current system, and not just in the form of the 'More tabs' page.. ..Hm, of course unless we just treat that subpage in the same way, but..(?) (19:35) ..Nah, let me uprate the 'Subclasses' Relation as well initially (despite the overlap with the 'More tabs' subpage (unless you use the sub-tabs for the Subclasses (sub)page)).. (19:37)

%(20:23) Oh, and I should mention: the standard Settings are just determined by the scores of the relevant Qualities of the.. Well, where exactly to uprate Qualities that have a strong correlation with another Quality?.. I guess at a '<Quality> -> Sub-qualities \langle Correlation \rangle' Scale.. Hm..? ..Yeah, something like that. And that can generate us the most common Settings (when making a cutoff).. But actually, we don't really need that. The users can just make a specific Settings entity based on the average.. Well, but users still need that scale to uprate.. Hm, but since this is just.. ..Argument templates.. Hm, I should think more about it, yes, but still: '<Quality> -> Sub-qualities \langle Correlation \rangle' seems like a good and fitting option.. (20:31) ..Yeah, that must be the answer. So this is a important Scale to nudge the users to score, not least because of their own UX of scoring something that really matters a lot: "Exactly what qualities is part of determining a given Quality, such as 'Good,' especially?" This is a very useful thing to answer.. :) (20:34) ..And yeah, then Settings proposals can be inspired by these scores. (20:34)

%(24.11.24, 13:14) Jeg sov virkeligt dårligt i nat, men jeg fik til gengæld også tænkt nogle tanker, og så fik jeg dog også sovet meget her i morges/formiddags..

%I don't think I will use 'Quality classes' after all. If we'll use them at some point, it could be like '<Class> [<Quality> \geq <score>],' and then the bots could make these Sets automatically, disregarding the 'Member' Relation scores, and instead only looking at the given Quality (and the 'Member \langle Relevant \rangle' score of the given Class, but not of this "Quality class"). But yeah, nah, I will not include this in the early version. Users can instead just formulate their own versions of these "Quality classes" and uprate their members manually.

%A much more important thing, however, is that now that the Qualities are much more absolute, in a sense, I actually don't think it makes sense to use my 'Scale = Object->Relation[Quality]' system anymore. We probably don't need to attach each Quality to a Set after all. Of course, the bots still needs to do this in order to make the entity lists. But users can just score the Quality alone for the given Subject, not relying on the Set. And when you want to make Qualities like 'Good as a movie,' and such, then you absolutely should, first of all. Again, this greatly help reduce the free parameters of the "measurements," which leads to better data (used for search and feed algorithms/results). But instead of specifying a Subject class, the users should just formulate the given Quality more elaborately, manually, like e.g. 'Good as a movie.' Now, we still of course need to rate the Relevant/useful Quality all the time. But here we can just use a seperate table for this, and therefore just remove the 'Relevant and useful' Quality and make it implicit in this table instead. And then if we want Qualities like 'has a good performance in <Object>', Object being e.g. a movie, then we could maybe make and use Quality templates instead for this. But this is something that I need to think about.. (13:29)

%Now, these changes would then actually not mean a whole lot to the overall structure of the app, as I see it.. Now users just don't have to uprate the same Quality for more than one Set: They always just uprate it for the "Set of all entities" (which is mathematically speaking a class, not a set, the Entities class, of course). Then it is the bots jobs to copy these scores to the relevant subclasses as well. And we can do so by sying that the bots only have to handle the N most popular subclasses of a given subclass.. where N reduces for each depth level.. And where they only have to handle the N most popular Qualities for any given (sub)class.. I think this might be possible..

%I also had an idea this morning, or rather this midday, as it were (..were?.. was?..), which is that for Sets larger than some maximal number, e.g. 4000, or whatever, the bots just make that cutoff when it comes to searching. So for all other Qaulities.. well, now 'Relevant' is no longer a Quality, so I should say: for all Qualities, the bot then also only copies the scores for the members that made this cutoff threshold. And the more novel idea was then that one could also do the exact same thing but with the "fresh"/"trending" scores/points (that I talked about yesterday evening(/night)) instead, such that you get to search among the fresh/trending entities in the Set as well, and not just the most popular of all time. I should definitely implement this, 'cause this will then give us an user-adjustable feed already.:) (13:39)

%And with such cutoffs, we also don't have to think about uprating when classes starts to be (small enough to be) useful for searching. We just have to uprate their usefulness, period. (13:41)

%..Hm, we could indeed implement having e.g. 'Had a good perfomance in this movie' as a Quality template, and then just uprate '(relevant/useful) Quality templates' for a given class of entities. And then the app can in principle just take these templates and construct the Object- and/or Set-dependent Qualities from these templates. These created template instances will then be fully first-class-citizen Qualities, that like all other Qualities are scores without a context (so no longer with any Set as the context for the score).. (13:46)

%..Something unrelated: Let me make a back button that also un-pins the given app "column"/page. Then if the user on the other hand scrolls, or clicks on the neighbor column/page, then this column/page will be "pinned" afterwards, meaning that a newly opened column/page will not overwrite it, as opposed to when the users click on this top-left back button to get back to the previous page/column. (13:49)
%..Oh, and for most non-class entities, let us make the Qualities tab the default one, for newly opened columns. (But for classes, we'll keep 'Members' as the defult tab (which then also displays the subclasses tabs above the page body).) (13:51)

%..I've also had a thought of making the standard entity classes, like Relations, Classes, and Qualities more distinguished by given they EntityReference span a (semi-transparent (translucent)) colored background, and perhaps with a gradient that gets light towards the middle (or the opposite, if that turns out better, but as I imagine it, I like the former better..).. (13:55) ..And then page headers, tabs, and entity (list) elements could also copy this behavious, taking on this gradient background from the Entity reference (which would then mean that the entity reference should have supressed their background..).. (13:57)

%..(13:58) Oh, the fact that we only choose N Qualities for the \emph{entity lists} doesn't mean that these Qualities can't be used on the enity page itself. It just limits the search options, which is totally fine. And its also totally fine to limit the number of Sets that can be Quality-searched in. So yes, I will make these changes.:) (14:00)

%..Oh wait, maybe I do want to include "Quality classes" in the early stage..(?) (14:02)

%(14:09) By the way, for my more elaborate voting system with representatives and several iterations and all that. We can just make more elaborate bots that coputes these points. (By the way, one can also even combine the representative voting system with ranked choice; only the computation time sets the limit, and that should matter very much, at least not for important votes.)

%..Hm, instead of choosing the $N$ most popular subclasses, e.g., I think it is much better to just look at the overall votes or points given out amoung all classes and Sets.. Well, amoung all Sets, simply. And then you just take the list of the N most up-voted/up-ranked Sets and make the bot copy the scores of the relevant Qualities to those.. Hm, and how exactly do we then find the most relevant Qualities for searching for each given Set?.. By the way, maybe the "points" are better than the "votes" in this instance, since some users will contribute much more to this, and their point shouldn't be reduced just because the up-rate more Sets. No, the "points" rather than the "votes" (the latter of which has the aforementioned normalization to them) are better for this job. (14:17)

%Quick aside: I should also do something similar.. actually exactly similar.. for when users vote for their representatives and make sure never to mormalize by dividing with a score sum less than 1. So yeah, we should just use this exact "vote" system when it comes to determining each user's representatives and their weights. (14:20)

%Now back to the question of determining the searchable Qualities for a given Set.. ..Hm, for relational Sets, we'd like to use the 'Qualities for members' Relation.. or 'for subjects,' perhaps.. (14:23) ..Oh, and we actually \emph{don't} wanna uprate every single relational Set, but we'd rather want to uprate just the Relations. And yeah, then the Qualities should of course also be up-rated/up-ranked for the given Relation.. ..Wait, maybe not..? ..Oh, the smaller the Sets the easier they also are to handle, so yeah, I think we should uprate Relations, and uprate Qualities for them. ..Right, yes.. (14:28)

%(14:48) Hm, I think we need Quality templates to be a more central thing for Relations if this is to work. In particular we need it for Comments; otherwise foreign Comments will also get a high rating. So yeah, no, we definitely need that, and maybe it should just be the standard?.. ..Hm.. ...(15:00) Maybe I should use the 'Subject class' attribute after all..? ..Or 'Set' rather than 'Class,' perhaps.. ..Hm, perhaps, yeah: a 'Subject set' attribute..(?) (15:05) ..Hm, and you could just uprate 'Entities->Members' Qualities for a lot of classes.. Hm, do you uprate Qualities for each individual class, or.. or do they also inherit from parent classes..?.. ..Yeah, you could mix the Scales, going up through the line of ancestors, taking the N highest scored/ranked Qualities.. (15:09) ..Or even better: You prioritize Qualities uprated specifically for the given class, but until some cutoff in rank, you start to take from the parent class. And the list for the parent class is just constructed first (making sure to prevent infinite recursion), such that you only have to look at the parent class, and not ancestors further up. Okay.. (15:12)

%..Hm, and we could render the Qualities as '<Label> (<Set>)' (where '<Set>' = '<Object> -> <Relation>') (and where we still shouldn't shy away from making long Labels, I think).. (15:15) ..We could also make two subclasses of Qualities, one that has the trivial Subject set, and ones that has specific ones, which are then shown when rendered, and which by the way also has the implicit semantics for 10-star scores that the predicate is scored w.r.t. the given Subject set. I think this could be a good idea.. (15:17)

%..Hm, so do we really use Quality templates, or what do we do..? ..Hm, the "Set-specific" Qualities could also be constructed explicitly by taking a Set and a Quality template, or whatever we end up calling the latter.. (15:22) ..Well, 'Quality template' is not bad.. (15:22) ..Hm, and maybe we could just render these Qualities with the placeholders (such as '*Object*' or '*Set*') un-substituted, and then add that '(<Set>)' parenthesis at the end.. (15:24) ..I might choose to do this.. (And this can always be changed later on; this is just rendering..) (15:25) ..(15:29) But often the placeholders won't be used, but just be implicitely understood, like I did for e.g. 'Relevant and useful.'

%Now, this actually brings me back to almost where I were: I can still use Scales the same way as I did. The difference is just that non-Set-specific Qualities are now always scored.. with respect to their Subject set, which they still do have, right?.. And then another thing is that maybe 'Relevant and useful' is no longer a Quality, but is implicit in some a special Scores table, unless I want to change this back..(?) (15:33) ..Hm, maybe we shoudn't differentiate between Set-specific and non-Set-specific Qualities, and just give them all a Subject class, and a.. Ah, yeah, the Label and/or the Description are just free to make use of the placeholders. Okay, this brings me back to almost where I was, especially if I also go back to using the 'Relevant and useful' Quality again/still.. (15:35) ..(But of course still with my new ideas for how the bots just copy the scores between Sets, meaning that the users never need to do so, at least they shouldn't..) (15:39) ..Hm, and I like having 'Relevant and useful' stay a Quality for sure, so let me indeed do that. ..Ah, but it is not a Quality now, but a 'Quality template,' then..? (15:40) ..Ah, no, it's a Quality still, but just one with a variable (not a constant) 'Subject set.' :) (15:42) ..And this could be the standard: When unspecified, the Subject set is taken to be the relevant Set.. Well, or rather, when specified, we app just prevents scoring of the relevant set if it is not the Subject set, and instead scores the Quality in relation to the same Subject set, always, regardless of the current Set. Yeah, that's the point.. (15:44) ..So basically almst exactly what I had, except that some qualities can have a 'Subject class' attribute, which means that they are always score w.r.t. that, and where the bots, however, can copy this score to other sets.

%Oh, but I just realized: What about sub-relations of e.g. Comments: How do we transfer the.. the set of the given Relation, 'Comments' in this case, to the sub-relation, e.g. 'Informative comments'? Hm, do we try to pass this via the context of the app, or do we actually make sub-relations hold a 'Parent relation' attribute?.. I think we might want to do the latter.. (15:49)
%..(Okay, let me take a break, and see if there's any photons left out there...) (15:50)

%... (16:54) Okay, maybe I should actaully remove the 'Subject set' attribute again, and then always just let that be implicit. And then instead we'll just let the Set be dependent on which ancestor the Quality is inherited from. This is by the way also what I intend with the Sub-relations; the Set is just dependent on which ancestor, looking at the 'Parent relation' attributes, the Quality is inherited from (if it doesn't ome from the (sub-)relation itself). And then instead of doing that complicated scale mixing that I talked about, we just do a simple one where we just join the two lists, keeping the scores for all lists as they were. This means that in order to get ahead of a parent's Quality, a child (class or relation) has to then have a higher score than that, i.e. for a Quality that should come before. And this makes sense when you think about it: The Qualities are generally less relevant the bigger the Set, namely if it contains more members for which the Quality is not so relevant as a consequence. So it makes sense that you should generally score Qualities less on the 'Relevant and useful' scale for the big Sets, which then makes room for them to be overwritten by a child. (I mean, there are always room, since we take the 10-star scale type to be an open interval, not including its end points, but it's still all the nicer the less decimal points you need to go to in order to get ahead of an ancestor's Quality.) :) (17:02)

%..(17:04) Oh, and another thing: I think I should start rendering the Scales as '<Quality> (<Set>)' instead, then.. ..(Instead of '<Set> \langle <Quality \rangle' like I have until now..)

%(17:09) By the way, something completely different: I thought that maybe I shouldn't implement mean bots for unbounded scale intervals, and maybe I won't for the prototype, but it can be done: We just have to remove outliers, of course.. ..(And it will for sure find some use at some point..)

%..(17:15) Nå, selvom jeg da fik sovet her i formiddags, så tror jeg nu alligvel ikke, jeg kommer i gang med at programmere nu her så sent.. Men ja, virkeligt nice til gengæld, at føle at jeg har så godt styr på det nu (7, 9, 13), og også virkeligt nice at disse ændringer her fra i dag faktisk ikke kommer til at betyde så meget for processen, andet end lige at det dog bliver en mere presserende opgave at sørge for at lave bots, der også kan kopiere of transportere scorer mellem Sets (mængder). Men det er kun godt, at jeg får gjort det, uanset hvad..:)

%(25.11.24, 12:02) (Har sovet skønt i nat! Men lidt irriterende, at jeg åbenbart har skullet have indhentet noget søvn, for nu er det anden nat i træk, hvor jeg har ligget 11--12 timer. Det må gerne lige skæres noget ned, det kan ikke være rigtigt..) *(Første nat af disse var så godt nok fordi, at jeg \emph{ikke} sov så meget af tiden..)

%I'm wondering what to do, then, for the Quality tabs. Shouldn't we then also combine then from parent classes and parent relations?.. ..Well, or the bot could just uprate them automatically.. But then there's the issue of getting the right Set. I've just been thinking a bit about providing the index of the tabScaleKey as well, along with the tabID, to define the pages (and the tabs). But writing this, now I'm also thinking: Should we just make Subject set a mandatory attribute of the Quality..? ..That would mean that they are essentially scales, right, well, no: except that the bots, and the bots alone, can also score scales where the Set of the scale doesn't match the Quality's Subject set.. (12:09) ..Yeah, this could be a good idea. Then 'Scales' will actually become something that the app is more concerned with, whereas the users mainly just need to think about the Qualities.. Yeah.. (12:12)

%..Hm, I shouldn't make the bot up-rate inherited Qualities automatically, 'cause we also need these Qualities for all the unpopular classes. So I should actually make the Quality tab bar more complicated, I think.. ..'Cause I guess we shouldn't the context classes, but the parent classes.. Well, only.. Ah, no, the Quality tabs only make sense if the bot has rated those scores.. ..Oh, me might want to make some sort of request that the users can make (a button/link that they can click, and confirm), to get the bot to copy the Qualities, using the user's weekly allowed data to do so.. An then the updating of these Lists could then actually be initiated/scheduled, not automatically, but by these requests from the users..!.. Hm.. (12:20)

%..I do like this, 'cause then we don't even need any system for measuring popularity, other than the system that gives various users their upload data allowances.. And the request link/button could just show the time since the last update as well. Okay, I really like this approach in general: To replace all scheduled event at frequent intervals with actions that are scheduled by users instead (and where the action just checks that it hasn't run a too short while ago (or is currently running), either waiting or aborting if it has, depending on the request, I guess (and it also checks if the user has the required "upload data" allowed)). (12:28) ..Yes, nice!.. (12:29) ..(And this could of course then also be the approach for things like calculating the medians and other estimators, etc.; the approach for all bot actions.)

%(12:50) Hm, maybe I should then start centering the app more around Sets, instead of Scales..? ..If I then also mae a new table, actaully, where the 'Relevant and useful' quality is implicitly understood, and where you use the setID rather than the scaleID..? (Hm, luckily I don't use a bunch of different states with different names anymore (I only use 'setState,' and that's it..), so 'setID' doesn't clash with that. ..Well, except it might clash with other setters.. Hm, but we could call it entSetID instead, then..) (12:54) ..(12:57) Hm, a Set is just like a derived Class, though, so then we might as well go back to creating Classes for each Set..(?) ..Except that I don't intend to treat these the same as the regular classes. ..But one could then make a subclass of Classes, i.e. 'Relational classes'.. (13:00) ..Hm, or I could just make Qualities hold a Subject set.. / 'Domain'.. \emph{key} instead of an ID..? (13:02) ..Yeah, and then I could keep things like they are now, I think, and also keep 'Relevant and useful' as a Quality.. (13:04) ..Yeah, and I could even just make the attributes: 'Domain object' and 'Domain relation'.. ..Okay, I think I'll do this.. (13:09)

%..And yeah, for the Qualities tab bar, we can just get them from the entity itself, and not from parent classes/relations, since the bot ought to uprate all Qualities that it governs for the given Set. ..Wait, uprate it for the Set? So we do need to have Set entities (or have 'Relational classes,' alternatively)? (13:12) ..(13:15) Hm, unless we just want to fetch Qualities like I have until now, but then just.. Ah, we could show the request link/button in the page body, even when the list is missing. Yeah, so we actally \emph{shouldn't} make the bot uprate the qualities automatically, but just use my existing technology for getting the Qualities (plus any recent idea that I might have had).. (13:18)

%..Oh, but keeping 'Relevant and useful' then means making a new one for each Domain. ..Hm, so back to making the Domain optional?.. (13:21) ..Or removing it, and then make the Quality tab bar able to fetch the Qualities from the parent classes and relations?..

%..(13:26) I mean, I could also introdue 'Relational classes' and then make a Subpage specifically for these, using my ClassSubpage only for non-relational classes.. ..Yeah, where this just.. well, where it is actually just the RelationSubpage renamed.. ..Hm.. ..Hm, alternatively, the RelationSubpage and/or the Set/ScaleSubpage components can just look up the given Relational class (Set entity), if they need it.. (13:34)

%..(13:42) Actually, maybe the Domain should always be a Class.. ..And also I \emph{can} just cheat and make 'Relevant and useful' an implicit part of a special table (where its Domain is then determined by the given Set).. Hm.. ..Hm, and I \emph{can} also just use Quality templates/functions for Relations were you have some useful Quality that is, however, derived from the Objects as well (like 'Actors' with the Quality function 'Has a good performance in *Object*').. (13:47) ..Hm, I'm thinking: yes to all these things.. (13:50) ..The last point means that the Quality tab bars will take two serial queries to fetch and combine the tabs, but since the 'Relevant and useful' tab is alwasy open at the beginning, and doesn't take several queries in series when I'm "cheating," this should be okay.. (13:54) ..Wait, maybe I don't even need two serial queries here..? (13:55) ..If we instead use.. Ah, if we use 'Quality functions/templates' as a separate class, and then implement that thing I talked about earlier of using the tabScaleKey index.. (13:57) ..Ah, but then we can just do the thing of making the Domain attribute optional, meaning that a Quality is automatically a "Quality template/function" if it doesn't have that attribute..(?) (13:59) ..Then we need to query for this attribute, but only when loading the page body, and we don't need it for 'Relevant and useful,' since we know that this will not have said attribute beforehand. And yeah, this will once again make 'Relevant and useful' an actual Quality, which is nice. (A 'context-dependent Quality,' we might say, but a Quality nonetheless..) :) (14:01) ..And note, since we just require the Domain to be a class now, I don't need to deal with creating and querying for entSetIDs: If the Domain is not a non-realtional class, well, then it is just always the Set relevant to the given context, which means that the Domain attribute is not needed in this case, and therefore we can omit it. :) (14:04) ..Oh wait, but what about Qualities inherited from parent relations? (14:06) ..Here I actually did want the Set to be that of the parent relation.. ..Damn, so perhaps no to making the Domain always be a class.. ..Well, unless we solve this problem by using 'Relational classes,' but let's see.. (14:08) ..Oh, but taking my example of 'Comments' and 'Informative comments,' as two Relations, then the Domain of most of the Qualities actually ought to be the Comments Class, and not the <Object>->Comments Set. So maybe we \emph{can} just let the Domain always be a Class after all?.. (14:11) ..Yes, I do think so.:) (14:18)

%(14:44) I wondered about renaming Statements to Texts, but maybe I should keep it as Statements..? Or can I find a better word in between?.. ..(14:48) Hm, I was considering 'Proposition,' but 'Postulation' could also be a candidate.. ..(And 'Thesis,' but I'm not too keen on that..) ..Hm, 'Postulate' seems like a good candidate, actually.. (14:51) ..But then again, why not 'Text'.. ..Oh, should I actually not wrap Texts as JSON object entities?.. ..Oh, for XML text entity references, we can render the main header.. ..Hm, maybe I should just use XML text entities, and then use Comments, which are texts that also has an Object, as well as a boolean for.. Oh no, I forgot: The creator can just score their own Agreement with their Comment. Okay, but does this mean that.. No: The Text class stays, but it should just.. Have some attribute that.. ..Ah yes, we can add an optional attribute to Classes, which can escape the JSON object datatype. And if this attribute is set, then all Special attributes are just automatically interpreted as 'removed.' (15:02)
%..(15:11) Wait, Comments can just be Texts as well. There's no need to specify the Object/Topic, is there..? ..Hm yeah, couldn't that just be done as a meta header for the XML text?.. (15:13) ..A 'context header,' if you will.. Yes, I think so.. (15:13)
%... (16:41) We could also call it the 'topic meta header.' And actually, if we want to implement subclasses of texts, we might actually implement this directly in XML, via such a meta header. Something else: Maybe we actually want an XML-\emph{like} markup, since it might be nice to be able to escape the special characters with just '\'. And it might also be nice to actually use '[]' instead of '<>.' Then it's easier for the users to type, and it also means that people immediately know that we are parsing this XML, and not leaving ourselves open for HTML injections..

%(18:14) Oh, I shouldn't really need to render the Domains of the Qualities. So I can in principle just stick to rendering Scales like I did, and make them very visible as a concept to the users. Or I can move it around as '<Quality> (<Object> -> <Relation>)'.. ..But then it might be a bit confusing whether Scales are Qualities or not.. So maybe I should stick to '<Object> -> <Relation> [<Quality>]'.. ..Well, no, maybe not.. Hm.. ..Hm, maybe I should stop having ScaleReferences, and only SetReferences, and then use Qualities more. And maybe I should display the Domain, which is a class, in a parenthesis, and display the Set of the given context sometimes, if the Quality does not have a Domain.. ..Hm, and I can also turn the '<Class> -> Members' Sets into just '<Class>' when rendering them, I guess.. ..Hm, yeah, maybe the parenthesis is only included, when the Domain breaks the context, or sometimes also for more "full" renferences, like those that go in headers, e.g.. (18:23) ..Yeah, I think so.. A bit like my previous 'context class,' which I have since then discarded, but in this case it might be useful/required.. (18:26) ..(18:31) Hm, on the other hand, maybe I do want to keep using the Scale references like they were.. ..Yeah, and maybe we just want to sometimes render the Quality in the ScaleReference with a parenthesis as well, namely exactly when the Domain isn't expected from the Context, meaning the <Object> isn't equal to the Domain, and <Relation> isn't 'Members.' So in other words, ScaleReference just needs to pass that context.. Well, no, actually: We can handle this in CSS instead, I think. ..Well, but we also don't \emph{have} to do so: I can also pass the Object as the context when the Relation is 'Members'..

%(21:00) Wait a minute, why couldn't Qualities just be absolute, having Entities->Members as their domain, always?.. ..Oh, because some, like 'Relevant and useful,' change semantics depending on their domain.. ..Hm, we actually could make Quality templates like 'Relevant and useful for <Class>,' though.. ..(21:06) Hm, maybe that would actually be nice.. And the template instances can just use the same Description, which can also reference the tempalte and explain that the given entity is an instance of that..
%(21:22) Hm, I don't think I will render the Domain at all, even if I keep it. Instead we just fetch the Description on hover an show at least the header of the Description as a mouseover text. ..Therefore we don't need the Domain, but I'm not sure yet that I want to reimplement the Domain-less Qualities as templates.. ..On the other hand, who say that e.g. 'Had a good performance in <Movie>' is only relevant when viewing the <Movie>->Actors set?.. ..So yeah, I probably do want those templates after all. And 'Relevant and useful' can also be implemented as a template, unless I want to "cheat" in this particular case.. (21:29) ..Yeah, and I do, actually.. (21:30)

%..Hm, and my "Scalars" then turn into just a Quality and a Subject.. ..Yeah, and my Scales get replaced with Qualities, actually.. (21:34) ..Oh, maybe since I no longer use Statements, these could be use for my formerly-known-as-Scalars..?.. (21:36) ..Hm, that actually works.. (21:38) ..And if we need it, perhaps we can call a specific User + Statement a.. Well, an 'Assessment,' or perhaps just a 'User statement'.. ..Well, I guess we need a score first to call it a statement.. ..So let me keep calling at a 'parameter' of some kind, perhaps a 'Qualitative parameter,' or something like that.. (21:41) ..Unless I want to start calling Qualities 'Scales' now instead.. (21:42) ..Hm, I do like 'Qualities,' and I also need to call the.. entity lists.. Or pre-userID entity lists.. something.. (21:44) ..Hm, except I don't, really.. I think I can just refer to the formerly-known-as-Scales together with the specific user/bot always, and then just call them (entity) lists.. (22:46) ..Hm, I think 'Scale' might be more suited than 'Quality,' since the "Quality" also holds information about the interval and the unit of measurement, and all that (and not just abstract.. ..quality is itself).. ..(Ah, I was looking for the word 'quantity,' but 'quality' suit better..) But anyway, I think I might turn 'Quantity' into 'Scales,' and then start using these for the Scores table, since I no longer need the set-defining part.. (21:51) ..And then I will use Scale templates.. ..And I'll probably make a special Scores table specifically for 'Relevant' Scales, where we supply the Set --- or perhaps the obj_id and rel_id?.. --- instead of the Scale.. (21:53) ..Let us say obj_id and rel_id, such that we don't have to query fot the.. Well, we can just use the hash.. Hm.. ..Hm, maybe 'Quality' is more user-friendly, even if 'Scale' technically makes more sense in terms of its function.. For a tab or a header saying "Qualities" is just more telling than a tab or header saying "Scales".. ..Hm, and maybe if I use a capital P, we can just get away with calling the formerly-Scalars just 'Parameters'.. (22:00) ..Ah, alternatively we could call them 'Semantic parameters' (which can maybe be shortened to just 'Parameters' at times..). (22:00) ..Good, 'Qualities,' 'Parameters,' and 'Statements'.. (22:01) ..Hm, or maybe 'Assessments' works better for the latter. Let me say that for now.. (22:02) ..Or 'Estimate,' but I'll see..
%..Ah, maybe 'Esimate' is a very good choice, actually, since it then also goes well together with using 'estimators' to refer to e.g. medians, means, votes, points, and such. (20:05) *(Ah, but I probably won't need to use a term for that (i.e. 'Estimates'/scores) much anyway..)

%..Ah, couldn't I make a button/link to automatically copy the scores of the relevant.. Nah.. Hm, so I might want to look at parent classes/relations for Qualities, and Quality templates, and maybe I should then just ignore the context classes/relations, then.. Hm, which might mean that I don't need to use these.. ..Well, I might need them, expecially if I don't implement a dropdown rating menu for the entity elements; then I have to have them.. (22:13)

%..Yeah, since 'Relevant (and useful)' is always the first tab anyway, we have time to gather all relevat classes and relations to query for Qualities, both looking at the combined context and at the parent classes/relations, and then combine all that. I think I should aim for that. Especially since the lists will most often be chached; you don't have to make all the context queries each time you go to a deeper tab level.. (22:22) ..Oh, but it's hard to remove a Quality from a parent, then, if it goes down in relevance (and is not just surpassed by others). This problem might be a bit theoretical, but still.. (22:23) ..I \emph{could} also make a request button/link to ask the bot to copy scores from all parents, and then only do it for those who haven't received enough votes already at the relevant child.. (22:25) ..Hm, or I could just forget about looking at parent classes/relations altogether for the prototype, since I'm not actually sure it will be worth the effort---also the computational effort, come to think of it.. No, it might be best.. Well, on the other hand, the parent queries can be cached more easily.. Hm.. (22:28) ..But yeah, this does not need to be something I have to figure out for the prototype: I can just postpone implementing these parent lookups, and then only implement them if they turn out to be desired at some point.. (22:29) ..And if I do in fact still use the context classes and relations, then it is just even less of a pressing matter.. (22:30) ..Hm, and let me indeed try to implement that context classes/sets/relations thing.. (22:33)

%(22:52) Hm, we actually don't need to "cheat" for the 'Relevant' lists. We should just be able to use an entListFromHash procedure.. ..Yeah.. ..Ah, well, that works well for classes, but not so much for Sets, unless.. Well, one could make it work, also without having to get the setID as an intermediary step, but I'm not sure that we want to.. ..Then again, it could be both illustrative, and more consistent as well.. (22:56) ..'Cause I can just use a template of 'Relevant and useful for <Object ref> $\rightarrow$ <Relation ref>,' and for the 'Members' relation specifically: 'Relevant and useful for <Class ref>'.. (20:59) ..Well, on the other hand, it actually makes a lot of sense to separate them, since relevancy scores and Quality scores are quite different.. ..Hm, why don't I just call it 'Relevancy,' btw?.. I think that works better.. ..Oh, and why not measure it in percentages instead as well, and then just say that 100 % means 100 % relevant for 100 % of the set members?.. And then e.g. 90 % means an average of 90 % relevancy. One can then "measure" the individual relevancy in terms of how likely it is that a user want to use the given Parameter. Okay, this is nice, thank you..:) (23:08)

%..Oh, and I don't need to construct the Label like that. I can just make a subset of Relevancy qualities that all take both an Object and a Reletion as attributes. :) (23:11) ..And yeah, then I can for sure just use an entListFromHash proc.:) (23:11)

%(26.11.24, 11:32) No to using the percentage scale type for 'Relevant;' we should use the predicate, 10-star scale. ..Oh, maybe I should reintroduce the Scale type class, rather than having subclasses of.. No, maybe not. Subclasses of Qualities is better.. And I will then use an 'Elaborate label' instead for the mouseover text. And I then don't think I will fetch the Description for EntityReferences in the prototype / beta version. Relations can then also have an optional elaborate title as well, I think. For the qualities page, I will then show the Elaborate label. Oh, and for 'Relevant,' this will just be: 'Relevant for <Object> -> <Relation>.' (So I will just insert the entity references in the Elaborate label after all, rather than making a subclass of Relevancy qualities.)

%Now, I'm thinking about how to implement the Quality searching and the Settings.. (11:39) ..Which means reimplementing the Quality tabs, probably, unless I just want to use a getTabTitleFromID() that returns some more complicated JSX..
%..Yeah, I should definitiely at least put the Quality tab bar below an EntityList/SetSubpage header. And yeah, I should probably make a special one.. ..Hm, but maybe I \emph{can} just make them have more complicated JSX tab titles.. (11:50) ..Sure I can build the Quality tab bar from SubpagesWithTabs and well, and then just wrap it in a component that has some more actions, to respond to adjusting the weights.. Hm, but what about Settings, then?.. (11:54)

%... (12:48) Just took a walk to get some (actual) sunlight (the two past days have been grey and dark), and there I realized: The "Settings" should just be Compound qualities, i.e. Qualities that consist of some defining formula, of atomic Qualities, weights, etc., where the bots then don't look at users' scores for the given compound Quality, but compute the scores based on the formula instead.

%Another thing that just struck me: Maybe we should actually make 'Estimators' a class, and then provide this as well to the bot Scores table.. ..Or use a '<Bot> + <Estimator>' compound instad of just the bot_id for the table.. (12:53) ..Well, a '<Bot> + <Quality> + <Estimator>,' then.. (12:54) ..Hm, and maybe I should call them just 'Uer groups' instead of 'Bots'.. (12:55)

%..Now, I'm thinking of making a Compound qualities tab as a sub-tab to 'Relevant'.. But that does mean we get a lot of those tabs popping out.. Maybe we could have them be collapsed at the start, where the users then has to press them to expand them in the first place.. That might be the solution, yes.. (12:58)

%..(12:59) Ha, a very simple way of forming User groups could then in principle be to just look at who votes for which Compound qualities, and then divide according to that. Not sure that I will use that idea, but it's pretty neat that it might actually work..

%..Okay, this this, I think I might know what to do, then.. ..(Oh, and I should mention, the user can then either request the bot/DB to update a certain Compound quality for a given Set, or they can also just let their app compute the compound instead, namely by querying for all the lists and combining them.) (13:03)

%(14:18) Hm, and I'm sure I don't want to "cheat" with 'Relevant'?.. ..Hm, yeah, I think I shouldn't..
%..Hm, and did I actually want to keep the Domain as an optional attribute, and then render the Member title in parentheses?.. (14:22) ..Yeah, let us do that. Then this will be the formal way of saying '... as a member of <Class>' (for all Predicate qualities)..
%..Hm, there's no reason not to make a subclass of 'Relevancy qualities,' actually.. (14:24)

%(14:48) Hm, it just dawned on me that I need to change the database again, and work upwards from there. Okay.. So the Scores table should just have an ent_list_id, which includes both the user (group) and the Quality.. Oh, and also the elements of the Quality, perhaps.. That would be nice.. And this will then also include the overall Estimator, and of course any nested Estimators in a Compound quality.. Hm, so how to make these Entity list entities?.. ..Hm, we probably need several subclasses of them, if I really want them to include all the data atoms.. (14:52) ..Yeah, so we'll have e.g. 'Relevancy entity lists,' and so on.. well, unless the nly other class we need is the overall 'Entity lists'.. (14:54) ..Ah, we could also have a 'Compound entity lists' subclass.. Hm.. ..No, that makes no sense: The user generallt selects a compound quality from a list. So yeah, do we only need those two?.. ..Hm, we should probably also include the overall Estimator as well, at least for.. 'Aggregated entity lists' (bots' lists), and then it doesn't matter that the Quality can also be a compound one that includes other Estimators on a lower level. ..Hm, but maybe the choice of Estimator for a user could be a way to implement uncertainty.. (15:03) ..Oh yeah, in particular we could just use an 'Uncertainty' Estimator for this. Then we don't need all that complicated mess of defining very-high-level Qualities for this..! Okay, I'll plan on that: We can use Estimators to implement things like uncertainty. :) (15:05)

%..Okay, so in general we want Entity lists to consist of User (group) + Quality + Estimator, but then for Relevancy entity lists in particular, we might want to explode the Quality attribute..? (15:07) ..Let's do that indeed.. (15:07) ..Ah, this will then make 'Relevant' a virtual Quality, and perhaps we will then not even need the actual Relevancy qualities, really. This will then essentially be the same as the "cheating" I talked about, except that the Description of the Relevancy entity lists can make it all legit, giving these Entity lists Description-defined semantics. Okay, that's very nice.. :) (15:12)

%..(15:15) Oh, and I forgot about the Object and the Relation as well for 'Aggregated Entity lists'.. Ah, but let us then just make these standard attributes. We can then either make them optional or mandatory.. Nah, let us make them optional with the default values of 'Entities' and 'Members'.. (15:17)

%..(15:20) Hm, I could also be more general and make a subclass of Entity lists using "Quality functions" instead of Qualities..(?) ..Yeah, and then one can also make Qualities from these Quality functions, but where the app makes sure to explode them before querying for them or inputting them.. That could be a good idea.. (15:25) ..Yeah, and since I wanted to actually use the Quality templates directly before this as well, then it becomes a solid yes: Let me call them 'Quality functions' instead, and make a special class for them, with a special rendering scheme, both for them and not least for the Qualities made from them.. (15:28)
%..Hm, should our Entity lists then just have an overloaded Quality / Quality function attribute?.. ..Well, or they could just both be optional. And if none are there, we could just interpret that as Quality='Relevant', I guess.. (15:32) ..Well, Quality function='Relevant', rather.. ..Hm, we could also just take all Qualities to be potential functions, but some without any inpt parameters, so to speak.. ..And this could just essentially be determined by whether e.g. '@obj' appears in the Elaborate label or not (or any potential other "input" placeholders).. (15:35) ..Hm, and since Qualities record their Object class, we can automatically render this in a parenthesis next to '*Object*' when rendering the '@obj' placeholder.. (15:38) ..Ah, and when scoring a Quality function, i.e.\ a Quality with placeholders---and maybe we also want and 'Is a function' boolean---the app should just make an input field where the user can select one of the Sets that is relevant to the 'context'..:) (15:40) ..And when you quary for Quality tabs, the default Set input will just be the most relevant Set in the 'context.' So for instance, if we are looking at Quality tabs under '*some movie* -> Actors,' then this Set will automatically be the input of any functional (with the 'Is a function' boolean set) Qualities. Nice, that seems to solve that..:) (15:43)
%..Oh well, no, 'cause the app needs to \emph{know} the perticular input Set when showing the list of relevant Qualities, unless we get that you can only have one instance of a Quality function in this list.. ..Yeah, I need to rethink this.. (15:48) ..(14:52) Hm, we could make a 'Derived qualities' class, consisting of a Quality function and a Set, and then just make the app explode these before every query and input..
%..Hm, I guess I should try not to go into too much trouble to pave the way for something that, outside of 'Relevant,' won't be used too much.. ..And a decent solution \emph{could} also just be to introduce and use Quality \emph{templates} at some later point, which is then backwards compatible with what I get, if I just use the 'Relevancy entity lists' subclass as the only thing that (implicitly) uses a Quality function.. ..I mean, it might even be the preferred solution.. (16:04) ..Hm, yeah, it seems so, actually.. (16:05) ..Ah, we can also introduce Quality functions instead as well, and then use 'Derived qualities.' So as long as I just make this 'Relevancy ent..' well, except that 'Entity lists' then won't be open for using Quality functions directly.. ..But one can just introduce 'Functional entity lists,' then, and Bob's your uncle. So whatever we choose to do, it can be backwards compatible with using the  'Relevancy entity lists' class for these particular entity lists.. (16:09)

%(17:54) Oh, I can also the Estimators to implement "Wish ratings"/"Wishes" as well..!

%(17:56) Hm, I could still use actual XML for the texts, and then just at some point implement a more handy intermediary markup language that the users can then use instead..

%(18:24) Hm, but what happens to the scale when the Estimator changes the semantics? Should the Estimator then hold the interval information?.. ..Hm, that actually makes more sense semantically as well, since a Quality shouldn't need to be defined by a unit of measurement in principle.. (18:27) ..Great, so the Estimator also holds the interval specification.

%(19:23) I we do need the user_id as well for the Scores table in order for the "bots" to do their aggregations, or rahter for the DB to do so.. ..I thought earlier of calling the pre-userID entity lists for 'Parameter sets,' so maybe we need to use those instead, or whatever we end up calling them.. ..Yeah, 'Parameter sets' makes sense.. ..(since a Parameter is a Quality + Estimator + Subject..) ..Okay, and when I do this, this should allow the aggregation procedures to work..(?) (19:28)
%..(19:30) Oh, I can also start using 'Scales' again, now that the Qualities does not hold the interval and unit information. So Quality + Estimator can turn into a Scale, and with Subject as well, this becomes a Scalar parameter.. ..Ah, so my Scores table ends up look like it was (unless I want to change the name of user_id, or divide the table into two, which I might do (i.e. the latter thing)..).. (19:33)
%..Hm, if I stick to using 'Estimators' like I've described here, and not use e.g. a 'Scale type,' then I guess Aggregation estimators should be a subclass of Estimators that are formed via monadic Estimater -> Estimater functions.. ..Yeah, it makes good sense to it like this, actually.. (19:38)

%Hm, since the users will generally request updates of a specific Entity list at a time, the scale_id should come before the user_id in the (User)Scores table.. ..And let's also do the same for the AggregateScores/ScoreAggregates table as well, namely since a "bot"/procedure might use several user groups to define some aggregates.. ..Well, or rather: a Compound quality might specify that the procedure should do so. (19:43)

%..(19:44) Ah, with Compound qualities, I should consider using more stand-alone Arguments, which can then be compound ones. Then the users can just uprate all the best Aguments, and never care about the inter-correlation between them, since the derived score, if any, will only ever be calculated from just \emph{one} Argument (at a time), where this one Argument might then just be a 'Compound argument,' so to speak.. (19:47) ..Yeah, I might be on to a great.. forbedring.. ..hm, a great thing-that-makes-it-better.. here.. ..Hm, one could say 'a great advancement,' I guess.. ..'Improvement'!.x) (19:51)

%..(19:55) We can just call the standard user Estimator 'Opinion.' And then if we take the user Uncertainy, this will then actually be a Estimator -> Estimator function \emph{taken} on Opinion. ..Well, unless we implement it otherwise, but it could make sense to do it like that. And to be more precise, let's refer to it as 'Opinion uncertainty'.. ..Maybe.. (19:58)

%..(19:59) Ah, but in terms of function vs. template, 'Uncertainty' is actually a template here, namely if I implement it as a subclass of Estimators that takes an input Estimator as their only attribute. And if I do this, then I should also do the same for Qualities when the time comes. I should thus also here make a subclass of Object-specific Qualities that all takes an Object as an attribute, as well as.. Wait, no, now I'm defining a function.. ..Well, come to think of it, doesn't always make the most sense to use functions. 'Cause a function-less template is just the same as a function-using template, only where the function attribute is exploded into more attributes. So yeah, for both Qualities and Esimators, I should make a subclass of 'Functional Qualities/Estimators,' which all take one function attribute, as well as some input, and then forms a Quality/Estimator. And in case of e.g. 'Has a good performance in @obj,' the user can then uprate the Quality function (under a 'Quality functions' Relation), which the app can then use to construct the Functional Qualities from. Okay, glad I figured this out (unless I'm forgetting some weird problem with this approach, but let's assume for now that I'm not).. (20:11)

%(20:15) Hm, I guess we should also still let the Scales include the Set, i.e. the Objet + Relation as well. The app should then just make sure to remove these for all non-'Relevant' Quality score inputs, which then makes them take their default values of 'Entities' and 'Members.' And for 'Relevant' scores, I will stick to using, what will now be 'Relevancy scales,' now that this is not even "cheating" any longer. (20:18) ..Oh, and all Qualities should actually have a Domain (although it can be the default 'Entities' value if missing), also the Functional ones. ..The Domain is then the where the user scores.. Ah, but shouldn't the Domain then be a Set, i.e. because of the Functional Qualities?.. ..Ah, but I'm already using tables with named columns, so I can just use an Object--Relation table/array for the Domain attribute. Great, let us do that. (20:24)

%(21:16) Oh! User groups can now also be implemented much more dynamically and fluetnly, it seems!.. We can actually just implement them by aksing/requesting the DB (backend procedures) to save a given existing entity list of users..!..

%..And let me just remind myself of that exciting Arguments idea once again as well.. (21:18)

%(27.11.24, 11:14) I need to use the RecordedInputs table for the user scores. And I've had the idea that we can just make the datetime part of the primary index, and then just require that the user chooses a time that's after the current one. And when updating an Entity list, we then increase the pointer from one time in the past to the present (or a time just before that). This all then makes it so that we don't have to record the timestamps of the inputs, and the app can randomize the times at some point, if the given user want. (It's nice that a central algorithm doesn't rely on data that is personal to some.)
%..I'm also thinking about giving both an option to compute an entity list from the ground up, or to update it using RecordedUserScores (which might perhaps just become our UserScores table).. And about User groups in general, if I simply introduce constant Scales, i.e. ones that can't be updated after their creation, then User groups can just be User scales, really. Of corse, it takes a lot more to update entity lists of dynamic User groups, since you then also need to update the weights some times, updating all scores on the given list in the process.. *(And when I'm referring to 'Scales' now, I'm also including the Set (Object + Relation), meaning that they're like before, only with the Estimator as well as the fourth component.. ..Hm, but I could also call them Parameter sets, and then reserve Scales for just Quality + Estimator.. ..This does make a bit more semantic sense.. (11:30) ..And 'Parameter' is short for either 'Semantic parameter' or, perhaps better now, 'Scalar parameter'..)
%..Now, my biggest question right now is about the Domains: Should we once again require the users to be economical about choosing the right Domains, such that this makes the aggregation computations easier?.. (11:24)
%...(11:35) Another thing: Filtering all non-relevant entities away now does not need to be an especially fundamental thing; it can just be a specific implementation of a filtering of the aggregate Entity list. And this also means that one will just as well be able to filter away e.g. Spoilers or NSFW/NSFL. :) (11:37)
%..But about the Domain..? ..Let me try to go over the various algorithms that I need again (in my head, and maybe also on the keys here..).. ..Hm, building an entity list from the grounds up means taking a user group, i.e. a user list, and then for each.. Well, it depends on the filter.. If we consider Relevancy lists.. ..Wait, I could also call Parameter sets 'Tables' instead.. ..Or something to that effect..:) ..Well.. ..Anyway.. For Relevancy lists..
%
%..From the ground up we could go through each user on the given list (perhaps with a score/weight cutoff as well) and then perhaps use each user's own entity list to get the contributions. So yeah, maybe a good thing to keep that.. And for Relevancy lists updates?.. ..Then for each new score, we look at the previous score.. ..then calculate the.. Well, for means, we can do this, but for medians we need to keep a list of at least all scores close to the current aggregate.. Hm.. ..Well, we could just bin the scores, especially if all Scales are bounded now.. (11:58) ..And then it's just subtracting form one bin (if any previous score exists) and adding to another. Okay, so we can do that. And for points, this is even easier, and for votes..? ..Without the multi-iteration voting system(s).. ..Votes are more complicated. Maybe I should just only do those from the ground up, and then therefore only recommend these for small entity sets.. (12:03) ..(I plan on using the "points" instead for feeds as well..) ..Oh, and recency points / trending points?.. ..Well, I guess we need to keep a recency/trending weight for all entities on the given list, then update these after every time tick.. but perhaps still initiated by users instead of a repeating scheduled event.. and then we also update them from new scores, on request as well, where some freshness weight is also added to those entities that receive new points.. ..Yeah, we could actually just calculate it in terms of "recency points," and then just have an automatic combination af "points" and "recency points" into "trending points".. (12:08) ..Okay, and for non-relevancy Scales?.. ..Then the Domain starts to become something to consider.. ..Filtering according to Relevance, with some cutoff, is easy enough in and of itself.. ..Hm, and when updating, not from the ground up, I guess it also doesn't change to much..? ..Oh, as long as the RecordedUserScores table just record the Scale (Quality + Estimator, as I'm using the term now) seperately, such that we can quickly go past non-relevant inputs.. Hm, or maybe this Scale should then be part of the index.. Well, it might as well.. No, it absolutely should: The Scale needs to be the first part of the primary key. ..Good. (And then the datetime (the live_after time) comes as a subsequent part of the key.) ..And yeah, since the Scale technically knows its own Domain (and we \emph{can} actually parse it, even in MySQL, if we want to..) then for.. Well, or the request to update it can also just.. no, the backend needs to check it, never mind. (But we can also do that in PHP..) So when updating a given Quality entity list, the procedure can go to the.. Oh, I should mention: The Set is also always part of the PK, both for RecordedUserScores and for UserScores.. And then we can go to the Set pointed to by the Qualty when looking at new scores with which to update the entity list, which might have a subset of the Domain as its set.. (12:22) ..And doing it from the ground up?.. ..Yeah, then you just use the UserScores table instead, and also make sure to go to the Parameter set where the Set is equal to the Domain of the Quality.. ..Okay, so for non-dynamic User lists, I should be good as long as I have UserScores and RecordedUserScores, with the Set and the Scale as the first parts of the PK, always.. ..And what about making updates when a User list changes..? (12:26) ..Hm, I think we should in principle always use constant User lists, at least on a fundamental level. ..Yeah.. ..But then: How to implement dynamic User lists on top of that?.. ..Hm, by having a table of all constant User lists, which are all snapshots, essentially, of a dynamic User list, pointed to by a User scale and another User or User list entity.. ..So we can store all those snapshots there. And each Entity lists then needs to have a pointer to the last snapshot used. And for dynamic User lists, the users can then request that that pointer is moved forward to the most recent snapshot, getting the DB to update all the scores accordingly as well. ..Okay, and how do we do this..? ..We can take each individual user and then compute the weight difference.. ..Ah, but maybe we then need the UserScores table here, which might mean that each 'update User list' request should be immediately preceded by an update of the scores, not as a request beforehand, but as part of a full transactional procedure.. If we can manage this, and we can therefore know that the UserScores table is fresh, and true and consistent for the remainder of the procedure, then we can perhaps update the User list by looking at each users difference in weight, including new users who had 0 weight before, and then boost/reduce each user's score by a corresponding amount.. (12:39) ..Oh, wait, the UserScores table should just only be updated in response to an update of the given entity list, or a refresh/econstruction from the ground up. So new user inputs just only goes to the Recorded/RecentUserScores table at first, and only enters into the UserScores table after an update.. Ah, but this means that we can't refresh from the ground up.. unless of course we make a third table to record.. Nah, it can just.. ..Yeah, we can just make refresh/recompute/reconstruct requests, or whatever we might call these special requests, which computes the things from the ground up. And these are then just run after the procedures to update the single-user entity lists, if you want them up-to-date.. Well, no, each user is responsible, in principle, to make sure (or rather the app will do this) that their own single-user entity lists get updated, and if they don't, well, then the rest of the network will just keep using their old scores, even when the have new ones recorded in the RecordedUserScores table, I mean, unless in principle another users request an update of the other users entity list.. (12:49) Okay, I think this can all work out. I'm not sure I have it completely, in terms of these last thoughts, but overall I think the whole thing is doable. :) (12:50)

%... (14:36) Votes are not \emph{that} hard: We can still update them for one user at a time. And I've gotten the idea that maybe we can implement them simply by having the user request on update of the votes, based on some already-updated scores, and then the DB just adds new RecordedUserScores for all the entities in the given list that has their votes changed. Note then that this means that votes aren't something that the users can submit themselves, as they shouldn't be, be they can request that there votes are computed on updated.

%I forgot to mention something from last evening/night: Under the Arguments, we should have both a (for-)arguments tab and a counterarguments tab, where the Argument here are supposed to be more atomic ones, and not the combined ones. And then under a third 'Combined arguments' tab, we see a list of all proposed combined argument, which are supposed to be Combined scalar parameters consisting both of for-arguments and counter-arguments. And you are supposed to then choose only one from this list (as a user community).

%Something else I thought about on my walk just now: The database should actually implement Relevancy scales.. Oh, let me start with something else first: Forget 'Parameter sets.' We just have Scales, which always have a Domain. But then we have a class of 'Sub-scales,' which are derived from a parent Scale and a subset of the Domain (although no one checks that it is a subset, but that's just the intended use). So Scales and sub-scales, there we go.

%Now, we don't actually need to have the DB parse anything about which a Relevancy scales and which are Combined scales, etc. Instead we should just make a scores table for filtered Scales. The PK then consist of a list of IDs, formatted as a comma-separated interger list in a string, which represent.. well, both the Scale IDs of the filter scales, and the cutoffs, so actually a list of ID and cutoff pairs. And then we have the target Scale, which is the one to be filtered. This scale can then.. Hm, hold on.. ..Hm, if the DB shouldn't parse any JSON, this means that all Entity lists should have their definitions written fully exploded..? ..Unless we want to introduce special Entity list entity datatypes, which are parsable.. yeah.. by the DB.. Hm, we might do that.. (14:51) ..Ah, and we actually need to have Entity list IDs regarless, 'cause we don't want to repeat the whole exploded PK for all rows, I think, even despite using table/index compression.. (Because I think the decompression might result in some added computation time which might be significant..) So yeah, I think we will make a special Entity list datatype.. (14:53)

%..Right, and then the def_str consist of a string of some syntax that defines the entity list. And an important part of this is the filters, which can be used for generating cutoff Relevancy lists, first of all, and then subsequently to create all the Quality entity lists (at least ones whose Domain is not Entities->Members), since these will then be.. Well, the root Scale of the Quality will be filtered according to the cutoff Relevancy list, which is already a filtered list itself, filtered against itself, actually. And then for all Sub-scales, we then filter the root Quality list (i.e. the one where Set = Domain), with anther Relevancy list. So actually four filterings in total: Two to create the two Relevancy lists, where these are filtered against themselves, and then one to create the root Quality list, and then another one to create the child Quality list.. ..Sure, something like that. ..Hm, but maybe we actually don't need the cut-off Relevancy lists, if.. Oh right, if we just include the cutoffs in the syntax, which is what I wanted to do, talking about those scaleID--cutoff pairs.. (15:04) ..There should then also be syntax for making weighted combinations of several entity lists. And Estimators should also be a part of the whole thing. But here we are \emph{not} using Estimator IDs. At least I don't think so.. Instead we implement the estimators as part of the syntax, perhaps using integer codes for different estimators as well, but these should then be completely independent on the IDs of the Esimator entities. The Esitmator entities are on the other hand handed by the app. Oh, let me mention: I think the median Estimator should include an atrribute that defines the bin width/number (where the backend then only accepts up to some maximaum number, at least for the prototype/beta version). And this attribute, as an example, then needs to be parsed by the app, and the app can then construct the corresponding Entity list definition from that, but with different codes for the estimators, as I said, and not using the entity IDs at all. (15:09)

%..And there we are... :) (15:10)

%(15:18) Oh, and I should also at some point implement an entity list type that takes a user list, a cutoff, and another entity list, and then forms an entity list of the users on the list above the cutoff scores according to their (opinion) score for the given.. oh the given entity, I guess, not entity list.. ..Yeah, a user list, a cutoff, and an entity, oh, and a Scale, of course..

%(15:33) Hm, do we then want Scales in the Entity list definitions to be referred to by their SK: Object, Relation, Quality, Estimator.. well, Esitmator is already part of the syntax.. ...(15:43) Well, I think it is a good idea to make the Entity list entities' definitions be verbose, yes.. ..Yeah and for every part in the compound, the DB can then just look up if that entity list exists as well, and then update it as well, before computing the compound. So yeah, maybe they should just be fully verbose.. Well.. Or maybe not.. Hm.. (15:47) ..Well, since the whole thing is parsed by the DB, then we might actually as well use scaleIDs instead. For the app can still get to just submit the whole thing, and then the DB can find the IDs of all the parts (via the hash SKs).. (15:49)

%(15:56) Hm, if an aggregate procedure goes through each user on a list individually, isn't it better to just refresh/reconstrut from the grounds up..? ..Hm, I could also making a user group tree, where you then only need to update the ancestors of a given user on updates.. Hm.. (15:58) ..It by the way seems that it is the multi-user aggregates that will be the heavy ones to calculate, not the compound scales from there, nor the filtering.. ..Hm, if I make user trees, these could then be created from binning user weights/scores on the given User list.. (16:01) ..Hm, maybe the tree idea is a good one, 'cause it might allow us to make the changes just propagate slowly to the top in several steps, instead of having to change the top level for each single new update.. Hm.. (16:04) ..Hm, the updates can then propagate upwards in waves.. And users can then also use the linking nodes by themselves, since these will also be User groups/lists. In fact, a User group could just always be created from a bounded number of sub-user groups / users.. Hm.. (16:08) ..I think this might be a sound idea.. (16:09)

%..We could also just not make Esitmators into individual entities, necessarily, but then instead just give the Scale entities an Estimator definition attribute. This definition is then free to follow the same syntax as in the backend (if we want to let the two syntaxes diverge at some point, we can just do so). (16:14)

%..Hm, with the user group tree idea, one then has to decide whether the individual weights are governed from above, or if they are governed independently by the user group itself.. Hm.. (16:17) ..But we can just have user group the are defined from two lists: I short list of.. well.. a short list of user (group)s, and then a list where they derive their scores/weights from, but then that means we need to have and update weight lists for each tree node, which I don't think is a very good idea.. (16:19)

%..(16:21) Oh, it's actually also not great if we \emph{have} to use snapshots underneath dynamic user groups, at least not if it then entails that we have to store and keep the scores for each snapshot.. And I'd also rather not have to store the snapshots themelves for good. But then again, with this tree idea, maybe we don't need to, since we might then be building the entity lists "from the ground up" always, only we might do it in waves, so to speak.. (16:24) ..Which makes me think that this tree idea will be the way forward, even though I still have to figure out a parts of it.. ..Hm, by the way, since the individual lists of users/user groups are then quite small, we might also implement it via the entList defStr syntax. (Recall that we use entListIDs for all the parts of a compound; it's only that the app is \emph{allowed} to (but doesn't have to) explode these references into their SKs, rather than fetching and using the IDs.)
%..But having dynamic user groups then means that the combined entity lists should also be able to have their weights defined by some non-constant way.. (16:31) ..Oh wait, the linking nodes in the tree do not have to have any weights, or rather the weights can all be 1!.. Well, unless the user group count in each is different.. Well, but if we instead just record the total weight for the data structures that we propagate upwards, then we only need to know the weights at the root node, and this can then just be given by referencing one User list. And this list can then easily be a dynamic one, especially if we don't care that some weights are e.g. added somewhere before they are subtracted another place. And I don't think we should. It's not important that the weights sum up to the same thing on every single iteration, as the data is propagated upwards in these steps. So yeah, I think that'll be the solution!.. (16:36)

%..(16:39) Ah, and propagating data up "in waves" through the tree also opens up for making it easier to catch e.g. spam locally, before it moves all the way "to the top".! :) (16:40)

%..Then we just need a way to agree on the user group trees, such that the app knows what to search on. Hm, I was thinking that we could "just" derive it from the overall User list, but maybe this isn't ideal.. (16:46) ..Well, it can of course query for popular user groups, and it and the user can also save specific user groups in private settings. And yeah, I guess the app don't need to look up the structure of the tree; that might just be a concern of the backend, i.e. when a user requests that there scores are updated.. Well, and/or if someone requests that a new iteration of the wave-propagating is.. Nah, it should come more from the ground up, I guess.. Hm.. (16:49) ..Ah, but if the individual users' places in the trees don't change (only their weights might, and they might receive sibling nodes.. Hm, which might be use groups, come to thing of it.. ..Yeah, we can mix users and user groups, and then just only look up the weights for individual users, but read them off of the entity list metadata that part of what's propagated upwards when it comes to the user groups).. If the indivdual places don't change, then the app can, I guess, more easily look up where the user is in the tree..? Hm.. (16:54) ..Oh, we could also make trees where the Users are always inserted according to their ID. And then we just need to sometimes turn a user group of individual users, which was therefore a leaf node until then, into a group of user groups instead. ..Hm, but does this mean that entity lists are editable..? ..I mean, yeah, maybe, 'cause this could allow us to do this exact thing, without then getting an old user group dangling, where it then becomes a problem to delete the entity list data.. (16:58) ..Hm, but this then destroys me current system where having a hash SK also means that it is non-editable.. Hm, so do I change that, or what..? (16:59) ..But let me then point out first, that with such structures, the app can then look up the given user's place in the user group tree easily, and make any update requests that the user asks it to, making the updates propagate further up.. (17:01) ..Hm, and couldn't we just update the hash as well when we edit entities that have hash SKs..? (17:03) ..Hm, alternatively we just make it so that.. Well, maybe that.. entListIDs can be discontinued, hm, maybe not.. ..It's probably better to have them be editable.. ..Well, but by who? It's not great if maintaining a large User group hinges on some user making regular updates of the entities. No, I need to figure out something else.. ..Hm, not "semantic" determination of the sub-entity lists and their weights, right..?.. (17:08) ..Nah, maybe it's better to just let this tree structure be handled in a seperate way, not really using my Entities for it..? ..Yeah, the whole tree structure could be automatically governed by the database, and then it's all just defined from one outer User group. But users can still also query subparts of this User group tree (which can be useful for detecting spam in a distributed way), and the app also need to make userID-specific requests to update parts of the tree. Okay, I think I might do it this way.. ..So again, the sub-parts of the tree are not themselves User groups, but are just generated automatically by the DB. They can, however, still be queried for, almost as if they \emph{were} individual User groups, but not quite, since they don't have their own entID, but are pointed to instead by the entID of the full user group, and a given userID, plus potentially a depth or height level (from the top or from the leaf node where the given userID sits). (17:19)

%(17:26) Oh, I actually think I will drop the Wish estimators, and then instead just make a tab of whiches, which are then comments. These can then be scored both according to desire, but also according to e.g. how easy they are to fix (potentially in a future version and/or similar product/thing), which then gives us a good opportunity to combine these two into a useful compound, which could b something like 'Desired (for being fixed)' \times 'Easy to fix.'

%(18:22) I think we should actually still make a table kind of like RecordedScores, and then make the User group update procedure scan this for new updates, and then update the local tree node, first of all. And once this is done, then the procudure can insert a new Recorded/RecentScore from the linking node just above. And on the next call to update, that producere can then, when also seeing the "recent scores" from the linking nodes, update these notes as well. This way we only need one update request for a given user group's entity list, while still being able to update only the parts of the tree that has changed.

%..I'm also considering making a delete entity list (data) request, which then takes some time to go through, and is annulled if the entity list becomes updated in the meantime. Well, I \emph{should} do this. But the question is then if I want to go back to having each node of the User group tree be an individual user group as well, after all..? (18:28) ..I don't think so.. ..Yeah, nah, let's just handle all that in the backend, especially now that we only need one update request.. ..Well, but the users might still be able to query for Lists of subtree nodes, if they won't to take part in spam detenction, i.e.. (18:32)

%(18:53) Hm, I'm completely closed to the idea of making all the nodes user groups.. We could also just update the tree from the top down, going through all the leaves, and then not care about the Recent/RecordedScores... ..Yeah, maybe that is actually better.. Hm, but if only there was some way to.. ..Oh, we could still make that thing about making the tree like a userID index, essentially, and then make the app request the updates.. Only thing is, we might also want.. Well, to put it simply: Do we want the user/app to keep active track of which groups the user belongs to and make updates there?.. Hm, we might do this, but..? (18:59) ..Hm, yeah, we could do that.. (19:00) ..But it seems like a lot of effort just to save some.. effort.. So I should think about it.. (19:01) ..Hm, maybe we should let the app keep track, by using the DB, of course, of the user's user groups.. ..Yeah, it's a two-way subscription, so to speak. And we want to keep track of it anyway, I think.. ..Hm, and I guess each user group defines its own tree structure, and then just also makes public some data that links each user to their node in the tree.. (19:14) ..Hm, and then there's no RecordedScores, and all updates are then called in response to user requests.. (19:17) ..Well, I like for these trees to be generated automatically, still, so I'm back to where I was before, I guess, where each User group tree is handled by the backend, and where there are several types of update requests per user group---and for any given Scale, but the point is to reuse the tree structure for all Scales.. (19:20) ..Then the DB stores intermediary data for all User groups and Scales pair, but only for a time, and it can always delete it, since it can always be remade from the underlying data.. ..Yeah, and the same goes for the outer entity list; this can always be deleted if no one uses the User group anymore, at least not for the given Scale.. (19:24) ..We should by the way make sure to make each node pretty large, in terms of having a large maximal number of children, since this thus prioritizes saved storage space over saved computational work.. (19:26)

%..Oh, and a 'constant entity list' is just a special subclass of entity lists, that also include a timestamp, and won't be udated by the database (only possibly deleted). And these const be both User entity lists, as well as any other kinds of Entity lists. (19:29)

%(21:57) Hm, maybe User groups should also be a special kind of entities with their own datatype as well.. ..With requests to construct the tree, i.e., and where this tree can then be used for all relevant Entity lists that use the given User group.. (21:59) ..On the other hand, a User group/list is also just, at least essentially, an Entity list, so we can just use the same datatype for this.. ..('l' for list..) ..Yes, and it can also be parsed the same way, for instance when the app sends the full, exploded definition, instead if fetching the individual IDs; here the User lists can just be parsed from the same syntax.. ..Yeah, and this means that one can make arbitrarily complicated user groups/lists in principle, which is a good thing. (And they are dynamic by default, unless the syntax codes for a constant list.. (22:07)

%(28.11.24, 12:04) I should make sure to not expose the whole user group tree and such. I should make an interface in terms of the entity list definitions and requests that hides things like that. So the user can instead just provide their own ID to their update requests, and that's it, then the rest is handled by the backend. ..About the user groups that the user subscribes to, maybe we can implement this via an entity list over user groups by the user.. ..(12:17) I think we should do this, yes. And maybe I should make a RecordedScores-like table over update reqests.. ..Yeah, and the backend can then also add its own requests to this table, in order to update things in waves. ..Ah, and it can use a counter for update requests for a user group and/or an internal/linking node in a tree, and then onlly update this after a threshold is reached.. ..And when the usr group itself is updated, then the whole thing is just recalculated from the ground up, and all old data is thrown away.. ..And maybe we can throw it away via a garbage collector procedure, if we want.. (12:23) ..By just giving all such data an expiration date.. ..I like this idea about using that counter. It also means that we probably don't \emph{have} to record user upload data as part of updating the entity lists, I think.. ..Okay, so the app just makes update request, providing the user ID, to all.. ..to all user groups above a certain threshold score on the user's own list, and for the specific Scale in question, after the user has submitted a new score to that Scale.. ..And then the rest is handled in the backend (where I will likely try to use those user group trees with counters on each node for each Scale..). (12:30)

%..About the syntax, cutoffs should never result in a new entity list on their own, but always be part of making another.. Oh wait, that's not exactly true, since User groups needs to have an ID, and also needs to have a cutoff. But we can perhaps say that cutoff entity lists are just never stored seperately by the backend. Yeah.. (12:33) ..Okay, let me try to formulate it in a symbolic, "intermediary language"..: We have Cutoff(entList, score), first of all (which are never stored seperately by the backend, as I just said). Then we have Filter(targetEntList, filterEntList, score). Hm, and the filter input could be a list, but we could also just make several calls. I think we ought to do the latter, perhaps, 'cause this is all visible to the frontend, and the intermediary entLists can thus also be used on their own.. ..Hm, and what more do we even have.. Oh, estimators?.. Ah, we have: Combine(entList, weights).. Wait, estimators first.. ..We have UserEntList(scale, user, userEstimator).. And then Aggregate(entList, estimator).. Nah, here we need a list.. Ah, a user list.. ..Or a user group, rather, which is.. also a user list, since now we also see Cutoff(entList, score) as an entity list on its own. So Aggregate(userList, scale, estimator)..? ..Yes.. ..And then Combine(entList1, weight1, entList2, weight2).. (12:43) ..Hm, maybe I should add an VARCHAR/VARBINARY data string to UserScores, which is then either the last part, or not a part of the PK, depending on whether we want to be able to fetch it from the SK.. ..No, whether we \emph{have} to have it be a part of the SK, so I guess it shouldn't be a part of PK.. Well, I'll see. This data can then for instance be the uncertainty/agreement interval, e.g. And actually, maybe I do want to have flat aggremant curves, not Gaussian ones, as part some thing that users can choose to add to their scores. ('Casue this might result in much more submitted user data, and the falt curve actually makes more sense, I think.) (12:50) ..Hm, but this can then just be part of our standard 'Opinion' user estimator.. ..And we can then let that be the default Estimator, if the input is missing from the UserEntList(scale, user, userEstimator) "call" syntax.. (12:52)
%(13:03) Hm, at the bottom we have UserEntList(scale, user, userEstimator?), and then Aggregate(userList, scale, estimator), which is what will require the most work. This is where the submissions and update requests plays a part.. Well, the ones I've talked about until now. And the update requests for the higher-level lists like the filtered and combined ones?.. ..Well, this is just all the more easy to implment: Yes, there should also be update requests for those, but these are typically given as part of querying for them.. ..Hm, maybe we could actually also make a counter here, that updates after enough.. Well, or is just more quick to update the more counts. By the way, for update requests with low counts, depending on the thing, me might put a lower threshold on the cunter for when the thing is even updated at all. But for low counts above this threshold, we might also have a scheduled event that runs infrquently, that updates the things for the requests with a low (but not too low, perhaps) count. ..And ones a count reaches its max threshold, an update event is sheduled to run soon thereafter. (13:13) ..Hm, but for the beta version, I might just shedule an event on the first count, for all requests, and then just let it run a few seconds after. And since this event will end with a request one further level up the tree, the whole tree will then be updated in a matter of some seconds (if it's not too high, which it won't be in the beginning). (13:16)

%(13:17) Okay, so the times where we might want to parse an exploded entList definition is for the filter(), combine(), and cutoff() calls.. ..Hm, maybe I want to just implement combine() only in the app, by the way, for the beta version.. (12:20) ..Well, or maybe I just want to implement it \emph{only} in the backend.. ..Well, ideally, in the end, we probably want that a query request can return nothing, but increase a counter, and once that counter reaches a threhold, the backend constructs and start serving the list, meaning that the app then doesn't have to contruct it itself, which is what it will do until the threshold is reached.. (12:28) ..But I can always postpone implementing the combination by the app if we just start out with a threshold of "above 0" (meaning that the list will be constructed and served already on the first request).. And note that all aggregated data can be deleted, and will be after a time, when implement that garbage collector.. (13:30)
%..Hm, I could also make a procedure where the app sends in a full high-level entList definition, and then the DB send the same string back but with IDs replacing all the exisitng entLists. And then the app must query for those, and if the returned string is not just a single ID, then it also has to them combine and filter (but not cutoff, since this is done by just querying until some lo value) to get the full result.. (13:35) ..That indeed sounds like a very good plan.. (13:35)
%..Okay, then it actually sounds like I know what to do, for the beta version, and going forward from there.. I mean, of course a thousand things will come up along the way, but still.. (13:37)
%..Ah, but about the syntax, since I would then need to be able to next syntax in other, I should just use function call-like syntax, like I did for my "IL" here, but only where the function names are just shortened to names that are typically one character long.. Hm, but wait, if we take the input to always be of fixed length for each function, then I could also make it as just a comma-seperated list, if that's somehow easier to parse in MySQL.. ..Well, but why not parse it in PHP, though, anyway..? (13:41) ..Oh, it could be easy enough to implement in MySQL, I think, if we do that one-(comma-separated-)list thing.. ..Cause we could just call subprocedures for each.. Well.. Maybe.. ..Well, we can certainly make a for loop that just used a stack to implement it's context.. ..But no, I can also call subprocedures that can then just tell me how maybe list items was consumed by it. Okay, that's the smart thing to do here.. ..And then it just returns the same list back, but with all inputs replaced by IDs when they exist (or rather when they have data.. Ah, but one also just might return ID--bool pairs for each element in the list, where the bool is whether the list is ready to be served or not.. or perhaps a countdown to when the list is ready to be served, i.e. in the form of the counter threshold minus the counter..), and there importantly, the function in a function call is "replaced" with the resulting ID (and a bool) if it exists. Okay, I can do that.. (13:51) ..Hm, maybe it requires creating a tempory table, which is then inserted into by each sub-procedure, and then selected at the end of the main procedure, but that's also fine..
%..Hm, but maybe I don't want to expose the counter, but just let it be something internal. ..But as long as we return 0 on success, then the non-zero integers is left open for future implementation, such as returning a countdown, etc. By the way, the counter should actually really be a FLOAT, and it should increase by the user weight specifically, rather than just increasing by 1 for each update request.. (13:59)

%(14:03) Hm, I think I will just make all user group trees one node deep below the root for the beta version, meaning that it is actually just \emph{one} list of users, and a counter that still, however, increases propoertionally to the user making the update request.. ..'Cause then all I need to store for these "user group trees" will actually just be said floating-point "counter," I think.. (14:06) ..One for each Scale + User group, i.e..

%... (15:29) I think I will not implement Combined lists in the backend, other than in that prodedure that returns the individual entListIDs. And for all other entLists, I think I will just let each query schedule a later update, unless one is already waiting. That can then be our starting point.. Hm, and for submissions, let's still add a request to update the User group + Scale of the subscribed to User groups, where we also provide the ID, even though this is not used for the early stage when the user group trees are just comletetely flat..

%..By the way, all Qualities now hold a Domain ([Object, Relation]), and so does all Scales as well, where the Domain can then be different (a subset, usually) than the Quality's Domain. And when users submit a score for a Sub-scale, as we call it when Scale domain \neq Quality Domain (and usually with \neq being replacable by \subset), I think we should just score it for both Domains at the same time, then, such that the app can always query the exact relevant Scale for the user's own scores for it.. (15:37)

%(15:41) Oh, and for the entList function characters, we can choose: 'u', 'a', 'f', and then for combine, 'c', and for cutoff?.. ..Hm, maybe 'combine' is to vague.. It could aso be something like 'union'.. ..Oh, and the userEstimator is now a required input, btw.. ..We could also say 'atom', 'aggr', 'filter', 'cut', and 'union'.. ..Sure.. (15:48) ..Then let us also give the basic estimators small keywords, like 'op', 'mean', 'med', 'pos' (for positive scores), 'neg', 'vote', ..and maybe '1st' for 'first.. ..'first quartile', and '3rd' for 'third quartile'.. (15:53) ..Let's put estimator as the first input, btw, jst in case.. (15:54) Oh, and 'rec' for the recency score.. Hm, and is this \emph{just} computed by looking at positive scores in the.. well, yes, the positive scores in recent times, where the more recent the less is subtracted from the contribution.. (15:56)

%(16:08) Hm, I \emph{can} make a separate table for all these kinds of entLists, btw, if I want.. ..And without 'union' and, of course, 'cut', this is just for 'atom', 'aggr', and 'filter'.. ..Hm, unless I want to make separate 'aggr' tables for each estimator as well.. (16:12) ..Hm, maybe that's a good idea, as it's most efficient, and probably also easiest, even though it does mean a lot more Scores tables.. (16:14) ..Oh, and for median aggregates, we actually need to store all the bin data for each subject.. ..Hm, and the mean can then just be calculated from a the median data entList, actually.. ..Oh, but I guess I also want a final median table that doesn't have this histogram data on it, used for querying whole entLists.. ..Ah, and for the intermediary structures, it's better to sort after subj_id anyway, so let's therefore divide the former PK and SK indexes into two tables, where the histogram is then stored by the formerly SK-one, namely the one sorted after subjID..:) (16:19) ..That's great, actually..

%..Let us also implement a priority ('pri') aggregate at some point for the early stage, btw.. ..Well, but also one that's just handled by the app alone in the very early stage..

%..Oh, the median aggregation procedure needs to know the bin size and interval location as well.. ..Well, but if the histogram data is encoded as a string, then the table itself might not need to know this, only procedure that updates it.. ..Yeah, true.. ..Oh, but I guess this means that the parameter number will be dependent on the aggregation estimator.. Yeah, so each estimator should really have their own function. Hm, but so far, we can just use the same keywords as I mentioned above, I think.. ..Yeah, it seems so.. (16:30) ..And 'op' or 'opin' then also replaces 'atom'.. Yes.. (16:31) ..Ah, and 'hist' replaces my former 'med,' and the new 'med' and 'mean' is then just calculated with a single 'hist' entList ("histList") as their only input.. ..Ah, and we can then make plenty of these, including e.g. my old "mean/median with offset <n>" aggregates (which I actually plan on using, still, for the early stage).:) (16:34)

%(16:47) Hm, the "points"/'rat' can also be calculated from the hist data. But 'rec' is a bit harder. ..Should we make recency histograms as well?.. ..Well, we could, or we could also just aggregate recency in its own way.. ..Yeah, let me just do that instead.. (16:51) ..Well.. ..Yeah, let me do that.. And for votes, I'll also (because I need to) compute it in its own way..

%(17:02) Hm, maybe I can gather 'med' and 'mean' and such into a single ScoreEstimators table.. ..Well, let's call it AggregatedScores, and use it for all entList where there's just one score aggregate column as the payload data.. ..Well, let's be more general and call it FloatingPointScoreAggregates.. (17:05) ..Or just FloatScoreAggregates for short.. ..Well.. ..Hm, and let me just use that for all these various entLists, that are not either the (atomic) 'opin' ones, or any intermediary ones like 'hist'..

%(17:57) Hm, we could keep some of the entList function calls exploded, for instance if we want to always store userOpinionScores by the user_id and scale_id, instead of ent_list_id. And another question: Do we want to then also explode the Scale part of these entLists..? ..Well, of those \emph{and} of the aggregated lists..? (18:00) ..Hm, maybe it's best to always store and use the ent_list_id. But we could still explode the scale parameters..? ..Well, or it could just use the exact same system, is the thing. The app can just send in the full string, and then get the corresponding ID list (with booleans attached as well) back. Or if the app requests, it can just get the entity list back, if it all turns into just one of these.. Well, but at the early stage, let me just make do with the request that always gets you an ID (and bool) list back.. (18:04) ..Great, yeah, let me include the scales as part of this syntax as well, hm, even though these are implemented as JSON obejct entities.. Hm, but this is okay, right..? ..Hm, technically, a Scale is also a list, can't we say that?.. A Parameter list, i.e..? (18:07) ..Oh, alternatively we can call it an 'abstract list'.. (18:10) ..Yeah, that actually makes a lot of sense, I think.. (18:11) ..Yeah.. ..Well, this is quite nice, isn't it?.!.. (18:12) ..And then a 'Concrete (entity) list' is an 'Abstract (entity) list' + a User or a User group.. ..I really like that, I think.. (18:15) ..And I'm letting 'User group' refer to a cut-off User list here, by the way, since the cutoff is often quite important in this matter.. (18:17) ..Yeah, I'll absolutely make this change, since it also cuts out a lot of redundant hashing. So yeah: An Abtract entity list (formerly a "Scale") is an 'l' entity that consists of just Obejct + Relation + Quality + estimator. And note that the first three of these are generally implemented as JSON entities, and this is also true for Estimators, but each Estimator JSON entity holds an estimator keyword, and this is the one that is actually used in the definition of the Abstract list / Scale, and not the Estimator entity ID. ..Why? Because the backend needs to be able to parse the estimator, and the backend shouldn't depend on the entity IDs (i.e. on what entities get which IDs). (18:23) ..Hm, I might still keep referring to the Abtract lists as 'Scales,' though..:) (18:24) ..Well, but I also might not.. (18:25)

%(18:57) Hm, the mean and the meadian can't actually be calculated from the same histogram, unless we want to distribute the contribution into each bin that's within the score_width, rather than copying it. But I guess we do..(?) ..(So the contribution to each bin will essentially be divided by the score_width, i.e..) ..Oh, this is also what we want to do for the histograms used for the medians anyway, never mind. We'll divide by score_width (rounded up or down to a multiplum of the bin size, of course). (19:00)

%(21:37) It shouldn't take us too long to also implement ML entLists as well, where this 'ml' function then needs to take a user group, and a list of Scalar/Semantic parameters as well, and then spit out PCs.. ..Well, this gives you a list of lists, then.. Ah, but that's fine: A lists of PC vectors, and then you can derive entity lists from that by combining this PC list with an index. *(So the list function keyword could also be 'PCs'..)

%And let me also underline that all Qualities are now free to be dependent on their Domain, and actually also on the scoring user. This way we'll also get User qualities for free. (And these can be used e.g. for ML, i.e. by including..) Wait, how to construct the User statements/parameters from these user-dependent Qualities, then?.. ..Ah, the Set can be one of 'User qualities,' and then you use a Quality that says 'fits *User*'. Yeah, we can do it like that.. (21:45)

%..And something about the querying entLists: We can just prepend the list of IDs to the entList table, where the score can then represent the aforementioned boolean value. I think I will do this.. (12:46)

%(22:12) Let me just mention something: It's really nice how my regular JSON entities now plays well together with the other datatypes. For instance, we can (and will) make a class of Texts, the the app can then know to insert 'x' texts rather than JSON entities when a user inserts under this class. And just because the standard Texts are 'x' entities, doesn't mean that all classes need to be; the Texts class can have subclasses that then break out of the 'x' datatype again, and back into the JSON ('j') type. The same can also be said about our 'l' entities here. And this also means that we can make a class that describes these entities, and it can even describe the syntax. So all our datatypes can thereby have a class with a class description, how nice is that. (And we also have the possibility to make e.g. Text sublasses via XML instead, as long as we just describe what's going on in (an edition of) the Texts class description.) So everything is possible here, semantically, and the different datatypes can work nicely together. :) (22:19)

%(29.11.24, 10:22) I will use 'f' for 'formal' entitis instead of 'l' for these formerly "list entities." And I should also include Estimators as formal entities. These should then first of all consist of interval bounds and a step/bin size (since I'm binning, it doesn't matter to say whether the end points are included or not), and then a 'Metric,' which is a JSON entity, purely defining the semantics of the interval (and defining the unit and its symbol and such for the app to use (but is not itself used by the backend at all)).

%I've also thought about potentially dividing the defStr into a searchable part and the rest. But no, I have a much better idea now. I'll another table like EntityHashes for small strings to be searched on directly. And then I'll actually also do something else: I'll add the entID to the PK (for both these tables), and also, after the string/hash and before the entID, I'll add the isEditable bool. This then allows for editable entities, and in particular texts, to be editable, while still hash/string-searchable. The convention is then to always take the first non-editable entity on the list when searching for formal.. well, searching for entities with a specific format in general.. And in fact, let's just allow for only one non-editable entity in the table for each string/hash. (Note the non-editable also \emph{have} to entail non-deletable, by the way.) Now, let me just see something.. (10:34) ..Ah, 767 bytes UNIQUE INDEX length limit for some versions of MySQL. That sounds about right: I think I might have such a version. ..But anyway.. oh let me see for other dialects.. ..Well, it's InnoDB that sets the limit, it seems.. ..Oh, VARCHAR(255) is 255 UTF-8 (normally) characters, not bytes, I didn't know that. But it makes sense; it just means that the reader procedure parses UTF-8 while it reads the strings in the rows.. ..Ah, and that explains why I got prompted a 3072 byte limit (this fits what I recall) and tested more like a ~767 \emph{character} limit myself. Okay, nice to know. So this means that I \emph{can} make a convention where we make a split in terms of the number of characters that then determines whether we use the hash table or the string table. Oh, by the way, a string should only ever be iserted if it is the full defStr of the entity (when the datatype char/string is also prepended), and if it meets the given length limit, of course. ..Hm, but if I'm making this convention, couldn't I just hide the hash vs. string question from the frontend, and make it automatically handled in the backend..? ..I don't perticularly like JSON-encoding JSON strings, but then again, with the 'formal'/'functional' entity datatype, I really don't have to do this very much. I actually don't really see when the user will ever search on a JSON entity.. Well, maybe sometimes, but then.. Oh wait, no. It's a good thing that the app can query via the hash, e.g. for long texts. So should we just keep it completely open to the front end, and just make it up to the.. Hm, or maybe I could at least require that a string is always inserted as well, along with the hash, for small enough defStrings.. (10:52) ..Yeah, let's do that. That seems important to do, actually.. (10:58)

%..I'll let go of the IndexedEntities table that I have now, and then let me think instead about full-text indexes.. ..Hm, I think I should have just ne of those. I could have more, but I need a table for each one I think, so it's not something I want to try to let the users be able to request and get automatically. And implementing several ones also seems a bit redundant, not least in terms of effort. So no, let's say only one full-text index for the beta version at least. So how do users submit texts (XML and UTF-8 (which is also a datatype that I'm going to include at some point)) and get them added to the index? Well, first let me mention something else:

%I forgot to mention that I actually won't make an inclusive class for all 'f' entities. Users can do so, if they want, but I won't. I'll instead create a class for each individual type of 'f' entity, such as List entities, Estimators, and perhaps also a seperate class for Scales (which I might call 'Scales' rather than the longer 'Abstract lists,' actually). And I then won't give these classes any Parent class attribute. (11:07)

%..But back to the previous question.. ..I guess by letting the developers manually choose a fitting User group, and then use a Scale of texts to search on. And then we can just make a scheduled event (perhaps a triggered one) that once in a while updates the full-text index, probably by constructing it from the bottom each time (unless there's a better way). Okay. But often we want to direct them to another entity directly after clicking the given text.. And maybe we even want to display the entity rather than the text in the search results list.. ..So do we.. Hm.. Ah, the user just submits the text \emph{and} the entity ID to the fulltext table. ..Ah, but then they should actually uprate a kind of (Text + Entity) entity for the given Scale. Okay, let's say that. ..And then the Entity can just also simply be the Text itself. Okay, so that's how we'll populate the fulltext index used for searching. ..Oh, but it would be nice to be able to search before the users are schooled in all this. So maybe we just also pluck entities from the Entities->Members list, and then just make sure to make all JSON-syntax characters part of the stop words of the index, if they are not already so by default. And \emph{that's} how we get the first search bar, although I will also combine this with that (Text + Entity) solution at some point (where we can just mix the two things in the same table; it just means that the text column of the fulltext table that the fulltext index sits on needs to be gotten and parsed in different ways). Okay, I'll do that. (11:21) ...Oh, we can just remove all stop words from defStr before inserting it in the fulltext-indexed column. (12:02)

%(12:09) I was about to add a table for constant entity lists, but no, the constant-ness can just be determined by the definition of the given entList. And then all these constant list can just use the same tables as their normal, dynamic counterparts.

%... (14:57) Hm, maybe I should use BLOB instead of TEXT for def_str, and then I should probably CAST() from utf8mb4 to BINARY for the standard insert proc.. ..And I could CAST back for the standard entity query proc.. ..Yeah, I should do this.. (14:59)
%..And then I'll only except defStr indexes for def_strs that passes the conversion to utf8mb4(_bin). And with this, I could then set the.. ..Hm, let me think actually, 'cause my current strategy for the entity index tables needs changing.. (15:02) ..I didn't sleep a lot this night.. ("last night"..)
%..Well, as long as I allow only one hash/string of the same value, at least for isEditable = 0... ..But I could also, yeah, just make the proc "find" when the hash/string already exists, like I've done until now.. ..I'm so tired..
%(15:36) I think hashed entities should just always be non-editable. So if a user want to edit a hashed text, they should just replace it, or give it a better representation.. Hm.. ..Hm, when are we actually gonna search on text hashes anyway, instead of fulltext indexes?.. Maybe searching on binary files will be useful, though.. ..But here I think we wold also just prefer to submit the hash--entity pair to the fulltext index.. ..So maybe we only need the EntityDefStrings table, and only for the 'f' entities..? (15:43) ..Yes, I actually think so. But I can keep the table open for other datatypes as well. ..Hm, but it wouldn't take a lot to expand it to automatically also use hashes for large defStrings, simply by keeping an is_hashed bool, and then by letting the procedures convert automatically when LENGTH(def_str) > 700.. ..Oh, but then it should be the binary length.. ..Sure.. Okay, so just one table for entity SKs, and where the procedures always just compute in which part of the table (the is_hashed part or not) they should query from / insert to. Okay.. (15:51)

%(30.11.24, 11:50) Forget about casting to UTF-8. We just hash depedning on the binary length, and then use a binary type only for the defStrings in the backend. ..Hm, URI encoding four-byte UFT-8 characters will mean that only 170 such characters can be fitted in a 2048-character-long URL.. ..So should be just always hash the SK after all..? ..No, but should we also add the.. let me think.. ..I'll get back... (12:01)

%... (14:00) I'm thinking of making all non-editable (and therefore public) entities have a hash. And then it's just: what about the defStr keys..? ..I could just make them for all 'f' entities, and require a max length for these.. ..Hm, and we could open up for having user-defined functions, such that the app can just use these (but where they are not necessarily used by the backend in any way).. That seems like a good idea. Then I just need a syntax to tell that it is a function, but this could just be 'f<ID>,' and in e.g. 'f123'.. ..And let's say that 'f' stands for 'functional,' then.. (14:09) ..Yes, okay, let me say that for now: The only string-searchable entities for now are the 'f' (functional) entities, and these then has a max length of something like 3000 bytes.
%And then maybe I'll just give all non-editable entities a hash, but let me think for a moment.. (14:12) ..Hm, it makes sense to give all JSON entities a hash.. ..Hm, and I do like for that not to be a user choice, but.. ..Hm, I think I should just do it.. ..Oh, easpecially since we can just make the texts of e.g. comments and such editable by default.. Well, yes and no.. ..Hm, we do have representations to escape trolls, but that then require us to copy it.. Yeah, and then you might as well.. Well, if other users can see that it is editable, they might upload a non-editable copy.. ..But this gives us some redundancy where they question becomes, why not just not have editablili.. wait.. Could we let other users request that an entity loses its editability..? (14:24) ..Nah, that's too messy.. ..Hm, but we could give all editable public entities an editability expiration date.. (14:26) ..Wait, now that we have the 'f' datatype, it isn't \emhp{as} critical to make sure that other types of entities can't be edited or removed by the user.. ..Hm, so maybe no hash at all.. (14:30) ..And then hash-searching can just be done via the fulltext index when relevant.. ..Hm, I like this.. ..I do also like some way to make sure that JSON-entities are non-editable.. ..Hm, could one make a system where one entity can have several owners, sorta like a 'shared memory' situation..? (14:34) ..Well, or better yet: just introducing a flag to 'copy on edit'.. (14:35) ..Wait, with my 'representations,' we shouldn't really need editing in the end.. Except for removal.. ..Okay, now I'm thinking, let's hash all JSON entities, and make them non-editble (but replacable by 'better representations' still).. (14:39) ..Hm, and for texts, we could introduce a kind of 'constant entities'.. ..A monad where we copy-on-edit..(?) (14:41) ..constant \emph{text} (/xml) entities..
%..Oh, by the way, if the community at some point wants to start using functional entities instead of JSON entities, then that's also okay. Then we just need to implement a lot of things over again (and I don't expect that this will happen, but it \emph{is} an open possibility, which is a good thing)..
%..(14:51) Hm, it'll not be introducing new datatypes; it's just that some datatypes allows for isEditable = 1.. ..And whenever it is 0, we also insert a hash?.. (14:52) ..Or a string key, at least for the 'f' entities.. ..Hm, maybe 'f' entities should be pure ascii, by the way, also since e.g. the scale units are contains in the 'Metric' JSON entities anyway. Then it will make no sense to use 'f' entities instead of JSON ones at any point, but I guess it wouldn't really anyway.. ..Yeah, let's use ASCII only for the 'f' entities.. ..And then we use hashes for the 'j' (JSON) entities' secondary keys.. (14:56) ..And also (use hahses) for the 'x' (XML) and 'u' (UFT-8) entities when isEditable is set by the user to be 0.. ..I like this a lot.. (14:58)

%Hm, about the 'x' entities, though: Shouldn't it be 'h' for 'HTML entities' instead?.. ..'Cause it \emph{is} supposed to be a mark-up format, not a data format, and I'm also now kinda thinking making it fully HTML with a <head> and a <body> element always.. (15:01) ..Well, and even if we don't require all that, we can still call it HTML. Okay, yes, I think we should call them 'HTML entities' instead, and use 'h'.. (15:03)

%..Oh, by the way, I'm imagining that we will be able to implement and allow for more and more complicated HTML as time goes on, and even ending up by allowing for JS <script>'s and so on at some point (of course with a process of parsing first and then not least also a thorough inspection and validation process, but nonetheless..). :) (15:07)

%... (17:10) Okay, I'm thinking: never any hash for text entities, 'u' or 'h,' and always a defStr key for 'f' entties, and always a hash key for 'j' entities.. ..And yeah, keep the isEditable for text entities.. Hm, and for JSON, entities, or no..? ..(Or is it always 0?..) ..Hm, shoudn't it always be 0, and then we can use representations to edit?.. Or..? (17:14) ..Yeah, I think that's the way, actually. Then I just can't edit the.. Oh wait, this ruins my current strategy of using '@[...]'. Hm, but I could set an editability expiration date instead, then.. (17:17) ..Well, unless we \emph{really} want to lean into the rep.. no, we don't.. ..So let me implement an editability expiration date, I think.. (17:18) ..Which can then be used also for the text entities, why not.. (17:20) ..Cool, let me do this.:) (17:20)

%..Hm, I should actually store the hashes in binary form, and not as hexadecimals, but then let's still use hexadecimals for any query proc that uses a hash, though.. (17:24)

%(17:57) Oh, so for JSON entities, we replace the hash when editing.. well, yeah.. ..Yes..

%(19:54) We should actually also make a 'code'/'listings' entity datatype, I think code with 'c' as the symbol, where.. Well, come to think of it, why not just implement this via the HTML <code> tag (or whatever its called), and then just have a script that automatically colors keywords, etc., in this code, depeending on some language attribute. I think I'll do that. And then all the users need is to wrap their code in this tag. Yeah, let's do that.. (at some point, i.e.). And then we can forget about 'c' entities. (19:59) *(We should just use the class attribute to specify the language, perhaps even if another one already exists (a language HTML attribute for the code element, i.e.), but certainly if not.)

%(21:31) Ah, https://www.w3schools.com/mysql/mysql_datatypes.asp actually wrongly (it seems) says that a TEXT stores a maximum of 65,535 bytes, but from the MySQL reference manual, it seems to indeed be up to almost 4 bytes more, depending on the character set.

%Something more important, though: I should put a limit on the 'j' entity defStrings, such that we also don't need to deal with the isContained parameter. And maybe I should even choose something that's below 65,535 bytes (I'm using BINARY now for defStrings).. ..Like, 2000 bytes seems plenty, even with a lot of four-byte characters.. which would also be unusual.. ..Wait, since we might then get well below 3072 bytes, this means that I could us the defStr as the defKey, at least internally.. ..Hm, and if we choose something below 1500 bytes, then we could even query on the hexadecimal defStr directly as well..!.. (21:43) ..So let's do that.. ..Sorry, 1000.. ..And 1000 bytes are also a lot, just to define a thing, and albiet also to make it include data useful for rendering.. ..But this still ought to make 1000 bytes plenty enough.. ..Well, 900 should be the max, then.. ..Unless we want to.. Nah, let's not go beyond hex, right..? ..Oh wait, I don't want any non-uft-8 characters for 'j' (or 'f') entities. And the URL max length is measured in characters.. ..Yeah, okay, so I should.. Oh no, that doesn't help, 'cause uft-8 characters aren't URI-safe. So hex is actually more compact in general.. (21:57) ..Hm, I could say 756 just so that DB implementations are also free to just use VARCHAR types, and not do anything special to make it, oh no, 'cause the tables also need more data.. ..Hm, maybe I should just hash after all.. (22:01) ..(22:13) Wait, AJAX shouldn't put any restrictions on the URL size for GETs, regardless of the browser, so maybe this doesn't even matter at all.. ..Oh, maybe it AJAX does so, after all.. ..(22:22) Hm, I could also just make my DBRequestManager POST instead of GET for too long URLs --- and perhaps even make it dependent on the browser.. ..Hm, that actually seems like the smart solution.. ..I'll probably still put a size limit on 'j' entities (and on 'f' entities, of course), but.. Well, but this means that it could by something like 3000 bytes instead, and we'll then still be able to not use hashes, exactly.. (22:26) ..I could still hex-encode the 'j' entities (since these are not ASCII), but maybe I won't.. ..Yeah, nah, this will most likely make most URLs bigger, rather than smaller, since ASCII characters will probably still be the most common ones. So let's not hex-encde it; let's just URI-encode, and then POST if the URL is longer than the browser allows. (22:30) ..Hm, and let me just always POST when the length is above 2000, and thus not try to check the browser. I think that's pragmatic.. (22:34)

%(02.12.24, 11:02) I should make an entity list of CSS sheets that users can choose to use. This can then open up for user-participation in terms of styling the website, and open up for having personal CSS settings.

%Something to think about: I do need long JSON entities, potentially, for private entities. Ah, but shouldn't I just make them a seperate datatype? Either a JSON data type, or just a general binary datatype..? (11:05) ..Or UTF-8.. (11:06) ..Maybe I'll implement it via UTF-8 --- that also fits the fact that we'd like them to be displayed as the JSON texts themselves, and not as something else. Okay, so UTF-8 ('u') entities for private date such as workspaces?.. ..Sure, at least for some private data, and in the future, we can also potentially use private 'b' (binary) entities as well. (11:09)
%(11:35) Wait, since JSON isn't very readable without whitespace, we don't necessarily want to display JSON data structures as plain UFT-8. So I probably do want a JSON data entity that is not the standard ones. Now, which one of the two to call 'j', then, and which one to call something else..? ..Maybe 'r' for 'regular, and then if it ever becomes not-so-regular, we can just change the meaning to something else, like 'real-world,' or 'oRiginal'.. (11:39) ..Well, yes, but let's just say that it stands for 'regular' \emph{or} 'real-world' from the beginning, i guess. And then 'j' becomes the typeIdent of the possibly longer JSON data entities. (11:41)
%(12:08) We could also call it 'a' for 'attribute-defined' entities, especially since we won't need 'a' for 'ASCII,' since we have UFT-8 texts already, and ASCII is just a subset of those.

%(12:15) Hm, do we even want attribute-defined entities to be able to be private..? ..Maybe not..

%(12:41) (Oh, so while we're using "just" CHAR for type_ident, we actually get up to 1,112,064 possible datatypes. Now, we \emph{can} change it easily, but we won't need to, then.^^)

%(16:24) Considered implementing selectFunctionalEntityAndChildren() to output strings.. Well, I might, but I'm also thinking if I should implement it (more) in PHP instead..? ..Then we can easily combine the output with an entity list.. ..Well, but we can anyway, come to think of it, as long as we just do the combining in PHP at least..

%(22:50) I've realized that we don't want the request to get the entity list right away, since the functional entities can be cached (for virtually forever in principle). Another thing: Let us use '@<ID>' literals for IDs, and perhaps 'f@<ID>' for custom functions, if we ever get to use those. And then '<num>' for numerical literals, and '"<string>"' for string literals. ..Oh, and I plan to add a number just after the function in a function call that tells how many inputs there are. This number is then not included in the actual 'f' entities' defStrings (which also does not allow for nested function calls, at least until we at some point allow this, if we ever do)..

%(03.12.24, 11:34) I guess HTTPs makes caching unimportant, though, perhaps.. ..And about string literals, let's not include these, at least at first, such that we don't need to escape commas. But I could inlude 0x<hex string> strings. ..Yeah, and then we can still have that functions starts with a name.. Ah, and I could also have constants as well, since these are just functions with 0 inputs.

%... (14:55) I have perhaps figured this out before, but if we want to add scripts on top of CSS sheets that users can choose from, we should do it by compiling React modules into JS, in the same way that the React preprocessor does, why JSX is turned into JS, and imports and exports are translated into reading and writting to a global object. And then one can just run such a script after the default one in order to overwrite a module with another version of it. The users can then upload such modules to an entity list, and the developers can then check them, and see if they want to process and serve it. And when they do, a user can make a personal entity list over their modules to load. And the first time these are loaded, the app can also store their URLs in localStorage, such that the app does not need to fetch the script URLs again the next time the user loads the webpage.

%(15:14) Hm, could it make sense to explode list_id in Scores tables, and then only insert the given functional entity when needed..? ..Is this possible..? ..Yeah, so we only create the list entities when we make compound lists from them.. ..Oh, but we also saves some space in each row by having just list_id instead of e.g. user_id and scale_id, even though this isn't as much as if we let the tables be uncompressed.. ..But I'm pretty sure the InnoDB algorithm uncompresses the table first thing before it reads from it, so it does matter in terms of the space it takes up in memory while reading from it (I think).. (15:26) ..Hm, I think I will keep list_id for all Scores tables for now.. ..I kinda want to use the "secondary key" for private scores, though, and also for 'better representations' and such.. ..When they are small lists, and they aren't typically used for compounds.. ..And I probably shouldn't worry about decompression time versus the time to find the given listID.. (15:38) ...(15:51) Hm, would it make sense to make two kinds of functions: ones that are replaced with IDs in other function calls, and ones that are always kept written out with all their input parameters?.. ..Wait, or maybe this should be a quality of a functon's input parameters, i.e. whether each individual parameter has to be an ID, or a written out function call..? (15:54) ..Hm, maybe we have all the ID parameters first, and then seperate with ';'.. Hm.. ..Hm, that \emph{could} potentially work.. (15:55) ..I could also add '[<idParamNum>,<otherParamNum>]' at the end of all functions (e.g. 'op[2,0],...').. (15:59) ..Hm, that also means that we can rid of all the '@'s, by the way.. (16:00) ..Well, it should be 'op[1,1],...' if we also want to use the sec. key for the Scale.. ..And note that this would mean that you \emph{have} to use the 'scale' function call, and that you cannot insert the ID.. well, except that I could reimplement my selectFunctionalEntityAndChildren() to also be able to go the other way, turning IDs of 'f' entities into function calls.. (16:07) ..This seems like a good idea, and then I might just explode list_id in the Scores tables after all.. (16:08)
%..Yeah. But if I also turn IDs into function calls, then my current strategy of returning outSubStr isn't really helpful.. ah, but maybe we can just compute and return the.. Oh, right, with these changes, '[<idParamNum>,<otherParamNum>]' is always part of the defStr, for all function calls, so I should just compute this and return it for the outSubStr. Okay, so I think I will do this.. (16:11)

%..Wait, maybe it should be 'op[0,2]' since the userID should not be looked up as an 'f' entity and have its ID changed to its defStr..(?) (16:17) ..Oh, which is why it should be 'op[1,1]' exactly, never mind.. (16:19) ..But there is something to consider here, at least if the query proc should also fetch function calls and subsitute them in place of IDs.. ..But that just means that we should still use '@', at least for the "verbatim" parameters, as we could call them, such that the proc knows to then look them up and write out their defStr.. (16:23) ..Yes, let's do that, and let's keep using just the numeric IDs for the "ID parameters," since this also aligns with a general thing of letting '@' signify that "the thing is to be replaced with something else".. (16:25) ..(So we'll generally not see any '@'s in the defStrings of the 'f' entities; we only see that for inputs to the selectFunctionalEntityAndChildren() query proc.) (16:27)

%(16:32) Oh, I might not explode for the histogram scores table, or the aggregates.. Hm, but what abut 'better representation,' then?..

%..Maybe I'll use '*funName*<*idParamNum*,*otherParamNum*>' instead (even though that messes up my own notational convention (of using '<...>' for placeholders), as can be seen here)..

%..Hm, I should think some more before I commit to this plan, though, since it \emph{is} also just a lot simpler to just use ID placeholders for all entity inputs, and we can also still make query procs that does it all at once, i.e. finds the IDs first and then return the entity list in the same request. It'll just take some more lookups by those procedures..

%...(16:50) Hm, we could also just allow for dublicate function entities, in principle, and then just let the app (i.e.\ the developers and the community) decide which parameters to write out and which to replace by IDs.. ..And then whether to interpret a number as an ID or somethign else is just dependent on the context, and more specifically on the agreed upon semantics of the function. And then all '@' ID references are always replaced by the defStr of the given 'f' entity by default. (16:54) ..Hm, and for the selectFunctionalEntityAndChildren() query proc, we then just also need a way to say "replace function call by ID".. ..Maybe by just prepending '@' to the function? (16:55) ..Sure, why not.. (16:56) ..Hm, I could also use '&' here, such that it becomes a lot like C, only where '*' is replaced with my '@'.. (17:00) ..Hm, and I can also use '&', then, for the entity constants (which are the 0-ary functions).. ..(If we ever get to use these.)

%..It's woth noting that with this plan, we don't even necessarily need to create a given scale entity before we can submit elements to it, at least if I also explode scale_id in the Scores table.. which I might not do, though.. But still, worth noting.. (17:05)
%..(Yeah, let's not go as far as to explode the scale_id for any of the Scores tables..)

%(17:11) Okay, and for 'better representation,' I should just make a query proc that looks up the listID, probably by using the fully exploded defStr.. ..well, except for the Estimators, which should always be referenced by ID, I think.. such that it only requires one lookup in the DB, before the list query itself. ..Hm, I wonder if I even need my recursive selectFunctionalEntityAndChildren(), then, or if we can always just search for, well, for the defStr directly in order to get a given 'f' entity..? (17:16) ..Well, it might still be useful especially for unioned compounds, so let me still include it. But I should also include a list query proc that gets it from the entList's defKey (i.e. the defStr (since I don't actually use hashes anymore, at least for the time being)). (17:18)

%(17:41) Hm, I've decided to use scale_key VARBINARY(3000) now instead of scale_id for PrivateScores. Should I actually do the same for UserOpinionScores?.. ..Thus exploding scale_id after all.. (17:42) ..Hm, since scaleKeys are typically only something like ~50 bytes long, maybe a bit more *(well, no, not if we convert the IDs to binary strings.. (17:52)), it might atually be more efficient, they way I see it (since it spares the scaleID lookup, while only resulting in a bit more memory usage.. well, more than just a bit, but still..).. (17:45) ..And it spares some storage space as well (more than the extra row bytes take up once compressed).. ..Wait, no, since the same scaleID will often be used by many users here.. ..Hm, keep the scale_id unexploded here?.. (17:49) ..Hm, \emph{if} we explode, then it's better to just make a row for each of the four IDs that make up a Scale.. (17:53)
%..By the way, it does seem a bit overkill with 4 mandatory constituents when we mostly only need to know the Quality.. and maybe also the Estimator.. ..Well, in principle, but this will also often be the same one, just like the Object and the Relation. So yeah, it does seem like a bit of overkill.. ..But we could then make them optional, and then we could exactly use a VARBINARY scale_key instead, such that we can leave out any constituent that just has the defualt value.. (17:57) ..But still, using such a binary scale_key is still much more trouble than jut making one more lookup for the scaleID (where the defStr can also just have optional parameters)..

%..(18:02) Wait, we should let Qualities define their Estimators as well, shoudln't we, so that you get the intervals automatically when you look up popular Qualities?.. Yeah, I think so.. ..And they already include their Domain as well, but we can of course use a different one in some circumstances (in particular for filtered lists).. Wait, so do we even need Object and Relation in the Scale definitions any more?.. ..No, we don't.. ..So we only \emph{do} need the Quality for UserOpinionScores?.. Yeah, I think so.. (18:07) ..Right..

%..(18:10) Oh, and maybe we don't need the Domain (Object + Relation) at all for Qualities, other than potentially as some way to aid in defining the semantics.. ..For we're going to implement filtering via single-entity score lookups anyway.. ..Hm, so goodbye to Scales and Domains (except as part of the semantic definition)?.. (And also hello to always having a Quality define which Estimator it uses itself?..) ..I think so.. (18:14)

%(18:50) I guess I need to implement Qualities as 'f' entities, then.. ..(so that we can parse min, max, and step..) ..Hm, but this is not that simple, so I need to think.. (18:53)
%(19:00) Oh, we could also let min, max, and step be a quality of the entList, defined by some parameters in an entList function call, such that a scores outside of that interval is just filtered out there, and the scores are binned according to the chosen step (but where different users / user groups can choose different intervals and "steps" (bin widths, i.e.) for the same Quality). Okay, I think this might be the answer. And then the Qualities, which can then be 'a' entities, can still define a default interval, potentially. (19:04) ..And for some subclasses of Qualities we might not even \emph{have} an interval, but the rating bar could instead just be removed, leaving only the input field where the user can type in their score.. (19:05) ..Yeah, and it will of course just be the 'hist' function that defines mix, max, and step/bin width. ..Great.:) (19:07)

%(04.12.24, 11:00) I will make a subclass of Context-dependent qualities, which are a virtual kind of placeholder qualities with undefined semantics. An example could be: 'Has a good performance in *Movie*.' The point is then that the app always pick the Object in question from the context and then insert it into the Quality. Oh, so we do also need sec. keys for 'a' entities?.. ..Yeah, we do.. But then the sec. key, which is also just the defStr in this case, and not its hash, just has to be edited along with the entity.. (11:04) ..Yeah, okay. This is also nice for helping to prevent some dublicates. So yeah..

%..And I'm just going to render Relevant (and useful) Qualities as '*Object* -> *Relation*'. And filtered List entities can be rendered as.. ..Oh, I also need the User (group) for those.. ..Hm, should we make "virtual," context-dependent List entities as well?.. (11:10) ..Namely with user group placeholders.. ..(11:17) Hm, it would be nice if we could leave the user group(s) as the very last decision, and then just construct 'compound qualities' until then.. ..Yeah, so I guess qualities should introduce variables to.. nah, but maybe we could call them Estimators, or something like that, intead.. ..Well, or maybe yes, maybe we could have compound qualites that also introduces user group varaibles along the way in their construction.. ..Hm, and this could actually just come from them having an attribute with values such as 'factual,' or chemistry-related, or programming-related, and so on.. Hm, but this is of course a lot, so ideally this would have to be "semantic attributes" instead.. (11:23) ..(11:29) Well, this ought to make it simpler in some sense, 'cause this then just means that the app looks at the.. Well, okay, no, it's not simple. The app looks first at the semantic quality type, and take the topmost quality type on the list.. ..And then it chooses a User group from that.. Hm, it gets a bit complicated, but I guess that this would be the ideal way.. (11:32) ..(11:36) Hm, we could perhaps also skip a step and then just uprate User groups for each individual Quality, letting the reasons for why we choose a given user group be underlying. I think that's more pragmatic, actually..
%..So the app constructs an List entity from a given Quality through this process, and then queries for the entity list via this List entity.. (11:39) ..Okay, I think I will actually let this be the plan. And of course, if no User group is uprated above a certain threshold, then a default User group for the given user is just used instead, naturally.. (11:42)

%(11:59) Hm, if we have to delete sec. keys, then I should add a sec. index for EntitySecKeys on ent_id.. *Oh, we have the Entities table for this, never mind.x) (12:17)

%(12:07) No, we shouldn't skip that step that I talked about. We should first look at the quality type, and then query for the best User group for that type (although this will be repeated often, so this will often be cached).. ..Hm, and we can't just use a "defining" (JSON) attribute instead?.. Maybe that's not the best, but let me consider it.. ..Well, or a quality subclass.. (12:10) ..Oh, but in principle it doesn't need to require more queries to get the quality type/subclass from (the top of) a list, rather than getting it from the JSON defStr.. (12:13) ..So let's get it "semantically," indeed.. ..More precisely: let's get it from the list..

%(12:23) Hm, it could happen that someone wants to edit an entity from one type into another, but that complicates my insert procs quite a bit, unless I can find a good strategy.. Well, as I'm saying this, there's always the strategy of using 'better representations'.. ..So could I just not implement editing from one type to another, at least for now?.. (12:25) ...Sure..

%(12:57) Hm, I just considered some 'ON DUPLICATE KEY UPDATE ...' hack, instead of my current 'INSERT IGNORE INTO EntitySecKeys ... SET outID = LAST_INSERT_ID();' solution, which won't work. But it just dawned on my the 'SELECT ... FOR UPDATE' don't block reads in the meantime to the state before the given transaction. So all my former double queries \emph{are} unnecessary (luckily; otherwise it would have been a mess..). Okay..

%(15:03) I'm looking at entity query procs now. I guess I might need to either always hex the outgoing defStr, or I should output an extra variable representing the outgoing format.. ..Hm, maybe hexing is the way.. ..But maybe I should rather see about implementing.. well, maybe giving the Entities table both an utf8mb4 column \emph{and} a BINARY defstr column as well, maybe where exactly one of them has to be NULL..? (15:08) ..Yeah, that's a better solution for sure.. (15:09) ..So a textDefStr and a binDefStr column, with a CHECK that ensures that 1 and only 1 is NOT NULL..? ..Well, I need checks anyway for each given datatype when inserting, so I can just do it there. (And then I can also prevent us from confusing NULL with "NULL".. well, unless MySQLi interprets "NULL" as null.. ..Hm, I bet it does..) ..Hm, I could also just make both column NOT NULL.. ..Surely InnoDB stored short TEXTs and BLOBs, including "", inline..? ..(15:22) It does (using the default COMPACT row format, or beyond).. ..(And maybe also for REDUNDANT, but this isn't important..) *(Oh, it is important, since the COMPRESSED row format inherits from the DYNAMIC one (but DYNAMIC also stores small TEXTs and BLOBs inline).)
%(15:32) Hm, and I should only return one defStr, still, for the query procs, such that my implementation of using two columns is hidden to the app, and preferably even to the control server.
%(16:13) Hm, or should I just serve the two column.. Hm, I guess I need to hex the bin_def_str, 'cause I don't think MySQLi can handle anything else; the default is probably returning "0x..." strings anyway.. ..Oh, so I can just return one column, then..

%(17:11) Okay, I think I can do some more programming tasks for today if they are simple. But I should also take some time while I have some sharpness left to contemplate the Qualities some more. I think I basically need to implment all my entList functions mentioned above as quality compound functions as well now.. ..Oh, so a lot of Qualities will then actually be 'f' entities..(?) ..Yeah, they will. So only the atomic ones will be 'a' entities.. ..Hm, will it be problematic to determine the User group for compound entity lists consisting of compounds themselves.. well, maybe not, since the user group should also come right above the atom, meaning that all atomic Qualities are wrapped in User group declarations, for the List entities, and no User group is introduced beyond that, right?.. (17:22) ..Yeah.. And this is good, since it means that getting the List entity from a compound Quality, or any Quality for that matter.. well, more on the context-dependent ones in a second.. will just entail wrapping each atomic Quality in a User group declaration function. Except for the context-dependent Qualities, where we also need to get the Object (and/or Relation) from the contexst, but this is done before the Quality \to List entity process, yielding an atomic Quality derived from the context-dependent one first.. So yeah, going from Qualities to List entities should not be too hard, i.e. once we implement that quality type lookup, and subsequent User group lookup.. (17:28) ..Well, there's also some automatic determining of cutoffs, I think, unless we.. no, we don't want that to be part of the Quality. So yeah, things such as the relevancy cutoff should also be generated in this process, and also be potentially looking at user preferences.. And I guess histogram parameters should also be generated automatically, but let me think about how.. (17:30)
%(17:34) Maybe there are defaults, and also Quality functions that can break out of the defaults. But let me also just remind myself that atomic qualities should actually be separated from compound qualities / compound list entities, where the latter is only used for searching..
%(17:50) Hm, I think I ought to then not make any compound qualities, but make what we either might call something like 'variable list entities' or call something else entirely, like.. ..I don't know.. But maybe something like 'variable list entities' isn't a bad idea.. (17:52) ..And then we don't need to use them at first, but at some point, we can start using the "variable"/"user-dependent" list entities, exactly in the same way that we might at some point use the "context-dependent qualities" in place of normal qualities, also where the app then generates the actual quality or list entity, respectively, depending on the context and/or the user.. (17:55) ..Yeah, I think this might be a better solution, rather than the one I had with using "quality types" and all that. 'Cause with these user-dependent List entities, we also get the same result, both in terms of users being able to categorize Qualities (namely by chosing the right user group variable for them), and for users to then choose their own User group for each User group variable.. (17:58)

%(19:53) 'List templates' and 'Quality templates'..

%(22:15) It's okay that some semantics is definied "semantically" (not very fitting term in this case), i.e. by uprating the right definition. I's not much different from uprating a 'better representation' with a expanded/changed definition. And it's totally fine, so we can let the semantics of the various functions for the 'f' entities, which all defines a subclass of the 'f' (Functional) entities, be defined by up-rated Descriptions..

%(05.12.24, 10:29) Hm, and we can let the functions, such as 'hist' or 'op,' without any parameters, alone in the defStr, be the Function entities themselves. These are then the entities for which we can uprate the Descriptions that define these functions..

%(11:54) Oh, MySQLi must be able to handle binary data (BLOBs)..

%(13:09) Wait, why on earth do I want binary, non-UFT-8 entities anyway?? Shouldn't we instead just use URIs?.. Well, let me think.. ..Yeah, we should absolutely just use URLs instead. Nice, this makes it a lot less complicated..

%(14:30) Oh, I should mention: I now plan not to use 'Elaborate label,' but to fetch the first part of the Description instead, and then just let the header (<h1> or <h*n*>) of the description constitute the elaborate title/label. And I will then do those for all EntityReferences, unless supressed, i.e. fetch the first part of the Description on mouseover, and then show it on mouseover when fetched (after ending any not-yet-finished HTML elements (by adding their end tags prematurely)).

%... (15:32) The functional entity subclasses should be defined using a format. I might even implement this via a function as well, taking a function name as an input and returning a Class, which is then the given subclass of 'f' entities. And this function could then also be taken on its own name.. ..Hm, this might actually make good sense to do.. (15:35)

%(15:58) I should consider opening up the User group trees to the frontend after all, making each node a List entity. I'm then considering making it possible to upload requests that waits for other requests to be fulfilled before the activate, meaning that when they do so, the request will initiate and/or decrease some countdown, not that the request fill be fulfilled when 'activated.'
%Now I'm also thinking something else: If we implement a seperate table for each type of entity list, then we don't need as much parsing (of the functional entity defStrings) in the backend.. (16:02) ..For then it technically doesn't matter if an 'f' entity appears in the wrong Scores table..

%..If I haven't talked about it before, I should also make a entList function to cut off Scores w.r.t. the positive scores count.. ...Or by the combined "positive" weight, for that matter.. ..Oh, and more importantly just by weight, not just "positive" (nor "negative")..

%(16:22) Hm, parsing should be easy enough, since we always use entIDs for lists, user groups, and qualities. So I can't even think if any functions where we don't want to replace all nested function calls with their IDs instead. So no need to make a lot of tables, which is also something that I don't want, actually.. (16:24)

%..And yeah, I think I should try to open up the User group structures.. Well, except that then requires some decision making, I guess.. ..Oh, and it requires too long functional entities.. Well, unless we use an indexing syntax, namely where we automatically pick out a sublist of a user list.. (16:27) ..Hm, maybe I should just hide it..(?) (16:28) ..Well, I could also make a 'tree' function, perhaps with inputs as to how large the nodes are (in terms of the number of children).. (16:29) ..Yeah, that could work, and then of course with an indexing syntax as well, probably similarly to how SUBTRING_INDEX() in MySQL does it.. ..Yeah, an let's just index each node by a single unique number, instead making a tree-structured index as well. I can instead just make it computable where to look in a tree, similarly to how B trees work.. (16:33) ..I like this.. (16:34) ..It's especially nice that you can then use the same tree for several different cutoffs for the given User list. So a tree can besically serve several different "User groups" at once, namely when they only differ by the cutoff. This however means that the outer part of the tree needs to be special, such that the top part is combined with the next part, then that combination is combine with the next part still, and so on, instead of creating the tree in the most natural way.. (16:37) ..But this is all okay, we can work with such funky trees. And it only needs to be so for the outer structure of the tree: For each of the "parts" that I just mentioned, the tree should just be constructed in the natural way, i.e. by dividing the interval smaller intervals of eaqual length and then.. well, then them all as the children of the one node. And for the outer structure, we also divide into equal-length.. well, "length" either in terms of element count, \emph{or} by weight. But that can be part of the paramters to the 'tree' function, which of these two it is. But for the outer structure, to continue the thought, we then only combine two parts/intervals at a time, and in a skew way such that the combined top is always combined with just one more part/interval at a time, in order to create the full tree. (16:43) *(20:45) I'm thinking that this User group tree function should have a node size parameter, and a depth, meaning that the outer trees will consist of (node size)^{depth} users. And yes, let us measure the node sizes in user count, not in weight. And then a third parameter as well for how many of these trees to include in the outer tree. So the full tree will have (node size)^{depth} \times this third length parameter. ..Hm, and when the User group grows, do the users then change to a bigger List entity, or what..? (20:49) ..Well, yes and no: They just increase the length.. Well, \emph{or} switch the List for another one with a larger depth if this is better. ..But of course, the app should generally do this automatically, shouldn't it..? Hm, generating it from the Compound qualities.. Hm.. (20:52) ..Ah, maybe we can choose a weight cutoff rather than a depth, and then the depth is instead just set automatically, namely to be just large enough to fit all the users with weights above the threshold, when all the leaf nodes (and the linking nodes) are limited by the node size parameter.. (20:54) ..I like this better; this is a lot easier; then the app just needs to determine the node size, and the users can do the rest. (20:55)

%(20:41) The idea about using 'List entity templates' doesn't work for getting the right scores when viewing a list of Qualities, or viewing the Qualities of an entity element, such as a Comment. So I have to instead implement the thing about uprating User group variables to Qualities, or rather Quality types, or whatever we want to call them, and then look up the user's preferred User group to query for the given Quality type..

%(20:56) Okay, so I'm back to the idea of letting the Compound qualities be made from functions that corresponds to the List functions, but where the User group is omitted for the Compound qualities, and also even the Quality type, as both these things are instead queried for by the app, first the type, then the prefered User group, accounting for the user's preferences. (20:58)

%(21:32) The app should actually determine most parameters in the List entities used for querying, bt also with a principle of querying the user's preferences (in a similar way as for the 'Quality type \to User group' query) where this makes sense. And then when it comes to entity lists, the User should also be able to adjust some settings, such as the weight sum cutoff for the elements (which is a function that is taken after the User group is used, namely since the latter is used to generate the histogram data, which is then used to get the final list (in the way specified by the structure of the compound quality)) in particular.. (21:37) ..But things like using the median over the mean, that should just be standard for the beta version of the app, and I don't even think I will present the option of using the mean to the users (for the early stage, i.e.).. (21:39)

%(06.12.24, 10:50) I'm gonna drop the function names and instead make all functions entities instead. And what I'll do is actually to include the Function entities as part of the 'functional' entties. The Function entities will just start with a '(', namely since the function definition syntax will be '(param1, ...)=>{...}', where the curly brackets is a JSON object, and one that uses a '%<param>' syntax for the placeholders where the parameters. (So a very similar syntax to the arrow functions in JS, only without the parenthesis around the object, and also without any whitespace (except inside strings, of course).) (10:54)
%..This means that the backend needs to reqognize these functions, but for this I will just make a new table, created and populated in a separate SQL script, where the functions that are in use are connected to a name, such that you can quickly get the ID from the name. give thi table a reverse index as well, such that the app doesn't need to know the names of the functions that the backend uses.
%I think I will also replace ',' in the function calls with '\t' instead, such that commas can be freely used in string parameters. Not that I plan to use such myself, but it's nice to keep the system open for that, if perhaps users want to start using functions for more efficient entity definitions. And it's perfectly okay to not allow '\t' inside the JSON(/attribute)-defined entity defStrings, as well as '\n' for that matter, but let's just use '\t' for the function call delimiters.
%Hm, this get's me thinking: Since we have the key parameters.. Oh, let me first say this: For the Info page, under the Attributes section, we should just divide this into two subsections for 'f' entities, namely a 'Key parameters' section, where the chosen parameter names are shown, each followed by a ':', and then the input entity from the given function call. And in the next subsection, which could just have the header 'In,' we see the attributes with the placeholders kept inside, not substituted. That's for function call entities, for Function entities.. ..Maybe we can do almost the same, but instead of listing the 'Key parameters,' we just list the 'Placeholders' instead..
%Okay, but this got me thinking, since we typically only need the key attributes for rendering, wouldn't it actually make sense to remove the redundency and just use 'f' entities solely instead of the.. Well, yes and no; only if I then change the 'f' entities to allow for editing. Hm, so maybe I'll just keep them seperate.. I think I might.. (11:09) ..Or I could try to implement a way to make cyclically defined functions, but.. (11:10) ..But no, I like my 'a' entities, so I'm just gonna keep it like this, despite the fact.. Wait, can't you make.. ..Hm, maybe you \emph{can} make e.g. the Class entity with itself as it's Class (attribute-wise) with functional entities, without editing.. Hm, I kinda hope to keep the 'a' entities, but this is worth thinking about, however.. (11:17) ..Hm, you could just implement a 'this' parameter, which the.. Well, which the backend \emph{could} replace right after insertion, but which we might as well just keep like that, why not.. ..Okay, but what about e.g. the Class and the Entities entities depending on each other..? ..Well, it doesn't atter, actually, 'cause I like the ability to edit afterwards too much.. Oh, and it's also needed for text.. Well, these are not 'f' entities, but anway.. ..Well.. (11:23) ..(11:27) Oh, using functions instead of my current 'Special attributes' system would actually be easier to implement. Especially if we allow the Functions to have defualt values when the input is missing. And we could e.g. just use the JS-like syntax of '(<param1>=<defualt value>, ...)' for that.. Okay, I'm very intrigued now.. ..Yeah, so we should replace 'Special attributes' with a generating function instead, regardless of whether I keep the 'a' entities or not, actually.. ..Ah, but then I'm probably still leaning towards keeping them.. ..Well, but almost all entities will be functional ones, then, outside of the text and data entities.. ..Oh, could we perhaps make editable and non-editable functions, or rather functions whose outputs are either editable or not, depending on some attribute or subclass of the function..? ..Oh wait, maybe we don't want to overuse functions.. Well, as long as we can just get the class of the.. Hm.. ..Hm, I might keep the things the way they are, except change the 'Special attributes' to a generating function, but not actually use that function to create 'f' entities when submitting a mamber, but instead construct the full 'a' entity from the function definition instead.. (11:40)
%..And for optional attributes with no default, like 'Description,' we can just use a '?' syntax for these.. ..Oh, and the inputs are typed, so let's use ':' for that..

%..(11:49) Hm, it does seem like a step backwards in some weird way *(since it's very similar to what I had before..), but I think I might actually drop the 'a' entities, and just use 'f' instead.. And then I will "just" make the app fetch the function definition and insert in the JSON oject, then JSON.parse() the result, and \emph{then} render the EntityReference, if that's what we're talking about, or the Info page.. Hm, but do I really want this..? (11:51)
%..(We can still make a 'Functions' class for all the functions, by the way, and then just make the app know its ID, and also uprate it "sematically" as the class of all the 'f' entities..)
%..Well, but if we go to all that trouble anyway, about always inserting the placeholders, then we \emph{can} also easily include the 'a' entities, and we might as well. In fact, maybe.. Oh, we could just my idea just then of making it inherent to the function, whether its call produce 'f' or 'a' entities.. Hm.. (11:58) ..And also give the app a say in the matter.. (11:58) ..Hm, and then I \emph{could} just make the 'a' entities the standard, at least at first, and then only use 'f' entities for the common derived ones, i.e. for list and quality compounds mostly. And then instead of doing all that insertion and parsing, I could just.. Well, I need to implement it regardless for the insertions. But still, I could just render the function call entities in a special way.. Oh, or better yet: I first switch-case the function, and render it right away if it's a known one, or else I do the insertion and parsing, and then render the EntityReference the 'a' way. Okay, I think I'm starting to land on a conclusion here.. (12:04)
%..Okay, I think I will do this..

%I also think that I might split 'f' up into 'f' and 'c', where 'c' stands for 'function Call' (or 'compound').. (12:10)

%..Oh, and with 'better representations,' we can always change 'a' entities into their corresponding 'c' versions (without it costing any additional lookups, since we're just fetching the 'better representations' first). Great, I'm confident that I will go this route, then.:) (12:13)

%(18:34) Hm. I could also make a 't' (or 'd') datatype which are 'datatype entities,' and which are inserted not by users, but by developers, and which then only includes 'f', 'c', 'a', 'u', 'h', and 'j', and then also 't'/'d' to begin with. ..Yeah, this sounds like a good idea.. Hm, but maybe the app should never need to query for these, and therefore we also don't.. Oh, never mind, the query procs are combined for all datatypes. Okay.. (18:38) ..And let's call it 't', btw..

%(18:55) Oh, it's a problem for the function/format entities that I can't edit them, especially for the 'Member formats'.. So I need to think of a solution.. ..Oh, it's easy: They should just be editable, and so should.. perhaps.. the function calls.. Hm.. ..I mean, for a little while. I could just limit the daysLeftOfEditing input parameter for the relevant procs.. ..Sure.. ..Good.. (18:58)

%(07.12.24, 15:40) Let's escape format placeholders ('%<n>') by '%%<n>', and thus not change any substrings of the form /%+([^0-9]|$)/. ..(So we only change substrings of the form /%(%%)*([1-9][0-9]*)/.)

%(15:46) Did I figure out what to now call the "Scalar parameters"?.. ..Oh, (Semantic) Parameters, just.. ..Or 'Qualitative parameters,' which is more straightforward now.. ..Yeah, 'Qualitative parameters,' which can also be abbreviated to just 'Parameters,' as long as we just keep the capital P..

%(15:57) Oh, I should allow for function calls inside attribute values as well..!..

%(08.12.24, 8:55) Slept well and woke early, compared to how it's been in the recent time (can't remember the last time I was able to get up before 9.. *(today I got up around quarter past 8, btw)).:)

%I'm thinking about about User groups and trust, and anonymous profiles. I think the thing of making it standard of not saving the e-mails, I should throw that away. People are too reliant on their e-mails for accounts now (which is a good thing). I should btw perhaps also make a 'user' datatype. ..Hm, which letter.. wait, maybe I don't need it. Maybe it can just be 'a'.. ..entities with the entity itself as its creator_id..? ..Anyway, back to the profiles: Users should then be able to rate trust and what not, and also particular whether the user profile is true, if the user claims to be some specific person IRL. It is then a good idea for most users to have a public profile, even if they.. Oh.. Could it make sense to have semi-private entities, such that you can have a friend and/or follower network, where only users uprated on some list owned by the user can see their entities. I think it does. Then you can basically implement Facebook, which is a good thing on its own, but also quite important since it means that people can then much easier verify each other's public profiles, whithout requiring a lot of direct messages/contact to do so (just like how on Facebook, it is easy to trust that the people are who they are, without any special effort needed). Okay, I should do that, and that's important since it requires me to make changes to the database, adding some 'is_semiprivate' field.. (9:07) ..Well, the field should contain the User group (a user List entity) ID.. ..Hm, and then we should also make a semi-private scores table. Hm, maybe we can join the two, and just make it so that when the User group ID is just the user ID, then that's the only user that can see the list, or get the scores. And we can also join is_private with user_group_id, potentially, if we want..
%..These local User groups also potentially provides us with good way of limiting spam in a distributed way.. Oh well, no, because we should also still use anonymos profiles to a great extent.. ..Well, but these local groups don't have to be of just the public profiles. ..So yeah, it could be useful for that as well.. (9:17)
%..Let's include the cutoff directly in the Entities table columns, such that we don't have to parse it from the List entity. ..Yeah, adding a nullable FLOAT to the table.. ..And replacing is_private with a user_group_id, which can be 0, which then in these case represents the full set of users.. (9:23)

%..For a lot of users, it would then make sense to choose an extended network of people who can see their.. well, yes or no, maybe not for so many.. Hm, but this might entail some heavy filtering in the local feeds, since you have to sort out all the entities you can't see.. Hm.. ..(9:29) Hm, one could ask the DB to do the filtering.. ..Yeah, take the n top posts of this feed and give me only all I can see.. ..But oh well, maybe it's also fine to just serve the full list, and then just not append any entity elements that the user can't see. Then the elements of the list just loads a little slower, but that should be fine. Okay, but how to users find a fitting larger group for them, how do they decide as a group, especially when there's no clear edges of that group?.. (9:33) ..It has to be generated automatically somehow..

%...(10:01) Hm, we could also make it possible for a user to request that their private scores are included in some larger user group aggregate, where the information of where that score comes from is lost.. This would essentially make private profiles redundant, except for the fact that the information that connect the score to the public profile is still stored on the hard disk of the DB, and can in principle be hacked.. (10:04) ..But on the other hand, you also allow other databases to store payment information and such, so the risk is not comparatively very great, I think.. ..So yeah, a lot of people might then not feel the need to use more than just their one profile. And then for people who want more security and/or has less rust in the DB's security, they can take part in the procedure that I came up with some years ago (~2½, I think..). I btw also kinda plan to go over that idea again this pre-noon..

%..But imagine if we can get private profiles, with they weight and trust of.. Oh, and the data needed for the aggregate could even only be stored temporarily.. Hm, then.. ..Hm, let me think.. ..But just to continue on the first thought: It would be really nice if we can get all that, but without the whole complicated procedure to divide the keys amoung the private profiles. So these thoughts are really important.. (10:14)
%..(10:16) Wait, couldn't we encrypt the private scores?..! ..And then only decrypt then for the DB as part of a request to inculde them in an aggregate..! ..Now we're talking..!.. (10:17) ..Yeah, this is awesome..!.. (10:18)

%(10:38) Okay, I think it's best if users can submit encypted scores to a private version of a User group's entity list.. Hm.. ..Several things to consider, several options.. ..But yeah, we want to gather.. Nah, maybe not.. ..Well, we want to gather the requests, yes.. And then the database can query the user inside a time frame, if the user logs on in that time, to send the decryption key, which is then only held for a brief moment, until the score is added onto the aggregate list (which is constructed one decrypted score at a time), and the encrypted score is then ticked off. ..Okay, but where and how do we store these private scores.. Well, on an encrypted table (which can just be implemented via an encrypted JSON entity, potentially), which is also decrypted in the process. Hm, and we can make sure.. Well, first of all, it shouldn't be \emph{one} encrypted JSON entity, it should be many, and the user also points to which entity to decrypt to get the relevant private score.. scores.. ..Yeah, the user can point to several encrypted entities with scores on them for the Quality in question, and then the database can aggregate them all at once. This process is btw started by the app requesting for the user, if there is any private scores to be aggregated, and the.. Well, no, this is complicated.. ..Ah! Only the uer that has updated a score needs to send their decryption keys, in principle. Then they should just verify their old private score, and their new one. And then the database can just remove the old and insert the new value for the histogram. I guess the same goes for when a users weight is changed, but then there's the trouble of when users' weights are decreased.. But let me think some more.. (10:52) ..(11:01) Ah, we should generally not update private scores one user at a time when it comes to changing weights, since this might be tracable back to the user, then. So for changing user groups, I guess we need to do it all from the beginning. ..Hm..

%..Hm, couldn't we also just make private profiles with keys to.. Well, essentially where the private profile verifies that they belong to a public (or any other one) temporarily, in order to inherit a weight?.. That sounds better.. ..Oh, and all you need to do is then to make sure to require that the private profiles updates/confirms their weights with respect to the user groups that they are part of at not-too-infrequent intervals, and then you just store that weight for each private profile--user group pair.. (11:09) ..Oh, and then this also allows us to do ML for private profiles. Okay, this is absolutely the way..!.. (11:10) ..And you of course never store the precise weight for the private profiles. You potentially add a little random value to it first, and then you also potentially round it up/down as well.. Yeah, why not do both.. Okay, I think this is the way.. ..Well, except for the fact.. Ah! The public profile can just store an encrypted entity with the passwords to its private profiles! And then we don't need the e-mail addresses for the private profiles. Great.. (11:13) ..Yes, absolutely great, and then my mentioned (circle) procedure from 2022 is only potentially for the future, and only for the users that don't trust the DB whatsoever.. ..Which maybe will be almost no one, 'cause why would.. Well, could be some.. ..But probably only so few that they can just implement the circle procedure themselves, then.. And whatever the case, it's only a future thing, it won't at all be a thing for the early stages of the app, nor possible before well after the take off.. ..Okay, this is really great news. I guess I've been thinking too much about a distributed database, oh yes, and with 'open data,' in the past, which then meant that I had to be extra creative. But now that the DB can have some private data, that it won't share even in a potential future where we get a distributed database (then the DB will just only share \emph{some} of all its data) *[Well, the DB can still in principle share almost all data that it stores, still, whithout outing the anonymous profiles, so I guess I just wanted to think of.. Oh right, I did work with the idea of the users not necessarily trusting the DB at all back then..], then it's great news for me realizing that we can actually get out anonymous profiles that can inherit weights quite easily..(!):) (11:23)

%..Hm, and let us distinguish "anonymous" from "public" profiles, and only request the e-mail addresses from the public ones (but not even \emph{require} them).. (11:25)

%...Okay, but back to the public profiles and to semiprivate Lists.. (11:36) ..Or maybe I want to do some programming first.. ..Yeah, possibly.. ..Hm, then again, I have been on a bit of a streak.. (11:38) ..So let me go over the thought again for a bit, and then see..

%...Let me just also mention that this of course not too important for the beta version; at first the network will mainly consist of engaged users, and there also won't be nearly enough for a (Facebook-like) FOAF network anyway..
%..Let me program for a bit.. (11:54)

%(11:57) Hm, maybe I should make 'c' entities the standard rather than 'a' entities, 'cause it means that we can be more generous with giving the subclasses constant attributes, which the app can then use without special implementation for it.. Yes, now that I say (write) it, I think I should do this. I'll keep the classes themselves as 'a' entities, though..:) ..Hm, and let me then just make it standard to only print out one attribute, either 'Username,' 'Name,' 'Label,' or 'Tilte,' in that order *(choosing the first match on this list), as well as any other standard "reference title attributes" that we add along the way. And then if the users make classes that needs special rendering, we'll take it from there and implement that specially. Note that this is similar to what I do in CSS now, but here I'm also talking about only inserting the attribute value in the JS rendering procedure (which I'm currently just letting print out all attrubte, both their names and their values). (12:05) ..But when we are only printing out the important one, then we can indeed be generous with the number of attributes that the 'c' entties have. (12:05)
%*(12:13) Ah, and we can also allow ourself to use key--value objects instead of arrays more in the formats, for instance with the case of '"Domain": {"Object": "%1", "Relation": "%2"}'.

%(12:07) Hm, I think it's pragmatic to make a user ('u') datatype, and then let's change the UTF-8 type identifier to '8' instead..

%... (14:50) Hm, shouldn't Domains either be '{<Object>, <Relation>}' \emph{or} '<Class>'?.. ..So let's introduce conjunction types, using '|'?.. ..Sure.. ..Oh, and I could introduce syntax to name the various options in the conjusction, by appending ':<Name>' to the all types inside the conjunction.. ..Oh ait, I mean 'disjunction,' of course.. ..Hm, make that '::<Name>'..
%..Hm, I kinda want to switch to '@{<path>}' then for the temp. placeholders.. ..Nah, let me just keep it..

%('c' can also be interpreted as 'compressed,' alternatively..) (15:11)

%(15:14) Okay, let me continue thinking about local user groups and such..
%..Ah, we could utilize lists where entities shared semi-privately with a user can be uprated, either by a bot or by the given.. super- User group.. ..Well, maybe that's not a good solution.. (15:16) ..Well, maybe it is.. ..Oh, and yeah, it can be a "bot," except that's not what happens anymore, or rather that's not the interpretation anymore. We simply have some special type of Lists with a special set of rules for how the DB can update it (on demand).. (15:19) ..So yeah, we can have a List type of 'incoming posts'..

%..(15:22) Hm, I could make an 'index' datatype as well, potentially. These could then potentially be used for encrypted private entity lists.. ..I like that..! ..One good ue for such encrypted indexes could then be an index over.. Well, that's a bit heavy, though, but I was thinking an index over all the things the user has seen recently, namely in order to better the feed for the particular user. But I'm not sure, and maybe a list temporally ordered list would be better here.. ..But that's of course just an index with times as the keys.. ..Hm, it could actually work, potentially.. (15:26) ..But then again, we could also just use the PrivateScores table, and then the user could just switch to an anonymous profile when wanting to view things inkognito.. (15:29) ..And they should also get a page, much like the history tab in a browser, where they can remove the 'have seen' scores.. Okay, that's a better solution, I think.. ..Well, for the beta version at least, and for the early stage. Then one can always implement encrypted indexes-via-entities later on.. ..And I can postpone adding the 'index' ('i') entity datatype.. ..(I'm thinking B-tree indexes, btw..) (15:32)

%..Hm, something completely different: Let us call the "User qualites," 'User attributes' or 'User descriptors' instead, and implment them as text statements, perhaps where an italicized I appear in the text to reference the scoring user.. ..Hm, and the I's don't even need to be italicized, as long as it is visible that the statements is a 'User descriptor'.. ../'User predicate'.. (15:37)

%..Oh, we can also implement Direct messages---and group messages!---in the same way. ..Yeah, let's divide it into Posts and Messages, but these can then still be implemented in essentially the exact same way.. (15:40)

%(16:10) (Ah, I probably just read it wrong with that 'editableUntil = NULL' bug.. ..Yeah.)

%(16:13) Okay, but the question remains: Should we, at some point early on, make use of the local user groups..? ..Hm, I'm thinking no.. Oh, how about adding things that one's followed users like to one's feed, btw?.. ..Well, it's the same. I should note, however, that we should also use a list of 'outgoing posts/messages/liked entities' for each user, from which the lists of 'incoming ...' can be generated. And there we are with that.. (16:18)

%(16:20) Now, these FOAF/social networks will serve in making the large User groups, no matter what, since it provides a good way to distribute trust/weights along the edges of this network. But the question is: Do we want to make subgroups out of these "local groups," or should we just go forward with my 'User group tree' function from above.. Hm, I think we probably should, at least for the early stage.. But I should think some more about it.. (16:23) ..Yeah, no, local user groups are just created by the users themselves if they want, just like how user groups are created on Facebook. And then it's simply up to the larger User groups whether they want to use these smaller groups in any way to help with their weight distribution, or if they will only use the FOAF graph for this (looking at some specific Qualities like 'trust' (if a public profil is authentic), 'esteem,' and 'user--user agreement'). (16:27)

%Okay, really great to finally conclude on that question. But another one popped up when I was writing this, from last night in bed before.. ..falling asleep (how could I blank on that.x)), which was.. ..Hm, I let it slip while writing this, so let's see.. ..Hm, what \emph{was} it even?..x)x).. ..Oh, it was about implementing the modertor groups and weight distribution in a more formal and automatic manner.. Let's see.. (16:31) ..Let me think a little..

%(16:46) Oh, there \emph{is} a very good way to use local user groups: Just give each user group an overall weight, or a weight factor, and then distribute the weight onto each user in accordance with that, and then join these User groups by taking the greatest score whenever a user appears in several User groups that are being joined!.. ..How great is that..!.. (16:49)
%..I mean, the procedure of looking at the entire FOAF/social network and then go along the connection edges is perhaps advantages in some cases, and perhaps even in most, who knows?. But this procedure could also \emph{definitely} come in handy..!.. (16:51)

%..Hm, the 'moderators' of a group would then in this case be the ones that distribute the weights to the smaller groups, or in the going-along-the-edges-of-the-entire-FOAF-network procedure, they would be the ones to limit or enhance certain edges, or something to that effect.. (16:54) ..(By 'edges' I of course mean the graph edges, not the "outer edges"/the fringes of the network..)

%(17:02) Hm, so I guess I/we should at some point implement these algorithms as well, one for joining smaller User groups, which is pretty straightforward, algorithmically, and then also an algorithm to propagate user weight along FOAF edges.. ..Ah, where the moderators can set the initial.. ..what's the opposite of sinks?.. ..Set the initial 'sources'.. (17:06) ..And then they can also set sinks/drain/whatever-to-call-them on either edges or specific users, that dampens the flux to that node.. (17:07) ..Yeah.. This is a more complex algorithm, but it's not hard either, at least not to plan out overall.. (17:08) ..And I kinda need something similar for the representive voting system.. only with a lot less sources, potentially, but a lot more iterations on the other hand. But yeah, that's it, at least if I decide to implement the FOAF algorithm in a way where the whole thing is calculated in rounds, rather than in a process of following each edge at a time.. Ah, but I think it's better to implement it via rounds, i.e. in the same way that I have described above about my voting system (minus the last part that adds the concept of "ranked choice voting" to the whole thing). And that's not so complex..:) (17:12)

%..Yeah, these will be the essential algorithms for the User groups, I think.. ..Really important.. ..Great..(!!..) :) (17:14)

%..Oh, but the moderator groups themselves.. I was thinking, last night, if we should make a system where a moderator group updates itself at frequent intervals via votes.. Oh, but I have already kinda covered this, then, 'cause this is essentially just deriving one User group from an existing one.. ..Although we would probably want to make it an automatic process, i.e. where the updated User group keeps the same ID as before (and where one needs to save a user group as a Constant list, if one want to back up the current state of the User group).. (17:17) ..So I should implement this as well, yes, i.e. a special kind of User group that gets updated automatically at fixed intervals (not necessarily 'frequent,' as I think I said), namely by letting the old members vote on the new weights and taking the median. Ah, but there's some trouble with that, as I also realized last night!.. If they just vote on the weights, it could end up in a race to the bottom (of the FLOAT numbers). So I need to consider how this voting could work instead.. Well, we do have points, so can't we just use those?.. Yes, well, 'votes' rather, i.e. the aggregates where the scores from each user is normalized if above 1 (and also multiplied by their weight in the end). Great, that was easy.:) (17:22)

%(19:31) Hm, if anonymous user participates in too many votes.. ..Well, voting probably won't happen for many different user groups. But users ought to only make a anonymous profile inherrit the weight for \emph{one} user group, and then use a different profile for another. Luckily, I don't think that there will be that many User groups that hold anonymous votes, so we don't get a total explosion in the number of anonymous profiles. Okay.. ..And I should then make a List type that is of the all anonymous user profiles that has been paired with a user (anonymously and quickly forgotten), and has then inherited their weight. I should then make update request for this list where each individual user can request to transfer their weight. ..Hm, I guess the DB should also check off the users when they have given their weight over.. Ah, maybe this can be done by just hashing the user's otherwise private key to identify them, and then keep the hash until the end of the procedure, when the dealine is reached for adding a private profile to the list. (19:39)

%..(19:40) Let me wrap disjunction types in parentheses so that it becomes more efficient to parse.. ..(Namely if I keep the current '<Type>::<Name>' syntax..)


%(11.12.24, 11:11) Hm, isn't it better to use nonterminal symbols instead of '?', '*', etc..? Well, maybe the '{n}' and '{n,m}' options makes it worth it.. But it does seem.. Well, that could also just be done in the test function.. And it is easier to make error reporting without these quantifiers.. (11:13) ..Oh, then I just need the empty symbol.. ..which I could just call "empty".. ..Hm, but maybe it doesn't actually make error reporting easier.. ..Now I've included the "empty" symbol.. (11:23) ...(I've decided to keep the qualifiers..) (11:35)

%... (13:33) I will not insert any sec. keys for 'f' and 'a' entities, and I will not let 'c' entities be editable at all. I also think I will turn the 'days' request parameter into a boolean again, and thus let the backend decide for how long.. Well, maybe we could even drop the boolean.. (13:35) ..Yeah, let's just do that..

%(14:46) Prøver at power igennem det, men jeg er virkeligt træt nu. Er btw begyndt at falde nemmere i søvn igen, og nu står jeg ovenikøbet også ret tidligt op. Men flere dage i træk med et par timers søvnunderskud har så nu hobet sig op, til gengæld.. ..Hm, bør nok prøve at gå tidligere i seng i aften, så..

%(15:10) Oh, error priority should just be the position (pos).. ..Hm, then I can actually get rid of the recordHolders, and just make it an error priority record holders array instead.. (15:13) ..But I need to postpone that to tomorrow.. ..Hm, and I think I just have to halt now.. ..Call it a (short) day.. (15:14)

%(16:46) I was able to think a little bit. First of all, something that I don't think I've mentioned: We will not need the 'i' entities, as we can just implement them as '8' entities instead of we need them, or as 'j' entities.

%But I now think I will make a 'p' datatype as well for 'posts.' The special about these entities' rendering is then that the posting user is always shown above the post/comment. And yeah, I'll take Comments t then be a subset of these "posts," and thus implement these as 'p' entities, along with contextless Posts. Also.. Oh, I should first say that the defStr of these 'p' entities are also still just HTML, like the 'h' entities. And then: Also, the 'p' entities will then be the only ones that are editable, I think, at least for now. And what I will do instead for the 'a' and 'f' entities, and possibly also the 'c' entities, is that I will allow the creator to edit the entity only by replacing some '@[<Path>]' substrings with '@<ID>' strings. And apart from that, I will not let these types of entities, nor any type other than the 'p' type, be editable (nor deletable). For the thing is: these are not supposed to ever hold personal information about the creator, nor be personal in any way to the creator. So having the option to "anonymize" the entity should be enough (and also warning that these entities cannot be edited or deleted, and therefore shouldn't contain anything that the creator might regret; use Posts instead for such things).

%And the last thing I thought of: There should also be a boolean in the Entities table that says hide_creator, which can then be used to "anonymize" the entity partly, but without forfeiting the option to de-anonymize it again at some later point. (16:58)

%..Let me btw underline: For non-Posts, the creator is normally not shown, unless the user clicks into the info page, namely since these non-'p' entities are not meant to be personal anyway.. ..Well, for answers and such, we might want to.. Hm.. ..Yeah, it just depends on the context, whether we want to render a 'h' entity (which is the standard text entity, btw; '8' entities are meant more for being data entities).. (17:02)

%(20:45) 'Closure' is another possible interpretation of 'c'..

%(12.12.24, 11:17) Okay, I think I will remove the score_err/score_width again, and then probably actually replace it with a weight (between 0 and 1, endpoints included).. ..Now I'm even considering reducing the histogram data, but.. Hm, maybe by just using a variance/SD instead..
%..(Hm, in English we should start using "spread" as well as another term for the SD, like we do in Danish.. ..Well, or something else fitting.. ..Nah, 'spread' could be fine, I think..)
%..Oh, maybe I do actually have to include \emph{both} the.. Well, first of all, yes, it could be beneficial to just reduce the histograms to their.. mean, spread, \emph{and} median?.. (11:29) ..Hm, in principle one could store the.. ..the SD/"spread" as a float, and then store the mean, meadian, and possibly also the quartiles, as just a few bits representing the relative value compared to the SD.. ..Oh, we do need to store the median as a float if we want users to be able to.. search with it.. Hm.. (11:34) ..Or we could compute and use the skewness, instead of the quartiles.. ..Oh, but with histograms, we only need to store a factor for each bin, and we can just make the bins wide.. I guess.. Hm..

%(11:47) I also just had the thought about making automatic (sub-)User groups from the user--user score correlations ("agreement," if you will).. ..Hm, and 'sub-' might be the keyword here.. ..But I guess this is similar to "just" making ML on top of a User group.. Yeah.. ..Hm, but then one \emph{could} perhaps save data by using this ML to make the tree structure of the User group.. ..Hm, that actually sounds more beneficial, rather than making it so that the cutoff is finely adjustable, ah! especially if we are also always propagating the weight of each aggregate score now anyway!.. (11:55) ..Oh, and if users with a very low weight, like a miniscule one, want to "pay" (upload data and/or computation quota..) for requesting an update of there score, dispite it not really mattering anything, then they should be able to go ahead.. So there's a change we don't even really need the weight cutoff, as long as only positive scores are included.. ..Hm, yes, but on the other hand, it might not be very easy to utilize ML for make the tree structure.. (12:02) ..Especially since it is Quality-dependent, whereas the user weight is not. Hm, okay so maybe never mind about that particular thing.. ..Yeah, let me just stick to my current plan for the User group trees for now at least.. (12:05)

%(12:25) Hm, I could also start using means instead, and then store the weight next to that.. The good thing about means is that the add easily.. ..And with the great powers of all the various entList contructors, and not least because of user groups, where users can be downrated if the spam or are consistently unhelpful in other ways, maybe we don't really need the medians after all.. ..Hm, and varainces are also easy to add, so we could then just propagate these three estimators instead of the histogram data, perhaps.. (12.29) ..The mean, the weight, and the variance or SD/spread, i.e.. ..Oh, but I also need to count the positive scores.. ..But I could just propagate "points" and "votes" in different tables.. (12:36) ..Hm, and I could also use use the median instead at the leaf level, but then just propagate it as if it were the mean.. ..The 'mean of the (leaf-level) meadians,' why not.. ..Oh, but if I'm propagating the spread, then I can add the medians together approximately. Okay, that might be a neat solution.. ..Yeah, then we can relax a lot more about policing users' scores inside a given user group..:) (12:42) ..And I think I should propagate the spread (or variance) anyway.. ..Yeah, so this can possibly reduce wat was the 'histogram data' to just median, variance/spread and weight instead, which then in turn means that we need to care all the less about storing each node, which in turn means that we can allow ourselves to use nodes of "sizes" in the tree, which then means less computation needed for updating them.. (12:46)

%..But the questions are then: What about the user scores, and what about the final aggregates?.. ..I don't think the spread will actually serve very much use for the user scores, other than for the sake of UX.. (12:48) ..Wait, that's not true.. (12:50) It matters exactly in terms of how the medians are added together, for the leaf nodes, i.e.. ..Hm, and we could also keep the weight (factor, in this case (between 1 and 0)), also for the sake of UX, even though it is a bit redundant when we already have the spread as well.. But some user might prefer to adjust their weight instead of their spread.. (12:53) ..(12:58) Hm, it's not completely redundant, 'cause the user spread also affects the propagated spread. So the weight might actually be prefered for the sake of aggregating. So yeah, maybe we should allow for both things. And they don't have to be FLOATs. We should be able to just use TINYINTs for both the weight and the spread/variance, appended at the end of the clustered index.

%Another thing that I just thought about: The recency scores should decay exponentially, such that when we aggregate them, we can always just record the modification time of the aggregate as well, and then the whole thing just decays with the same rate, regardless of its composition.

%And another older thing that I don't think I've mentioned yet: ..What was it again.. (13:03) ..Oh, that slipped.. Hope I'm not getting old already.. (13:04) ..It'll come to me again..

%..Okay, so I might then be able to use the same table for user scores and the intermediary aggregrates, and, well, the weight and the spread (SD (standard deviation)) will also surely be useful for any list, although we could also query for that afterwards.. ..But I think not. I think it might be a very good idea to make that adjustment, i.e. of how the weight and spread should affect the ordering of the final list, to be something that happens at the very end, on the client side. 'Cause this will likely be very helpful in terms of.. well, in terms of making it easy for the users to adjust the filter of how many not-so-certain entities they want to see, which then means that they can then make the not-so-certain entities more certain by rating them, organically, without having to choose a different list entity in order to be able to see the more "fresh" entities, but where it is instead just a matter of adjusting two sliders (all client-side).. (13:13) ..Ah, and again, the weight and the spread does.. maybe.. not need to be two full floats.. Of course, the exact weight..

%Oh, there it was. The thing that slipped was: For the anonymous profiles, speaking of something completely different, the weight should only ever be rounded down, of course (regardless of whether a random value is added or not; then the added value should then always just be negative). (13:15)

%But continuing on: The exact weight is probably quite important for the intermediary scores table..(?) (13:16)

%..Of course, I should \emph{also} make entList functions that uses the weight and/or spread in some arithmetical calculation. But the whole thing might then also still be adjustable client-side as well, namely by still also aggregating/keeping a/the weight and a/the spread.. (13:20)

%..(13:22) I think it's fine to just use a TINYINT for weight propagation. It's thus okay to just approximate the combine weights of the leaf nodes thus (and it's certainly also okay, I think, for any linking nodes, and of course for the root node(s)).. ..Great, so now I can combine all these three tables. But I should on the other hand then make seperate tables for the points, votes, and recency scores. But that's perfectly alright.

%Now, the next question is then: What about the PrivateScores: Should we also give a user group to each.. ..to each List entity..?.. (13:26) ..Ah, I could make a seperate table to store the.. ..The user group\emph{s}?.. (13:28)
%..Hm, it should be normal for users to keep some of their entity lists private, while still requesting that they can be aggregated by the database. We also have the possibility for using anonymous profiles, but until we implement those (and also definitely afterwards still), it will be very useful to just be able to submit one's anonymous scores this way. So yeah, let's add a user group to each entList, which are the ones that can query for data on it. And maybe let's do this via a new table storing List entity--User group pairs.. (13:37) ..And we'll just have one User group per.. Well, in principle, each entList could then have up to at least a few user groups, through which any given user might qualify to see the data. But I could of course just say that the cap is 1 at the early stage, if this makes it easier.. (13:39) ..And when aggregating private/anonymous scores, we should make sure to let the intermediary nodes have the empty user group as their "allowed users group," until the node exceeds some user number threshold.. (13:41) ..Oh, maybe we can use the same, or a similar table, when it comes to who can view different entities. Then we also don't have to store this info on the Entities table itself, when it is only really used for 'p' entities in practice anyway. :) (13:44) ..Great, I like this a lot.. (13:44)

%..Oh, but a user giving an extreme score will then still f** up that given leaf node, if we use the variance to add the medians further up the tree from there.. (13:46) ..Oh, I just thought of a differnet solution: Maximizing the spread (SD) for which the inner product with a Gaussian curve is greatest..! (13:48) ..That's actually quite great, I think.. (13:48) ..(Obviously where we center the Gaussian on the median when we maximaze.) (13:49) ..(Hm, what a neat way to handle outliers in general, btw (when expecting a Guassian curve (but you could then also use higher SHO solutions as well, potentially..))..) (13:51)

%..Very good. And then for the points and votes and recency scores, I'll just use 2--3 different tables for those (as I can maybe also gather points and votes, who knows..).. What else, then? (13:54)

%..Hm of course anonymous votes also give the possibility to troll, but as long as there are just enough open user.. Oh wait, we need to only aggregate private scores together with private scores.. ..Hm, maybe the anonymous profiles is just nicer, since it also doesn't lock the whole thing in terms of things like ML, and such. And it's not so hard to implement: A user just requests to get a anonymous profile for a given user group, and then the database just marks privately that they have one, such that they can't create another (if we're just allowing only one anonymous profile per user for a given user group in the early stage..).. (13:59) ..Hm, but alternatively, we could just divide the user group tree at the top level into two parts, the public and the anonymous part. And then you could even add a factor to all the anonymous scores (below 1).. (14:00) ..And then we can just.. ..yeah, just do ML on only the pub.. Oh no, we can also potentally include some of the anonymous users' entity lists, if they allow it.. (14:02) ..Okay, I'm starting to like the idea again..
%..Oh, and if there's an anonymous troll, then the given subgroup can just request to be split up. And then if the trolling then starts in another user group, who has received new users, they can just request to be split up again. And this only needs to happen a few times before the trolling user can be rooted out, if they don't stop their behaviour.. ..Sure, I think this might be a good idea (even if it sounds a bit complicated.. and much more complicated then just using anonymous profiles, hm..).. (14:06) ..But anonymous profiles might run a greater risk of revealing themselves, right..?.. Hm.. ..Oh, there's a problem that we can't expect the scores for a user entity list to be either all non-sensitive or all sensitive; it might very well depend on the entity. And that's where anonymous profiles makes much more sense.. (14:11) ..Hm, but we could then just divide each user entity list up into two (or several; one for each 'allowed users' group).. (14:12) ..(14:16) I think anonymous profiles are safer.. ..And they are also a must, regarless.. ..Ah, let the anonymous profiles display the profile count, not the.. Well, no 'cause then everyone can see these numbers go up at the same time.. Well, unless we allow for a random delay.. Let's do that, maybe.. Then everyone can still see roughly how many other profiles an anonymous profile has. ..Yeah, and we can just choose a big delay.. ..Well, but also cap the number.. (14:21) *(14:58) Oh, we could also just say that each \emph{new} profile stores and shows how many other profiles the user whom the weight is inherrited from had \emph{before} the given one.. ..I like that.. (14:59) ..Oh, and the inherrited weight (at the time of creation, or update), is also just shown in principle, and then the given User grou can always down-, or up-, rate the given anonymous user afterwards, irrespective of the inherrited weight.. (15:02)

%(14:36) Okay, but I also need private score lists, so what do we do there.. ..In particular the list of users that can see.. ..certain posts.. ..Yeah, things like that should be private. But I guess I could just keep the PrivateScores table..? (14:39) ..Yeah, I guess, but then the user can also choose to use other (public) User groups as well.. (14:40) ..Yes, alright. And then the table that stores the List entity--User group pair can just store whether the List is a private or a public one as well, if that speeds up the query.. (14:42) ..It can store which table to quary in general.. ..And then it should also store the user group / user that has access to both see change this.. Hm, but how to do that with a user \emph{group}..? (14:44) 

%..Wait some List entities needs to be private.. So it's not just the 'p' entities after all.. Hm, that requires some thinking.. (14:46) ..Oh, 'j' and 'u' entities for sure also needs to be private sometimes, and we could then just implement the private List entities as 'j' instances instead.. ..Or as private 'a' entities..(?) (14:48) ..Hm, but then I should rather just give all entities the option to be private, again.. (14:49) ..Hm, so reintroducing is_private, and is_editable, and then for certain non-private, entities, is_editable don't mean free reins to edit all you want, but maybe you can only substitute path placeholders. And then you have to mark this substitution as finished, before a sec. key can be inserted for the entity, and it only will be \emph{if} one does not already exist.. ..(And then we still only actually insert sec. keys for 'c' entities.. Well, unless we do want to do the same for.. Nah, it doesn't make sense for 'a' entities, 'cause these will generally have a Desciption attribute, which will mean that collisions doen't occur, even for samantically eqaul entities..) (14:55) ..Oh, but I can change is_private for allowed_users, still..

%..Then back to the question of having semi-private lists potentially for whole user groups, and how to handle that, if we do.. (14:56) ..(15:03) Well, surely the User group just votes for the desired 'allowed users' group, and then all their lists gets that user group.. But then, shouldn't we just say: one user (group), one 'allowed user' group..? ..Hm, and also: Shouldn't user groups' scores just always be public, at least for the early stage?.. (15:05) ..Yeah, let's say that, and then we also space that implmentation. And users, on the other hand, can just choose for.. Oh wait, shouldn't it just be part of the List contructors. I think so. And then these will just mainly be used by individual users in the early stage, as very few (virtually none, perhaps) User groups would probably wish for their aggregates not to be public. ..at least not at the early stage when there no benefit to it.. (15:08)

%(17:25) Hm, I think I ought to move some responsibility over to the control server.. ..Oh, but I just realized that I don't only have SUBSTRING_INDEX() to use (somehow I noly realized this now..); I also have REGEXP matching, which also means that I could have easily used normal function call syntax for the 'c' entities if I wanted to.. ..But in terms of the mentioned question, then I need to do ML in the control server regardless.. ..But it doesn't hurt to implement things in the DB layer when possible, I guess.. (17:32)

%(18:02) Oh, there's also the modified_at time/date of the user scores.. ..Hm, and we don't just want to also propagate this, do we..? ..Well, we can't.. Oh, but we could.. Nah.. Hm.. ..Hm, we could propagate a decayed.. Nah.. Hm.. ..But we could propagate recency points.. ..Hm, I guess I haven't thought recency scores/points, points, and votes through.. (18:07) ..Wait no, I had, but they of course depend on having a time/date for the user score.. ..Well, I should certainly give ListQueryRestrictions, as I now call the List--'allowed users' group table, a char as well to denote the given Scores table to look in.. ...And that's really just the solution, and then use my UserOpinionScores table once again.. Should I then keep the weight \emph{and} the error, or?.. Hm, maybe I should.. ..Hm, couldn't we just have a seperate user points table, where all for all uploaded positive scores, points, with dates/times, are automatically added to that table as well?.. (18:26) ..That sounds good, and then I actually \emph{can} join the UserOpinionScores with the aggregates.. ..That table is then used for points and recency scores, which are just decaying points, and then we actually need another table for votes, since we don't want to compute these automatically.. Well, but the table could still be the same, I guess, but just.. Well, unless we just want a.. Ah, I guess we only want to store the date / rounded down time for the 'user points and recency scores' table.. And for votes we want floats, and we only want them updated on the user's, or others', request.. (18:32) ...(18:48) Hm, so let me make a FloatingPointScores table, without weights or.. Wait no, we do want weights for votes as well, right.. ..Yeah, so maybe we can just use what I now call FloatingPointScoresWithWeightsAndErrors for aggregating votes as well.. ..Yeah, then we have the dangling score_err_exp (TINYINT only), but that's also fine, and only unless we find some use for it for votes.. (18:51)

%(19:02) Hm, for the 'f', 'a', and 'c' entities, I can actually just check for '@[...]' occurrences to see if the entity can be edited. ..But I should probably keep is_editable anyway, right?.. ..Well.. ...(I'll just do that.) (19:20)

%(13.12.24, 10:54) The recency scores are actually quite easy to aggregate: we can just aggregate then as dates. If they are all decaying with the same half life, then we can eaisly sum then together, and update them when an individual contribution changes, and then we just work backwards from the resulting number, between 0 and 1, to get the datetime, which we can then use as the recency score.

%Something more impotant, though: I think we.. Well, first of all, if we were to indeed use that whole User group tree, then medians of medians is the best thing. But I don't think we need to. We could instead just make that (ent_list_id, subj_id, score, user_id) index for the user scores. And then updating the combined, weighted, median of \emph{any} user group will be quite simple: Just go to the previous median, and then.. Well, this assumes that each.. Okay, let me think for a bit.. ..Hm, it could be done with a kind of RecentInputs/Scores table.. (11:02) ..Hm, which could actually also double as the current PositiveScoreTimes table, or rather take over its responsibilities.. (11:04) ..So each entList--Subject gets an index over all the recent scores (rounded down to the nearest day, or half-day, or quarter-day, or couple of hours, according to the users.. Well.. ..Okay, I should also think more about.. Oh, we could just add a random time to the datetime..).. (11:07) ..Then each User group just needs a timestamp for when they were last updated, and they can then just go to their.. Hm, maybe that score times table should then also have a column for the previous score.. And then updating medians, and means for that matter (although medians will be much more interesting, I think), for the whole user group at once will be quite simple. Well, they have to go through many new updates.. Oh, we could actually use a BIGINT AUTO_INCREMENT PK instead of a datetime.. ..And to get the recency scores, we just need a table that stores the most recent score at approximately every whatever-interval-we-choose, but only approximately so.. Well, that sounds complicated, but it could be one (quite data efficient, albeit complicated) way.. (11:15) ..Well, we can just store the time at every 5 min or so (or whatever we choose to do), and just keep this data private (non-public).. ..Now, every user group update then needs to go throug.. Oh no, 'cause it's only for one entList--Subject at a time.. Well, but then the BIGINT AUTO_INCREMENT PK doesn't make sense.. ..Nah, let's just use the timestamp instead, and if we get a dublicate key error, we just handle that by waiting a millisecond (if needed) and trying again. ..And let's make this timestamp collumn nullable, such that we can remove timestamps that are old enough.. ..Well, just remove the whole ScoreTimes entry, then.. (11:22) ..And call it RecentScores indeed.. ..The recency/freshness score is then trivial to calculate, and can be done from the buttom up. ..Well, but we can also make it so that it updates only from the most recent scores and up, let's do that. And the same for the medians (and means).. (11:25) ..I should then make a table over (user_group_id, ent_list_id, last_updated_at)..
%..Hm, and should we actually forget about the score_err_exp?.. (11:28) ..Yes. Let's just use only the weight, both in terms of the user opinion scores, and in terms of the aggregates.. (11:31)

%(11:39) Right, why on earth try to update whole entLists at a time, when we should just be updating individual subjects on the entLists at a time..

%(11:54) So a entList--subj--score--user index, and a entList--subj--timestamp--*(user--)score--prevscore table, and a {userGroup, entList}--lastUpdatedAt table.. And for the votes?.. ..Hm, here we probably need to update a whole ent_list of user votes at a time.. ..Oh, or just update a sum of all the user's points for that list in a seperate table.. So an {entList, user}--pointSum table..? (11:58) ..Hm, and when we update the votes, we could maybe.. ..go over an index like the entList--subj--score--user index, where score and user is at the end of the index, and then for each user and score, we divide the.. Oh right, votes are proportional to the score. So we take the score and divide it with the combined positive score, which is what we should store in that {entList, user}--\emph{score}Sum table, as long as this sum is greater than 1, and then we also multiply whith the user weight of the given group, rounded up to 0 iff negative. And that should take care of that.. (12:03) 

%..And for "points," we just only count.. Oh never mind, for points we update using the RecentScores table. But we can't use that for the "votes," right?.. ..No.. Let's just compute the votes from the bottom up each time (on enough requests), at least until a smarter way of doing it is thought of.. (12:06) ..Yeah, 'cause for feeds and such, we are actually more interested in using the "points," and thus votes will only really be used for small entLists anyway, I think.. (12:08) ..When you need to decide between a (small) range of options, and the exploitability of the points starts to matter.. ..(in terms of people not rating, or even down-rating.. things the don't like as much.. hm..).. (12:10) ..Well, this is something to think more about, along with ranked-choice voting, and representative voting. But this doesn't change the plan of using the above mentioned indexes and tables for the medians, (means,) points, and freshness points.. ..Hm, and to be honest, we don't really need anything else, even including the votes, for the beta version. And when we need them, it shouldn't change how we implement the other aggregates. So I can just go ahead with this plan.. (12:15)

%(12:24) Hm, is it crazy to use a CHAR instead of a TINYINT for the score_weight_exp? It gives us more than 2^{20} options (equivalent of two and a half byte) in principle, and it is also transfered more compactly in general over HTTP..

%(12:27) Something else: The entList query proc should how an input to state the wanted precision of the FLOAT score.. ..(And we will almost always use this, then..)

%(12:30) Lets just be a little wild and use a CHAR for the weight exponent.. Oh no, we might need it in the backend, I mean, unless we want to store it as a FLOAT for the aggregate tables.. But yeah, nah.. ..Okay, but TINYINT it is then, probably unsigned, though, since we want to choose our own offset.. (12:32)

%(12:40) Hm, a qual--subj--timestamp--user--score--prevscore table, rather.. ..Well, if I keep the modified_at column, then it can just be a secondary index of the User(Opinion)Scores table.. ..Oh no, this is not true, 'cause of the prevScore. Alright, two tables, then..

%(12:58) MySQL doesn't seem to want to return the current fraction of a second, so I should use some counter somehow anyway.. ..Hm, can I do that in a DEFUALT expession?.. (12:59) ..Oh, AUTO_INCREMENT can at least be just a \emph{part} of a PK! (13:03) ..Awesome!.. ..Oh, not for InnoDB tables, but for sol-called MyISAM tables... (13:14) ..But we can just make it a.. ..oh, keep it a qual--subj--timestamp--user--score--prevscore table (omitting the score weights here), and then include user as the last part of the PK.. ..Yes.. (13:19)

%... (15:20) HTML entities should have some meta tags (implemented as a subset of true HTML, and not just a HTML-like language) that can denote the source (or inspiration) of the text, inside or outside the DB/Semantic Network. For instance, Post entities can serve as the source/inpirations for a given text.

%I'm thinking about what to do exactly for the 'better representations.' One good idea so far: The representations of 'a' and 'c' entities should actually be 'j' (JSON) entities, namely such that the act of creating a better representation doesn't also produce a dublicate of the given entitity. And its only a good thing if this means that the represeentations can potentially hold more metadata about the entity than the 'a' and 'c' entities can. (15:25)

%(15:36) Wait.. Editable texts where the user determines the editing User group..!..(?) ..Ah, so also a userGroupID instead of is_editable as well..!.. ..And an editable_until, perhaps.. Or maybe just a creation date, and then a standard.. Hm.. (15:39) ..Ah, and the ability for the creator still to change it to.. Well, to change the editor group.. (15:40)

%..Something else, btw: User groups should not have cutoffs, actually. Instead we just always have 0 as the cutoff, but then have a list function that just subtracts a constant from all scores, and which, like the cutoff function would, also don't need to be stored separately in the DB.

%But back to the "editor groups".. (15:43) ..Well, maybe the editor group can just vote to make the thing constant.. perhaps simply by uprating an edit above a certain threshold, which means that this should be the final edit.. Hm, I think I like that.. (15:45) ..And the creator cannot change the editor group, only finalize the edit, I guess.. (15:46)

%..Hm, we \emph{could} remove the 'p' entities, but no, I think they should stay.. And btw, 'p' can also be interpreted as 'personal' (whereas 'h' entities are not meant to ever be personal to the creator, not directly so (i.e. not anymore than e.g. a scientific paper is personal to the author, which it generally shouldn't be (although I've broken that rule myself a bit sometimes.. ..just to add that little unrelated personal note to the conversation..))) (15:49)

%(15:50) But this will mean that most of the entities (I guess except the User and the Type entities) will function the same way: The editor group.. and only the editor group, can edit the entity.. Well, and the creator as well, sure.. And only the whitelist group can view the entity (but NULL means that everyone can view it). The creator can however, even when editor_group_id is NULL, which means no editing, still always substitute any '@[...]' occurrences with an actual '@<ID>' reference; the creator can \emph{always} do this, until there are no more '@[...]' occurrences left, of course. (15:54) Both the creator and the editor group can determine when an entity should be finalized. The creator does this simply by request, and the editor group does this by scoring an edit above a certain score, \emph{and} weight, threshold. (Both the score and the weight needs to be above a certain threshold.. which will be chosen somehow, perhaps by the creator.. sure.. (but in practice by the app, most of the time).) ..And that's pretty much it, isn't it?.. (15:57)
%..Well the edit proposals could be formulated as diffs, though, at least once we implement this.. (15:58) ..And then when finally finalized, the def_str is generated from these diffs.. ..And until then, the app queries the diff, and then make the substitutions in the original to show the user.. (16:00) ..Oh, which means that diffs actually needs to be a new data type as well (namely since it should be rendered in a special way in the app (where it is normally rendered as the full text, unless the user clicks to see some details about it)).. (16:02) ..Hm, either 'd' for 'diff(erence)' or 'e' for 'edit'.. (16:03) ..Hm, maybe 'e', and then we can also call it an 'edited entity'.. (16:04) ..Right, this is good.. (16:05)

%(16:06) Hm, and any usr can at any point in time, also after the finalization, query their own favorite user group (for doing so) for a better representation/version of a given entity (such as a text, in particular). This could also perhaps be used for making annotations. But we still need the same entity as the underlying reference point, which is why it is important that all entitites (that are used popularly) gets finalized at some point, even if they'll still se more "weak editing," we might say (rather than "hard editing," we could say), after this finalization.. (16:10)

%..Great, great, great.. And I love those 'e' entities.. (16:11) (As they are never exactly dublicates.. Oh, and I should say: So we are not using 'j' entities for 'better representations' after all, but the 'e' entities.) Hm, and I guess this means that.. Nah, let us not require the edits to follow the format of the entities they edit, but instead make it the apps job to check that the syntax is still fulfilled after substitution of the diffs.. At least we can say this at the early stage, and then we can always change it later, and delete any 'e' entities that doesn't fulfill this, if we want this.. (16:15)

%(16:39) Hm, the creator should also be able to forfeit the editing privileges.. ..Hm, so a creator_can_edit bool?.. (16:41) ..Sure..

%(16:52) (You would think that I wouldn't get tired earlier for a thinking day like this, but it seems that this makes me tired sooner than for a full programming day, at least in these recent times..)

%..Let me try and revisit the topic of the votes.. (16:57) ..These dark winter times.. The tiredness really hits like a brick sometimes.. (17:01)

%(18:23) Okay, about the votes. Votes should actually not be something to be generated continuously anyway. A User group can request a vote, and then the DB schedules one. Then all participant have time to cast their votes, and the result can be seen as final, which is not really the case if it was a continuous thing instead, since when does the vote really end?. Additionally, we should use whole user groups as possible representatives, which means that we don't need to use several rounds/iterations for the process, well, except for the ranked-choice votes, but I'll get to those. But instead we just let each voting user have a list of user groups, where a "user group" can also be a single user. So a list of either userIDs or user entListIDs. And when we calculate the individual users vote contributions, we then just look at all the user groups on this list who has cast a vote, and then we normalize.. Well, let me say first: We take a whole list of candidates (the things that is voted for, which could be persons, solutions, and all other kinds of things.. game of the year.. you name it..) from each representative group on the user's user group list (which we could call the 'representative group list'). So for each user group / user that has cast votes (which all user groups will tend to have, except very small ones), we take the.. n highest-scored candidates, and then we multiply with the weight attributed, by the given user, to that representative group. We then collect all the n \times m, candidates, where m is the number of representative groups, and finally we pick the n out of these \leq n \times m candidates with the highest score/weight, and normalize the scores, potentially. This way we get n candidates from each user in the voting user group, together with some, yes, normalized scores (that thus sum to the same thing for all users). ..Oh but we don't look at these scores for the next step, which is to just join all the \leq n \times userNum candidates into one list, where the score is then the number of users that has votes for them. Then we make a cutoff on this list, such that we are left with some number, call it N_1, of possible candidates. Then we go back and repreat the process, but where.. Oh wait, the n that we pick from each representative group and the n candidates send to the combined list from all users don't need to be the same number. The first "n" can be greater. ..So call that M for now, I guess.. Then with the N_1 candidates, we now go back and reduces the M candidates for each user to some new M_1 for each user (sorry if I'm not explaining this well), simply by removng all candidates that didn't make the cut of the N_1 top candidates overall. Then we repeat the same process, perhaps with a smaller n, but definitely with a different N_1, call it N_2. And this way we can thus reduce the list all the way down to the second-last number, which should always be 2, and then for the final round, the users then just each vote for the one of these two should win (so "n_final" is definitely smaller than n_1, since n_final is always just 1). And there we are. Near-perfect representative ranked-choice voting. Oh, and I forgot to mention, if the user cast some votes.. Well, no, let's just say that the user themselves are typically.. Ah, no, the user themself.. No.. ..Okay, let's say that if the user is missing from the representatives list, then their weight is just assumed to be 1.. or maybe 100.. (but they can also change this by including themselves on the list). And there we are. So in principle direct democracy, but with possible (and probably very often used) representatives, and with a ranked-choice voting system that can be arbitrarily "smooth" in principle, such that users don't have to strategize about voting for something else than they want, really (namely if we just let N decrease slowly enough, and also use generous values for the other parameter( set)s), and don't have to deliberately not vote for things that they like. (18:51)

%(18:58) Oh, about "condtional votes," can't we just join votes.. ..Namely by voting for compound solutions.. ..Well, things can get more complicated: Conditional votes really only makes sense when you have different groups with different kind of power (i.e. over different things, and in particular over themselves, of course).. So let me not try to dive more into that now, I've already treated that subject enough for the time being, I think.. (19:01)

%(14.12.24, 10:18) Okay, I'm not going to implement the 'e' entities for the beta version / early stage. They might be implemented at some point though. I have anther idea of what to do in terms of editing, but I'm not sure. And I feel a bit stuck in my thoughts, so: 'Thinking on the keys'.. I'm thinking about making it as a kind of request table, where the edit proposals aren't entities, but er just temporary requests that will be deleted after a time.. But I guess the.. Hm, what's the big trouble exactly..? (10:21) ..Hm, I guess trust, not least that the user group understands the intension.. Oh, and the danger of the app automatically making users use the same editor group.. Hm.. ..And I also don't like that a user can just pull the plug on an entity, if it has received a lot of activity and scores.. ..Hm, so we should probably just make the '@[...]' substitution the only allowed editing of non-personal ('p') entities.. ..Yes, okay, and then we should use 'better representations' from there. Okay. But then what about the better representations; what do we do exactly in practice?.. (10:29) ..Oh, we can just use '8' (UTF-8) entities for the 'better representations,' and tehn we just make sure to also query for and return the entities original data type, even if there's a better representation('s defString) to be returned instead of the defStr of the original entity. Okay, this seems good.. (10:32)
%..Then the only thing is: Will people really use this.. well, it's just an optional query proc; we don't have to use it.. ..Yeah, and maybe in the future, the app (with normal settings) will use it in some circumstances, but not in other. (10:36) ..And in the beginning, let us just make the app use the "normal" variant of the query proc everywhere, actually.. (10:37)

%(10:38) Wait, maybe the difference between the 'h' and the 'p' entities should really just be whether they are editable or not, and then if they are, that just changes the rendering to a 'p' entity..
%..Hm, I should probably make a table with entity ID timestamps, which are inserted for every new entity, unless not enough time has passed since the last timestamp.. ..Better to let everyone know that the IDs can be used to tell the time of the upload, namely by just making it a public feature instead of trying to be hush about it. Okay, I should do that.. (10:43)
%..And yeah, I think we should gather the 'p' entities into the 'h' entities exactly like that.. Well, and the '8' entities, which is a good thing: Then we can have posts of both types. And then all entities just gets the is_editable column/field, even if some of the data types don't use it.. (10:45)

%(11:03) Oh, we could also use the same boting system for handing out payment/prices to contributors, only where we select several candidates, and where the prizes then decreases the lower you get on the list. This means that you get more power the more money you have contributed, directly or indirectly (by watching ads), but it still doesn't pay to vote for something unpopular (such as yourself of you haven't made useful contributions). That actually sounds very great.:) (11:06) ..\emph{very} great..!.. ..(And the possibility to use user groups as representatives is also very important here..:))

%(11:50) Hm, about having a separate table for the entLists' userWhitelists, can't we just make the userWhitelistID part of the function call that constructs the entList..?

%(12:00) Let's just be hard core about the 'c' entities and not allow for even path substitutions..

%(16:05) Oh, I guess I should use ent_list_id instead of just qual_id for UserScores, then, since we need to parse the userWhiteListID as well..

%..Oh wait, my 'c' entities can't be used for private entLists!.. So what do we do..? ..Make a private EntitySecKeys table, including the user IDs as well?.. (16:08) ..Hm, I didn't immediately like the sound of that, but I guess: maybe yes.. (16:09) ..(16:12) Well, no: It should include the userWhiteListID, and in fact, we could just combine that with userWhiteListID = 0 representing public entities. ..Or userWhiteListID = NU.. oh no, 'cause it's the PK. So userWhiteListID = 0.. (16:14)

%..(16:20) Ah, and this makes it really easy, 'cause then we can just say that the userWhiteListID of who can view the entity list is excatly the userWhiteListID who can view the List entity.:)

%..Oh, but if we turn user_id and qual_id into just list_id, then we can't make that index with the users at the end to update the medians.. (16:23)
%..Hm, we can also just query for the given List entity, if it exists, and then pasre the userWhiteListID from that, and assume it to be.. hm, either userID or 0 if it does not exist.. ..Wait, no, 'cause we need userWhiteListID to query for it in the first place.. So what to do..? (16:26)

%..Hm, we should maybe just make another table for the users-at-the-end index, but let me think.. (16:28)
%..Well, or maybe we should just keep the ListQueryRestrictions table.. (16:29) ..Ah, and we can then just say that when a.. Well, first of all.. Wait.. No, maybe we don't actually want it to be such that users can only see the List entity if they can also see the entity list.. So yeah, maybe I should make two ListQueryRestrictions tables for both my two kinds of Scores tables. And then, as I was about to say, we could maybe make it so that if a list_id/{user_id, qual_id} is missing.. ..hm, we don't show the entity list to anyone before the user (or the backend) has set it to something, is what I wanted to say. But I also just realized that we don't necessarily want.. well, we might want to make it possible to let some users see the scores of some given entities.. well, what I'm trying to say is just that: It is not the qual_id that should determine if a score is private or not, not alone; the subjIDs will often be the sensitive ones.. ..Hm, but I can't add a userWhiteListID to all UserScores rows, can I?.. (Well I guess I could, but should I?..) (16:36) ..We can make it compressed on all indexes, I think, so we shouldn't worry about space, I guess, and what else is their really to worry about for user scores, not really RAM, since we they won't often be queried much from anyway.. (16:40) ..And it doesn't really hurt if a user can show one opinion to one group, and another to others, as I see it, why not.. (16:41) ..(So we \emph{can} let userWhiteListID be part of the (actual) UNIQUE constraint as well..)
%..Hm, and we can just say, then, that the procedure for aggregating for a given user group then only looks at public scores and scores with userWhiteListID = <the given user group>, and that's it; no other user scores are queried.. (16:44)
%..Oh, then we also get the users-at-the-end indexes for only specific user groups for free, essentially, this way!.. (16:46) ..Oh, well, no, 'cause they are then not viewable by outsiders.. ..But let me not worry about that anyway, I guess.. (16:49)

%(18:01) I was about to propose having both a whitelist for who can see the entity list, and another one of which user groups can aggregate the scores from it (but not necessarily be able to see it, not as an individual member in that group). But again, I think it might be better to just rely on private profiles for privacy.. ..Oh no, 'cause we would also like to use lists for inboxes (and "outboxes").. (18:04) ..Hm, so maybe a bit of both things?.. ..Hm, but maybe where the user group only aggregates.. Well, it could be like I said before: The scores have to either be public, or at least puplic for the given user group.. Ah, with a separate Restrictions (/ Permissions) table, then the user can also green light several user groups at once, which is quite important.. ..Hm, but.. ..Yeah, it's not so easy.. (18:07) ..We would need a table where the aggregating procedure can go and query for a (user_id, qual_id) pair, and then get a list of all the green-lit user groups for aggregating the scores.. (18:09) ..No, 'cause, again, it needs to be entity-specific.. (18:10) ..Hm, we could.. ..Let the users make some identifiers, which can then act as a sort of sub-profile, where more precisely each "sub-profile" is a list of green-lit user groups, and where the user can then use this list for the UserScores. So the procedure looks at the users "sub-profiles," and finds one that includes the given user group, and then go and look in UserScores with that.. Hm, this seems to complicated, though; there must be something smarter we can do.. (18:13) ..(18:15) Well, we could just make the index like (user_id, user_group_id, "sub-profile").. ..Yeah, then it's just one quick query to get the "sub-profile".. Wait, couldn't it just be a boolean instead, i.e. the "sub-profile" (instead of some identifier)?.. (18:17) ..Yeah, so just an index of green-lit user groups like that, which then gets the right to aggregate all the users scores at once (but not show where they come from). Okay, I think that that is the way.. ..Oh, but how do users then trust other users.. Ah, exactly because the user scores cannot be dublicated.. well.. the point is just that you can still see some of the user's scores. And if you are not satisfied with this, then you should just start urging the members of the given user group to create anonymous profiles instead.. ..Hm, and we \emph{should} urge users to do that, and we should maybe even urge them not to let their scores of their public profiles be public, at least when it comes to matters of tastes (and opinions).. (18:22)

%..Hm, side note: I should allow 'a' entities to be private.. ..Yeah.. ..(18:28) Well, maybe nah, 'cause we would probably rather want to use the 'j' entities after all, just like we do for the "workspace entities"..

%(18:30) Okay, but in regards to the UserScores, I think a green-light table for both the aggregating user groups and the user groups where the members are allowed to view entities and the scores on the list is the way forward.. ..Ah, but again: Green-lit to view \emph{all} entities on the list?. No.. ..(18:34) Hm, maybe the user could make a semi-private entity that stores the.. hm, the "sub-profile" key.. to use when then going to the UserScores.. Hm, this doesn't help, I think, not with the whitelist-should-depend-on-the-entities problem.. (18:36) ..Ah, but I guess they shouldn't.. Maybe the user should just.. well, generally make all scores that regards tastes and opinions in general be private, except maybe for the public profiles where they then just don't rate much at at in terms of such things. So maybe we can allow ourselves to (semi-)lock whole entity lists at a time, instead of locking each element individually..!.. (18:39) ..Okay, that could help a lot.. And it sounds reasonable.. (:)) (18:40)

%(15.12.24, 10:46) Okay, we can't hide the scores for a user group that can aggregate the scores 'cause the weight of the individual might change. So I think we should only aggregate public.. Well, or at least scores that are viewable to the given usr group, yes.. And now I'm thinking that we should probably compute the histogram for each qual--subj--user group triple..
%...(11:00) Oh, I got an idea for how we could make and store the histogram: It could maybe be done quite easy if we always make bins, from one end to the other, that has the same size in terms of area.. ..That is both easy to compute, with no real preprocessing required, and it's efficient to store..(!..) (11:02) ..'Cause it's just essentially a much-expanded median--quartile set.. ..Oh, but I'm assuming that I still have the users-at-the-end index, which I should also maybe keep.. ..Yeah, this is good.. (11:05) ..Of course, updating this histogram efficiently is another matter.. well, actually: maybe not since we only have to update.. well, not one, but two points (in the "expanded quartile set"), i.e. at the prevScore and new score for each changed score.. ..Yeah, so maybe not so computationally hard, but otherwise I was thinking of just keeping the.. well, if we keep the bin intervals, but add and subtract, then we need more storage space.. ..Well, we could (since we are already cheating anyway) just expand the interval, then, instead.. (11:10) ..But maybe it's more pragmatic to just change those.. Oh, no, if we pluck out a score, then the whole histogram could change.. So we should probably cheat for updating, oh, and then the idea is just to refresh them from the bottom up from time to time, upon requests (or more specifically by havig a countdown that get's reduced for each little cheaty update (and also gets reduced with time, potentially)).. (11:14) ..So how exactly do we wanna chease it..? ..Maybe do what I thought about and, well, shrink an interval when we add a new score there.. Hm, but then all other intervals needs to move, and that's no good.. Hm, maybe it's better to just double the storage.. well, unless we want to use less space to store the bin height, which we very well might, and then just add and subtract to each bin.. also counting some obligatory end bins (unless the whole interval is already bounded.. well, all qual intervals are always bounded in some way, so yeah, we'll just always have some end intervals.. Hm, that we probably want to always cut off when aggregating further from there).. But yeah, I think this could be a solution.. (11:19) ..And when calculating the median, and mean, you then just approximate by assuming that the scores are uniformly spread out within each bin. Oh, and one might note that.. Oh no, never mind: If we add a score to a bin, we don't keep track of where that score was added within the bin. So where the median is located within a bin is really decided by how many scores (when accounting for their weight as well) are on either side of the given bin. But the scheduled update from the bottom up refresh means that.. well, how will the median be places after that?.. ..The same way.. ..But I think that's also fine, 'casue the bins are also all the smaller near the most frequent score point(s). So okay.. (11:25) ..But the refresh from the bottom process then means that this is also true even if the most frequent point changes, when we then refresh.. Okay, I think this is the plan.. (11:27)
%..Well, it does almost sound easier to just not update so often, at least for large score sets, and then just update from the bottom up.. And for small score sets.. No, that's not necessarily easier if we only have \emph{one} shared users-at-the-end index.. Hm.. (11:30) ..We are also saving a lot of space, and trouble, without the RecentUserScores table.. ..Yeah, and how long does it take to.. Well.. ..No, I think I'm actually gonna end up choosing just to do it all from the bottom up, but I'm not sure so let's see.. (11:34)
%..Hm, each qual-subj pair will probably only get some hundreds of scores normally.. ..Oh, maybe the users-at-the-end indexes could be nice to have anyway at some point in the future, where the score_weight_exp is then the user-determined weight times the user's weight within the user group. And if we have these indexes, then we can absolutely just update from the bottom up, I think, 'casue then we don't even have to worry about spam (from a developers perspective, 'casue that is then the job of the user group itself to handle).. (11:39) ..And then the thing is: For small score sets, the computation from the bottom is easy, and for large set, the urgency to update frequently is just all the less. So yeah, I think I'm nearing a conclusion here.. (11:40)
%..Oh! And the user group \emph{can} just make the update from another users-at-the-end index (and in particular the all-inclusive one), as long as it just holds all the groups members.. Oh, then we have a trade off.. well, unless we just make sure to.. Nah, let's just always store the user-determined weight alone, and then the procedure just needs to make a lookup for.. Well, nah, having the weight within the user group right there close to the end of the index could also be useful.. (11:45) ..Sure, but let's also store the user-determined weight, oh, not least becasue it's also handy to be able to browse both things, and not the product. And then, yeah, smaller user groups can just make use of larger user groups lists when updating their histogram. And I should say: Getting the users-(and-score-and-weights-)at-the-end list is of course something that each user group.. well, the users of the user group (or others), rather, need to request from the DB in order to get it. (11:48) ..Great..

%(12:37) We can remove.. Hm, if we reomve the big users-at-the-end index and just use the user--qual--subj--rest index instead.. Well, then we don't need these.. But we also kind of want those.. (users-at-the-end user group indexes).. ..And it will also mean that we can't use the above procedure for generating the hitograms/score curves..
%..Hm, I think user groups should only aggregate public scores; the possibility to choose only some part of the overall user community is redundant anyway. You can do that for entities, but not for scores.. ..So (semi-)private scores are only used for things like inboxes and user preferences, and such..
%..So we could make a new users-at-the-end table, and then update the one for the all-inclusive user group on every new score.. ..(12:48) Ah, yeah, and then the only entity list restrictions that we'll use is the user--qual ones (none for the aggregating user groups, which always use public scores only anyway).. (12:49)

%(12:54) Oh, I also ought to reconsider score_weight_exp; is this really what I want, and is it the only thing: Don't I want the score_width_exp as well?.. ...(13:15) Hm, we could still use almost the same procedure for generating the histograms when including the score_err/width, only where we then.. well.. ..Ah, I guess where we iterate a second time afterwards to subtract from each current bin and add the subtracted amount to the neighboring bins.. (13:17) ..And then we also have the option of aggregating the most frequent spot, namely by taking the tallest bin, and then perhaps even going further in and doing the same process over for that bin, if we want more precision.. ..But yeah, anyway, my older system with those agreement then also becomes a possibility, which is good, 'casue the did make very good sense; I only dismissed them becasue it seems a bit to complicated when compared with the benefits.. (13:20) ..But then the user_weight_exp now seems redundant, and it is also much harder to explain than the error/width, so I guess we should drop that.. (13:22) ..Yes, for sure.. (13:22)

%(13:43) Oh, the recency scores. Hm, I guess we use a *(rounded down.. and also nullable) timstamp for all UserScores, that can't be queried by itself, btw, and then just.. ..just.. ..well, just aggregate these directly from the GroupedUserScores table instead of the histogram.. (13:46) ..Not the the Grouped.. well.. should we put the timestamp on GroupedUserScores instead, or no?.. (13:48) ..It's nullbable, and this table is much compressed anyway.. (i.e. it will be)..(?) (13:49) ..Oh, and the recency could also be a factor for other kinds of aggregates in the future, i.e. where one might then include it when generating a new kind of histogram. Okay, let's just put the rounded timestamp (nullable) on the GroupedUserScores table.. (13:52)

%... (15:07) Okay, I've actually decided that I should remove the current PK index of the UserScores, and then make the UNIQUE index the primary one. And then I'll instead make a separate table for score-sorted user entity lists, which is then only updated by request (and can be deleted). Unlike the AggregatedFloatingPointScores.. hm, that's actually a better name.. this table will then not get the user--qual--subj index, 'cause this already sits on the UserScores table. This little maneuver will then save a lot of unused data per entity on average: If an entity generally has around 100 or 1000 scores, or more, namely from all the different kinds of relevant qualities, then we will save hundreds or thousands of bytes per entity this way. ..Well, we could also say hypothetically a billion users times a thousand scores.. Hm, so only perhaps up to a some terabytes.. But that's also.. well.. Hm.. (15:13) ..Hm, well, whatever we do, we can just hide it under the DB layer's interface.. (15:16)

%(15:21) Oh, we could perhaps have non-public user scores that can then only be aggregated once there is enough changes.. And in particular, the user group weights.. Ah, Or we could just add some randomness!.. ..Some randomness whenever we create a new histogram.. ..And then keep all user scores priivate, unless the user requests to get a public entity list.. perhaps even where the user can filter the public list.. Okay, this might be very important.. (15:24) ..So both the UserScores table and GroupedUserScores, with their datetimes (or dates \emph{and} times, btw..), are kept completely private.. ..And the users then only see the histograms and the AggregatedScores, which all has to contain some randomness, plus whatever we need for the votes, and any other type of score aggregate that we might add in the future.. (15:27) ..Well, and then we also have some semi-private lists used for inboxes.. And the user can also query their own UserScores.. (15:29) ..Oh! But how do people then.. Ah, damn, how do we get to trust a user, and how do we.. Well, things like ML and such can still just happen under the hood.. Oh wait!.. What about being able to.. well, to first of all have the user group weights be private, potentially, and then.. Or maybe semi-public, but where we add some randomness to them.. And then they can be updated by voting to reduce users that has scores such and such, and reduce certain ML PCs, not least. Hm, this would actually make generating and governing a user group much easier in general, regardless of whether the weights, or at least the reasons for the weights, are private.. Okay, very interesting thoughts here.. (15:33)
%..It will be easier for the user groups, but of course also harder to implement (but possible worth it).. ..Hm, it would entail requests to reduce the weight of certain bins/intervals on any given histogram.. ..And requests to reduce a given vector, combined of potentially several PCs, once we get to doing ML.. ..And maybe that's really what we need, and not really anything else at the beginning..(!..) (15:37)
%..Hm, and it still doesn't exclude the possibility to use those private profiles, exactly like I have described them (and also where the "inherrited weight" is only guiding, but can also be changed by the user group, for any one of its private profiles (which all "belong" to a specific user group (but other user groups are also free to use them (by including them)))).. (15:40) ..So people can potentially start with just the "public" profiles, where the rate all kinds of stuff, privately, but then they can also, and should (despite the randomness; 'casue it can only do so much: at some point a profiles scores will in principle be traceble back to it.. well, at least if we make the approximate weights public.. but perhaps only so.. hm..).. (15:42) ..they can also make a private profile, or several (which they also should), was what I was about to say (write).. ..Ah, maybe the users are responsible to update their weights themselves, like I have thought about before, and then if it's a long time since the last update, other users might then just be suspicious about whether it holds..
%Hm, okay it seems quite cool, but it also seems like a lot of work, especially when users could just make sure from the start to make several private profiles, and make sure to only use each one for a specific topic, with not much overlap with each other.. ..Well, then again, reducing the scores of certian intervals does not seem very hard.. ..(and boosting scores, for that matter).. ..But then you also have entity contributions.. oh, and money(/ad-viewing) contributions, and also the public user weights, potentially from other user groups, as well as the time since the user last updated them.. ..Then again, you shouldn't really be boosted nor reduced for any entities you produce, in a fair user group.. ..Only how you rate these entities, e.g. in terms of agreement, if it is a statement.. ..Oh, and we can still.. well, we \emph{must} still use scores on user lists as well, and these are just public no matter what, anyway; nothing that the user can do about that.. ..And then in terms of money (including ad-viewing) contributions, we can just have an administrator or bot that makes sure to score the users, if they accept it, acoording to their contributions (perhaps in different times as well (meaning that the given quality could be period-specific)).. (15:54) ..Hm, this actually sounds quite awesome.. (15:54) ..Ah, and the public weights are just implemented as entity lists as well, where the users on the list are scored according to their weight w.r.t. the user group in question. And then the users just requests to be scored, by a bot, in order to get on that list. And what about the dates for when a given weight was last updated?.. ..Oh, should the lists just also be period-specific?.. (15:57) ..Yeah, that's the simple and natural solution to that.. Okay..:).. (16:00)

%..And we \emph{can} also implement requests (at some point) to lower a user's score based on them being the (publicly known) creator of a given entity.. (16:04)
%...(16:40) Oh, and we can make requests to boost individual users, which can then serve as a sorta crude initial way to get some FOAF-distributed weights..

%(16:44) Oh, I can't put the timestamp just on the GroupedUserScores table. So no, it needs to be part of the UserScores table..

%(16:56) An approximate distribution curve/histogram over all the user weights in a user group might be queryable, where users that have not requested their weight to be public for the given period are also still included in the aggregated histogram/curve.. (16:58)

%(19:17) It's quite a change.. So no more 'open data,' as it were. But we can still extract all the non-sensitive aggregates from the DB, as well as all data that the users make public, and it's still done in a way such that no one is more privileged to see some data data aggregates that others aren't: You can't buy the users data. Everyone can see the same aggregates. And the backend does not look at any of the private data as well, but treat it as sensitive information just like passwords. (And we will also even encrypt the user scores at some point, in case of being hacked.) (19:21) ..So yeah, a shift, but it will still achieve the same thing. And it's nice that we don't \emph{have} to school the users in how the have to protect themselves, i.e. by making several private.. no 'anonymous' profiles, rather. Instead they can also just use a single profile, like they are used to everywhere else, probably, and then that's also fine security for them, at least for the vast majority, I think. So yeah, a big, but also great, shift.. (19:24) ..Oh, and it also helps to protect the SRC from others just copying its data. So the SRC can still be open source, but with partly closed data. That's very good, actually.. (19:25)

%(21:37) Something else that's really good about all this is that it actually makes the selling point about not needing to "pay" with sensitive data in order to get personalized feeds (and search result) easier to explain, and probably also more convincing..
		%-With this system, you are in control of the algorithm, and exactly what you feed it, and what you don't feed it. And also, you get to shift the parameters of the algorithm yourself. And you \emph{don't} need to feed the algorithm any data before being able to select these parameters. (And your selected parameters won't be logged anywhere in the backend, at least not in any accessible state; the user might store preferences in an encrypted way, encypted with the user's own key, or the preferences might be stored in localStorage, only accessible by the user's browser. And the backend doesn't record anything about the user searches, or what they view, or any other detail.) The user is in control.

%And another little thing: I'm also glad that this change solves the '(user_id, qual_id) vs. list_id' puzzle. Now we can just use the former for the UserScores (which are private), and the latter, i.e. a list_id, for the publicized user scores, which can just be included as part of the AggregatedFloatingPointScores table (or whatever we end up calling it). (21:47)

%(16.12.24, 11:04) Okay, these changes mean a lot of things to figure out. But I might have found a solution now. First of all, something very important. It shouldn't be a request to a given user group to create an anonymous profile. Users should instead just be able to create anonymous profiles as much as they want, with the exception that they also have to choose some areas of interest/concern for each profile, and they can't choose the same ones for different profiles; an area of concern can only be used by one profile at a time. It is then the user's own job to make sure to restrict the profile to the given areas, and if they don't, the various user groups might down-rate their trust in the profile. Also, like I've described before, a profile can request to "inherrit" a user weight from any given profile, and I actually think that they should just be able to request this at any time, and to do it for any nmber of user groups that they want. These weights are then not actually transferred, but they are shown for the profile, and can thus be used as a shortcut to (re-)gain trust for the given profile. (11:10)
%..That was the first thing, and the second thing is that the user, for each of their profiles, can give a list of user groups that are allowed to aggregate any of the profile's private scores.. ..And I guess also to be able to down-rate users without the members knowing which of the other members they downrate.. I'm talking here about those requests to downgrade, or boost, the weights of users on certain intervals, w.r.t. some scale.. (11:14) ..Oh, this should by the way also be possible for recency histograms, such that a user group can also crack down on users who tries to recency-spam something not very useful.. (11:16) ..But I hope that all this can be something to postpone to a later stage.. (11:17) ..Well, some of it won't be too hard to implement, and then we can just expand it more at later stages.. (11:18)

%(11:21) I should also mention, the user needs to log in each time they switch profiles, but the browser can of course store the passwords. And the browser can also store the various profile IDs in localStorage, which can then be updated on a new device, or for an erased local storage, by loggin in to one's main account (whichever one one want, and possibly more than one), where a user-encrypted entity with all the profile IDs can be fetched (and decrypted on the client side). ..The user can also come up with aliases for the profiles, if they don't want their user names to be viewable if someone else sees them (over the shoulder) browse their profiles.. Sure, why not.. (11:25)

%..But back to figuring out what to do about aggregating private scores, and such.. ..Well the interval down-rate request should actually be a feature regardless, at least at some point.. (11:29) ..But to be honest.. if we just make this multiple-profiles thing a really central thing, and also make sure to help users remain within the profile's area of concern, and also help the user groups note when going beyond it (or even better, just not aggregate scores not within one of the profile's areas of concern), then isn't this better than anything else we can achieve with private-but-aggregatable scores..? (11:35)

%..Well, wrong areas of concern can just be detected if we show the areas of concern at the end of a score--user list (coming from a "users-at-the-end index"), not by including these as part of the index, but just by fetching them for each individual user when shown as an entity element of such a list.. ..And the areas of concern can be shown everywhere where the user entity (i.e. the user profile) is shown (together with the username, and potentially a user icon/picture).. (11:41) ..Yes, and that is enough of that, at least the early stage..

%..Oh, but I do like if the users can send requests to downrate (or boost, for that matter) users anonymously..

%(11:56) Oh, the user community should then rate the overlap of different 'areas of concern' (which are entities), and a user profile should then be able to request a certificate of how much their areas of concern overlap with their other profiles (which again, the DB can check by recieving some keys and/or encrypted data, then varify it, and finally deleting all these keys/data again (treating it all as highly sensitive data just like e.g. passwords)), similarly for how they can get ("inherrit") the user group weight certificates. (12:00)

%..Okay, this system really beats the private-but-aggregatable scores. I still need to figure out about those requests and such. But I can potentially let go of implementing private scores.. (12:02)

%(12:12) Oh! You just.. well, you just.. use an anonymous profile with a certificate "from" the given user group that you are interested in. Then with that you upload a request/argument to an entity list concerning the user, namely one with 'arguments about the user's trustworthiness,' and then other members of the given user group can just browse that entity list, see the argument, and see how they are rated by profiles with certificates from the user group, or from other trustworthy user groups. Oh, and you then also uprate agreement for the given argument that you have just.. Well, I guess the list is sorted by.. Nah, we have both a relevancy score and an agreement score, just like for any other Set, and the anonymous user can then choose to score either of these. (And they can also even choose to make the argument entity completely anonymously, i.e. with no creator_id.)

%Okay, there we are..:) Let me think for a bit more, but I kinda think that I'm getting there.. (12:19) ..Yeah, no, I think I can actually call this a plan..:).. (12:21)

%(12:58) Oh, we could actually use a signed TINYINT for score_weight_exp, and then simply let the weight be given by w = 1.2^{score_weight_exp}.. ..I think I will do this.

%..Hm for score_err_exp, we can use 1.1^{score_err_exp} \times |score_val|, that should be more than enough range.. ..Well, maybe  1.2^{score_err_exp} as well.. ..Sure, then they follow the same scheme. Okay.. (13:12)

%(13:30) We can also just represent datetimes as floats for the recency scores. (Then these floats just increase and increase (linearly) as the time goes on.)

%(13:34) Oh, I \emph{can} use that trick of saying that an entity list is viewable by a given user iff that user can view the given List entity.. ..Hm, and then the UserScores table.. ..Nah, I have a bit of a puzzle on my hands here indeed, so let me think.. ..Or maybe I don't: just use a private user scores table for (semi-)private scores..(?) (13:38) ..And I can even go bak to my previous UserScores table, also with the full "users-at-the-end index".. ..Ah, but maybe I can still drop the previous PK, i.e. the score-ordered entity list (subject-at-the-end) index.. ..Yes..

%(13:57) Hm, I don't think GroupedUserScores will be useful before a very "late" stage anyway, so let me just remove it (out-comment it) for now..
%..Great, that leaves me with only a public and a private user scores table, a histogram table, and an aggregated scores table, where only the first and last of these table has two indexes rather than one, namely the first and the last one. (14:01)

%(14:05) I've now made a PrivateUserScores table, with a user_whitelist_id column/field, where they point is that all users on the user whitelist (with scores above 0 on the list, of course) can add scores to the list, and delete from it.. Hm, and what happens when a user is kicked out of a whitelist?.. Can we find a way to delete the scores automatically?.. ..Well, yes, we can make a procedure for this, but how do we call it?.. ..Ah, I guess we just set a request automatically (with a reducable countdown and all that) each time a user queries from the private list.. (14:09)

%(20:52) Oh, the GroupedUserScores will be used for browsing also, so I think I should in-comment the table again..

%(20:57) Oh, I'm gonna use scheduled events for the requets. (I think I kinda forgot that they don't have to be recurring..) ..And I think I can ALTER them.. ..Ah, but I can also use tables at the same time to store data, and there we go, then we can do everything that we want.. (20:59)
%..Oh, and I should make the time to execute the event dependendt on how big the task is. In particular, for qual--subj pair with not very many scores, the events to update things like the histograms and such should take a short while to get triggered.. (21:01)

%(21:25) Hm, I think I should let relevancy be a more fundamental part of the lists, not a filtered list. And I think I should just always use the median.. or the mean.. for relevancy.. Hm, but couldn't a user group.. oh right, a user group should be able to use another user groups relevancy lists.. Okay, things to think about (tomorrow, most likely).. (21:27)

%(17.12.24, 11:12) Okay, I don't think I should in-comment GroupedUserScores. And the first step on the ladder then has to be that a score-inserting/updating user requests updates for the given qual--subj pair for a number of user groups that the user likes.. ..Then when updating, they first make the full histogram for that qual--subj pair. And what happens from there.. ..I guess they also update some specific AggregatedFloatingPointScores lists for the quality, updating only the score for that particular subject, and maybe we just update one such.. well, both the median and the maximum point could be good to update. So those two lists are updated immediately.. ..But we are missing out on the relevancy filtering here, so could it be, that the domain of the quality is parsed before updating said lists, and then the relevancy quality of that domain is checked first, and only if the subject is on that (with a score above 0, and potentially also a weight above some.. yeah.. some threshold as well), then the lists get updated.. (11:21) ..Hm, so I guess I should make Qualities be 'c' entities as well.. ..And if the quality itself is a relevancy quality, then we just skip any such check, and always update the lists.. (11:23) ..Hm, that's almost it, it seems, except that a user group should also be able to use another user group for relevancy scores.. ..But that should just be taken from the top of an entList of 'relevancy-determining user groups' by the given user group. And if none is there, then the user group itself is.. well, or we could default to the basic, all-inclusive user group.. ..Hm, or to the best 'relevancy-determining user group' \emph{determined} by the basic, all-inclusive user group, if we want to be more advanced.. (11:31) ..Let me just set the default to the basic, "full" user group at the beginning, and then I can always change it to something else (i.e. the backend developers control this). But if the given user group has rated a 'relevancy-determining user group' above some weight threshold, most importantly, and also above some low score threshold (perhaps as low as 0), then they will use that 'relevancy-determining user group' instead. (11:42)
%..(11:43) Hm, maybe I'll just use the maximum point, instead of the median, as the defualt thing that is updated an used, by default, in the beginning. Then the scores will function exactly like I've described before about my "agreement curves".. ..And the users can think of the score sigmas both as uncertainties and/or as 'agreement curves,' they are both good interpretations of the sigma.. (11:47)

%..(11:47) Okay, that turned out to be much easier than I feared, it seems. Now I have a plan for the basic layer of the scores, i.e. before we get into compound qualities and all that, but that doesn't seem like it will be any harder.. (11:49)

%..(11:57) Oh, and I then actually have to make the standard Qualities always take an Object--Relation pair, and not let them take just the class.. ..Yeah, I think that's best..

%(12:06) Hm, should we just make the user group (to update) list be part of the score insert proc?.. ..Why not..(?) ..Well, then again, maybe it's better with a request proc.. ..And one where there's just one user group per request..

%..Hm, I think I'll get some sun while it is there.. And then when I come back, I will start implementing those request procs and all that.. (12:10)

%... (13:50) Oh, managing all the different kinds of requests requires some planning, though, since I need to figure out a good way of doing this, that can be reused for future kinds of requests as well.. Well, unless I just want to make a data table for each new kind of request, why not.. Yeah, that sounds better.. ..Yeah, good.. ..Well, unless.. (13:53) ..Yeah, maybe I want to add them to the same table, with the countdown, have a common continuous event for counting each one down, and the a common request initiator procedure is then called afterwards, which branches according to 'request_type,' which could then just be the first column in the pending requests table.. (13:56) ..Yeah, that's better, 'cause then all the handling can be done in the same way, and by the same few procedures and scheduled events, and then each new request type just needs to be included in the switch-case(-like) statement, and you need a procedure for accepting and inserting the request. And come to think of it, any request that needs a special data table can still have that. Then we just insert a key in the PendingRequests table instead, and then go and get the data with that key when it is carried out.. (14:00) Okay, so this is the plan, yes.. And then I need to figure out: Do we just always insert a foreign key in the PendingRequests table.. Well, yes, 'casue if some kinds of requests can use the same data table, well, then that's just fine. Okay.:) So I have a plan for this now..:) (14:01)

%(14:12) Wait, I don't necessarily want the request handling to be forced to run in sequence always.. ..Hm, I could use columns of scheduled_at and delay_time instead of countdown, and then just use the scheduled event from there.. (14:19) ..Yes, for sure..

%(14:23) Hm, but if I just use a VARBINARY instead, and use a single data table.. Which can then just become part of the Pending/ScheduledRequests table, then it might be easier to handle.. ..Sure that actually seems better.. (14:25)

%(15:12) Hm, I can't make an event for each request, since I can only use name identifiers for the event names, which are their primary keys, it seems.. ...Just tested it: You can't create event names from functions or system variables.. ..(15:27) Hm, but we could make each event lock a certain interval on the (req_type, req_data) index, and then immediately set another event to take care of the next interval, which are then free to run before the current one is finished.. Hm, yeah, I like that.. (15:29) ..And we can even just count a number of requests to handle at a time.. ..Wait, B-Trees don't keep track of the row count??.. (15:31) ..Hm, I guess to make concurrent reads and writes easier.. ..Well, I could then try to use a AUTO_INCREMENT PK for the Requests table.. ..Sure I could do that.. (15:40) ..And make the (req_type, req_data) index the secondary one.. ..Yes.. (15:42) ..Ah, and let's divide the table into "pages," then, i.e. in PK intervals of a certain size, and then we'll make $n$ different scheduled events, where the $i$th event, i \in \{0, \ldots, n - 1\}, starts from the smallest PK entry, then finds the first interval, m, starting from the interval that this low PK is on, for which m \% n = i. Then the event runs on that. And when it's done, it doesn't start over immediately, but goes instead to the (m + n)th interval, and do its work there. And only when the (m + n \times j)th interval is empty, where j is the number of these sub-iterations, \emph{then} it starts over. And if ever the first interval of the sub-iteration is empty, then it halts.. Well, no.. Then it just waits a short while, and then it restarts itself again.. ..Well, we could also.. Nah.. ..Nah, they just keeps on running; they will do so eventually anyway, once we get enough users.. (15:54)
%...(16:05) Oh, it's easier to start from the greatest PK, though..
%..(16:06) Oh, but maybe we also want to layer them in terms of how far down.. ..Nah, never mind..

%(16:16) Hm, I just thought of an alternative: We could also let the PK be the time to execute.. Well, combind with the unique (req_type, req_data) index. So a (time_to_exec, req_type, req_data) PK.. And then we could let the "pages" from before just by of size 1, meaning that each of the "threads" will just.. Hm, then they have to scan.. Oh, but that's totally fine.. ..The idea is then to let a seperate event/thread be the one to scan and reduce the time_to_exec. And this then also don't need to scan the whole way every time.. Well.. Well, maybe one thread/event could update the soon-to-be-executed requests, and another one could update all the rest (so splitting the full time interval in two parts, a near end and a far end).. So two threads/events to update the time_to_exec, and then $n$ threads/events that only runs up to.. yeah, probably and even lower time, so like a "near near end" interval as well, and where each of the n threads then scans and only take.. Oh, but how do they agree on the offset, then?.. (16:24) ..Wait, they don't, they just always takes the first one on the list, and then removes it straight away. Oh, this also means that requests won't be waiting unnecessarily. THat's actually very nice. Okay, so I like this plan better. Only thing is that it does require a lot of quick deletions, in a table with two indexes.. (16:26) ..Well, they need to be deleted sooner or later.. ..But one could also mark for deletion, and then have a "near near end" thread that both updates \emph{and} deletes requests that are marked as done.. That sounds more pragmatic somehow, but I'm not completely sure whay.. (16:29) ..Well, it would only have to delete, I guess.. ..Nah, it shouldn't be better; I think the n threads should just delete when they come to a new entry.. ..Hm, and they should require a FOR UPDATE lock on it first, actually, but then I need for the other threads (events) to be able to skip if locked.. ..Well, I could then just mark as being executed, which is also being marked for deletion, and then the other threads are free to read them once the lock is released, and can then just skip the enty, and try to get a lock on the next one.. ..The thing is, the locks should be released very quickly, since all the holding thread needs to do with the lock is to mark it as to-be-deleted, before moving on to a much heavier computation. Okay, but what should the "garbage collector" thread then do (as to not interfere with the "worker threads".. ..yeah, I guess we can call them that)..? (16:38) ..Oh, nothing speacial: MySQL will never select a deleted row after waiting for a lock. So all the "garbage collector" thread needs to do is to.. Wait, but then we \emph{can} just delete right away.. ..Yeah, we can.. ..And exactly because B-Trees don't keep a count of the rows in each node of the tree, deletion is quick, even when there are two indexes. So yeah, well, just let the "worker threads" select for update, and then delete, after which any waiting thread can continue (simply by always selecting the entry with the lowest time to exec.). Good..:) (16:44)
%..(16:46) Oh, and if that weird thing where the procedures seem to dead-lock when selecting an interval, rather than a specific key.. if I'm correct, which I very well might not be.. if that happens, then I could just update the first entry.. Well, no, not when (req_type, req_data) is part of the PK as well. I was thinking something about reducing the first PK down to 0, before then selecting for update, but I'll cross that potential deadlock bridge if and when I get to it.. (16:50) *(It would be better to just select and select again, then.. (18:18))

%(17:25) Maybe I should limit the worker threads (and delay.. ..oh, let me get back to this thought..) to less than the number of CPU kernels, so that some are always ready to receive new requests, inserts and queries.. (But yeah, I'll just see about that later on, but for now, let me just use two or three threads in total..)
%But my other thought was then this: We don't need the other threads, other than the "worker threads" if we instead just let the delay_time float (for which I was using a float anyway) increase as time goes on. Then instead of setting the delay_time of a new request to some constant initDelayTime (which can depend on the request type, though, and some other parameters as well, actually), we just set it to MIN(delay_time) + initDelayTime instead (and then just rename "delay time" to something more appropriate). Et voila, then we only need our workers.. (17:31)

%(18.12.24, 10:33) I should replace scoreSigma(/Error)Exp in PublicUserScores with scoreWidth again (or scoreRadius). So a uniform distribution rather than a Gaussian one. And then I'll display the score as '<scoreLo>--<scoreHi>' rather than '<scoreVal>\pm<scoreRadius>'. And then there will be three main types of aggregates, I think: Median without accounting for the scoreRadius/Width, median when accounting for it, and then also the type.. well, the maximum of the histogram, but only where we then do account for the scoreWidth. (10:38) ..And for relevancy scores, we'll just use the.. Hm, I should say first: This also means two types of histograms (which the users can also query for).. ..And let's indeed just use the simple one, i.e. without accounting for the width, when it comes to relevancy scores. And then for all other lists, it's the users (or the app's default choice) wh decide.. (10:40)

%..I'm also thinking about a weekly request parameter, just like the weekly download and upload limit, and I'm thinking that maybe this parameter/counter could increase proportional to the timeReductionOnAdvancement float.. (10:42) ..But there should probably also be an increase in the upload counter for any new list..

%And this then brings me to something I have left to figure out: How do we decide when to delete aggregate lists again?.. ..I was thinking a bit about a LRU (least recently used) list.. ..Hm, constant lists should however just be a matter of upload data, I think; let us perhaps not delete any of those in the early stage..(?) (10:47) ..Nah, I think that should eventually be up to voting, among the money-contributing users, which lists to keep.. ..Ah, and we should use "points" for this.. ..If not "votes," but "points" might be more practical.. (10:50) ..And for the lists in general, we could make a table with a counter on it for each list, that counts the number of queries.. ..Hm, and maybe just count a year at a time..? ..Starting from the creation date, which is of course also recorded.. (10:53) ..And then at each month, we might look at all lists that are a whole number of years old, and then decide whether some of them should be slashed.. (10:55) ..Yes, I think that should be the plan for now..

%And in terms of (update) request and upload counter, the counter should just increase for each uploaded score, and.. Well, and the update request counter always just increases with timeReductionOnAdvancement.. ..And then perhaps we should use a fourth counter for creating (and updating?) compound aggregates..? ..(11:02) Hm, or maybe the first one to create a request should just have their upload counter increased.. ..Oh, or we could have a separate.. ..Hm.. ..Hm, maybe we could just increase the counter after the update.. ..Wait, the taxing part is just to get the histograms for each (requested) userGroup--qual--subj triple, well, and what about the userGroup--qual entity lists?.. ..Is just adding some bytes on top of that for each userGroup--qual--subj update request.. (11:08) Hm.. ...(11:49) Okay, first of all, each new lists should count as upload data, and it's just the user who first requested it that "pays" it.. Oh, and for non-relevancy lists we actually know the length of the list beforehand. Nice.. ..Well, and for all others, we'll know it at the end, and we can just remove the computed list right away if the user exceeds the upload limit, and then also mark the user.. well, also increase the upload counter still, even though it gets thrown away.. .."All others," these are just the relevancy lists, I should remember that.. ..Oh but this means that we ought to count the length of a relevancy list before updating it, which we don't want to do, unless we want to make users pay the full amount for each update request.. Well, it upditing the list does mean that the list stays longer in storage, if it was on its way out.. But still, updating an existing list should not count as much as creating it..(?) (11:57) ..Oh, you are just updating one subj at a time!.. (11:58) Great so the upload data spent is constant for each userGroup--qual--subj update, and the you just also update the update request counter.. Which we could also call something like 'computation counter'.. (12:00) ..Great, and for the.. wait.. In order to know the length of the relevancy list, without having to COUNT(), we need to store this length in a table.. ..So a table of entity list lengths, that doesn't sound to bad.. ..Oh, I just realized: Maybe I should remove the offset for the list query procs, of this is requires scanning through each element before it, which it does. Alright.. (12:02) ..And then for any compound list, we can look in this EntityListLengths table in order to increase, and check, the upload and the computation counter, before the request scheduled. And for relevancy list updates, we also know these counts beforehand, as I just "said," and can therefore also increase and check the counters first. (12:05)

%(13:00) Shall I drop the computation counter..? ..Nah, 'cause maybe not, 'cause otherwise a few users would potentially be able to dictate the order of the scheduled requests.. (13:02)
%...(13:30) Ah, but maybe the download data counter should go, since.. Oh, right! Not only does checking it all the time cause extra disk reads for each query, but the users aren't even supposed to provide their userID when querying! So yeah, no download counter after all..

%(17:08) Oh, maybe I do want to drop the computation weight counter, and then just also drop the whole timeReductionOnAdvancement thing..(?) ..Yeah, 'cause for the long computations, which will.. ..Well, I also need to consider long aggregates.. Well, but also here, the initDelayTime (or just 'delay time') should just depend on the length of the list (which is known beforehand). So yeah, let us just drop the thing about advancing a request (and just be fine with some requests taking some time to be carried out).. (17:13)

%(19.12.24, 10:32) We should just store a (salted) hash of each user's e-mail joined (with some delimiter) with the username. And then the user can type in the user name and their e-mail to request a password reset, and if it matches the hash, then the control server sends a mail, a few seconds later to the e-amil address, and genereates a URL that resets the password and navigates the user to a 'type in the new password site' (and sends some keys that the browser can use for the 'set new password' request). The user who types in the username an e-mail are always given the same reply back, and at the same speed: "An e-mail will be sent to your address" ("if you typed it in correctly," implicitly)..

%Now, something more important: This might be what I said previously as well, but then let me repeat: The 'area of concern' correlations are only.. Oh wait, the certificate can just state which user group was used to check that there were no correlations/overlaps. Okay. But that was a side note. The 'area of concern' correlations/overlaps are then only checked when requesting a user group weight certificate, and only between the two profiles. When you request a certificate, you do it from the requesting profile, and is then navigated to a page where you type in the credentials of the target profile (from which to "inherrit" a weight), and then the backend takes care of the rest from there.. oh, and you also state the mentioned user group, along with the target profile.. and then at last the backend forgets the request again, and the connection between the two profiles, and only keeps the certificate. (10:44)

%..I can also limit the number of profiles that you can have per e-mail address in the beginning, namely by having a table that connects the aforementioned hash to a number. Then at a later stage, this number could turn into a number with a significant random error on it (meaning that the exact cap will effectively vary from person to person, or rather from e-mail to e-mail). And then at some point, we might just remove this table altogether.. (10:47)

%..(10:49) Hm, I could make a subset of CSS that only has static features (no reactive features (reacting to user input, or any thing else), and no animations). And of course also without any urls; only with a few select number of functions. And then we could use 'style-determining user groups' to get our style for the website.. ..And let's also keep login forms, and all such things, on a seperate page that is not affected by this custom CSS, just for good measure.. 
%..So we'll have 'relevancy-determining user groups,' 'style-determining,' and what was the third thing I had in mind..? (10:53) ..Oh, but before I talk about that, I should also say, the browser could then just store the CSS sheet in local storage, such that it can apply it right away when the user visits the site. And else the query just happens pretty early I guess.. well, maybe that's just better then; to just make the query at the beginning (while the page is loading). Okay, let me do that.. (10:55)
%..Oh, we also had 'area of concern overlap-determining'.. (10:56) ..And there'll probably be many more. (10:56)

%(11:10) Wait, what was I talking about that the "upload data spent is constant for each userGroup--qual--subj update"??.. It depends of the number of users who has rated the qual--subj.. So what, do we.. ..Ah, the \emph{upload data} is constant, yes, but the problem is, I need to set a delayTime.. Okay, so there's something to solve here.. (11:13) ..We could of course count the users that have scored any given qual--subj pair.. ..Wait, we could also say that the first step is for someone to request the user-at-the-end-for-a-given-user-group list, or that that always comes as part of making a histogram.. ..Wait, the user could request to be on that.. ..Hm.. ..I need to buy some food, so let me take a thinking (and shopping) break... (11:22) ..(11:24) Hm, should only have the one users-at-the-end index, unless a (small) user group request a list that is a filtered version of that user list.. ...(12:05) Oh, maybe there could be something to making a users-at-the-end index (there have to be a term for that..) for each individual user group.. ..'Cause we do very much need for users to query those lists exactly.. Now, they could of course just filter them client-side, but.. ..Oh, but it's also a good thing to have for constructing user lists in general, which can be used for new user groups.. (12:09) ..Hm, nah, I don't think it should be a requirement at least: We should be able to just update a qual--subj for a given user group from the full users-at-the-end index.. Hm, but then again, these user lists are deletable.. Hm.. ..Hm, could we do something where a user group can request to be verified as a subset of another..?.. (12:14) ..And then an update request could come with a superset as well, from which to generate the histogram. And then the users-at-the-end superset list just has to have been updated recently, or else the request fails.. (12:16) Hm, that could be a potential solution, let's see..
%..Hm, we could then implement the superset bools as an entity list, where positive scores means that the subject list is a superset, or subset, of the object.. ..Come to think of it, we can also in principle implement the EntityListMetadata, i.e. lengths and query count, via two entity lists. But no, let's keep using a table for that.. (12:24) ..(12:30) Okay, whether I'll use a table or not, this is not very important. The important thing is actually just to know that this can be implemented in the future, in a backwards compatible way, and then I can just use the full users-at-the-end index/list until then.. Hm, but maybe it should be a list then, or rather, a list for each qual--subj pair.. That seems to be the case.. (12:33) ..Okay, I think I have a plan here, then.. ..Yeah, and with a user list for each qual-subj pair, we also can also store the length of each such list, and then we can get our delayTime from that.. (12:37)
%So let me try to sum that up: We use lists for each qual--subj--user group triple in principle, but initially we only do this for the basic, all-inclusive user group (which must therefore be filtered client-side when wanting to see it for a smaller user group). And at some point in the future, we'll also implement requests to verify a user group as a super/subset of another. And then we'll also implement shortcut requests for calculating the histogram (I mean, if this is deemed worth the trouble..) where a superset of the given user group can be used instead.. Well, you can use the user group itself, or any of its supersets, including of course the full, basic user group. Then that user group's users-at-the-end list just have been updated recently for the given qual--subj pair that you are interested in. Okay, I think this plan works (and again, we might also just decide that this is not worth the effort, potentially)..:) (12:42)

%(12:56) Hm, but what about just adding to existing histograms, and then just generate the histogram from the bottom from time to time, at a much less frequent rate?.. ..That would seemingly save a lot of computation.. (12:57) ..And again, it just means that we add.. ..Hm, could be just a tinyint or a short, and certainly is we use probabilistic addition; then we could just have a weightExp at each bin.. Well, why not just store the full FLOAT.. I think I will.. ..Ah, and the histogram data can actually store the length, instead of using another lookup to the EntityListMetadata table (Formerly just called EntityListLengths, but I added queries_this_quarter to it).. (13:01) ..Wait, the whole thing was that when the list is long, you also don't need many updates.. well, unless the user group is small, and also if there's spam.. ..Oh, but this would require the whole RecentScores table..

%(13:13) Hm, we could do something where a score-inserting/updating user needs to attach a list of user groups, and then the histograms are only immediately updated for those user groups, and the rest will just have to wait until they are reformed from the bottom. Hm, that actually sounds like a good idea..
%..Well, maybe there is also something smarter we can do, maybe something where a user has to choose a "patron" user group for storing their user score.. Hm.. (13:19) ..Oh, it also be some kind of.. user group family.. Hm.. ..Hm, I'm just brainstorming here, but I'm thinking very big user groups, where maybe the smaller ones then has to choose one to be a subset of, and then they can only use scores from users of the big user group.. ..And the big user groups are then meant as a means to prevent spam.. ..So "user group family" is not the right terms, as it is not the right way of thinking of it.. (13:23) ..It's just that each user group needs to choose a superset, except of course the all-inclusive one.. (13:24) ..Yeah, now we're talking..
%..And I guess they can change their superset group whenever they want, be uprating the favorite one semantically, and then in the beginning, we might just.. yeah, before we implement this, we just use the default, which is of course the basic, all-inclusive user group. Okay, really great..! (13:20)
%...(13:35) Hm, and I think that the users then do indeed request a specific (super/patron) user group to "take" their given score, but a user is then free to upload the same score, or different ones for that matter, to several different "patron" user groups.. Hm, what should we actually call them, btw..? ..Hm, maybe we don't need to call them anything special; they are just user groups, like any other: A user group can choose any other user group as its superset/supergroup, and a user can choose any user group, in priciple, to "take" their scores. Okay.. (13:40) ..(And a user group just "takes" a score automatically when the given user has a positive weight in the group, and it is always the user themselves that pays the upload data cost..) (13:41)

%(15:12) A user should not fear losing data for being kicked out of a user group. So we store the user's public scores and a single-index table, and then I guess we ought to contruct a List entity for each user_group--qual--subj triplet.. ..And the request is then to copy your newly created/updated score onto these user lists..

%(15:24) Oh, since using an offset for queries is potentially heavy, how do I implement the histogram generation in a way that I can be sure is efficient?.. ..Hm, do I really pass it to the control server..? ..Oh, there's 'cursors'.. What did they do exactly..? (15:34) ..It does what I need them to do; it means that I can iterate through a table in a for loop.. (15:35)

%(15:47) Hm, I'm considering making a list_id for all these table, and then actually join them by making the score a VARBINARY instead.. Only thing is, VARBINARY(255) might be too little for the histograms.. Hm, but what about just two aggregate scores tables, then?.. ..Hm, although, the length bytes will be compressed anyway.. (15:49)

%(16:57 *(that's wrong..)) Or maybe we could go in the other direction, and make a separate table for each type of aggregate lists, and then let it be up to the users to use the right listIDs.. ..Hm, but then we should.. ah, also make sure to denote the list type explicitly for compund lists.. Well, no, it can still be parsed by the.. Well, we don't need to care about the length of a request, really, so yeah, maybe the request could just point to the table in which to find the given entity list.. ..Oh, one could even reuse the same List entity for several kinds of aggregates, meaning that their can be several versions of the same List.. Hm, that's maybe not bad.. (16:54) ..as it can then hide it at the top layer whether we tend to use meadian whith or without score_width, or if we use the maximum point, or something else (like the mean, e.g.).. The List entity will then still look the same, even if we change the convention for which of these types of aggregates is the standard/default one.. (16:56) ..Okay, I think I like this a lot.. (16:58)

%(20.12.24, 9:50) Wait, that all didn't make sense.. damn.. I still need to parse the List entities, since they still need to have the right parameters within a given score aggregate table.. ..But still, I don't need to parse the functions if we instead.. use identifiers, wait, why is this a good idea again?.. ..(9:56) I guess it isn't.. Shouldn't we just have a one-to-one correspondence with a list function and an aggregate table?.. ..Btw, since functions are 'f' entities, I can just insert them right away with '@[...]' plaeholders, and with initial_admin as the creator, and then just substitute at a later time. ..Yeah, and then I don't need a whole bunch of similar tables.. ..Hm, and I think the query procs should just.. ..Yeah, maybe one extra read is better than transferring more data, I think so.. So we'll just query for and parse the listID's defStr for all entity list or score queries.. (10:03)

%..Oh, why don't we use named function declarations (definitions) instead of anonymous ones for the 'f' entities, so that we can then render the function entity using its name?. Let's do that..

%(10:31) Hm, I was thinking: I probably need that advances 'c' query proc, but now I'm also thinking, maybe we should use higher-order functions as well. In particular, the histogram function could be a.. No, never mind, the median function just knows which kind of histogram to use.. ..Well, or maybe not. Maybe that histogram function input is what decides whether the scoreWidth is used or not.. (10:34) ..Hm, I don't hate it..

%(10:42) Maybe I should use more regular function call syntax as well.. ..Well, maybe not quite.. ..No, let me keep it as it were, and then use those param numbers (right after the function) for the advaced 'c' query proc.. (10:44)

%..If a functions parameters aren't typed, it means that the user is not supposed to construct these themselves, really, but rather the app is supposed to take care of it.. ..Well, might as well give them types anyway..

%(11:19) I'm just gonna let the standard qualities be 'a' entities, and then parse the domain via a RegEx match of '"Domain":"@<ID>"'. ..Oh, no, I mean: I should parse Object and Relation this way..

%(11:35) Oh, maybe my plan for the recency scores is now a bad idea, given that the histograms will be updated less and less frequently. So I should instead add a request to increase a recency score, and I guess then just automatically send this request when scoring.. Well, but maybe we want to send it for the Object and/or Subject, and not for the Quality--Subject pair.. ..And I can then let that be a thing for frontend; as long as I just implement this 'boost recency score' request, then it should all be good.. ..And then the user/app should provide a user group along with these requests, which will then yield the weight to use.. (11:39) ..But let me make the fundamental requests to get the median lists first..

%(12:00) Oh, this means that I can remove the modified_at columns, right?.. ..Yes..:)

%(12:04) Hm, shouldn't User group also just be 'a' entities, and then we can also just parse e.g. the Cutoff, or the Superset, if these are indeed some of the attributes from that?. I think so..

%(12:06) Hm, shouldn't I actually make a new table for user groups..? ..Where the List pointed to by list_id then determines the user list it is based on, as well as the cutoff.. ..Ah, and maybe the superset as well, namely via some kind of filter() call..(?) (12:08) ...(12:19) Oh, when I'm using that ScoreContributors (formerly GroupedUserScores) table, then we don't need a Superset/Supergroup attribute. The filtering can just be.. Right, users are only inserted if they have a positive score anyway..
%..And yeah, I think I ought to make a.. ..wait.. (12:22) ..Hm, no, we don't necessarily need a table for the user groups.. ..We can just parse the (optional) Offset/Cutoff attribute and use that.. ..So user groups are 'a' entites with the attributes: 'Name,' 'List,' and 'Cutoff'/'Offset'.. ..And User contributor lists are 'c' entities with a User group and a Quality.. ..And then we need to parse the Cutoff from the User group, and the Object and Relation from the Quality.. Oh, as well as whether or not.. well, if Relation is the 'Relevancy' relation, then we handle it differently.. ..Ah, but we also need the 'Relevancy-determining user group' in that case.. ..Well, no, in the opposite case we need it: when the Quality is a Relevancy quality.. (12:38) ..Sure, fine. (12:41) Let me take a walk first, and then qet to work on the request to be marked as a score contributor...

%... (14:03) If the procedure can't parse a 'User list' attribute from the input user group's defStr, then we just assume that the Group is recorded in FloatScoreAggregates instead, and look there.

%I will implement recency scores via the private user scores, where the top entities on this list will be the ones to boost in a request. And we can btw also implement 'entities that the user has seen recently' with a PrivateUserScores list as well, which can be used to make feeds more fresh each time (if one wants that). (14:06)

%(14:37) Åh, det så åndsvagt tidligt at blive ramt af træthed.. Men jeg sov godt nok heller ikke ret meget i nat.. ..Jeg er altså ikke på mit højeste i den her tid.. Selvom jeg godt nok har fået rigtig mange gode idéer til gode ændringer og løsninger i de her dage, så alligevel.. ..Føler mig virkeligt ikke på mit højeste, nej.. Håber januar bliver bedre (ift. daglige arbejdstimer (og søvn) især). Og ellers kommer foråret..

%(17:06) Hm, maybe I should make a list for each user group, even if it means copying data.. ..Otherwise I have to parse each time we check if a user is part of a user group.. ..Hm, plus maybe it's a good idea to update the user list first, and then only update the (dynamic) user group some time after, which gives the users time to react on changes.. (17:12) ..Oh, and we should actually also use TINYINTs rather than FLOATs, so there's also that. Okay, let me make a special table for user groups (or user group weight, if one will).. (17:13) ..Wait user weight exp assumes.. Well, of course: we don't have to include the users with 0 score or less on these user group lists. So we just tranform any positive score into a weightExp (approximately so).. (17:17) ..Oh, and since we'll have the underlying list, maybe we don't need the sec. index for this UserWeights table.. I mean, unless we might want to look at e.g. 'Money-contributing users' sorted according to score. But I could of course also just implement that via a FloatScore list.. ..Yeah, or some other way. I need to store the values more precisely anyway, so yeah, no sec. index, and only the userGroup--user PK, for the UserWeights table.. (17:22)

%(21.12.24, 9:05) Okay, slept well tonight, and now I have several good ideas/solutions: The medians() function should indeed take functions as input, in particular a histogram function. And it should actually also take the interval limits for the histogram as well (meaning that the histogram functions should also take those inputs). And then, importantly, the medians() function, as well as its "singlings" (similar aggregate functions), should also take an optional input (yes, we can have optional inputs.. ..oh, and we could even let 'null' represent.. well, we'll see..) that is the list with which to filter the subjects in the histogram in the first place. (And this input propagates all the way through to the ScoreContributors table (right?..)..) (9:10) ..(Yes, it does..) So we actually do let the relevancy score filtering be a responsibility of the app layer after all. And it's only a good thing that the users will be exposed (potentially) to all these parameters, if they investigate the given List entity. 'Cause this might only inspire them to get more knowledge about it all, which might inspire them to invent new types of lists. (9:13)

%I think I will actually also include a tinyint public user scores table, even though the float scores will be compressed.. And then if I do, then I will use that table for 'is interesting/useful' up/down-rates. I will then make arrow-up/down buttons on each entity element in a list where users can press these. These will then be used instead of my "points," I think.. And for recency..? ..The thing about using the private scores..? (9:16) ..Or just implement it solely via requests where.. ..No, it should actually be public scores (when it comes to recency/freshness scores, not when it comes to viewed entities, as I talked (wrote) about here yesterday).. ..Okay, I'll just see about that.

%Another thing is, I should bring both the weekly download counter, and the computation counter back. And the download count will then just only get increased when the user queries 'as' themselves (i.e. query reqTypes ending in 'AsUser'..). And then the idea is that the control server, at least at some point, will get a temporary(!) log over IP addresses and their (temporary) download counts. The control server can then delay or deny IP addresses that have queried for too much data lately, when the servers are stressed. And when it denies a request, it sends back an appropriate response, which the app can then react on and try to query 'as the user' instead. The control server IP "firewall" (not sure if that term applies fully here *(maybe it does; it's just an advanced (and dynamic) type of firewall, isn't it..)) should then make sure to let all '...AsUser' requests go through, and these are then only denied if the users download counter is too great (which is checked inside the database, by the given query proc). Isn't that a good plan?. I think it sounds good.. (9:28)

%(9:37) Wait, it doesn't make sense to have optional input in the middle of a function. It makes sense for parsing the input fields for an entity submission form, but it doesn't make sense for the function itself.. Unless we can optionally name the input in a function call.. ..Wait, or use the 'null' input. Okay great.. By the way, I have considered whether string input (which might be useful at some point (or it might not)) need to be wrapped in '""' for function calls, but yes, they do.. Well, the typedness (and not least the lack of whitespace in the language) actually means that we could omit them. But let's just not, especially if this 'null' input will be important, which it seems that it will. But I'll still keep omitting the '"'s for the input names.. (9:42)

%..Oh, the lower and the upper bound should also propagate right down to the score contributors list, or what?.. (9:44) ..Ah, no, 'cause it can serve as a way of removing outliers for unbounded qualities. And in that case, it's still nice to keep the outlier data point at the lowest level. Okay..

%(9:52) I should also mention: I intend to just save the histogram bin limits as complete floating point number literals.. Hm, I should think about that some more. But then I've also had the idea to actually let the hist_data of the hist_wid() histograms start with the ID of the corresponding hist_cen() histogram.. ..Hm, let me actually make those function names more verbose: There's no real need for them to be short.. And then following that ID is then just a list of all the (altered) bin heights. (9:56)

%..(10:02) Oh, if we only compress the.. hm, the histogram tables.. ..But then I should also make sure to round the interval limits somehow.. ah, I can just look at the interval widths, and then round the floats to not be too precise compared to that. And then when we compress the table, is shouldn't be too much difference whether we store as compressed binary representation of the float or if we store them as literals, right?.. (10:05) ..Well, it could mean a byte or two for each number, but not even necessarily (for some hist_data values, it might be the same). Okay, so that's the plan.:) (10:07) ..I'll keep it as a VARBINARY, just in case, just like I also do for the req_data in the ScheduledRequests table: just in case.. (10:08)

%(10:21) Oh, I should also make sure to go over the structure of the Qualities again, just so that I have a complete picture.. ..Right, and in particular, should we still use the Metrics.. ..Well, we might, and then the app can just parse the upper and lower bounds from that.. ..Yes, so I keep that exactly like it was, only where the Metrics now don't have to specify the bin width.. ..Yeah, no, they don't (I'll stick to my current plan for the histograms (as it is a very good plan)).. (10:27)

%Oh, I should probably gather computation_usage and download_data still, since that's really the problem with downloading anyway: the resources it takes.. (10:50)

%(10:54) Hm, and I don't want any timeReductionOnAdvancement?.. ..Nah, let me just keep it like this, at least for the time being..

%(11:01) Wait, should we actually just handle when to request a histogram or a list update in the app?.. Then the app could maybe keep track of the user's upload and comp counters.. oh, and maybe roll a "dice" to see if the user should make the request, and where the roll is lees likely to succeed if the counters are high already.. And then the histograms and list should also just include data about when they were last updated.. (11:03) ..In the case of the histograms, we could.. Nah, it could just be a separate column in the HistogramData table. And for lists, this should of course be in the ListMetadata table.. Okay, I actually like the sound of this idea.. (11:04)
%..Oh, I should by the way split the userID input of requestUserGroupScoreUpdate() into the requesting user and the target user, such that other users also can make the request.. Yeah, and especially since the user might not have the particular user group among their own favorites, but other users might want to have the user as part of that group. Okay.. (11:07)
%..And yeah, let it be up to the app when to request histogram and list updates..:) (11:07)

%..Oh, and some requests should just run straight away on acceptance, such as the requestUserGroupScoreUpdate() request.. (11:09)

%(11:16) Wait, why even queue requests now?. Why don't we just always run them straightaway now?.. ..Yeah, no point in delaying now; that was only meant as a way not to make a lot of redundant updates. But now the app and the.. Well, okay, one potantial reason could be as a way of preventing DOS, either purposefully, or in principle also accidentally, if a lot of clients' apps roll to request an update simultaneously. But then why don't we just put a limit on the period (in terms of shortness) after which the same update request can be made again?. .. (11:20) ..Sure, let's do that instead, indeed.. (11:21) ..So a goodbye to my plan for these scheduled event "worker threads," and all that.. We can pick that up again if it ever becomes useful at some point (but I don't see how it would become that (but what do I know..)).. (11:23)

%(14:26) Hm, I don't like to make an PK with floats in it for the ScoreHistograms table.. ..But it seems that I have to.. ..Oh, I could also "cast" to CHAR, and thus use float literals instead.. (14:32) ..Ah, or I could just create the histID.. which I alrady need for the center histogram, since the width histograms use those IDs.. (14:33) ..And then we can also use this histID in the (coming) histData query proc. Okay, I'll do this.. 'Cause I kinda have to because of these defining bounds, so yeah.. (14:35)

%(14:48) Oh, I guess I don't need to count the queries_this_quarter, then, now that the app is just responsible for help keeping alive the queried list, like I've described. Good, 'cause that's a whole write per query that we get rid of.

%(15:14) Hm, a hist function could also just take the scoreContr listID, and then just the two bounds..(?) ..That sounds easier and better, yeah.. (15:16) ..Or no? What should I do..? (15:20) ..Is it not better to keep it unpacked/exploded?.. (15:21) ..Yeah, maybe so, 'cause otherwise it will require two queries to get a histogram. Okay.. (15:21)

%(15:25) Hm, could I make the bin widths depend on the number of users rather than the weight.. oh, I can't make it precise anyway when relying on weights. Okay, so I'll maybe drop the idea of not storing the bin heights. Then I'll first get the bin user number from deviding the list_len, and then I can just iterate through the list once, while constructing the string one bin at a time, and with no editing needed afterwards.. ..Unless I want to make it so the the histograms using widths do not depend on the score center ones.. Maybe I do.. And maybe I actually don't really care to make or use the 'histograms of score centers' in the early stage.. (15:30) ..Oh, wait, I do need to loop through the list twice for the with-widths histograms.. So maybe I should keep the simpler score centers histograms as well.. ..Sure.. (15:31)
%..So hist_data will in both cases be a list of: weightSum, binEnd, weightSum, binEnd, ..., weightSum, and then you put the lower and upper bounds at the beginning and end, respectively, in order to get the full histogram data set.. (15:35)

%(16:36) Ah, no, we should just make the PK of the Histogram table (list_id, hist_fun_id, lower_bound_literal, upper_bound_literal) instead. And then we don't actually need the Histogram entities for anything, not for querying for histogram data either. But we of course still need the histogram function, anyway (and the user \emph{can} choose to make use of the Histogram entities if they want).

%(19:23) Let me just format the hist_data in the same way for both types of histograms, though, such that the width-using one isn't dependent on the other (in terms of the API).. (19:24)

%(22.12.24, 11:12) Hm, even though my idea for measuring the bin limts has some cool an interesting qualities to it, I think, it might still be better, actually, to just let the users+app decide the bin width instead.. (We are also ready letting the users+app decide the lower and upper bounds..) ..One argument for doing this is that it also allows us to construct the with-widths histogram from only one scan.. ..And it also becomes easier to read/understand visually when looking at a histogram, when the bins are of the same width (such that the height and the area is proportional).. (11:16) ..And actually, all we need to do is to pu the Bin width attribute back on the Metrics.. (11:18) ..Okay, so all in all: yeah, let me make my two histogram functions here use fixed bin widths instead (and then we can always implement other histogram functions in the future..).. Hm, should I then.. Ah, I could let the ScoreHistograms table inclde a data format identifier, right before the hist_data. ..Well, or right after the hist function.. ..(11:23) Hm, I guess I ought to treat the bin width like the lower and upper bounds, but I also don't want to fix the histograms to needing a bin width.. ..Wait, couldn't the hist function just take a Metric instead..? (11:24) ..Hm, and the Metrics are 'a' entitites, so we can parse the bounds and the bin width pretty easily, without requiring a lot of restrictions on the Metrics that are valid for this request.. (11:27) ..Okay, I like it. Let me do that.. (11:28)

%(11:42) Well, but take the percentage metric: Here we would for a lot of things want the first and the last bin to be much smaller (we want more fidelity here).. ..Ah, and in terms of showing the histogram, we can just label the intervals, and then transform the width such that they are all displayed with the same width, i.e. in terms of pixels on the screen, not in terms of width on the x-axis.. (11:46) ..Hm, could the metrics then just hold an optional array of bin limits, instead of the bin width..? ..Oh, maybe we want to label the intervals as '<x_1>--<x_2> <unit>' and center them right underneath each bin, by the way.. ..Then the reader/looker is more natural to get the the bins aren't of equal width in terms of the given unit (I think).. (11:50) ..Hm, we could do that bin limit array thing.. ..But isn't it better to "just" use my idea where we measure these limits instead..? (11:52) ..Hm, I could, btw, also scan twice, and then use the user weight rather than the user number.. ..Or I could make a finer histogram at first, and then combine bins afterwards to make them as equal in weight as possible.. (11:56) ..Oh, it won't be very visually.. good.. to get a lot of histograms where the bins all have almost the same height, and the labels are what to look out for.. ..Then I perhaps like the bin limit array idea better.. (11:58) ..(12:00) Oh, if we wanted to be more fancy.. Ah, the limits should be literals, which means that they are actually strings.. Oh, but the don't have to be literals anymore now, I think.. ..No, so we could say that a string in the.. mixed.. array.. means.. Hm, or we could just make it an array of dyadic arrays, namely of bin width and repeatFor numbers.. ..(12:05) Hm, couldn't I also just make a fine histogram on the first scan, and then just join all bins that are low (in height/area), and who has a neighbor that is also low enough to "want" to join..? That actually sounds like it might be a good idea.. (12:07) ..Hm, and then we could perhaps let the height rather than the area represent the combined weight in the given bin.. ..And then draw percentages on the y-axis.. (12:09) ..I think this might be the idea..
%..Then we can't do the width-using histogram on just one scan (as it seems), but that's also alright.. (12:12) ..Wait, that's not true, since the bin widths of the finer histogram is fixed..! Oh, and this bin width can just be given by a 'Minimum bin width' attribute (rather than a 'Bin width' one) of the Metric. Okay, I think I have it here.:).. (12:14)

%(12:22) Ah, no, we draw weight on the y-axis, of course.

%... (13:37) We can also have an optional Maximum bin width attribute, and then I'll just set 5 \% of the full interval length as the default value. (Having a maximun width means that we won't get a lot of broad bins of the same height.)

%(14:27) Hm, I could start using base 64 encoding for entIDs.. ..I think I will.. ..Well, or maybe not.. ..Hm, to do or not to do.. ...(15:17) I actually feel like no: Let's just keep the IDs as base 10 numbers..

%(8:40) Okay, I've figured out what to do with the histograms. My other idea wouldn't really work after all: to complicated (and questionable) to construct, and bad to render as well. Now, one could then instead just use the idea of having arrays with bin width and repitition numbers. But that's also not vry good. But now I have it: The metric should just contain two bin sizes, one for an outer histogram, and one for an inner one, which is done by splitting.. Oh, I guess we need to take three neghboring bins at once.. and then split that into a finer resolution. The idea is then to take the bin where the median lies, and then split that, and its two neighbors.. No wait, we don't have to use the same bin limits. So we just take a certain distance from the median, also defined by the metric, along with the two bin widths, and then make the inner histogram inside this inner interval. The idea is then that the user needs to click on the bin where the median lies in order to see this finer histogram.. Oh, but then we could also just make it two independent histograms, and then just specify the parameters in the request, and in the parameters to the histogram function.
%But there's something else as well: I actually probably don't want to store these histograms for every scalar. The thing is: A lot of scores will be given to commeents, where there are only a few scores on avagerage per comment. So no, I think I should actually scrap this histogram.. well, not all of it, 'cause we still need to construct them when calculating the width-using medians. But they will be thrown away, unless otherwise requested. And in the early stage, I might just not store them at all.
%I also don't think I will use the score center histograms at all, after all. The thing is: When scoring something, the user just always selects the lower and upper bound on their scores, as two independent (in principle) numbers, rather than selecting a midpoint, and then some "error" afterwards. I will then make two slider buttons, if I make such sliders, and then have it so that one follows the other.. no, that doesn't make sense. No, but we can say that.. ..Let's just make sure that they the lower one doesn't move past the upper one, and lest perhaps try to make it so that the right edge of the left (lower bound) button is where the value is at, and then the same for the left edge of the right (upper bound) button.. That could work. And until I implement all that, we of course also just have an input field, where the user can type in the two values.

%Now, there's then also a question of: Do I then even keep the histogram functions, or do we just say that if we want to make other types of histograms in the future.. Oh, I'm already giving the Metric as input.. Well, hm, I don't know.. (8:59) ..(9:06) I think I can forget pretty much all about histograms for the early stage, expect the the Metrics will still specify the step size / bin size.
%Hm, and if not for the score widths, we could even make the.. well, no.. ..Hm, we could store the combined weight in the ListMetadata table, but then we would need constant user groups.

%This also reminds me of: User groups should just.. Ah, they should have a "moderator group," which is a user group from which you query about any updates to the user group. More precisely, you query the group for the top User group entity on a '<Current user group>->Updates' list. And if there is an update, the app is supposed to store this information and start using that group instead whenever it would have otherwise used the old one (for queries).. (9:12) ..So similar to how a user group can choose its 'relevancy-determining user grooup,' it can also choose its 'moderator group,' or at least it can in its beginning; it's not certain that the app will query for updates on the moderator group, namely since this effectively removes power again from the moderators. Yeah, so I guess the moderator group actually has to be given in the definition (defStr) of the User group entity instead, or rather I think that that's better.. (9:16)

%..Hm, this means that we \emph{could} just make it so that all user group entities defines a constant user group, i.e. a time slice of the given user group (or "user group family," if you will.. but no, let's just call them several versions/editions/time slices of the same user group..).. ..And then we.. Ah, but that means that you are also supposed to update each score.. Oh, well, you are anyway.. ..And a user groups "version" can just make sure to point to its previous one, such that the app can just query that (or even an earlier one still) if the data is missing.. Hm, I actually don't think I like this.. I think it's better that the user groups can be dynamic.. (9:21) ..Yeah..

%..Ah, but we can still in principle store the combined weight for each score contributor list; we just need to update it on all updates of the list, including when a request is made to update the contributors' weights on the list. ..And then we \emph{can} find the median with just a single small select statement (summing the weights), but unfortuneatly not for the width-using medians, right?.. (9:26) ..Wait, we can also just store the median in the metadata table..!.. Hm.. (9:26) ..But again only if we are not using widths.. Ah, although we could approximate the median this way still, and then just add a reuest to compute it from the buttom up again, if anybody fears that it might have drifted too far from the actual value.. Ah, and we can even store the suspected error as well, such that users can see that, or rather their app can, and then it can roll to see if it wants to request a bottom-up update..(!) (9:30) Okay, so it seems that we can get rid of these table scans in the day-to-day requests, and then only rarely do such scans for from-the-bottom updates..! (9:31)

%..And then I \emph{can} actually indeed pretty much forget all about histograms (not the lower and upper bounds for the Metrics, though; these are still important) for the early stage..(!):) (9:33)

%(24.12.24, 13:17) It of course doesn't make sense, the thing I was talking about, not needing to scan at all. For you would then at least need to measure some kind of density, and that's compilated. So no, I should make the histograms, at least if I want to keep using the score widths (which I think I do), and at least as part of the computation, then (but I don't necessarily have/want to store them)..

%(14:31) Okay, I think I might make the initial, higher-res. histogram in the way where I put the bin limits according to either user number or weight.. Well, why not weight.. ..Yes, sure.. ..And when there are few enough users, this then just means that they get a bin each, essentially.. Oh, maybe I'll make gabs between the bins as well (where there are no users). Sure, I think that's best (for when there are few users especially).. (14:34) ..And then I use the score width to transform the distribution, and then we finally gather into a larger histogram with equal.. Well, unless we just want to return the median instead, of course.. ..But no, I actually think that we might want to make the histograms, and if no one queries for them, then we just delete them again (while possibly keeping the median). I think this might be tha plan..
%Hm, do I then keep the hist functions, or do I still let.. Yeah, let's say instead that the Metric is supposed to store the information, if you want to aggregate a different estimator (than this score-width-using median) at some point (and then I can just use that estimator everywhere for the early stage).. (14:38)

%..Okay, but how do I gather the histograms? Do I still.. ..Ah, wait, no: I don't construct or store any histogram, other than the "initial, higher-res" one for the early stage, as I also won't be implementing rendering of the histograms anyway. So the users just see the median scores, and then they can inspect the contributor list, if they want to see what's underneath that.. Hm, and is this enough?.. ..I shouldn't also just make those histograms?.. ..Nah, maybe the app can just fetch the whole list and draw a curve.. Well.. Hm.. ..Hm, or I could gather a low-res histgram right after the median is calculated, and then store the median together with the low-res hist_data, and possibly also the highest bin.. ..in what is now the ScoreHistograms table, and then make.. another request to take the median and.. Well, no: The relevant score median entity list should be updated automatically in the same process.. (14:47) ..Hm, let's maybe join the ListMetadata table and the ScoreHistograms table, then..(?) ..And then also let it include the time of update.. ..Well, then again, maybe we should keep them separate. But this is not a big thing. The point is just that I actually do compute these rough histograms as well when computing the median.. at least if the 'requestingUser' requests so.. ..(14:53) Hm, no, I think we do want to see those (low-res) histograms, when we see the score display in the entity lists on the app, if not at the early early stage, then soon after. So let me just always compute that low-res one while I have the fine-res one in memory (when computing the (width-using) median). Okay, good! (14:55)

%(27.12.24, 9:26) I've gotten a great idea yesterday. A much better idea than to take the maximum point of the histogram is to instead just record both ends of the score-with-a-width for each contributing user and then take the median of the whole thing. Then the estimator won't be changed by any new score that has the previous value of the estomator inside the interval (of the score-with-a-width). But whenever the previous value is outside the score(-with-a-width) interval, the new score will always pull the value towards it. And this was not the case for the maximum point / "agreement curve" idea. There you could have three islands of scores in the full distribution, where the third one in one end of the scale might win completely over the two others, even though that this third island of scores only contains little over a third of the combined weight. And this is not desireable. So this new idea is much better---and it's also easier to understand, furthermore. And I would even say that it's also better than using the width-using median, like I've thought about until now. So that's probably it: I will record both the minimum and the maximum of each user's score interval, then pile all these together and take the median of that.

%I think I will also actually just store it as two separate FLOATs.. I think that when we compress it, that this will only cost us about one byte in most cases. So yeah, that's what I'll do. And then I think: Let's order the contributor lists by the score minimums first.

%I will then not make any histograms as part of computing the median. 'Cause I will instead just make sure that the som of the weight is updated in the ListMetadata table. And for the computation, I will just construct a list of the maximum values along the.. Oh no, we should instead just store the lowest.. Ah, no, I have to record them in a new list as I scan the scores, indeed, and then keep a variable with the lowest one above the current scan index. Then we compare on each iteraction, and if the lowest maximum interval value is less than the next interval minimum, we use and remove from the constructed (partial) list maximum values instead, and update the variable to the new lowest maximum value on the list. We then do this until we have reached half of the combined weight, and then choose a value midway (or adjusted according to how close we are to the exact half of the weight sum) between the two scores that is neighbors to the median. (9:45)

%..Another very important thing: I should make a request to take all the users from one contributor list (often one where the user group is a superset of the target user group in practice) and query for and add their scores to another contributor list with a target user group. This will be very important to update user groups, where a lot of the users themselves don't request and update when they submit scores. So yeah: quite important, actually.

%Oh, and about histograms, then will then just be separate requests, i.e. to make the DB construct the histogram from a contributor list. And the requesting user just determines the parameters (but in practise this is almost always determined by the given Metric). And for these histograms, we actually just always construct two histograms for each contributor list: one of the score interval minima, and one for the score maxima. And then we always just serve this two-fold histogram. The app can then decide when to show one or the other, or when to combine them, possibly by making each bin consist of two colors, e.g. red and blue, where the red part of the column might be on top of the blue, e.g. (or the other way around), and where one color then represents the score minima and the other represents the maxima. (9:54)

%Now, in terms of handling stressed servers, I think the best solution is still not to make those worker threads, but instead just have some kind of initial control server / advanced firewall that stores all incoming requests (including queries, inserts, and update requests), as well as the IPs where they come from, and then decides the order in which they are sent to the control server, and also whether to sometimes return an error that the request was dropped due to stress. But I should read about DoS-prevention, and how it's normally done, and then see if I can get something like this (in a way that hopefully doesn't require too much effort (hopefully something like this is already normal to have (and it wouldn't surprise me at all if it is))).. (9:59)

%(16:07) I think I \emph{will} make those wroker thread scheduled events after all, and then just only use them for update requests with more than just a few reads and writes. And then we can also at some point make that WAF (firewall) thing that I was talking about (perhaps where we just program the whole thing ourselves), but this can then be at a much later point, when the system is actually in any risk of being targeted in some DoS attack.

%(28.12.24, 11:14) Quickly about (D)DoS attacks, we can also have different servers for different types of requests, and then just have more servers for the query requests in general. Not that this means that we won't have to implement any WAF at some point, but still worth noting.

%When querying for an entity list (or a specific score) from a certain user group, the app might then also query for metadata about the list and also about the corresponding list from a superset user group. And if there is much more weight on the superset user group's list, then the app could construct the list itself from.. Well.. ..Nah, there could always just be a tab to see the superset's list instead, next to the current one. And similarly an option to see the superset's score instead of the given score. And when the app then sees.. Well, no the app can then always in general send off a request to update the given subset user group's contributor list and also update the median.. ..(or what we should call our median-of-min-and-max-scores estimator).. subsequently.. ..Maybe there should also be a request to update all entities on the superset group's entity list. Yeah, maybe. And the app can then send these whenever it queries for them. And in the time when the given entity list is still sparse, perhaps because the members of the given user group don't know or don't care that they are members, and therefore don't request updates themselves, then the user can just click to see the superset group's list or score instead.

%Now, as I'm writing this, I recall that there's another idea I also need to mention in order for this to work well: I think I might make it so that the requesting user provides ether a percentage or a maximum upload data and/or maximum computation cost that they are willing to pay, probably the latter thing actually (so not the percantage thing). The idea is then either that the app just stores the sum of all paid costs for a request so far, and then only carry out the request once the sum exceeds the given threshold. Or the app could also just roll a dice for whether to make the update, and then make the probability depend on the paid cost relative to the required cost. In the first solution, we store a lot of current request, and thus need some space for that, but on the other hand, this also allows us to them make updates of requests that hasn't met the threshold if the ressources are available (and in particular computational ressources; if the server is pretty idle anyway, might as well do some those computations, then).. But all in all, the idea to make it so that a requesting user can choose to only pay part of the cost. (11:33)

%..And I think I might store the not-fully-paid requests instead of using randomness, and then I'll just add the percentage of how much has been paid as part of the index, before the delay time (or exec time), such that the worker threads can take from those requests that have been paid first, before they start working on half-paid requests.. Hm, and maybe we should.. Nah, let's perhaps just gather computation cost and upload data cost into one thing here. For the thing about upload data: We can always delete those aggregates again..

%..By the way, if the app then just sends these requests for most things that is has just queried for, then we herby also get a good way to see what data are being queried for, without having to make writes on any of the query requests themselves, as we can just make these writes on the (typically subsequent) update requests instead. (11:39) ..(So instead of keeping track of the query volume directly, we actually keep track of the update request volume, which ought to be roughly proportional to it.)

%(14:55) I should record both the upload data and the computation counter on the EntityListMetadata table, and then indeed.. ..Well, I could perhaps even just gather the ScheduledRequests table with the Metadata table, and then make sure to make the (payment percetage, exec time, rest) index that the update worker threads need. And yeah, the upload data counter doesn't have to always be paid. For as long as we keep track of the full "payment," the lists that got updated before the upload data was paid (when the server was idle, i.e. (and had enough storage space)) is just the first lists that we can slash again when we need some more storage space. With this system, we can then also reduce these "data payment" counters each month or each week, and then when the counter reaches 0, or some value below 0 perhaps, we slash (delete) the given list.

%And with this system, we can now just say that whenever the user queries and whatches an entity list, or they query a specific score, the app sends a small update request for that list or that score (w.r.t. to the given user group that was used for the given query). And that's really it; then we can pretty easily.. Oh, I should also say: There should.. Oh no, I have of course mentioned the request to.. Oh no, right: There should be a request to update a whole list at a time, which will then be the most expensive one among all the ones I have so far. This request implies that the DB goes through each entity on a given entity list. And then, for a given user group, updates each individual entity's score contributor list, as well as the.. ..uniform-error-using median.. for each one, in order to get the full list in the end for the given (smaller) user group.. (15:07) ..Again, this is the most expensive one, but this then just means that the "computation and data cost" will be all the greater. And in the eraly days, it will still be a pretty qiuck computation in most cases (and therefore also not with a very high data--computation cost as well). (15:10)

%And there we are, then all the app needs to do is to send an update request short while after each query, as well as send some appropriate update requests---perhaps---after each score insert. (15:11)

%..Oh, I should also say: The app can reduce the number of update request, and not least the paid data--computation cost on each of those requests, when the user is nearing their weekly limit. (15:13)

%(02.01.25, 12:29) Hm, should I just use FLOATs instead for the user weights..? ..Hm, nah..(?)

%(17:49) Hm, der skal næsten være en justerbar minimumnsvægt på filter lists'ne.. ..(Score-minimumet er altid bare 0, men det går måske ikke bare at sætte en fast minimumns vægtsum på samme måde..) ...(18:17) The filter list's minimum weight should probably be an input as well to the median.. function?.. Hm, but what about the score contributor list..? ..(18:23) Hm, maybe the score contributor list shouldn't be filtered.. well..
%..(18:25) Alternatively, we could divide each standard list into two versions, one with all the subjects with a weightsum above a threshold, and a second one with the rest.. ..Well, or we could just include the threshold next to the filterListID in the List entity, both for the score contr. list and for the median list derived from such..

%..Wait, the weight is a relative quantity. So instead of having 1 just be something like 'the normal weight of a normal member,' or whatever, it could instead be taken to be the exact limit for when an entity is considered of having a large weight sum to be included in the list for all viewers of it.. ..Yes, that's what we will do, except that this threshold doesn't have to be 1.. ..Hm, should we say 100 instead, and then let weights by FLOATs?.. (18:36) ..Ah, we can still use the weight_exp for the ScoreAggregates table. ..Sure, and then we can just round down to 127 if it's above that, and up to -128 if it's below that.. (18:39) ..Okay, I think I'll do this, and indeed use 100 as the.. ..yeah, sure: 100. Then that's the weight sum that a subject need to have before not being filtered out of the list by the app, unless adjusting the shown list to show more examples, and it is also (more importantly) the limit for when the subject as whitelisted when using the List as a filter List. (18:43)
%(19:06) Let's say 10 instead..

%(03.01.25, 8:29) It would be nice to have the ScoreContributors table divided into two: one for the min and one for the max score, at least for the median computation. And it would then also be more of an entity list again, each of these tables/indexes, i.e. a list of scores and subjects. And we could then make query procs for both of these lists, and not least also use them both for combining new lists.. But how do we then make this split exactly, 'cause it would require two list entities, right?.. ...(8:54) Oh, we could then just use the standard score + weight Aggregate table..

%(9:39) Let me call them 'min score lists' and 'max score lists' instead of score contributor lists, and thus say/assume that it's okay to sometimes name lists after the scores rather than the subjects. ..(9:43) Well, yes, except let's perhaps not call it 'min score lists'.. ..Ah: 'user group min/max scores'.. (9:44)

%(16:35) The filter list doesn't have to be a part of the user group min/max score Lists, I don't think. They can just be part of the median function instead.

%(04.01.25, 11:56) I'm btw calling the "user group min and max scores" simply the 'score contribution' instead now (implicitely 'to the given user group'). And then each score contribution has two parts: The 'min score contribution' and the 'max score contribution.'

%... (14:53) Hm, I've wanted to aggregate the number of positive scores.. well, and their weight more importantly.. before, so shouldn't I include these numbers in the ListMetadata table as well?..
%(14:59) Hm, I haven't mentiioned that I considered something about making the recency point decay dependent on some Estimator(-like thing, or a function input..)..
%..Hm, it would be nice to just implement points and recency points via ListMetadata, but..(?) ..Hm, okay, let's do it for the (non-decaying) points at least. And then we could also consider making a special table for recency scores, indexed partly with some Estimator(-like entity) perhaps, where users can then request updates for.. Hm, perhaps via a PrivateScores list, like I've written above (search for 'recency' up from here), but let me see.. (15:06) ..Hm, wait, or do we also rather want to implement the regular points via requests, such that the app can uprate an entity when the user is positively engaging with it somehow..? (15:09) ..Well, yeah, but I guess all points should actually have \emph{some} decay rate. So the "regular points" can just be points with a slow decay rate in reality.. (15:10) ..And then until I implement these (and potantially after as well).. I can use those other points.. ..Yeah, well, these will then be the non-decaying points, which are then not given for "positive engagement," but only for actual user score contributions. And yeah, indeed we can then use these until we implement the decaying 'activity points,' let's call them that instead..:) (15:14)

%...(15:30) Okay, now I know it. The activity points (formerly "recency points") should be implemented via the PrivateUserScores, indeed, such that for each quality, each user can store a list of the entities with which they have recently engaged positively. The DB then, upon request, updates the activity (points) list for the whole user group at once (for a given quality). And it does so by looking at all the members private lists for these qualitites.. Hm, well, then we need a way to make sure that these qualities aren't \emph{too} private ones.. Hm.. (15:35) ..Hm, maybe we could signal this somehow via the user_whitelist_id.. (15:36) ..Hm, perhaps by making it a "bot," but this would be a bit hacky.. ..Then probably better to make a separate table for it.. ..Sure, let's do that.. (15:38) ..'PravateUserActivityPoints,' perhaps.. ..Yeah. And then the point is to make sure to only take up to some number, specified by some Estimator-like entity, from each user. And number will then in practise determine the.. ..the decay rate, although, let's perhaps save some date for each activity point as well.. Hm.. ..Anyway, the idea was that when users overwrite old engaged-with entities on the list with new ones, this would automatically create the decay that we want, with a decay rate determined by the number to include from each user. Oh, and I should say: The DB/algorithm then gathers all these engaged-with entities from the whole user group at once into a single list, where the score on the list then represents how many users (probably weighted) has engaged with a given entity. But of course, as I've realized now, it would be nice to also make sure that inactive users also gets a decay on their contributions, automatically.. (15:46) ..Let's just give PravateUserActivityPoints a DATE column.. Hm, and do we really want it to be private?.. ..Yeah, all in all, yes, but.. ..Wait, "engaged with" you can only do that "pubilcally" (although from an anonymous profile). So yeah, no reason.. well, you might say: 'no real reason to keep it private,' but no, even better: there's no reason to make it public.:) (15:50) ..Oh, except that we probably want to update it by request, and then users could just request entities that they haven't actually engaged with. So maybe we do make it public, but perhaps instead just use those arrow up/down buttons as the only way to signal "positive engagement" with..? (15:52) ..Yeah, and then we could make them public (and queriable)..

%..(15:55) Hm, it \emph{would} be easier if we could just determine a fixed decay rate, and then just use ListMetadata as well for this.. ..It's also quite important that the users can uprate "activity" with some specific Qualities in mind, which makes the up/down arrow buttons an insufficient idea.. (15:58) ..I mean, we are gonna determine the decay rate anyway as developers for most users, since we need most of the users to use the same type of activity points anyway.. ..So I should probably just choose a decay rate, and then if we want more, we could then just add one or two more similar columns for the ListMetadata table.. (16:00) ..Oh, and we could still make it separate requests (such that it doesn't need to interfere with the other (sub-)procs that use the ListMetadata table)! (16:03)

%(I've just gotten a new keyboard, started using it yesterday, and I'm already feeling like I'm slowly catching up to my speed with the old (bad) one (still on the laptop; I have the new keyboard in front). How great. I really look forward to be able to write.. well, to write \emph{not} super slow.. again..!:))

%..(16:09) We could perhaps also just keep one with a quick decay rate on the ListMetadata table, and then keep the other ones in a nother place.. Well, or at least just only update the quick/short one automatically for new List updates.. ..And then the other are updated on requests, specifically when people query for them.. ..Well, but when and how do the update happen then?.. (16:12) ..Ah, it could happen via a scheduled event, that then only runs at fixed points in the day, perhaps only at one specific time, but I guess let's say at only a few fixed times.. ..The requests then schedules the next possible time to make the updata, if it has not already been scheduled. And when the event runs, the short-lived activity points are then transported over to the longer-lived points, contributing to increase the given entities on those lists as well. (16:16)

%..Great.:) (16:19)

%(16:31) I'll probably need a separate table for votes, though, come to think of it. Unless we can implement signals somehow.. well.. ..Hm, maybe that's not too bad of an idea: Adding a signal/flag column to the table.. ..Yeah, 'cause then we don't have to change the DB interface with the app when we implement some new things like that, i.e. where private scores can be aggregated in some special ways. Okay, I might do that.. (16:35) ..And then place it first in the index so that it can be compressed away almost completely.. (16:36) ..(You could put it several places for that, but I want to put it first..) ..Let it be a "list_type_ident CHAR".. (16:40)

%Hm, 'activity points' is then the wrong term.. (16:50)

%(17:24) Hm, pos_score_weight_sum is redundant: just query for the contribution median instead..

%(19:39) Instead of making the big update-all-medians-on-a-list request, we could also just let the app (roll and) send some update requests for individual elements that the user views in the given list.. ..Yeah, this is better, 'casue it also removes the redundancy of paying for the whole list or paying for each element separately. So yes, let's not implement that big one. (19:43)

%(19:51) Ah, about the recency/activity/engagement score/points, we can just multiply all new contributions by some root to the power of the UNIX time stamp.. Well, unless we just want to multiply for some reason, let's see.. ..No, we want an exponential decay of old points, or a exponential growth of new points, if you will. (19:53) ..(19:59) And we can subtract a constant (some time before launch) from this timestamp..
%..(20:03) Oh, but I can't just update this score in _insertUpdateOrDeleteScoreAndWeight(), can I? namely since user's would then just be able to boost it as much as they want. No, I need to do it via requests somehow..(?)
%(20:27) Ha. Just had the idea to implement it via another counter, one that when it runs out means that the user can't request anymore recency score updates until the next interval.. ..That could be a way to do it.. ..But it still shouldn't happen in _insertUpdateOrDeleteScoreAndWeight().. ..It should still be implemented as a request.. ..Hm, and maybe also implemented through FloatScoreAggregates (not FloatScore\emph{AndWeight}Aggregates)?.. (20:32)

%(20:33) Oh, about the exponetnial decay. We could then also have some very infrequent events that goes though all recency scores and divides them by a number, and then (somehow..) also shifts the timestamp offset to a new value, such that the FLOAT exponents don't overflow.. ..Well, maybe it makes more sense to store it at a time-like score, where the closer the time is to the current one, the higher the recency score..

%..(20:39) Wait, and we can't just make period-dependent Qualities?.. ..Ooh, maybe ones that can then only be scored in the given period..!? (20:40) ..Yes.. (20:41) ..I mean, yeah, come on..!.. (20:43) ..Yes. Only thing is: What about positive \emph{and} bad scoring activity? Should polerized engagement not also be able to be tracked and used?. .. (20:45) ..Ah, but that's the weight! Okay, great!.. (20:45) ..Yes! So I should implement those only-in-a-period-active Qualities, then!:) (20:47)

%(21:26) Well, you could also just attach a period to the specific score contributions list, and also for the corresponding score median lists.. ..Hm, or perhaps add a date to the score contribution lists (as nother column)..? ..Hm, I think the period-specific Qualities idea was a better idea.. well, maybe, but maybe not.. ..Yeah, maybe not.. ..Hm, so a FloatScoreAndWeight\emph{AndDate}Aggregates table for the score contributions?.. (21:33) ..Well, yeah, that \emph{would} be the easy solution.. (21:35) ..And then we don't need anymore in the fundamental layer other than that, and then we can get whatever decay rate we want for recency point aggregates, \emph{and} we can also get period-specific score aggregates. (Well, we still need to implement the algorithms and the requests for each new type of aggregate, but that's fine. as they can be made afterwards independently on the other procs.)

%(22:39) The PublicUserScores should then also contain a date, which should be the same one used for the score contributions.

%(05.01.25, 9:48) I'll change "weightExp" to just 'rank'.. ..Well, no, not for the aggregate tables.. (9:53) ..Hm, I almost want to just use FLOATs, but I'm not sure that I have enough reason for it.. ..And the ranks do provide a useful.. well, anonymous profiles can also just subtract and round down.. ..(10:04) Hm, FLOATs is the straightforward and easy to understand way.. And I only save.. well, maybe in most rows I don't even save a single byte *(no, it will always take up at least two bytes, but that's alright, I think.. (12:31)), namely when the exponents are.. well, when they are not all over the place.. ..Well, only if I round down for aggregates.. ..(10:11) Hm, if we round down to two decimals.. No, to.. ..two significant decimals.. ..Then it can be compressed.. ..(10:16) Hm, one could in principle.. store part of the fraction bits off of the score-ordered index.. But then maybe we could also just store the weight off of the index..? ..Or do what i just said..? ..No, let's just round down to two digits.. (10:21) ..always.. (10:24)

%(11:15) Uh, MySQL has some more FLOAT options!.. Let's see.. ..FLOAT(M,D).. ..But they are deprecated..

%... (15:23) Hm, don't we actually want to remove the weights from the normal entity lists?.. And then we could just reintroduce the ScoreContributions table, where the weight and date can then be stored in the score-ordered index.. ..Well, the weights are also nice for checking membership in a user group / whitelist.. ..(15:28) Hm, but we could then also divide each list into two parts: < 10 and \geq 10.. ..Hm, don't we actually wanna do that regardless..!.? ..(15:37) Okay, so let's make two tables for all the regular entity lists, and let's only include the weight in the score-ordered index for the one below 10.. well.. ..(15:41) Well, we could order that table's sec. index by weight instead, actually.. ..(15:43) Ah, but why don't we just make two separate indexes, then, for this table: one for the score and one for the weight?.. ..It would mean more storage space (/uploadData) reuired, but having the weight index for the large weights might also be beneficial, as it could for instance tell us about the activity/engagement.. (15:45) ..Yeah, let's do it that way, also for the sake of combining new lists from previous ones; here it is both beneficial to have the full list of.. Oh wait, the whole point was to filter the < 10 weighted scores out. Let's see.. (15:49) ..Yeah, we do want that.. ..So let's divide into two tables, indeed, and let's index make a weight-ordered index on the <10 one, and then I can decide whether to also make a score-ordered index on that table, but I don't think so.. (15:52)

%..(15:54) Hm, maybe we want to refer to these list differently, then. Maybe we want to refer to them as 'entity lists,' 'score contribution lists,' 'user score lists,' and private score lists,' respectively.. ..I think so.. And then we can always add more, if needed.. (15:56) ..And then the 'entity lists' are the only ones that needs to be as query optimized as they can, and only really the \geq 10 part of them (yes, these come in two parts (< 10 and \geq 10 weight)).. ..Oh, and since the < 10 lists are only temparary stations for the scores in theory, we should maybe not worry about making several useful indexes.. (16:00) ..But then again, I don't think.. Ah, except for combining new lists: there the score-ordered < 10 list might be useful at times.. (16:02)
%..Alternatively we introduce "bots" to aggregate such lists automatically, upon request.. (16:04) ..Hm, yeah, "bots" like that might still become a useful thing.. (i.e. automated "user groups" with a specific purpose, which you can then rely quite heavily on).. (16:05) ..Yeah.. (16:06) *No.. (16:20)

%..(16:08) Hm, would it be crazy to try to gather it all into one table..(?) ..For the public user scores, we could split it into the min and the max score, and then let the "weight" column hold the unix_time.. (16:11) ..Well, we need both for the score contributions.. ..Yeah, nah, forget it.. But it would be nice to.. Hm.. ..Hm, we could also give them list_type_ident's as well.. (16:13) ..Well, implicit ones, which can then be used to specify the table/type in function calls.. (16:14) ..Sure why not.. (16:16)

%(16:20) No, about the "bots," we can just make new list types instead, and then add procedures to create these lists (similarly to how e.g. the score median lists are made, or the score contribution lists)..

%..(16:24) Hm, so do we just say that users need to request some kind of new aggregate entity list, if they want to use e.g. score contributions or user scores to combine new lists?.. ..(16:30) Wait, VIEWs!.. ..Yes! I think I can find a good way to use those!.. (16:32) ..Wait. I thought of some quite complicated way, but maybe there's a much better way to use them: By making a joined table of all the list tables, where we then have a virtual list_type_ident as the first part of the indexes..! Can I do that?.. (16:35) ..I should be able to, yes.. (16:35) ..a union, rather.. ...(16:48) Yeah, I think this is the way. Then for all combination lists, you just make sure to always specify the list type next to the given listID argument (just before or after) such that the DB can just query this view of all list tables directly.

%(17:18) Maybe we should add a score-ordered index for the public user scores.. ..Or not..
%(17:41) Hm, what to do, what to do.. ..Hm, do we indeed just make public user scores into Lists as well, and then perhaps also add a score-ordered index?.. (And then make that combined view..) ...(17:59) Well, or we could just expand the view to potentially include a userID as another one of the first key parts.. ..Hm, I think we should just leave the public user scores out.. (18:01) ...(18:25) No, I think they will be very important for combined lists, actually, since it allows the creater to combine lists via a list of several users, e.g., or several lists.. ..So we add the score-ordered index to the public user scores, and do we even make a List entity for each user score list?.. ..No, it's probably better to just have a big first part of the view index/key.. ..The view only has one index, btw: the score-ordered one.. (and it \emph{can} only have one, I'm pretty sure..) ..Okay, so the view consists of (list_type_ident, userID/whiteListID, listID/qualID, scoreVal, subjID). And for the public user scores, and the score contributions, we just have two different types for whether we mean the min or the max scores. And for the 'entity lists' (unless this is still the general term..) we have a type for the \geq 10 list, score-ordered, and then another type for the < 10 list, weight-ordered (and then if we add a score-ordered index to the latter, we can then also add a third type here).. (18:34)

%..(18:35) Hm, question: Do we then allow for a listID to double as referring to several different lists, potentially, where the provided list type determines which one?.. ..Such that a list can have several versions, for instance the < 10 version and the \geq 10 version, or the min score or the max score version..(?) (18:37) ..Hm, I actually think s.. Ah, wait, how do you then specify which one semantically?. That becomes a bit of a mess.. So I think we should just use both a different type but also a different ID for each one (and where the type can actually also be looked up in the defStr, but the idea is to not make the DB do this..).. (18:39) ...(18:50) Well, you could make the DB do that, and then just use the view afterwards.. That way we remove the redundancy in the function calls, and save some space.. Okay, let's actually do that; that just sounds better, more neat.. (18:51) ..A list ('c') entity should then just always have the list type identifier as the first input.. ..Yes, alright.. ..Ah, and without this, we wouldn't be able to e.g. combine a list of lists into one list, as the DB then indeed needs to then be able to figure out the list ype itself. Okay.:) (18:54)

%(18:55) Oh, but I feel like i need to go over the PrivateUserScores again, now.. ..(18:59) Yeah, maybe we pull the userID up as part of the.. head of the index, just under the whitelist/user group. And then the app must just combine the list from all users, of which there are not supposed to be very many.. well, I can't be sure of that.. ..Hm, do I add another index, then?.. ..Well, it only has one now, which is also actually part of the problem, I feel, so yeah, let's see.. (19:02) ..Yes, okay. So (listType, whitelistID, qualID) will be the first part of both indexes.. And then we add (userID, subjID) to that for the PK, and for the score-ordered index, we add (scoreVal (as a FLOAT), userID, subjID).. Doesn't this work?.. ..Ah, and do we then add two sub-views from this to the view, i.e. one with userID at the end, and one with subjID? I mean, it doesn't cost anything really to do this.. ..(Especially since this view is only used in update requests, not in the regular queries..) (19:11) ..(Yeah, let me stick to that statement..) (19:12)
%..And so the view will consist of (listType, userOrGroupID, listOrQualID, scoreValOrWeight (always a float), subjID). That seems really great.:).. (19:15)

%(19:16) Oh, but I'm not sure I'd like to have two secondary indexes on the PublicUserScores table for both the min and the max scores.. ..So maybe we introduce a single-score (user score) table?.. (19:17) ..Where we could also just remove the unix_time as well, since it is not really needed.. ..Yeah, not bad. Let's actually do this, I think.. (19:20) ..Ah, and then \emph{that} table gets a listID as it's "head" of the indexes. That sounds great.:).. (19:22) ..Yes! (And no, we shouldn't merge that with the private scores table; three different user scores tables it is.:)) (19:23)

%..(19:24) Wait, could we even make it such that the user gets to decide which of the two tables counts for a given score? ..Oh, that's actually easy: If the single-score one is scored, then that counts, and if not then the other one counts, provided the score is there.. ..Or the other way around.. Hm, and we don't want to then make a third one \emph{with} a unix_time as well?.. (19:27) ..Wait, why don't we just make the max_score and the unix_time nullable.. well, because we want a secondary index only on the single-score one.. ..(19:31) Yeah, just forget the unix_time on the single-score table, and also just forget using this table to contribute scores to a user group with; the single-score table is only meant to be aggregated directly via some list combination functions/algorithms. (19:33)

%(20:21) Oh, I don't really need another index for the PublicUserScores now.. ..Yeah, as this don't need to be part of the.. Hm.. ..Oh, but we could also change min_score and max_score into score_mid and score_rad now.. (20:24)

%..I btw forgot to mention something from this morning (lying in bed still), which is this: We ought to start denoting more explicitly what \pm we are tanking about, e.g. by giving it a \sigma subscript. And for uniform distributions, we could then give it a \mathrm{u} subscript. And in cases where the distribution is more irregular than just a uniform one, but it is still bounded, we could then give it a subscript of $\mathrm{b}$. I think I will start doing that myself. And when there's no room for a subscript, e.g. in a mouseover text, we could just write the given symbol out in a parenthesis right after (the unit after) the \pm value. (So e.g. '4.54 s \pm 0.3 s (\sigma)'). I think I will do this, indeed.
%In particular, if we start using score_mid and score_rad instead now, and want to also display these, then I could use the $\pm_\mathrm{u}$ notation, specifically. (20:32)

%..Hm, now we \emph{can} gather the existing publicScores table together with this new single-scores table.. ..So I guess let's do that.. (20:34) ..But I should probably call it now and do this tomorrow.. (20:35)

%(06.10.25, 11:11) Oh, except that we only want the list_id for the single-score table. So let's keep them apart, I guess..

%..(Slept a lot, like 9--10 hours..:) ..Oh, and while I'm at it, let me mention that I feel such a good post-vacation *(/holiday) energy!:) I feel so focused again.:) Anyway, back to work.:))

%(11:17) Hm, maybe it's nice if users can query for only the entities that they've previously scored around a score that they are currently considering. So maybe I do want to gather those tables, and also turn min and max into mid and rad(ius) (which is free anyway).. ..Ah wait, what if we just do use that view for the query procs as well?..!.. (11:19) ..Ah, well, not for \emph{all} query procs, but we can make a combined one that can be used to also query non-standard entity lists as if they had.. well, as is they were stored as our view.. ..makes them look like.. Okay, good. Then we can keep the (userID, qualID) "head" for the PublicUserScores table.:) (11:23)

%(11:31) Hm, with this view, I guess I could also ask if we now need the score contribution List entities..(?) ..Well, we do \emph{want} it, I think, so yeah, let's keep it.. (11:33)

%(11:40) Hm, a weight-ordered index for the \geq 10 table \emph{would} perhaps be a very good way for users to be able to find some good entities to rate in order to find themselves in a PC-space.. ..(14:48) Yeah, and it shows the engagement. I think it might be a good idea. The alternative is to make it as a derived list, and then make update requests for it.. ..I think let's make that index.. (11:50)

%(12:41) Hm, if the list type needs to always come first in list entity function calls, then we need a function syntax for 'input has to be this'.. Well, just replace the ':' with a '='. And then in this case the '=' means 'has to always be' rather than 'defualt value (if null) is'. Okay..

%(12:56) Hm, the ListMetadata table whould then also have the same "index head" as the view, shouldn't it?.. ..Hm, should we include the PK of the PrivateScores table somehow in the combined view..? ..Namely where we just break the convention in this case that the list is ordered (by the score/weight value).. ...(13:11) No, no need, I think.. About deletion of scores, the users should just have "paid" the compCost for this, essentially, when they upload the private scores..

%(13:13) Oh, entities should also have a paid storage (upload) data cost counter on the Entities table.. ..Alright.. ..Well, or a query counter.. Nah, I like the paid upload data cost better..

%(13:21) If, we ever want to change the IDs from base 10 to base 16 or 64, we should just do it only in the interface between the control server and the client.. ..Hm, and do we want to do this, or no?.. ..I'm kinda still leaning towards 'no'.. (13:23)

%(13:26) Okay, since the update requests and sub-procedures need it, we do indeed need to make the "index head" of the ListMetadata table like our (still unamed) view.. ..And therefore we might as well just implement the metadata also for the PrivateScores, and also for the PublicScores (which is a must).. ..Ah, but I don't need to include the PK index of the PrivateUserScores.. (13:29) ..(13:34) No, and not in the view either..

%(14:09) Wait, with two sec. indexes on StandardScoreAggregates to choose from, we don't know which one given just the list_id. So I do need two tables, right?..
%..(14:15) Hm, we could make requests that only add one entity at a time to the weight-ordered list.. Hm.. ..(14:19) I think we should just make it a request to update the full list of weights; I think it's better that way.. ..So I'll do that, and thus also seperate the two tables.. Well, unless I just want to reuse the same one.. ..Then again, why, when we can just query via the view. I mean, unless we want to minimize the view-querying for efficiency purposes, but it shouldn't really mean that much at all.. %..(14:32) Whatever, I'll make two tables (or a third one, if you will) regardless. The view still means that I can do this with much less worry (about code complexity and clarity, of course).:) ..(14:35) Ah, then again, the StandardScoreAggregates are supposed to represent all kind of things anyway.. Ah, but then what about the weight of the weight aggregates?. So I think I should maybe make it two.. ..Or I could make a StandardWeightedScoreAggregates and a StandardScoreAggregates table.. (14:38) ..'StandardUnweightedScoreAggregates'.. ..Yeah.. (14:39) ..No, 'StandardScoreAggregates' and 'UnweightedScoreAggregates'.. ...(14:51) Or just 'FloatValueAggregates' to make it more general..

%(15:00) Shouldn't I seperate the view into a private and a public one, though?.. ..Yes, I should.. (15:01) ..Well, or just add a boolean flag.. (15:02) ..Well, let me do the former.. (15:03) ..Ah, I can also just union them in a third view, just in case.. (15:04) ..And if we don't end up using it, we can scrap it, and then we can potentially allow the.. list types to overlap.. nah, let's just refrain from that at all times.. (15:07)

%... (16:03) Wait, why don't we just make a table over list function entities and their associated list types. And when we can hide the underlying list types (somewhat) in the interface with the app layer. I think we should do so.. ..(16:10) Ah, and if not found (NULL), then it's just the standard ("s") list type.

%(16:14) Wait, wait, wait, \emph{do} we actually want to hide the list type?.. ..Hm, it tells both the user and the app what other data can be searched for from each element of the list.. ..(16:24) Ah, on the other hand, the lists types of the functions can also just be hard-coded into the app (or it can potentially query for them, if at some point we want to do this instead of having it hard-coded..).. ..'Cause it doesn't really make sense to use any list functions that is not somehow supported by the backend (or has been).. ..So yeah, I think let's hard-code it in the app (and "hard-code" it in the DB via the ListTypesOfFunctions table).. (16:30)

%(16:37) Oh, but I do actually need to differentiate between various list types, or 'classes,' in the app layer.. ..Hm, but that just means creating those classes.. (16:39)
%..Hm, maybe the types.. Ah! We could hard-code the list types simply by making ListTypesOfFunctions a view instead..!.. Cool.. (16:41) ..That \emph{is} actually quite cool; pretty neat..! (16:42)

%(17:17) Hm, the list types are only meant for making list functions that take float value lists able to take several kinds of lists, without needing to transport the values to the FloatValueAggregates table first.. ..Right.. ..Yeah, and since we are also hard-coding the list types of the various list functions into the app, we therefore also don't need the view for querying, btw. Okay, good to remind myself of; it's just meant for that and only that.. ..Ah, but my (unspoken) question was actually: what to do for e.g. public user mid score lists, when they are inputs, as they don't have a list_id.. ..Hm, well, we can still make a list entity when needed, and then make sure to parse the userID and (at least) the qualID from that.. ..But then the view doesn't help all that much.. ..Only my new ListTypesOfListFunctions view.. (17:25) ..Ah, yeah, 'cause that can at least funnel the given procedure's braching tree into just a branch for each different (relevant) list types alone (not a branch for all list functions).. (17:29) ..And then I should maybe actually just out-comment my All[...]OrderedEntityLists view again.. ..Well, no, 'cause that can still funnel at list all list_id-using tables further into one on top of that.. (17:31) ..Yep..
%..(17:33) Wait a minute.. Could I not make a more advanced AllPublicOrderedEntityLists view here by encoding into the "u" part of the view that the qualID and userID is looked up from the list's defStr!..? (17:34) ..Ha, neat!.. (17:35)
%..Oh, except that it probably would get too complicated, namely due to those SUBSTRING_INDEX.. well, then again..(?) (17:38) ..No, that should be doable, actually.. Pretty neat, in that case.. (17:39)
%(17:44) Ah! And then we can even further use the ListTypesOfListFunctions view to elinimate the first list_type column from the AllPublicOrderedEntityLists view as well! Neat!..
%...(18:04) Okay, I think I need some join to make it work. It might be too complicated compared to what it achieves.. But let's see.. ..(18:09) No, I think it's doable.. ..(18:26) Well, maybe so, but i think it's better to just make the procedures brach for the user_score_mids function themselves. And the app also just braches here (if not for all list types).. (18:27) ...(18:38) Okay, it can be done with a three part INNER JOIN, where we also join with the Entities table, and I kinda wanna make it anyway, just for fun, but no; I know I won't use it. It's better to just branch specially for that particular list function in the procedures..

%(18:47) Actually.. ..Oh, I was about to say 'let me out-comment it all, but I just recalled, we need it for maore than just gathering a few branches in a procedure. We need it for all list-combining algorithms.. ..Hm, so maybe i should actually finish it?.. (18:49) ..Oh, but I should drop the thing about.. no, maybe not (the thing about removing the list_type from the view).. ..Wait, I could also keep it, and go even further and include the defStr of the List entity as well..!.. (18:52) ..Yeah, let me do that: It makes the definition a lot less complicated, and it most likely helps the query order compiler as well..:) (18:53) (It certainly won't hurt.:)) ..Well, I could also give it the fun_id instead of the list_id.. ..Oh, no, but I could add the fun_id, why not.. ..(18:57) Ah, we might want more complicated lists in the future, such as lists with cutoffs, and such.. ..So yeah, let's add the fun_id.. ..Wait, why don't I just branch on the fun_id alone, and then just make the relevant parts of this OrderedEntityLists view take several fun_id's (like a switch-case case statement with several conditions (in a disjunction))?. I think that's better.. (19:01)

%..(19:03) Oh, but what about the ListMetadata?.. ..Hm, couldn't i just go back to the previous AllPublicOrderedEntityLists view, and then make the new view on top of that?.. (19:05) ..Ah, and and make the second view a view where you obtain the generalized keys for the AllPublicOrderedEntityLists view..(?) (19:06) ..Yeah, I think that this is the way.. (19:08)

%(19:18) Ah, I can also just implement this second "GeneralizedEntityListKeys" view as a sub-procedure instead, i.e. one to get the generalized (list_type, user_or_group_id, list_or_qual_id) "list key." And there we are; then all we need to do is to call that first in any list combiner algorithm, and then use the All[...]OrderedEntityLists view right afterwards in whatever select statement we need (e.g. in a cursor definition). (19:21)
%(19:36) And that procedure can also spit out any bounds (cutoffs) and such..

%(21:06) I think I'm going to collect all three, 's', 'a', and 'f', tables into one. Also, we essentially just make it so the the weight >= 10 check is done at the end of an algorithm like the score median update reuest to see if the element can be added to the given list. And then we don't have to make that check when checking e.g. members and filter lists, and all that. Hm, but maybe such algorithms won't work (well) when the weight is not part of the sec. index.. ..(21:13) Unless I should just embrace making a lookup for each element in an algorithm, saving space and complexity, but getting significantly slower update algorithms..(?..) ..I mean.. Now that I say it, it does actually kinda sound like a worthy trade off.. Hm.. (21:15) ..Hm, yeah, it kinda does, actually.. (21:17) ..Hm, it actually sounds like it would be better that way, even when we only think about the time-for-space trade. It seems so.. (21:20) ..And then we can just use one table for everything, at least except the PrivateUserScores.. Well, and some other date like the score_rad and the unix_time of the PublicUserScores.. (21:21)
%..(21:23) Should I even make score contributions lists, then?.. ..Yeah, sure, but we could potentially just store the score_mid on them.. ..Well, no, let's maybe keep the min and max scores. I can think about it, though.. (21:25) ..I \emph{think} I will keep them, though.. (21:28) ..And that actually means that the score median algorithm can be implemented in the same way, as long as I just also remember to make the initial check for the filter list, and also the last check of the.. oh, right, the weight.. ..Yeah, then it does require a weight lookup on each iteration of the loop, but as I've just talked about, it might very well be worth it.. (21:31)

%(07.01.25, 9:50) Should I go back to saying that one needs to create a List entity and store it in the standard ScoreAggregates table if one wants to use the public user scores in a way that requires an score-ordered list..?
%..Hm, we could also go back to factoring the user group / user out of the List entity, such that the key "head" of any list will be (user_group_id, qual_id).. wait.. (9:57) ..No, (user_group_id, list_or_qual_id).. Hm.. ..Well, we could actually go back to calling it "combined qualities" instead of "combined lists"..

%..(10:00) Hm, and maybe we \emph{could} drop the ListMetadata, if we instead just scan the full list for the weight sum.. Oh, which is much faster since the weight list can just be scanned, with no other lookup required on each iteration.. for the median algorithm, i.e. And then for the counters, we could perhaps just time the algorithm.. Well, then we can't necessarily make that whole system of paying for it partly.. ..Wait, the ListMetadata table can just have (list_type, user_(or_)group_id, qual_id) as its PK "head".. (10:04)

%...Well, lists are not qualitites; the former can't be scored by users. So let me keep using list functions for combinations.. (10:17)
%(10:47) Hm, a class of derived/aggregated qualities.. ..Oh, with user group placeholders, what happened to those?.. (10:49) ..Well, I guess it could just be the app's job to insert, but.. isn't it better to use those user group placeholders?.. And then we.. we essentially get list functions that takes some user groups as the last input.. ..We could call them proto lists, perhaps.. (10:51) ..Hm, and if we want user_group_id as part of the "head" of the list keys, then we could just use 0 when no user group input is needed.. ..Well, yes, but this can all just be happening in the app layer.. (10:53) ..But still, we could also do what I just said in the backend..

%(11:18) By the way, if I ever encode the IDs in base 64 or so, I will just decode it immediately in the app layer, and also use decimals in the app layer in general.

%(11:20) I guess, let's use proto lists and all that at some point in the app, sure, but let's still just use list_id's in the backend..

%(11:23) Hm, I had such a good groove.. And now I don't know what to do, all because I wanted to go from weightExp to weightVal, and becasue I wanted to make the PublicUserScores into score-ordered lists as well.. I think it's a good thing, but I do feel a bit stumbed.. (11:25)

%..Hm, maybe we should just indeed remove the sec. index for PublicUserScores, and then make it a request to make a new list from them, either with the score mids, or whatever, even with the unix times..(?) And then we still turn all the other public tables into one..(?) (11:29) ..Or not.. (11:31)

%(11:50) I think, even if we end up only really using one scores table, I should still make it open and possible to use other kinds. So I think I will keep ScoreContributions as is now, which means that we can still get quick updates, which might be important for "recency scores".. ..And then i will just use a procedure for insertingUpdatingOrDeleting from any scores table, which then also updates the ListMetadata table.. (11:53)

%(11:56) Wait, how about using a nullable "other data" column right before the subj_id in the sec. index.. And then combine it all into one table, all public scores tables, that is..(?..(!..)) ..Having it right before the subj_id also means that it can be compressed away almost completely when NULL (or some other very common value).. (11:59) ..I mean, why \emph{not} do this?. ..! (12:01)
%..Hm, making it a VARCHAR (or a VARBINARY) actually means that we should make it NOT NULL (but them it can just be the empty string).. (12:02)
%..We could even make both an on-secondary-index data and an off-secondary index data column.. (12:04)
%..And then we make a query proc. for getting the list with only the scores, and another one for getting the scores \emph{and} the other (on-index) data (as well as the subjID in both cases, of course). (12:07)
%..Okay, let me just absolutely do this, why not..:) (12:09)

%Now, do I then keep the weights for the standard entity lists, namely for the sake of further aggregation, mostly?.. ..I mean, the good thing is that this is now a not-so-fundamental decision, since it is then just a matter of that functions the DB supports, which can be seen as a layer above the fundamental one.. (12:12)
%..But yeah, let me maybe keep the weightExp at hand as much as possible, but let me maybe use 1.1 instead of 1.2 (now that we have this 10-is-the-magic-number system, it kinda seems that we won't need extremely large weights, nor extremely low ones (since almost all lists are now filtered in some way)).. (12:16)
%..We could also make the weightExp a base 128 number, and then use all ASCII characters as the base 128 decimals.. (12:18) ...(18:43) Hm, interesting idea, but i can't figure out how to convert from a tinyint decimal number to an ASCII char in MySQL.. ...(12:52) Well, you could convert to binary, and then remove the.. Hm.. ..(12:57) Ah, I can just cast from BINARY to CHAR.. ..Yep, 'SELECT CAST(CHAR(<num>) AS CHAR);' does the trick. (12:59) So it's an option..

%(13:16) Now, do we indeed use a VARCHAR/VARBINARY, then, Or do we use a nullable FLOAT, or BIGINT, or something like that?.. ..Let's use a VARBINARY, I guess, or a VARCHAR.. ..Hm, VARBINARY, I think..

%(13:23) Wait, it it really a bad idea to make proto lists a part of the backend as well?.. Let me just think.. ..The thing is that all the user group inputs could in principle be decided by the outer user group.. ..Oh, I almost think that this is a (very, perhaps) good idea..!.. (13:25) ..(13:29) Then all entity lists will be indexed by a user(_or)_group_id and a proto list ID, which we could then just call list_id, since "proto lists" will then be the new lists.. well, maybe not quite.. ..Yeah, nah, and we could also call them 'list templates instead, btw, for more clarity.. (13:31) ..So a (user_group_id, list_tmpl_id) as the entity list key, or as the "head part" of the key in out new all-purpose public scores table.. (13:32) ..Hm, and then (user_group_id, list_tmpl_id) forms a list, but maybe we don't really want to use such list entities all that much by them self, and instead just use the (user_group_id, list_tmpl_id) pair as the list's key as much as possible, also thereby postponing having to create and insert the given List entity itself.. (13:35) ..And let me clarify, the idea is that list templates can then have user group variables/placeholders in place of actual user group inputs in their function call, and these placeholders are them automatically substituted by the combiner algorithms in the backend, via "semantic lookups," of course, when a given combined list is created/updated. (13:37)
%..And Qualities can them also be seen as a subclass of list templates, by the way (if we want that, instead of making monadic list template entities, and i think we do).. (13:39)

%... (14:43) I do wanna keep that weightExp along as much as possible for all standard entity lists. 'Cause it can then be aggregated by other lists, which can then for instance divide all weights by a factor to get a more strict cutoff. And note hare that we shouldn't ever really want to take the < 10 part of a list and enhance the factor; it should only go the other way. For a user group shouldn't really be responsible for whats on the < 10 part of the list, except the the are responsible for not letting the wrong things exceed a weight of 10. ..And it's also a good thing that the app can query for the weightExp's as well, which it can then show to the user, as well as use for some client-specific reordering of the list. (14:48)

%..(14:52) Oh, and another thing: The DB can see whether an entity is a user, a user group, or a user group variable simply by seeing if it is a 'u' entity, a 'c' entity (since user groups are just lists (actual lists, not list templates)), or 'a' entities.. Hm, and we don't want user group variables/placeholders to also be list templates as well?.. (14:54) ..That \emph{would} actually make perfect sense, wouldn't it.. (14:57) ..Well, no, not quite, since we don't want to use the outer user group as the user_id for the list key here, but rather the user_group_id that we look up semantically from the outer user group. So no, the user group placeholders are 'a' entities, indeed.. (15:00)

%(15:19) 'List template' actually seems ambiguous.. Maybe 'proto list' is better.. Or..? ..Hm, 'list header' perhaps.. (15:21) ..Hm, that could work, but I think I will actually stick to 'proto list' for now.. ..Except that 'proto' is meant to be a prefix.. (15:25) ..Hm, 'list specification'..? ..Or 'proto-list'..? ..Nah, 'proto-' doesn't necessarily mean ~'pre-', so let's say 'list specification' for now.. (15:29) ..(15:37) Hm, I do actually like that, 'list specification'..

%(15:58) If a user deletes a profile, they can also request that their userID is removed from all entity lists (but they have to specify them), even the "constant" lists. And they can also be removed from all entities. In fact, whenver an entity is removed, their entRef in all entities should be removed, i.e. whenever it is discovered. Hm, and once in a while, the database might scan through all entities, and then remove all entRefs to removed entities, after which the relevant entity IDs can be freed for new use, potentially (if we want that at some point). And the important point in all this is then that i actualy need to reserve 'null' for meaning a removed entity, and then I should use something like 'undefined' instead for function calls, although I think I will just use having nothing between two commas as representing an 'undefined' input. (16:03)

%(16:28) Let's make a PublicListMetadata table, and perhaps a private one, although I actually think that we might not need one.. ..No, let us not make one for now.. ..But let's on the other hand add a private storage counter.. (16:30)

%(18:07) So how do we store score contributions exactly? Do we still store them on two lists.. well, yeah, that was the plan.. ..And with the weight and the time.. ..Oh, the time doesn't really make much sense for min and max scores, does it?.. Okay, let me think.. ..Hm, it does make sense, and I do then need to store it on both lists (in the on_index_data column).. (18:14) ..Okay.. ..Ah, but the public user scores?.. ..I can.. ..Hm, could one make a view for conversions? No?.. ..But one could make a FUNCTION, I guess.. ..Anyway, we can store the max_score as a binary, where we just use FLOOR(LOG2()) to get the exponent, and then also divide with that to get the fraction, after removing the '1.'.. ..Hm, unless we just also store this as two values.. ..Oh, and where if the user only uploads min_score, then this is just treated as if max_score is the same.. ..Yeah..

%(18:53) I should maybe make a list of.. ..hm, weight-ordered.. ..either subjects with qualID as the on-index data, or qualIDs with subjIDs as the on-index data.. ..And the point is that the float sum of this can then also be used instead of my former weight_sum for the score contributions lists.. oh no, that would require a (qualID, subID) entity at the end.. (18:57) ..My (loose) idea before that was to just make at least one weight-ordered score contributions (users-as-the-subjects) list.. ..Hm, i think it's better to put a nullable weight next to the (other) on-index data column, which we can then sum as well in the PublicListMetadata table.. (19:01)
%..Ah, and with a second nullable float, we can also just use that for the max_score instead.. (19:04) ..I think this is better; it also means that.. well, i wanted to carry that FLOAT-valued weight around for a majority of entity lists anyway, and now it is just much easier and clearer.. (19:06)

%(08.01.25, 10:35) Hm, maybe it's actually easier just to require a non-null float_val_2, namely since we are summing these values anyway. And it shouldn't make it take up more space, since a FLOAT column set to 0 should just be four zero bytes. So let us just that.

%By the way, I'm making it all easier by gathering it all into one table (of all public scores). And it doesn't seem that this will slow down the query procs by very much. But if it turns out that it slows them down significantly after all, then we can always just try to optimize by splitting it up, and then rewrite all relevant procedures to branch for all different "list types" (obtained from parsing the function ID of the given 'c' entity), and to maybe use a combined view instead for select statements (and such). But for now, let's use this neat all-fits-one table, and also at all times keep that as at least a virtual table as seen from the app layer; keep it as an abstraction, an interface, that all entity lists seems to follow when viewed from the frontend..:) (10:43)

%(10:55) Now that it's lists specs, not lists, I should call it 'score contributors' rather than 'score contributions'.. ..(And it will then consist of only a qualID and a subjID, both the min and the max versions..)

%(11:07) Hm, for the public user scores, it would be kinda handy if float_val_2 was nullable..(?..) ..Well, too bad.. Let's just go ahead with this.. (11:09) ..Ah, and if we ever want to optimize this, we can just use the minimum possible float number (a negative one, if not -\infty is supported), and use that for maxScore = float_val_2 when maxScore = minScore. (11:12)

%(12:08) Just tested the FLOAT(2,2) solution, and it doesn't work (only cuts it down to ~9 decimals for some reason). But maybe we don't generally want to "round down" anyway. For instance, we might be interested in an interval between 99.99 \% and 100 \%. So let me just not "round down" for now, I guess, and leave that for some potential optimization in the future?.. (12:11) ..Yes, let's do that.. (12:12)

%(15:06) In order for this new (user(Or)Group(Variable), listSpec) system to work, we then need for the userGroups to be defined by single entities, which means that we need a user_group() function, taking what we might call a 'moderator group' as the first input, and then a userListSpec(ID) as the second input (and that's it). Hm, and do we get a chicken-or-the-egg problem here..? ..Ah, we still need those initial user groups to be 'a' entities, and we also need "constant" user groups (where users can however request to be deleted from these iff they then delete their profile at the same time).. ..Okay, so we still need to parse the Class attribute of 'a' entities in order to know if they are actual user groups or user group placeholders, which is fine.. (15:11) ..I should make a sub-procedure for testing.. or for getting the moderator group, which can be 0 in case of 'a'-type, non-moderated and/or "constant" user groups, as well as the list spec.. (15:14)

%(16:31) Hm, and we don't just wanna implement the fundamental user groups via a "bot" that keeps some "user score" list over users..? ..The original/fundamental "moderator," if you will.. ..That actually sounds better.. ..Then all user groups are list entities formed with the 'list' function, taking a user(Or)Group and a list spec (of which 'Qualities' is a subclass).. (16:36) ..Oh, or formed from the 'locked_list' function (calling it 'locked' instead of "constant"), which also takes a 'Locked after' datetime input.. (16:37) ..Yeah.. (16:37)

%(09.01.25, 11:33) I've gotten the idea to call "list specs" 'list formulas" instead (and yes, let me just use that plural version).

%And something else: I should store it as score_mid and score_rad instead for the user scores. That solves the problem of wanting to compress away score_rad's of 0. And it also sets a good precedent for if we want to have other "agreement curves" in the future other than uniform ones. In that case it would only really make sense to order the score w.r.t. the mid point first. So let's do this..

%..(11:40) I'm also considering if we might want to just use the quality itself as the list formula (and 'List formulas' is still the Parent class of 'List formulas,' btw) for the score_median lists. It would then require the algorithm to parse whether the quality is a relevancy one or not.. And it would "hide the implementation" a little bit, which I actually don't like completely: As I've said before, I think it might be a good idea that the users get a view into what underneath, and thereby of the possibilities.. (11:43) ..But on the other hand, it would reduce a \emph{lot} of boilerplate code in the formulas of combined lists.. (11:44)
%..Wait, we could also make qualities not a subclass of list formulae (hm..?) if we just wrap them in functions. The smart thing about this is then that it open up for exactly the thing about having other ways to score a quality, e.g. with a Guassean agreement curve, and all that. Okay, I think this might actually be a good idea, also because, conceptually, I for one don't want to think about Qualities as List formulae/formulas (..?).. ..Question is then, should the Metric then be part of the list formula function, or still part of the Quality.. I actually kinda think we keep it as part of the Quality.. Yeah.. (11:49)
%..Okay, but now I'm getting \emph{more} boilerplate..(?) (11:50) ..(11:54) We \emph{could} introduce syntactic sugar.. Ah, but this this should just be in the app layer, right?.. ..(11:57) Oh, we are substituting all nested function calls with their entID, and we should actually also use ListIDs, I think, instead of user_group_id--list_form_id pairs.. ..So the "syntactic sugar" in the app layer is just how the app renders this List entities.. (11:59) ..Ah, yes, and this can also help with insertions, 'cause then the app parses the function, sees some List-typed inputs, and then guides the user to choose and input a List entity. Alright.. (12:01)
%..Hm, but then we do get all those List entities, that I kinda wanted to avoid.. Let's see..

%(I don't know why I slept so much today; I went to bed at twelve.. I didn't think that I would have needed it, but I guess I must have.. (12:03))

%..Nah, let me try to get back to (a convention of) requiring user_group_id--list_form_id pairs in combiner list functions.. (12:07) ..Yeah, and we can still say the same thing about list formulae, then: The app will help the user insert the entities, which can be chosen from a list (even potentially one with subcategories, and so on).. ..Let me indeed try to use 'formulas' instead, btw.. ..(12:12) Then again, about the List entities, as long as it's for the combiner lists, it might be totally fine to use List entities instead of user group--list formula pairs..
%...(12:24) Wait, for the combiner lists, we are constructing list \emph{formulas}, not lists..

%...(12:47) Ah, nu ved jeg det.. In the same way that one provides an "outer/context user group" along with a formula when querying for a list, one shouldj ust also provide a char denoting the standard for interpreting the formula. And then we just initially only implement one standard, where the score_median is always used for user group--quality pairs, and these are filtered iff the given quality is a relevancy quality. And then these form the fundamental atoms of the formulas, and you need to start using functions to get other kinds of aggregates, and of course for combining the formulas, from there..:) (12:51)
%..This standard should also let allos for user--quality pairs, of course, and here we shouldn't filter the relevancy.. well, I mean the \emph{non}-relevancy qualities.. (12:57)
%..Now, do we want the app to fetch the right user group for each Quality, or should we add a.. Hm, a quality-dependent user group variable?.. Hm.. (12:59) ..Nah, better a user group variable, that implicitly depend on the quality that it comes before.. (13:00) ..Hm, a user group variable variable, so to speak.. (13:00) ..Do we want this, or is it better that the app does some fetching instead?.. (13:01) ..(13:06) Hm, this does kinda make the user variables in the formulae sorta redundant, as they can just be found from the given quality instead, it seems (by uprating user group variables for all qualities, and then also uprating user groups for each user group variable, of course).. ..Yes, it does.. (13:09) ..And what about 'locked lists'..? ..Hm, you just insert that instead of a quality in the formulas.. ..Oh, instead of "user group variables," we can just use some quality classes instead.. ..Where we still look at the '<Quality> \to .. User group class..'.. Hm.. ..Well, maybe 'user group variables' makes more sense, but we'll see.. Well, I need it for the backend, though.. (13:14) ..Let me just stick to 'user group variable'.. (13:15) ..(13:19) Hm, all this does put lot of responsibility on the backend, and with caching, it might be better to just let the app fetch the various user group variables for the various qualities.. ..Which would then of course also be turned into actual user groups by the app (and here we can really expect a large degree of caching).. ..So letting the formulas be converted by the app, and thus also keeping the various "standards" out of the backend.. I mean, this does sound like a good move.. (12:24) ..Ah, but then I need to move away from using formulas in the back-end.. (13:24) ..(13:27) Hm, but we could still keep the "list \emph{specs}," like how I have it now..
%..Okay, I like this move, of moving all list formulas to be a purely front-end thing (with no special back-end support/implementation). But I still need to figure out if I indeed keep the List specs, and also if I keep the same names.. (13:30)
















\subsection*{ } 

\newpage

\chapter{Economic ideas}

(12.07.23, 18:39) I just got an idea for a different version, basically, of my SRC idea. The idea is to propose it as a policy instead, much like ``Økonomisk demokrati.'' In this version, the proposal would be to make it so that.\,. Well, basically that firms are required to become like SRCs.\,. Hm, it could maybe then be part of a sales tax that the government is then required to buy the given company's assets for (and the company is required to sell, I guess) and to give those shares to the buyers. Now, the reason why I think this idea is interesting, although it kind of ruins the whole freedom of my SRC idea --- and the fact that it does \emph{not} require policy changes --- is that this version is perhaps easier to understand and discuss as a first step, before moving on to discussing my actual SRC idea. And this might especially be true for people used to thinking about politics, which *(as I see it.\,.) will likely be the majority of the people who is likely to find the ideas interesting to begin with. So maybe proposing the as-policy version of the SRC idea might be a good place to start. I think I might go forward with this idea.\,. (18:50)

(19:41) There shouldn't need to be any laws required in regards to make the companies sell the shares to the state. So it will just be a certain new sales tax, where the state is then required to buy shares for the same amount (and give it to the buyers), which means that the companies basically owe the customers, through the state, a certain percentage of there shares at each interval. And precisely because the costumers are the recipients, and that the shares received is exactly proportional to the money spent by the given customer, it means that the companies can just raise their prices accordingly, such that no party, neither the company, nor the state, nor the customer, has any losses or gains in total by these transactions. (19:49) .\,.\,This is of course opposed to the ``Økonomisk demokrati,'' which, as far as I understand, was supposed to be payed by the employers. And another advantage of this model is that, instead of requiring a lot by each citizen, since they are forced to become investors and figure out where to put the money (.\,.\,which I guess was the case for the ``ØD'' model, but I'm actually not sure after all.\,.), all the customers have to do is to, well, be customers (much like before). .\,.\,Well, and then they have to also give votes, if they want to use the power that this system gives them over time, but they don't \emph{have} to do this, of course. (19:55)

(15.07.23) Hm, maybe I won't go forward with this right now after all.\,. The thing is: In order for the idea (both my original as well as this one) to really work well, we probably need the good discussion websites that I hope my web ideas can bring about first.\,. .\,.\,First of all the ideas need more thorough discussion before we can be sure that they are good ideas (of course with my SRC idea, we could also just try it out and see, but an initial thorough discussion would still be the best way to go). But what's more, the system itself would probably work best if it is accompanied with good discussion and negotiating/voting sites like the ones I imagine.\,. I feel like this is the case, anyway.\,. .\,.\,So I think I will just keep on focusing fully on my website project for the time being.\,.


%(22.08.23, 17:11) Jeg kom hjem fra forlænget weekend i går aftes, og i dag har jeg bare været til A-kasse-møde og først gået og nu løbet en tur. Jeg holder nemlig lige en lille pause med openSDB-projektet (og bare lige venter er ser om der er nogen, der svarer fra den der Semantic Web Interest Group). Jeg fik lige tænkt en tanke på den der løbetur omkring idéen her fra denne sektion, som jeg synes er værd at nævne, så den kommer her.
(22.08.23, 17:14) More on this idea: If the state buys a small portion of the lands assets at frequent intervals and gives.\,. no, lends them to its citizens (such that they get the.\,. %avance, afskrift på dansk.. hm.. ..dividends, tror jeg det hedder!.. *Ja.
.\,. dividends on those assets), then a very good idea would probably be that they cannot sell this kind of stock (that they get from the state this way). They can however trade them for other stock of the same kind, but not via personal trades. Only via a.\,. .\,.\,central kind of exchange instance run by the state, where people can submit trade offers, not to other individuals, but anyone has to be able to take the offer. (And it should also be illegal to make trades where other valuables and/or money is involved in the trade, so that if people find a way to game/hack that system, it is illegal). When a person dies, their stock of this kind is given back to the state and redistributed to all other citizens, at least partly: A state might also decide that relatives of a deceased person can inherit some (or all) of that stock directly. And there we go. A system where the citizens will slowly take over the assets of a land, not at the cost of the previous owners, at least not directly in any way (because they are reimbursed for them), but at their own cost. And while they then have to carry this cost, they also don't lose something of value, since they now get the dividends for the shares that they get this way for the rest of their lives. And if the state says that it can be inherited by relatives, then that stock will keep on benefiting the family once the person is gone. So the average people do not lose anything either. On the contrary, the system helps them gain more and value in the society, creating less inequality. (And this is not contradictory to the previous asset owners not losing money: What they will lose instead is just their dividends---over time---since less and less free stock in the country/world will mean higher stock price per dividend. So the rich people will not loose anything of value immediately (due to the rising price of the ``free stock''), but they will lose future earnings.) %(Ah! There was the word I was looking for!.)
Going back to the average person, for them the system will just mean that there pension plan will be slightly changed in terms of how it works, namely since (as I see it) the system will work pretty much just like an added pension system for these people. This was to drive the point home that, even though all people now have to pay more in taxes, they shouldn't see it as a loss---and in fact, it would be natural to just lower other pension payments in the state at the same time as this system is implemented. (17:43)





\section{Small note about cryptocurrencies}

(25.07.23, 15:15) I have pretty much abandoned all my ideas about blockchains. But I just thought about the fact that maybe it is actually worthwhile to propose a simple proof-of-public-history chain, namely where there is no mining going on: Instead the miners are exchanged for (independent) firms/parties that provides the service of keeping a public ledger over transactions, and also to spy on other such parties and make sure to sound the alarm if all of a sudden the checksums of any given interval of another party's ledger doesn't add up to what is used to. All these parties are also supposed copy all valid transactions from each other, and if one party fails to copy and record (and show in their ledger) a given transaction, that the majority of such parties have, then they can also sound the alarm on that given party. And as payment for these services, the parties can simply take a small fee in the transactions. A user therefore chooses a party/firm to send their transaction to, and can include a payment to that party as well as part of the transaction in order to make them record it. Hm, I guess that it would be handy to also be able to compress the ledger somehow.\,. .\,.\,Yeah, and the parties \emph{can} do this. They can declare a compression publicly, and then give the other parties (and all their users) some time to reject to this compression. And if no parties record and show their rejections, then all users (that trust a majority of these parties to not want to do anything that compromises their trust in the project) can trust that the compression was legal. So there we go. To begin the project, one simply defines an initial PoW chain, and define a curve that ends the mining rewards after a time. And after this time, the miners are supposed to have been replaced by a number of independent parties, that are willing to store and give public access to a ledger, and to make sure keep to the rules in order to accumulate trust among the users, all for the price of requiring some transaction fees for the users that chooses to send their transactions to them. (15:34) .\,.\,This solution is not very different than PoS, except that it does not give power to anyone in particular (as opposed to giving power to the big stakeholders). It is not vulnerable to users beginning to rely to much on it, such that the stakeholders are able to get away with small cheats to the system. For if any of the mentioned.\,. ``public history parties,'' we could call them.\,. if any of those do not comply to the rules, it will hardly cause any bump in the road for the cryptocurrency, since other parties will have recorded the same transactions.\,. Hm, I guess one should think more about a few things, such as the incentive to.\,. Oh, never mind about that particular thing: A transaction can be considered as having gone through once a majority of the parties have recorded the transaction and confirmed that it is valid (including that the money is there to be transferred). After that no other party that cares for their reputation will want to add any transaction that conflicts with the given one, and they will also make sure to record the transaction themselves in order to maintain their reputation of commitment to the rules as well. .\,.\,Hm, what was the other thing that I thought about.\,.\,? (15:47) .\,.\,Well, I can't recall, but I don't think it was that important either.\,. So let me just stop here.\,:) (15:50) .\,.\,(15:51) Oh, it was about the order of transactions, but I've also answered that now as well: The exact order is first of all not important, and one can rely on the parties to be able to agree to an overall order of transactions, sorting out any disagreements that might occur in order to let the transactions through ('cause if a party is behaves in a contrary way, not being very agreeable, it would simply damage their reputation for being a good and helpful actor in the set of PH parties). (15:55)

%(17:19, 18.06.24) Copied from above:
%"... (15:27) I went for a walk and had a few thoughts about the potential of a 'proof of public history' blockchain, or what we might call it.. ..I'm also considering `greenchain,' btw.. ...Something like 'proof of confidence (faith)' has also been on my mind, btw...
%(17:03) Hm, it's wild that PoS doesn't work at all, actually, without any reliance on a 'public history'.. ..Hm, but on the other hand, you can also just see PoS as implmenting PoPublicHistory in a roundabout way.. (17:10) ..As long as the users are aware of this and accepts it..
%..Yeah.."

%(11:41, 19.06.24) There are also some more notes about blockchain in out in the source comments above, just below where the previous (18/06) comment paragraph was copied from above.





\section{More on blockchain (20.11.24)}
(11:23) Okay, I had some thoughts about my PoW blockchain attack vector this morning% (as well as some other thoughts, which I have no written about above and below)
, and I actually now think that it \emph{is} worth writing a whitepaper about.\,.\,! The main thing is that I previously said that ``when people get to short cryptocurrencies, it will be even more of a problem,'' but surely there is nothing particular stopping a potential attacker from doing this already. And with that I actually think that an attacker could in theory make an almost 0-risk attack, given that they can set it up right. Now, before I move on, let me just reiterate that the could thing in all this is that I have the solution to the attack vector as well, which is to change the semantics of the PoW chain slightly. However, once you do this, there's really not much of an argument to not go even further, and turn the PoW into a green blockchain, like a PoS blockchain, thus saving the Earth a lot of electricity spent in excess.

There was also another idea from this morning.\,. which I might actually have had before, come to think of it, which is to use a privately mined block at the start of the 51 \% fork such that no one else can see where the fork actually starts. (Okay, I don't think I had this idea in full before, by the way.) This also adds to the idea. I will explain the whole idea here in a moment. (11:34)
%(11:51) Okay, jeg går lige en tur først, faktisk, og får rystet tankerne lidt bedre sammen omkring det...

(13:51) I just searched on it, and I can only find some articles about/mentioning ``short-selling attacks'' for PoS blockchains.\,!

(14:11) Hm, it's actually pretty simple: You short a lot of coin from a lot of anonymous wallets. Then you mine a new block to begin your fork from. Then from some other wallets, with a lot of, potentially borrowed, coin, you set up rewards for building side chain, following the same rules as the main chain, but where the first hash is the one from the new block from before, that is not revealed to the public until the end. Then even if miners realize that this is a 51-\% short-selling attack, they are still tempted to mine the new side chain to immediately get a higher coin income, which they can sell immediately. And when the attack finalizes, they will also get the mined coin on the new chain, as well as any fees for the transactions that they included in their new blocks. (These can be old, replayed transactions, and they can be new ones.) If the value of the coin drops significantly at any point during this attack, the attacker(s) will have already won due to their short-selling. The same applies if the value drops directly after the attack. And if the coin value stays roughly the same, the attacker(s) will still be able to replay their coins, which for the shorted coins means that they'll get to both sell and give back the borrowed coins at the same time. .\,.\,Of course the attackers should make sure to try to sell any non-shorted coins before they start buying back the borrowed/shorted ones, just in case the value drop is somehow delayed a bit after the attack is successful.

So that's it really, except I guess I should also think a little bit about the divide-and-conquer idea again, and see if there is a way to include that as well in all this (not that it is needed, but still.\,.).\,. (14:29)

.\,.\,Yeah, you could in theory make additional rules for the side chain as well, that states that no reward (apart from the mined coin when the attack finalizes) is given if transactions are included in the blocks of users who hasn't first payed a fee to the attacking wallet(s), in the form of a regular transaction. This transaction should then be included on the side chain, and can also be included on the main chain if the attacker(s) whish(es) to post them there as well.\,. well, or I guess the miners of the main chain (the ones that are left, if any) can include these transactions at will, if they also have a fee to the miner (which they probably should have (for else why should the side-chain miners include these transactions in their blocks)). With this `divide and conquer' addition to the idea, the idea of masking the fork's starting point will probably not be such a good idea. So the attackers would probably choose one or the other. (14:39)

.\,.\,And the idea behind masking/hiding the starting block of the attack fork would by the way be to allow helping miners to (almost-double-)spend both their rewards from helping the attacking side chain / fork and also the mined coins on the attacking fork once the attack is finalized. (14:42)

%..Okay, I'll fix my vacuum paper first, hopefully with some renewed energy, new that I'm finally more confident about the time that comes after, and then I will make a quick whitepaper about this PoW short-selling attack. (14:44) ...(15:07) Okay, I think I can't help working on the blockchain paper first..

(16:56) The coin that the attackers short can actually also be.\,. well, `triple-spent,' in a way, although one might say 2½-spent. For if the borrowed money can be paid back at any time, as per the contract, then the attackers can do it just before the attack is finalized, where the coin value will likely have dropped. They may then get the earnings from the shorting thereby, but when the attack subsequently finalizes, the attackers will then essentially even rob the coin back again from those they borrowed it from originally. .\,.Of course, if this is somehow against the law, they can also just pay it back afterwards. But yeah, if the contract doesn't guard against it, the shorting attackers should thus even end up with the coin as well. This coin might then not be very valuable at that point, of course. But it doesn't hurt them to have it, especially since the value of the coin might rise again at some point in the future after the drop caused (with all likelihood) by the attack. (17:04)

(21.06.24, 12:02) Okay, the problem is that it of course costs something to short an asset.\,. Well, unless the lender can sell their contract for what they are owed, in principle, I guess.\,. But even so, this would not sell as well as the real deal, I'm sure, so we get a cost no matter what it seems.\,. So I'm not so confident that the attack vector is a true threat in practice right now.\,. I'll think a bit more, though.\,.

.\,.\,On another note (and about something different), there are some disadvantages to a conventional PoS blockchain compared to what can potentially be achieved if you make a blockchain that depends directly on a principle of `never treating a fork that has clearly been a part of a 51 \% attack as valid.' In particular, the fact that the coin-wealthy users get to mint even more coin for themselves is something that we could do without. (12:10) *[Oh, I guess everyone gets to mint coin equal to their share, so never mind. Maybe PoS chains are fine as they are.\,.]

(12:13) Oh, about the attack vector, when you control the attacking fork, which you do throughout the attack, then you could also buy and sell coin in the middle of it, and then only include the transactions where you buy the coin, as well as some transactions that sends that coin to another wallet (so that the selling transactions becomes invalid afterwards, should someone try to post them to the blockchain). So you could in principle make infinite replays at once this way.\,.

(13:51) In theory, shorting could be made very cheap.\,. But I also just had an idea about insuring your coin in a rival cryptocurrency. The idea is that big stakeholders in the rival (possibly PoS) blockchain will be happy to do this since it's a win--win for them: Either they simply get the fee if the attack fails, and if it succeeds, they lose some of their holdings in exchange for some coin that's now of very low value, but they might very well cover this loss by the fact that people will then want to abandon the PoW chain, which might very well drive the value of this rival PoS cryptocurrency up. (13:57) So that's pretty interesting.\,.


%(23.06.24, 13:35) Disposition notes copied from the blockchain paper document:
%"
%Disposition:
%- Shorting attacks on PoS chains has been proposed in the past. The authors point out so and so. Here we will argue that a shorting attack is also a risk for PoW chains, and potentially a more severe one.
%
%- A 51\% attack is where so and so. In theory, an attacker could buy up enough mining equipment to gain control over more than 50\% of the hashing power/rate(?) and start builing a private fork of the blckchain that they are able grow at a slightly larger rate than the public fork. [...] Unlimited replays [...] *(by buying and seeling coin on the public fork, while only keeping incoming transactions on the private fork, whle replacing the outgoing transactions with different ones, thus making these transactions invalid once the private fork is made public.)
%
%- But a deterrent is that this might drop the value of the coin completely after the attack. However, if the attackers are able to create short positions for the cryptocurrency, they can in theory compensate for these losses in a way where they always win money as long as the value of the coin doesn't magically go up immediately after the attack when the new fork is revealed.
%(Such a giant short would require a lot of money, but it is theoritecally possible (for some actors).)
%(This is known as a Goldfinger attack.)
%*- ..Calculation of how much/when the attackers win. [...]
%- There already exist ways in which to short cryptocurrency. However, attackers will likely not rely on these ways to short the coin as they will likely want to remain anonymous to prevent legal prosecution. However-however, to overcome this obstacle attackers might implement their short positions via smart contracts in a different blockchain, whose value is not tied to the victim blockchain. Since this attack vector applies to PoW blockchains specifically, the attackers can choose a PoS chain such as Etherium to implement the short position smart contracts
%
%- *(Appendix, maybe:) The short position smart contracts can be implemented such that Alice deposits a number of PoSCoin as collateral *(equal to $a$ times the sum of PoWCoin involved) and promises to pay a number of PoWCoin to a PoWCoin wallet that Bob owns within the end of the time period. *(Bobs PoWCoin wallet has to be unused.) Bob then irreversibly transfers PoSCoin to a PoSCoin wallet that Alice owns equal to the current market value of the owed PoWCoin, plus a small compensation. Both parties also lock some additional PoSCoin to the contract to help settle it at the end. When the time period ends, Alice can then either choose to pay the collateral sum of PoSCoin, if the value of PoWCoin has risen by more than a factor of $a$, or they can claim that the owed PoWcoin has been paid to Bob's W-wallet.
%- (The settlement:) If this is not true, Bob can then challange this claim. Alice then has to reference and sign off on a recent block of the PoW chain. Bob can then either claim that this block is not a recent block of the PoWCoin blockchain, or that the blockchain does not include a sufficient payment to his W-wallet. In the first case, Bob then has to provide a recent block of the actual PoW blockchain (and so on..) [...]
%- The contract can also even be made such that Bob is able to sell what he is owed on to others. This can be done by creating a token on the PoSCoin blockchain as part of the contract. Instead of Bob choosing a W-wallet immediately, whoever holds the token when the time period ends gets to choose the W-wallet instead. Alice is then given a reasonable period of time to transfer the owed PoWCoins to that wallet, and by the end of that period, the two parties can then settle the contract in the same way as before.
%- With such smart contracts, the attackers, as well as anyone else, will be completely free to create short positions for any cryptocurrency, and even while remaining anonymous, thus making the danger of Goldfinger attacks more real.
%
%- Public 51\% attacks: In a different version of this attack, the attackers does not try to gain control over more than 50\% of the mining capablities themselves, but try to openly buy and persuade miners to work on their fork of the blockchain, rather than the 'honest fork.' In this version, the 'honest fork' will start out as longer than the 'attacking fork,' and the attackers' goal then to openly buy miners to help overtake the honest chain..
%They can do this either buy manually paying the miners for each block mined for the attacking fork, or they can set up smart contracts on a rival PoS blockchain that rewards the miners automatically for each block that they mine (for a period of time). Since the attackers control the rewards, they are free to also make restrictions on what transactions are added to the attacking fork, just like they would in the 'secret 51\% attack.' 
%- Now, this kind of attack is theoretically preventable, since the amount of shorted PoWCoin generally be lower than than the total amount of PoWCoin, meaning that there will be more combined stake in the PoWCoin retaining its value than the combined stake in its value dropping. Therefore, the stakeholders of the PoWCoin should in theory be able to counteract the attackers by out-spending them, offering even more money for the miners to continue working on the 'honest fork,' at a sufficient rate such that the 'attacking fork' will not overtake it. However, in pratice some PoWCoin owners might be tempted not to pay a portion of their PoWCoin to help it retain its value, and instead just to try to sell their PoWCoins as fast as possible, before others do the same and the market value of the PoWCoin starts dropping. The attackers might thus hope this will trigger an avalanche of sales of the PoWCoin, dropping its value considerably. And since the value PoSCoin that the miners are rewarded for working on the attacking fork are not tied to the value of the PoWCoin, the defenders of the honest chain will then get less money to pay the miners to get on the right track again, making the situation even more dire. The attackers can then hope that this will cause snowball effect that drops the value of the PoWChain enough that the attack is sure to go through, as the defenders will be unable to counter the offers of the attackers. And upon a successful attack, the PoWCoin is unlikely to retain its value, at least not until a soft fork or a hard fork is made that can guard the PoWCoin against future attacks. And the attackers can therefore cash in on their short positions.
%- This demonstrates that there is a real danger for Goldfinger attacks happening in a not too far away future. And to make matters even worse, we have not even discussed yet that the miners of the PoWCoin might also short the PoWCoin, either before or during the attack. Of course, few people would be willing to lend out PoWCoin \emph{while} the attack is happening---except for people who owns short positions already, and who wants to give the miners incentives to work on the attacking fork.
%And truly the miners are the ones who have the absolute most to gain by taking part in shorting the PoWCoin. For if an attack breaks out, they will not only be able to aquire some of the rewards from the attackers (which may or may not include themselves), but they will also at the same time be able to require some of the money that the PoWCoin defenders tries to counteract the attackers with. So even if an attack is not successful, they will still have amassed a great amount of money from the PoWCoin defenders. And if the PoWCoin somehow retains it value after all this, they will therefore only be all the more likely to short the PoWCoin and hope for another similar attack in the future.
%- The fact that the miners can secretly pay a part in the attack will thus make it less likely for the PoWCoin to retain its value after an attack, even if the 51\% attack itself fails. For the PoWCoin defenders will then have lost a portion of their assets, and will nonetheless still not disuaded future potential attackers thereby. So in reality, the attackers might be able to drop the value of the PoWCoin regardless, even if the 51\% attack fails.
%
%- How to mitigate these kinds of attack: The best and simplest thing that a PoW blockchain community can do to mitigate such attacks is to agree to a soft fork of the blockchain, peferably before any short positions are created, which adds an amendment to the PoW blockchain: The valid fork of the blockchain at any given time is the longest one that has \emph{not} been part of 51\% attack, meaning that it has neither been hidden from the public in a considerably time while some section (of considerably lenght) of it was constructed, nor has any section of it been constructed while a considerably longer fork was known to the public at the time.
%- The way to implement this soft fork is to give each node of the network an option for the owners of the node to maually mark any given fork as invalid on account of the above amendment. The node then has to automatically reject all forks that biulds on top of the given fork in the future.
%The community then has to take steps to ensure that the amendment is up for as little interpretation as possible, and that the community is on the same page in regards to the amendment, in order to prevent unintended hard forks to occur, if different part of the community (and in particular the node owners) disagrees on whether some fork is invalid or not. On might say that this leads to the blockchain being less decentralized, since the node owners now seems to have some interpretive power over what forks are valid or not. However, at the end of the day, the buyers of the PoWCoin are the ones who ultimately decide what forks are invalid or not. If the choose to buy coin from a fork that has been deemed invalid by some of the node owners, that then makes the fork valid in the end. And if they choose not to buy coin from a fork that some node owners has deemed valid, that makes the fork invalid in the end.
%"

%(24.06.24, 9:06) Jeg har det virkelig godt i dag..:)

(25.06.24, 10:43) I've been of several minds about it, but I think I will actually not write the blockchain attack paper at this point. The thing that puts it over the edge is that I can see that the price of Etherium has historically been tied to Bitcoin. So even though it could make sense for sure, it's not certain that the Etherium stakeholders will be convinced that a drop in trust in PoW chains will lead to an increase in the value of Etherium (PoS). People in general could of course still short e.g.\ Bitcoin anyway, in a Goldfinger attack, but still, I don't know that it's worth it for me to write that paper.\,. Hm.\,. .\,.\,I do have a good point, though: If we assume that the Bitcoin community will not make the mitigating update that selects `honest forks' rather than any fork that has been part of a 51\% attack, and which therefore marks a fork as `final' when it's by far the longest one known in public, well then the Goldfinger attack can be fruitful for the attackers. This means that such an update/strategy is necessary for the PoW blockchain. But then this of course calls into question whether the whole PoW concept makes sense. For why pay the bill of so much mining as a community, when the blockchain, with the mitigating update, could easily work with a much smaller hash rate (and still prevent 51\% attacks)?\,. That doesn't really make much sense to do.

I also thought of a version of the attack this morning where the fork that the attackers control also simply implements the mitigating update itself. This should be possible (since Etherium is Turing complete, etc.). And then the community doesn't really have the same clear reason to declare the attack fork invalid after the attack, as this fork itself, if the miners just keep using the same software as distributed by the attackers, will guard against future attacks.

But still, I'm not convinced that a paper with these arguments will.\,. Hm.\,. (11:01) .\,.\,Maybe I should just sit on it for now.\,. (11:05)

(26.06.24, 10:23) Okay, just because the ratio of how cool people think Bitcoin is versus Etherium has been pretty stable in the past, it doesn't mean that their isn't a battle here. So Etherium stakeholders could have an interest for sure in exposing Bitcoin as ultimately relying on community decisions to stop 51\% attacks. Now, a lot of such stakeholders might be stakeholders in both cryptocurrencies. But even if just a tenth of them are not, perhaps because the want to join in this exposure and then switches to only investing in Etherium, they still have plenty of money combined for a 51\% attack. They even ought to have enough to do it several times such that more and more miners join in the bribery each time, which would then likely lower the bitcoin value and raise the ether value each time. So there we are, Etherium can in principle bully and own the Bitcoin blockchain, despite being much smaller, and that should really gt people around to not relying on PoW blockchains anymore.\,.

So that should be the thesis instead: Goldfinger attack by a rival blockchain.\,. Now I just have to find out what the disposition should be. (10:32)

(12:19) I just got an idea which seems to drive the whole point home: The PoS stakeholders can make (collective) smart contracts where they collectively reward mined bitcoin (or whatever PoW coin) with ether (or whatever PoS coin). So they don't reward just any old bitcoin; they only trade with ones mined on the attacking fork.

And you can even do it so that it's not just the mined bitcoin themselves that gets rewards, but where the helping miners can submit (a hash tree of) their work, and then this work also gets a little bit of reward (but not as much as when the miners succeed, for otherwise they won't have an incentive to build the chain longer and longer). This helps ensure that mining pool participants have an interest in joining in on the attack(/exposure).

.\,.\,Oh, and one makes sure that the rewards are given a while after the attack, such that the miners have to sell trade bitcoin on the at that point longest fork (if the `honest fork' has somehow become larger). So the miners of the attack don't have to sell the mined bitcoin, rather the just have to prove that the mined a coin to collect their reward (or prove that the worked on the attacking fork, if the want to collect their reward for that, where they then of course have to have uploaded a hash tree of their work to the Etherium blockchain at the time just after the given block was mined). (12:39)

.\,.\,The only downside is that it's all a bit complicated, but I should be able to explain it.\,.

%..Hm, I have to then first explain it in terms of economics / game theory, just using general contracts. And then I can write about the potential for implementing the contracts via smart contracts. ..And the overall 'rival blockchain Goldfinger attack should be pretty easy to explain, right?.. (12:52) ..I think so.. So maybe it won't be a long whitepaper..

%(16:11) Okay, never mind 'unlimited replays' (I don't think it's an advantage after all), which makes the disposition (structure) easier.. ..It should be pretty simple to explain, until we get to the mining reward contracts, and then the smart contract implementation of this..
%...But I should mention the 'divide and conquer' strategy then instead..

%(27.06.24, 9:11) I had the idea this morning that the attack funders don't have to direct the whole attack. They can instead just make sure that the miners are plenty incentivized to make the attacks. And by rewarding the attackers with future options to trade bitcoin for ether, they make sure that the have incentive to try to make the value of Bitcoin drop as well. They can also give out bounties for almost attacks, where only a fraction of the hash rate is converted to an attacking fork. They can then make bounties for e.g. 5 \% of the hash rate, 10 \%, 20 \%, 40 \%, 60 \%, 80 \%, and of course >100 \% *(well, >50 \% of the \emph{total} hash rate). They measure this coverted hash rate by having a sidechain that continuously keeps track of the newest block that is considered 'resolved' by the Bitcoin traders (6 blocks deep, or whatever). The reward-seeking miners can then upload their alternative forks to this sidechain. By considering the time of the upload, etc., the rewarders and the reward seekers can then settle if the sufficient hash rate was indeed required to upload the given alternative fork at the given time. Oh, and I should mention, part of the rewards should always be future options (to trade bitcoin for ether), since each bounty achieved is likely to make the bitcoin owners more and more insecure (and wanting to invest in the rival blockchain instead, which seems to be able to take over in the future (unless the Bitcoin cummunity switches to something like PoS as well)).

%I then don't need at all to hypothesize about "unlimited"/several replays or 'divide and conquer,' since the miners are now likely to use the latest mined block for their attack instead of an old one in this version of the 'rival Goldfinger attack.' (9:27)


%More (newer) disposition notes copied from the crypto attack paper document:
%"
%Disposition 1:
%- The Goldfinger attack is a known serurity risk for Proof-of-Work (PoW) blockchains, where attackers carry out a 51\% attack deliberately to cause the value of the related cryptocurrency to drop. Among the possible motivations for such an attack is that the attackers might seek an investment gain. This possibility has, however, not... for instance by short-selling a large amount of cryptocurrency before the attack. In this paper, we call to attention the danger of a special kind of Goldfinger attack where the funders of the attack are stakeholders in a rival blockchain.
%- Introduction: See so and so for blockchain. A 51\% is when so and so (see e.g. so and so). ..But so and so would make the value drop.. A Goldfinger attack is where the attackers have something to gain when the value drops. Authors so and so lists 3 possible motivations.. As an example of an investment gain, short selling is put forward. But as the authors note, it would be difficult for the attackers to get such large short positions in practice. ..Rival blockchain instead.
%Costs and benefits for the attackers: The cost of a 51\% attack on Bitcoin is very low compared to the market value of Etherium. It's so and so..
%Bribing the miners via smart contracts: ..The funders lock a portion of their ether in a sidechain. Participants of the side chain then frequently upload and sign off on the latest public blocks of the Bitcoin blockchain to that sidechain, with timestamps. The miners who seeks to be rewarded for an adversarial fork can then upload the last block in that fork, either to the sidechain, the mainchain, or to the Bitcoin blockchain.. Hm, maybe I should explain the more simple reward for >50 \% hash rate first.. Here the sidechain can simply note when there is a reorg of a large enough length to trigger a bounty. Hm, I guess it could be done on the mainchain as well, so what is more efficient?.. ..Well, it actually doens't matter: I don't need to mention the concept of a sidechain, as it is not really relevant to the point. The point is just that participants can lock a portion of their ether to a smart contract, where.. Oh well, maybe it is actually nice to explain it as a sidechain.. ..Yeah, it should be a sidechain.
%(13:08) Ah, I can mention the two options: Either the attacking miners try to keep their fork a secret, or they attack from behind, in public. The latter option is more costly, since you start from behind, but it requires less organization.
%(15:40) So hot.. Let's say that all parties have to upload proof (hashes) to the mainchain. ...Hm, the funders can upload it to the sidechain.. ...I think I've got it..
%(28.06.24, 9:00) I didn't quite, but now I think I do..

%Disposition 2 (outline):
%- Goldfinger attacks. It's hard to take large enough short positions.
%- Rival Goldfinger attack.
%- Costs and potential gain.
%- Bribing the miners with options contracts. (Options implemented as smart contracts.)
%- Automatic rewarding of the miners via smart contracts.
%- Intermediary goals. (Spiral where more and more shift their assets to the rival blockchain.)
%- Mitigation: The Bitcoin community should plan ahead and figure out how to ensure that the blockchain forks back to the original fork. This requires a break with the underlying philosophy of PoW where every man is for himself, and requires community planning and actiaon. Simply saying that 'we'll figure it out when we get there is not enough.' Once the attack is successful, this will cause deep uncertainty, and investors would want to move their assets to rival blockchains, and prefarably ahead of the curve such that they might gain from this movement. As this might cause the value to drop rapidly, the remaining 'honest miners' might also halt their mining, which means that the adverserial miners could get absolutely free rein on the blockchain, free to cause DoS for the blockchain as long as they like. (<-This should be part of a Ramifications section before this ne instead.) If the Bitcoin community wants to prevent this, they have to update their consensus algorithm in a way such that 51\% attack forks can be identified from the 'honest fork' and declared invalid by a majority of trusted forks. However, in a decentralized system, nodes are only 'trusted' until they are bribed with options from a rival blockchain. Unless their trust depends on the stake they have in the blockchain. Ideally, Bitcoin therefore ought to update their consensus algrithm and start using the PoS concept as well. Etherium has shown that this is possible, and viable.
%"


%(12:59, 29.06.24) Some very early drafting of the first couple of sections of the paper:
%"
%\section{Introduction}
%
%%In a Proof-of-Work (PoW) blockchain\footnote{See. e.g.\ cite for details of...}, miners compete to solve mathematical puzzles in order to win the privilege of adding a new block to the blockchain, by which they also gain some cryptocurrency. When a new block is added, the puzzle is changed, and the miners will then start working on that
%%The game is then constructed in such a way that as long as a majority of the miners 
%
%The most severe attack that a Proof-of-Work (PoW) blockchain\footnote{See. e.g.\ cite for details of...} might face is the so-called 51\,\% attack\footnote{See e.g.\ cite.}. In this attack, a group of cooperating miners controlling more than 50\,\% of the total hash rate of the blockchain changes strategy from the conventional one, and either stops broadcasting their work to other miners, or deliberately start working on a fork of the blockchain other than the longest one.
%This allows the attackers to change the recent part of the ledger, which means that they can potentially steal a considerable amount of cryptocurrency, and one that far surpasses the amount that they would have mined through the conventional strategy. (Rewrite and elaborate)**
%
%So why do miners of PoW blockchains not always seek to do this attack in reality, if they can make a greater amount of cryptocurrency this way rather than by following the conventional rules? The answer to this lies in the fact that a successful 51\,\% attack would likely cause the value of the given cryptocurrency to drop completely. So while the miners might indeed end up with a greater amount of cryptocurrency than they would have had if they had followed the rules, when trading that for ... they might end up with less..
%
%However, as Kroll, Davey, and Felten cite points out, attackers might have other motivations to cause the value of the cryptocurrency to drop. They might be state actors who ... They might be activists ... Or they might be investors who seek investment gain from the drop.., for instance by taking large short positions in the cryptocurrency. Kroll, Davey, and Felten cite names this kind of attack a Goldfinger\footnote{The attack is named after the James Bond villain, who in the film plans to ruin the gold backing of U.S.\ currency in order to undermine it.} attack.
%
%%(15:54, 28.06.24) Åh, hvorfor er her så varmt..! Det er kun 22 grader udenfor, og jeg har lidt vind ind ad vinduet, men her er så varmt.. ..Der er endda rimeligt mange skyer.. ...Så, nu kom der noget mere vind..
%
%..These authors points out that it is unrealistic for anyone to take such large short positions. However, there might be other ways..
%
%%(29.06.24, 10:44) Jeg tror jeg vil prøve en skrivestrategi, hvor jeg bare fylder på og fylder på, altså hvor jeg bare tillader mig at skrive rigtig lange og mange paragrafer, og tillader mig at gentage mig selv mange gange (jeg skal i øvrigt også gentage mig selv i den endelige artikel: Lidt gentagelse er godt, især for skimmelæsere (som mig, rigtig ofte)). ..(Og så skriver jeg altså bare det hele om og skærer ned efterfølgende.) ...Tja, måske dette er en god idé, måske ikke (måske skriver jeg bare dispositionsnoter så på ny), så jeg ser lige på det..
%
%Kroll, Davey, and Felten cite points out that it is unrealistic for anyone to take such large short positions. And this might indeed be the case. However, there are other ways that someone could gain an investment gain from a cryptocurrency crashing. And one particular way might be a rival blockchain..
%
%
%\section{Rival Goldfinger attack}
%
%The two largest blockchains today, in the moment of writing, are Bitcoin and Ethereum. Bitcoin is a PoW blockchain, while Ethereum has become a Proof-of-Stake blockchain(**) as of [...].
%
%When investing in cryptocurrencies, investors have to consider two things: What will the general interest in cryptocurrencies be in the near future, and what will the interest in each specific blockchain be. Some investors might choose to invest broadly, eliminating the risk associated with changes in the relative values of the cryptocurrencies, but others might invest in only some, and hope that these increase in value compared to others.
%
%If for instance Bitcoin's PoW consensus algorithm is shown to have security risks, this might make investors migrate to Ethereum instead and put their assets there. Since Ethereum has a PoS consensus algorithm, is does not necessarily have the same issues, and trust in Ethereum might therefore be maintained, while the trust in Bitcoin drops.
%
%And this is how investors might gain from a Goldfinger attack on Bitcoin: If they are stakeholders in the Ethereum blockchain.
%
%
%The market capitalization of Ethereum is roughly 400--500 billion USD in the moment of writing according to BitInfoCharts\footnote{See cite for Bitcoin and Ethereum statestics roughly at the moment of writing, and see cite for present statistics.}, whereas the market capitalization of Bitcoin is roughly 1.2 trillion USD. So if Ethereum were to grow to even just the same popularity as Bitcoin, such that the two cryptocurrencies split the total market capitalization evenly, this would mean an increase of roughly 375 billion USD for Ethereum, making roughly double in price.
%
%In comparison, the daily rewards for the Bitcoin miners are around 27 million USD, which means that 375 billion USD could in principle pay for a 51\,\% attack lasting as long as roughly 14.000 days, which is around 38 years.
%
%It goes without saying that a 38-year-long Denial-of-Service (DoS) is more than enough to obliterate the value of Bitcoin if the blockchain sticks to its PoW consensus algorithm.
%
%
%\section{Bribing miners through options contracts}
%"

%(13:27):
%"
%One way for Ethereum stakeholders to bribe the Bitcoin miners into a 51\,\% attack could be to offer them options contracts to trade ether (which is the cryptocurrency of the Ethereum blockchain)** for bitcoin (which is the cryptocurrency of the Bitcoin blockchain)** at a price which is only favorable to the miners if the value of bitcoin were to drop in the meantime. (So a price that is a little higher than the present ETH/BTC price.)*
%
%This strategy has the benefit of not really costing the Ethereum stakeholders anything much, apart from it costs them to set up the contracts. If the bitcoin value does not drop, the miners will not use their options. And if it does drop, the Ethereum stakeholders will lose some of their ether, but if they only sign contracts for parts of their ether, and hold on to the rest, their total assets will have increased in value still.
%
%So by handing out such options contracts to the Bitcoin miners, the Ethereum stakeholders can share some of their investment gains if the BTC value drops, and thus hope that the miners will act selfishly and initiate a 51\,\% attack on Bitcoin.
%
%
%\section{Options contracts implemented as smart contracts}
%
%Some blockchains afford their users the ability to make what is known as smart contracts, where the terms of the contracts are encoded in a scripting language, which then executes automatically on the blockchain when some conditions are met to trigger the contract. Ethereum is one such blockchain, whose scripting language is Turing-complete, and therefore gives a lot of possibilities.. See e.g.\ cite for an overview of how smart contracts work..
%"

%(15:07):
%"
%In order to implement an options contract for trading ether for bitcoin as a smart contract on the Ethereum blockchain is to be able to prove that you have transferred bitcoin to a wallet.
%
%Let us say that Alice want to give/sell an options contract to Bob for trading $x$ ETH for $y$ BTC at a time period between $t_1$ and $t_2$. The smart contract can then go as follows.
%\begin{enumerate}
%\item Alice locks $x+s$ ETH to a smart contract on the Ethereum blockchain, where the $s$ ETH is an amount that is used to settle the contract.
%
%\item Alice also declares a Bitcoin wallet where Bob is supposed to send the $y$ BTC if he wants to make the trade. This wallet has to be unused.
%
%\item The contract should also declare a hash of some old block in the Bitcoin blockchain, which can be used as a reference point for later, if Alice want to contest Bob's claim.
%
%\item At any point in time before $t_2$, Bob can then choose to transfer $y$ BTC to Alice's wallet. And when $t_1 \leq t \leq t_2$, Bob can then trigger the options contract on the Ethereum blockchain.
%
%\item If Bob triggers the contract, he then has to supply a hash of a recent block in the longest fork of the Bitcoin blockchain, as well as a number stating the length of that fork. He also have to lock $s$ ETH on the Ethereum blockchain which he risks losing if Alice can successfully contest his claim.
%
%\item Alice now gets a contest period to respond. If she does not wish to contest that Bob has transferred the $y$ BTC to her wallet, she can simply wait out the period, at the end of which the $x$ ETH locked to the contract is transferred to Bob. The additional $s$ ETH that Alice has locked is then unlocked, and the same applies for Bob. \label{option_point_contest_begin}
%
%\item If Alice chooses to contest Bob's claim, she can first of all do so by contesting the fact that $y$ BTC has been transferred to Alice's wallet on the blockchain that Bob has referenced. She can also choose to contest the validity of the referenced fork, including the declared length of it. Or she can contest the fact that Bob has referenced a recent block of the longest fork of the blockchain.
%
%\item If she does the latter, she then has to supply a recent block of another fork that is considerably longer, as well as a number stating its length. If this length is not long enough compared to the length that Bob declared for his fork, Alice loses the contest. Otherwise, the process goes back to Point \ref{option_point_contest_begin}, but with the roles almost reversed such that Bob now gets to contest Alice's referenced fork, either in terms of its validity, or in terms of her claim that the $y$ BTC has \emph{not} been transferred to her wallet.
%
%\item If Alice instead contests either the validity of Bob's fork, or the fact that $y$ BTC has been transferred to her wallet on it, Bob now has to reference the block took place.\,. Hm, it does get a little more complicated all in all than I thought.\,.
%\end{enumerate}
%"

%(30.06.24, 12:07):
%"
%\item If Alice instead contests either the validity of Bob's fork, or the fact that $y$ BTC has been transferred to her wallet on it, Bob now has to reference the block took place.\,. Hm, it does get a little more complicated all in all than I thought.\,. .\,.\,Hm, I guess Bob could state the relevant balances in his snapshot reference, but then all balances should be correctly stated at the start of the contract (when referencing the old block), so that doesn't really work so well either.\,. %(15:13)
%\ldots\ Luckily, blocks are only valid if the transactions are valid, which means that they cannot transfer BTC from wallets that doesn't have the funds.
%
%\end{enumerate}
%
%%(30.06.24, 11:35) I know how to make it work (by the way, Alice should be the first to declare/propose the longest fork, I think) now. And it's not hard to explain, it can be done in about 10 point like I have here above, I think. But a problem is that if the mining halts completely for the fork, then Alice could in principle bully her way out of the contract, if we are not careful, by keep mining on a private fork, and only showing the results once in a while when the contest requires it. It probably wouldn't be viable for Alice to do, I think, but it makes it a complicated mess; irritating that we have to consider all that. And here's the thing: For the more advanced version where the Alices pay the attacking miners directly, as rewards for creating an attacking fork, we might not have the same problem at all.. ..Here the rewards are simply given out to the miners, and.. And the miners then just have to trade exactly the mined.. bitcoin in the options, no wait.. ..Oh yeah, sure. Since getting Bitcoin to hard.. no, soft fork.. Hm, is getting Bitcoin to soft fork away from.. yeah, it is: It is a goal to force a soft fork away from the attacking fork. And in that case.. Hm, well, in that case we don't even need options contracts, do we?.. The attackers just need to provide Proof-of-Attacking-Work.. Hm.. ..Sure, and then the sidechain of funders can just continously sign off on what bounties/rewards they will finance, and thus how much they'll pay for how much attack-work.. (11:49) ..Or rather, they get the opportunity change their rewards at frequent intervals. ..And they get to change the starting block for the desired attack as well at frequent intervals, and each time they do, they also set the deadline for when that work must be turned in.
%%..Oh, let me by the way note that my idea for recognizing an attacking fork from an honest one is simply to say the e.g. 10 \% or so of each block in (the recent part of) the attacking fork has to be unused / left blank. Since the honest miners will always try to fill out the entire block as much as possible with transactions, this would never happen on the honest fork (it might happen that a one or a few blocks would be like that, but not several blocks in a row). (11:56) ..So that's how the attackers provide Proof-of-Attacking-Work..
%%..And as part of the attacking block, the attackers can also simply provide data that declares the Ethereum wallet that the want the ether to be sent to. Then this is done automatically when they provide the PoAW to get their rewards.
%%Okay, so let me indeed just forget about the options and talk about this possibility instead, as this is much easier, and also a smarter strategy for the attacking parties.. (12:06)
%"

%(12:08) Also:
%"
%And as another comparison, the reward per block is roughly 200,000 USD. It is currently common practice to wait six blocks before considering a transaction finalized. So in order to fund a 51\,\% attack that is long enough to allow for replay attacks, it only cost around 1--2 million USD, which is only 0.0004\,\% of the 375 billion USD that the Ethereum stakeholders would stand to gain in this scenario.
%
%So if Ethereum stakeholders believe that undermining the competition could increase their assets' worth even slightly, they have more than enough funds to finance such a Goldfinger attack.
%"

(30.06.24, 12:22) In some disposition notes somewhere, I almost make it sound like PoS is the only (potentially) viable option for a blockchain, and this is true in a sense, but only if we require that the valid fork is always recognized automatically by the network of full notes. In reality, any blockchain that allows its community to hard fork the blockchain by declaring forks invalid that embodies a ledger which hasn't been public throughout whole of its creation (save for small (and insignificant) time periods in the process of making new blocks) is a viable option. As long as the community is of one mind of when attacking fork should be declared invalid, and how to hard/soft-fork to another blockchain that contains all the transactions that has been freely available to the public, then.. Well, no system is completely safe, but as long that there are a vigilant community and enough transparency to quickly notice when/if previously trusted arbiters of what the state of the ledger is, then the digital currency can work. But of course, PoS is a nice concept that help ensure that the arbiters of what the state of the ledger is won't try to cheat the community---especially if the community is also still vigilant on top of the PoS-reliance. (12:33)


(13.07.24, 14:13) I have had some thoughts in the past about `Proof-of-Public-History,' or what else I have called it. Now I think it's all even more simple than that: I will instead just propose a change to Ethereum such that the stakeholders can distribute their voting power amoung some validators, free of choice. So in practice, this becomes much like a blockchain based on ``trusted authorities,'' expect that it is completely up to each stakeholder who (i.e.\ what third party) to vote for---and they can each distribute their vote to more than one party, as mentioned. And on top of this system, there should just be the fail-safe mechanism that I mention in my paper, that I'm about to publicize, which is just that any stakeholder can alert some fishy goings on, and when an alert is called, block finalization is delayed, and the stakeholders are called upon to revote for the recent (not yet completely finalized due to the alert) blocks and/or to redistribute their voting power (potentially away from whatever party caused the alert). Calling a false alert, without any good reason \emph{might} also be sanctioned. But that's something to look into.

%*(14.07.24, 15:31) Actually, I didn't end up keeping that part in the now publicized paper. It was: "[...] But this vulnerability is rather easy to mitigate, either by encouraging or requiring more stakeholders to stake their Ether, or by setting up a fail-safe mechanism where the idle stakeholders can quickly join the voting process and call for a revote regarding the recent, problematic blocks."

So this will be my suggestion for a future blockchain, and I hope the Ethereum community might find the idea valuable and want to implement it at some point (it sounds like a good idea to me). (14:24)













\section{SRCs} \label{SRCs}

In section \ref{SRC_instead_of_open_data} above, I've written some thoughts about a Web 3.0 SRC, and in those notes, I realized that it might be a good idea to also have a fixed amount of shares at each interval be distributed out to workers of the company, proportional to pay. This way, the idea also kinda fusions with the overall idea behind worker coops, and I think it makes the idea a great deal more appealing. Now the philosophy can be said to be: If you are dependent on the company, either as a worker or as a customer, you should be part of who the shares are redistributed out to. (22:28, 06.05.24)

(07.05.24, 8:48) I think if workers should be included, the shares that they get should also be proportional to the total sales, as well as to their salary. And it might be smaller fraction of what the customers get, i.e.\ such that a dollar spent on buying products or services from the company as a customer gets more customer(/worker) shares than a dollar in a worker's salary gets the worker. For otherwise the workers would often be much, much more powerful than even the top customers, and what's more, since the shares are redistributed slowly, the workers are generally not shareholders in the company that they work at, but rather whereever they worked 20--50 years ago. So in that sense, I don't think it makes sense for workers to get as many shares per dollar received as the customers get per dollar spent. (8:55) .\,.\,Oh, but my web SRC might actually be an exception to this, where it might make sense to have it be more equal. But I still think that the contributors should get a little less shares per dollar earned than the users get per dollar spent/donated/generated. (8:57)


(16.07.24, 12.47) I think I will also try to discuss my SRC idea with people in the coming time, but maybe a specific version of the idea where it is more a state/government initiative, more like the `Økonomisk Demokrati' (ØD) idea from the past. But then as opposed to ØD, the bill should \emph{not} be paid for by the stock owners. Instead citizens are forced to buy a (very) small fraction of the shares of a company at market price each time they buy a product or service (proportional to the price) (and a small enough fraction that it only raises the price ever so slightly). Or instead of forcing this for all companies, the government might instead just give tax breaks to companies that implements this system, to cover the costs of implementing and maintaining it. (Further tax breaks might also be given in a more lefty version, such that companies have further incentive to switch to such SRCs.) And then the great point that makes all this create a better future for our kids, grand kids, and great grand kids, is that when a person dies, the shares are given on to their heirs almost tax free. There we go. So with this system, people are just getting basically a better deal for their pension, and at the same time the majority of the public will get a larger and larger share in the stocks of companies in the country. Everybody wins. Except only the future rich kids / rich grand kids (etc.) that would have otherwise (if our system stays the same) inherited large fortunes from their billionaire parents, in a society that kept on moving more and more wealth in the upward direction.

(9:32, 17.07.24) Something that I haven't thought of before: Families! Families should be able to share expenses and thereby also get a share that isn't dependent, necessarily, on who bought the given product/service at the time. Also, parents should have the option to include their kids (or maybe they just have to), which means that the kids will also start out life with some shares already when they are 18 (even without having inherited anything yet).\,. .\,.\,Maybe parents should also be able to make their kids pre-inherit some stock, tax free. Things to consider. But yeah, all in all: of course the whole thing has to account for shared economies as well, outside of companies and institutions.

(11:21) Hm, but couldn't another version of the idea just be, in the specific case of Denmark, that ATP just changed to having a purpose of growing larger and larger?\,.\,. I guess this removes the nice quality that you automatically become a shareholder in the companies that affect you as a consumer. And it also removes the democracy, 'cause even though an institution like ATP might have good intentions, and make good decisions as well, it's still not actually a democratic institution, I don't think. But then, that could just be another change. And with an additional change of having to invest in companies where the clients consume, and if we're getting more advanced, a democratic system where voting power is distributed according to where the clients consume, then it basically becomes an implementation of the consumer movement that I had in mind (and described in the first version of the SRC paper, as I recall). So yeah, this is interesting.\,. (11:29)

.\,.\,That actually sounds relatively easy to implement, in fact very much so.\,. A change of laws such that ATP has to, as part of its goals, capture more and more of the assets where its clients consume, and then furthermore also become (if not already) a democratic institution where voting power, in terms of the companies that the institution is a shareholder in---oh, maybe ATP also doesn't use its voting power in the companies, I don't know (so maybe it does.\,.).\,.---is distributed according to where the individual client consumes (i.e.\ buys products and services, and for how much). And come to think of it, why not go international, with that set of goals?\,. ATP could do this, or other investment firms might do it, and hope to get support (based on the social promises for the future that this system leads to) internationally, and perhaps to a very big international pension fund.\,. (11:38)

.\,.\,Oh, and let me underline: The point is then to deduct a portion of what the clients are owed, as the whole point is that the clients ``donates'' to a more social future as part of choosing that pension fund. On the surface, `taking more money from the clients' might seem like it would get \emph{less} support, but if people can see what will happen as a consequence, it really ought to attract \emph{more} support. (11:42)

(12:02) Oh, and another difference is that in my SRC movement idea, the money are not necessarily just paid back at the end of your life. So it's more of an investment fund, rather than a pension fund. And this is quite important, as a big part of the idea is to make a transition to a more automated future much better on a social level.

So in terms of implementing the idea by changing e.g.\ ATP's goals, it should then also be changed so that, at least in a near future, the dividends can also be paid out (to a lesser extent, of course) during the whole life of the clients, and not just at their old age. (12:07)






\section[Backward payment]{`Backward payment' implemented politically}

(11:03, 19.07.24) In the past, I have thought a lot about how to get what I've been calling `backward payment' %(or 'bagudbetaling')
(although one could probably find a better name) without any political change to the system. But now that I've thought some more about it (in the last couple of days), it \emph{would} actually be a better outcome if we could implement it through government. For then we get a much better tool to guard against freeloaders, without having to do all kinds of things to mitigate this risk.

And what is it, this thing that I've called `backward payment?' It's the concept that people can give ideas and creative works to the public (or a semi-public institution), and then be rewarded by the public at some time afterwards---and preferable when the impact that the ideas had on people and on society is known---without having to first draw up complicated contracts for the individual each time in order to ensure this `backward' payment.

Now, I just wrote below about a future where robots (or ``robot scaffolding'';)) can do all out manual work, and where the programming of these robots will likely be the bottleneck of societal development, so to speak. Oh, and the robots themselves also needs to be developed in the first place, which could also be a public effort (if fact, that would be \emph{very} good, since there is almost *(in fact, erase `almost' here, I'd say.\,.) too much power if such robots become privately owned IP).\,. .\,.\,So for this reason, `open source' might very well be even way more important for the future than just in terms of how useful the internet becomes *(and other computer programs). .\,. (11:16)
%...Let me take a walk and think about what to write... (11:29) ... (12:19) Shiiit, det er godt vejr..:)

\ldots\ (12:32) There are plenty of areas where backward payment can be very useful. There is open source programming, of course. And I've just written about robotics where both the programming can be open source, but also the buildings etc. that the robot scaffolding builds/fixes/rearranges. And if we stick to that topic for a moment more, there's also the plans of the parties/festivals/etc., which I've talked about below today. These both needs planning in terms of construction, but they also need more general planning. And that is yet another thing where a good backward payment system also can work: When you've planned a nice festival, or such, why not share the plans with the world and get some monetary reward in return. Then others can use your work as well, and further modify it to create new things from it. By the way, the same could also be said for businesses/companies: These might also choose to share their plans and business structure, as well as technology, of course, and get monetary rewards for it if the same information can help others in the society as well. And apart from these things there are also music, now that everything is digitally recorded, as well as games (including board games), sport activities, exercises.\,. recipes.\,. books.\,. you name it. The general point is that these are all things where the value lies in the information. And in a very well-functioning society, information should be spread as wide as possible, whenever it can have use. For this will increase welfare and productivity in society, given that the creators/contributors are still paid a just reward for their creative work. And what's more, sharing these things openly also makes it way easier for the society as a whole to build on top of the technology (when all the information is open, more people can join the work). Oh, and I should mention, there is no reason to limit the backward payment system only to works that produces information-based value, but this is just the things where you can really make a great case for why a backward payment system is very good thing.

So how can you make a system that still pays the creators/contributors for their work even though it is distributed to the whole public? Well, it's not exactly rocket science to find out ways to make such a system function. I believe that I've written about several versions of the idea. Here is one that I really like: Each government could first of all be responsible for paying its own citizens as the first priority of the system, and then on top of that you can have deals for cooperation between all countries where the countries agrees to terms of paying each other as well, if their citizens use ideas and technology created by citizens of another country. But at a very early stage of the whole system, each country can just start by focusing on rewarding its own citizens. And in terms of how, one could simply have a system where each creator/contributor signs their own contribution, and get an obligation in return for a (perhaps vanishing) payment in the future. When the impact of the idea seems to be measurable, the government can start paying the creator/contributor an appropriate amount. In order to not create real debt hereby, the government can just promise to pay a portion of whatever is the total amount of money that is paid out at the time of the payment. So citizens continuously vote for the total amount of money should be given out at each.\,. quarter, e.g., which are then all given to creators of \emph{past} inventions/ideas/contributions. And who gets what of this total amount? That distribution is also determined democratically at the time when the money is paid out. So a creator will not at all know what they are owed when they get their obligation.\,. .\,.\,Oh, it seems that they are not called `obligations' in English.\,. Oh well, I'm just talking about promises to pay some money here. And in this case, the money will be determined democratically at the time of payment. If an invention turns out to become even more impactful (positively) at an even later time, well, then the government/people can just pay the inventor/contributor some more money on top of what has already been paid. And there we go, pretty simple. Is there any more I need to mention about this.\,.\,? (13:05) .\,.\,Nah, that should do it for the technical part, I think.

Now, companies should still be able to function in the same way a usual, of course. So movie producers and record labels, game companies, etc., etc., can still choose to get their money the old way. However, I believe that when things like that starts to become more and more publicly available, then more and more people might make do with that. This then forces more and more creators to join the open source system. It's a bit like how I imagine it must be for Spotify nowadays: When the majority get their music that way, then it doesn't really pay anymore to try to sell your music the old way. So I think there will be a positive feedback loop where more and more artists and creators will join the system, and start getting their rewards via this democratic (`backward payment' (or what to call it)) system instead. (13:15)

And as a final point (I think (for now)), let me repeat what I've just written below, and that is that the project of my newly invented `robot scaffolding' future really means that we as a civilization \emph{needs} to have such an open source `backward payment' movement. The things we will miss out on if we do not do this are simply (way) too great.\,!\,:) (13:18)

.\,.\,Oh, there's at least one more thing, which I will write about above under the SRC section of the Web ideas chapter instead\ldots\ (13:20, 19.07.24)












\chapter{Hopes for the future}

\section{Some hopes for the future in terms of what my ideas can hopefully help bring about}
\label{Some_hopes_in_terms_of_my_ideas}

(16:35, 24.01.23) In terms of my SRC idea, my other economy-related ideas, and my ideas about happiness and local communities.\,. %Oh wait, I have some other stuff that I want to write about my web ideas.. ..Hm, jeg tager lige en kort pause og for samling på de og disse tanker.. ...Oh no, they are actually related to this section.. But.. Hm, let me just mention them in the Web ideas section first.. ..There..
\ldots I really hope that this can lead to a future where people generally are busy with activities/businesses that are much more efficient in adding to their own and other people's happiness and at the same time much more efficient in advancing our technological level. The way I see it, the two things goes hand in hand quite a lot, for in my opinion, the way in which we busy ourselves according to the current societal systems are just very wasteful. Most of these activities deals with bettering the lives of, well, consumers, but I believe that if you look at the calculation in terms of how much the activity of each person actually benefit or total happiness, this activity is generally very inefficient. The big trouble is that our society is geared towards an unspoken, unelected ``philosophy'' that consumption brings happiness, when in reality there are much, much, much more important things to consider, especially in generally wealthy societies. I thus believe that if we really think about it and start planning our lives and societies better (in a decentralized way, btw), we can achieve much more happiness as people for a fraction of the effort. And this means that we can generally spend much more effort into activities that advance our technological level as well, so its really a win--win: If we optimize our activities in terms of bringing more happiness to people, we can then also spend more energy on activities that advances us as a civilization.

Okay, so that was some very broad strokes in terms of describing what my ideas related to these topics might be able to help achieve, without giving any reason why. In terms of my economy-related ideas that aims towards less capitalism, I then believe that these ideas can basically help us get out of said unspoken, unelected ``philosophy;'' help us get away from that direction as a society. 

And then there's the idea, which I haven't written about in a long time, about being able to ``pay workers/contributors backwards'' (what I have often called ``bagudbelønning'' in my Danish notes). I think this could bring about a lot of good in the future, but probably in terms of the web more so than anywhere else. I have just written somewhere above that I hope that the scientific community will generally join and take big part in the semantic web at some point in the future. I then also really hope that this will bring scientists and all other people much closer together, so to speak (i.e.\ working together), with amateurs also being able to take quite a big part in science and knowledge sharing.\,. which they already do a lot, come to think of it. And I hope that a good community around giving donations to helpful contributors, both amateurs as well as professionals (to thus add to their total income), can do a lot of good for the web. Maybe this ``backwards payment'' could even be a big part of getting scientist to join the semantic web in a big way.\,.

And in terms of my happiness ideas, well that goes pretty much without saying: If these ideas can be efficient in bringing happiness to people, this can then save a lot of wasteful and inefficient work/business/activity at bringing consumers happiness, and this saved activity can then be used on other things instead. 

In terms of the ``planning'' of how to change direction as a society, finding what problems to solve, and in terms of finding out what can more efficiently bring people happiness, I think that the semantic (and user-driven!) web can really help with all these things, potentially. One of the reasons for this is of course that I believe that we will be able to discuss matters much better on the semantic web, both in terms of the quality of discussion (because they can be better structured and because more people can be engaged in a single discussion), and also in terms of the quantity of active discussions that we as a civilization will be able to handle at once. But apart from this more trivial point, I also think the future (user-driven) web really can help people find together with other people with similar interests, and can be used much better to find ideas for activities --- and also ideas to structure one's life. Ugly sentence (and I've had a lot of those today) *(I was a bit roasted when I wrote these notes), but I hope it makes sense (at least if one has read my previous notes.\,.). Thus, I think that the ideas regarding ``user groups'' and ``user-driven ML'' will make people able to much better find things that interest them, and to be able to much better find people to be friends with. And as I have written about in my earlier notes, I hope that we will get to a point in the future where it will be normal for people to move together in small or larger communities with others that share similar interests, a similar demeanor, a similar approach to life, and so on, and that people will thus end up living much more in communities with exactly people they want to be around, instead of just living more or less with a random sample of society around them, and then having to look for friends other places. 

Alright, this summarizes the some points of what I hope my ideas can help achieve. I know this section, what I have just written, isn't super well-written and easy to understand (without understanding the ideas mentioned already), but I just had to write these thoughts down, at least for my own sake. So here we are.\,.\,:) (17:58)


(15:38, 26.01.23) Copied from above: ``And just to make clear, there is also another great point, which might not be so easy to ``sell'' since it is hard to argue that things will go according to how I imagine them, but which is really the big underlying reason why I'm so interested in all this. The point is that I believe that this technology can get us to a point where all of science can also be structured in a great semantically linked graph such that is becomes easy to look at all point and counterpoints to a given question, and to look at all existing solutions to a problem (and see arguments for their benefits and drawbacks). The same can also be said for open source programming: I believe we can get to a point where all programming solutions (modular) can be ordered in a great semantically linked graph. I believe that my ``Web 2.1'' ideas here, as we can call them, potentially might be able to bring about such a future, and I really think that this will mean so much for our scientific (and societal) advancement.\,.\,! %(Let me by the way mention here in the comments that I have thought about this today and reconsidered if I still really believe that my Web 2.1 ideas can lead to this, and luckily I have sort of arrived at the point where I think I will double down on that belief. For the way I see it, having a semantic graph over web content can very well become very popular, and this might very well further lead to the scientific --- and open source programming --- community/ties also making use of this technology to structure all scientific knowledge and discussion (each individual scientist (or programmer or amateur) taking part partly of selfish reasons to make their work reach a larger audience). And once such a well-structured graph becomes a reality, I believe this will... Hm, let me actually write this in the rendered text instead.. )
Let me by the way mention that I have thought about this today and reconsidered if I still really believe that my Web 2.1 ideas can lead to this, and luckily I have sort of arrived at the point where I think I will double down on that belief. For the way I see it, having a semantic graph over web content can very well become very popular, and this might very well further lead to the scientific --- and open source programming --- community/ties also making use of this technology to structure all scientific knowledge and discussion (each individual scientist (or programmer or amateur) taking part partly of selfish reasons to make their work reach a larger audience). And once such a well-structured graph becomes a reality, I believe this will greatly increase people's --- scientists/programmers as well as all other people --- ability to look up specific knowledge and to engage in discussions and innovation/solution-finding processes. I thus see that this technology can maybe sort of create a giant online collective intelligence --- not an artificial intelligence, but metaphorically speaking still a big collective brain. These are large words, but I really do think that such technology will give us intellectual powers as a civilization that is many times greater than what we have now. Anyway, I hope so.''


(12:13, 27.01.23) Hm, I think that AI and semantic web technology can potentially both be really instrumental in the development of each other: I believe that the development of A.I.\ can be greatly accelerated by having an open ``predictive model,'' as I have called it (or we could it a predictive knowledge/statement graph.\,.), with a lot of active users, and I also really think that AI could help the the development of such graphs a lot, namely since AIs could help generating a lot of these graphs automatically. I really see the potential of a great ``symbiosis'' in this regard.\,. But let me point out, that AIs will not be able to give us such predictive knowledge/statement graphs on their own, since a big part of these are open and free way for each user to implement various algorithms for distributing trust. Furthermore, while future AIs might get the ability to keep an internal ontology over the (conceptual) world, such an ontology might be very unreadable to humans, unless said internal ontology developed ``in symbiosis'' with a human-readable semantically structured knowledge/statement graph. (12:24)




(02.06.23, 13:07) I thought about my ideas for a ``happiness currency'' (or whatever I used to call it) last night in bed. I believe that my last notes on that subject is actually in the comments somewhere in my 2021--22 notes. I moved away from the idea because I came to the conclusion that me idea that would later become my ``SRC idea'' was much more likely to be able to lead to a similar kind of future, and much quicker as well. I still believe that, but I just thought about how, once we reach that state where power and wealth will be quite broadly distributed, then.\,. Oh well, maybe I'm actually just about to reiterate what was already an earlier conclusion: that there is a small step from the future that SRCs (in my opinion) promises and then to further implementing a system where people's income will be decided by a democratically run system/model that rewards all kinds of behavior that creates happiness for others, both down on the level of the local community that the given person lives together with, and to any group of people above that. And the happiness-creating efforts are then not limited to working hours: If you are active in your community in your free time, this could in principle be rewarded just as much by such a system. So about getting away from that ``philosophy'' that down-prioritizes any free-time work/efforts that a person offers to their fellow people, such a system might help us get away from that. It might boost people's interest in working less hours at their daytime jobs, and then use some of that time to do, what we today would call ``voluntary work.'' Of course it has to be noted, that giving financial rewards to people for making other people around them happy, does have a little bit of an.\,. iery.\,. *(oh, `eery,' of course (13:30)) Hm, I can't find the right spelling for that word, so let me just say: an discomforting ring to it. And it will certainly be important to discuss such concerns thoroughly before implementing such a system.\,. well, at least on a large scale, but of course it could just be tested by small communities first (which is what should be done for all such suggestions (following what I have discussed in earlier notes about testing different societal ideas in small communities)).\,. (13:31) But maybe a system like that could really boost people's engagement in local activities in a good way; without causing much of a feeling of people ``faking it''.\,. .\,.\,I'm not completely sure, but I actually think it could.\,. .\,.\,Yeah, and for some reason I find that thought quite comforting, so that's why (for one thing) I wanted to take some time now to write these notes (this paragraph) about it.\,.(:)) (13:35) .\,.\,Oh, and just to underline, part of my point here is also that that kind of system would not require nearly as much.\,. todo.\,. as what I wrote about in my last notes about the subject (as I remember it). It would thus not require any future promises.\,. Oh, and more to the point: It would not require a \emph{currency} (which my last notes on the subject was about)! It would instead just require a democratic system where some money are pooled together at frequent intervals and then distributed out as a sort of income to people, where good deeds for the society (the local one or the more global ones) are then rewarded with a little extra. And I guess these are the points that are really worth mentioning here, namely that once the SRC future has already become a reality, one does not need a lot, and in particular not a new \emph{currency}, in order to still achieve what my earlier ``happiness currency'' idea sought. (13:43)



%(07.09.23, 18:49) En lille hurtig note, som jeg bare kan give her ude i kommentarerne, er at jeg jo stadig tror rigtig meget på min e-demokrati-app, og tidligere i dag kom jeg så til at tænke på, hvordan, som jeg før sikkert har skrevet et sted, det kunne være rigtig fedt med en grundig model over, hvordan samfundet (f.eks. det danske samfund eller verdenssamfundet, osv.) fungerer. Men så tænkte jeg, at lige netop denne app, der jo på sigt giver en model, kan vi sige, over forskellige grupperinger i samfundet samt deres løfter og holdninger til ting, kan være det perfekte udgangspunkt for sådan en samfundsmodel. På en måde vil den jo allerede føre til en slags samfundsmodel, hvis idéen kommer til at gå så godt som den kan. For så vil alle forhandlinger jo være med udgangspunkt i nogle instanser i samfundet, og hvordan de interagerer med hinanden. Så ja, man kunne faktisk få lige netop en sådan udførlig samfundsmodel herved.. Ja, eller i hvert fald hvis brugerne/grupperne så også i reglen sørger for at prøve at lave en (del-)model over, hvordan de instanser de administrerer / er en del af fungerer under forskellige parametre, hvilket jo kan være en god måde at formulere sine løfter til andre grupper/instanser, som er med i appen. Så ja, \emph{hvis} idéen udlever sit absolut fulde potentiale (og måske lidt mindre endda), så kunne appen altså også lede til netop sådanne (vildt detaljerede) samfundsmodeller. (19:00)
%(08.09.23, 8:47) Ja, så fra en e-demokrati-app kan systemet altså potentielt set udvikle sig til en hel model over alle mulige instanser (inkl. private virksomheder osv.) i samfundets forhandlinger samt stående løfter med hinanden (og muligvis inklusiv fremtidige løfter, der så kan afhænge af parametre).



\section{Some comments etc.}
(18.10.23, 10:38)
Let me just write a few notes here that I've had on my mind.

.\,.\,Okay, maybe this will actually be quite short. About the consumer union idea, I just want to point out that part of the motivation is create an environment where bad costumer experiences are more easily shares with the rest of the consumers as a group, and decisions to take these into account and do something about them will be greater. .\,.\,I feel like this should be unpacked a bit by some examples, but I'll just leave that for the reader to do here.\,.

Another point about the consumer unions, that I've probably mentioned before also, is that it could mean that the consumers could actually work together an help the industry in some ways, namely by ordering product in advance and in general behave more predictably. Of course, a significant level of unpredictability is necessary for consumer happiness, since people have different needs, and people also want e.g.\ their furniture (etc.) to be unique for them. But while everyone want some level of uniqueness to their combination of belongings, a lot of people, I think, could be perfectly happy---perhaps even more happy---with fewer choices when it comes to certain things. If consumers with similar.\,. indifferences would then join together and make big orders together, it might thus both ease their lives and the lives of the manufactures and sellers etc.

Then there's of course the big point about it being easier to be political consumers in a unified group, and a big example here is the consideration for environmental issues. But another example that I've thought about could also be to boost.\,. the wheels of industry locally to the consumer union. So that the consumers become a part of the bigger economic picture and help.\,. not \emph{plan} the economy, but to \emph{help} plan what the industry should aim for.

About another topic, namely e-democracy, I still really think that an E-democracy party like I've written about (maybe in some notes out in the source comments from 2022, I'm not sure.\,.) could become a great (and popular!) thing. And a big point about it that is worth underlining is really the transparency aspect of it. Present-day parties often have a big incentive to keep their fundamental, day-to-day workings---their deals and decision processes---hidden from the public. Not only would an e-democracy party be incentivized to the contrary; to make their decision making processes public, but since the party is comprised of all interest group in society, the ``inner'' dealings would be.\,. well, much more interesting.\,. when it's not just about dealing about what policies to follow between almost like-minded people, but is the very meaningful dealings across groups in society. An e-democracy party could thus make a whole show about their discussions, even one that might be entertaining to watch (for anyone who is interested in society and the questions and need for decision-making that it faces).\,. (Don't expect me to write very elegantly right now, by the way.\,;):)) I'm thinking televised discussions, where interested people can follow along, and where they can dive into any sub-party's own decision-making processes, before and after a debate. But anyway, even if this is a thought that is easier said than done in reality, I'm still sure that an e-democracy party could get a lot of positive attention and trust by publishing all decision-making processes, whether it be the cross-party debates/discussion/dealings or those of the individual sub-parties, which might be done before or after a big debate. Now, from the average person's point of view, they will probably not want to spent any time at all, really, following debates and issues. But all will probably follow something, and enough to get an idea of what representatives they trust on a decent enough level to give them their vote(/voting power). And the good thing is, that if you are skeptical towards the party, then only need to find someone who has the time to look closer at all the published decision-making processes and that you trust enough to feel assured that if something is off when your chosen person takes a closer look at the inner workings of the party, then that person will notify you and others about that. I'm not explaining this very clearly, but the point that I'm trying to re-make (I've made it before), is just that one about how you can see it as a pyramid of trust, where each layer only has to spent enough time an effort into checking that the ones in the layer above them is doing their job satisfactory as that requires, and not any more. That way you can get a structure where the top is completely accountable to the bottom, but where each individual at the bottom don't have the responsibility to keep up to speed about what's happening at the top (since a lot of that responsibility is then handed to the next layer in the pyramid, and so on). And the lower layers will always have some incentive to follow what the upper layers are doing, either simply because of their interest in the workings and governance of society, but since the ``middle layers'' in the ``pyramid'' might very well be employed by the party and/or be the people below them who has, well, essentially hired them to be vigilant on their behalf, there could also very well be an economic benefit to moving up in the layers (not that the party \emph{needs} to have \emph{many} layers; could be only a couple for all I know), which would then incentivize more vigilance (am I using that term correctly?\,.\,. .\,.\,I think so.\,.). (11:26) .\,.\,But yeah, all in all, I think that such an E-democracy party could become massively popular.

.\,.\,I also have a few comments about my Semantic Database idea, but let my write them under the Web ideas chapter.\,. (11:29)

\ 

(12:30, 18.06.24) About my e-democrats/direct-democrats party idea, I think the party should note how the mean opinions differ from those of other elected parties/people outside of the e-democracy party, and then make a bias for its decision making in the other direction, not necessarily by much, but enough to create an incentive to choose to vote \emph{for} the party rather than for an opposing party *(when first the party has grown big), and then still vote inside the e-democracy party as well (which everyone is always allowed to). Well, but this reminds me that people could then potentially game the system by voting for a party that represents the opposite of what they want. But I guess the fact that the bias should only maximally be enough to slightly punish voting for another party is enough that people don't want to do this reverse-psychology-like trick to to try to game the system, and that people would then rather support the `E-democrats'/`Direct Democrats' party. .\,.\,(And I really still think that this could become massively popular.) (12:41)

(15:31) How much to ``punish'' it when people elect members from other parties (or outside the e-party in general) should of course just be a (direct) democratic decision of the party, and thus one that can be changed continuously if needed.

(20:01) Let me also mention that my idea now is that all participants (online), which can include all people of the country as long as they verify their identity, should be able to upvote questions for the leaders (who are hired by the party, essentially, and can be replaced at any moment if the members vote to do so). Any question that gets enough votes to be part of the new round of question will then be put up for a vote by the members. (And remember that all people can vote on anything, and when they don't their vote just goes to a selected representative, which can be \emph{any} other member. (And if that representative does not vote either, then it goes on to \emph{their} representative, and so on.)) When the questions have thus been answered by the members, the leaders are then obliged to make policies that honors these votes (and if they fail to do so, they are supposed to be fired and replaced by the members. And on the other hand, the leaders (i.e.\ the ones elected to rule on behalf of all the members) can also add questions of how to act in whatever problems that might arise. And then the members are also supposed to vote on these, as a way of continuously guiding the leaders in their decisions. If the leaders fails to ask questions that they ought to have asked about before they made a decision, then this should also be grounds to fire and replace the leader (for someone that's hopefully better at following the protocol of the e/direct-democracy party correctly and honestly). (20:12)

.\,.\,As a bonus point, if I were to try to start an e-democracy party, I would also let it be part of the vision as a whole that the party should strive towards implementing an online platform where the members can argue, make deals, and also kinda map out the society, hopefully. I'm of course talking about my `e-democracy app' idea here, and I'm also talking about my idea of making a society model (which is also part of an advanced version of the app idea). But yeah, in short, it should be part of the vision to make it so that members (i.e.\ people of the society) can argue well and efficiently with each other online, make models of the current society and of various plans (to make changes, etc.) to consider and discuss, and also to make deals with each other (between the various groupings within the society). (20:21)

(10:54, 20.06.24) As a quick point, one of the selling points of an e-democracy party is that conventional parties has to span over a lot of groups in society, and a few representatives therefore has to be split between a lot of groups, with varying opinions in some regards. But this is not the case for the e-democracy party. Here anyone can get exactly the representative they want, and they can, in the advanced stages, get to make deals as a small group with other groups of any sizes, and thereby really reach their full political potential as a group.

Another practical point is that, if one were to start an e-democracy party today, e.g.\ in Denmark, one could just start out by utilizing a public Facebook group for discussions. Now online discussions are very important for an e-democracy party, especially because they help users select their representatives (which can be any other user---or can be a list of users, by the way, if you don't want to rely on your representative's representative, etc., but want to choose the alternative yourself). For not all people can be politically active at once, so most people need representatives. And here you can choose one of the very active ones on the forum/fora, whose opinions are therefore more mapped out, and whom you can also be more confident will cast a lot of votes. Oh, by the way, users should be able to choose to let all their votes be public, and it should then be expected that all people who wants to be representatives for others should choose this option. (11:06) Facebook also has votes---and also likes, of course---so the party's elected leaders (fireable at any moment) can see what questions the users wants to vote about. The actual voting should then take place on a private website, with better security. But the point is then that this website doesn't have to implement discussion fora necessarily to begin with. It only has to implement ranked-choice voting for the various questions (that the leaders are responsible for copying from the Facebook group an initiate on this private website). So we can therefore make do with a budget website to begin with. And then the website should then expand from there to include discussion fora, as well as a direct way for the users/members to vote a question into action, and down the line the website should also implement my whole e-democracy app, which I have written about in my 2022 notes, out in the source comments below.

By the way, the organization behind the e-democracy party that makes the apps for it could actually potentially be a private company, which means that when (\emph{when};)) the e-democracy party concept takes off and spreads to other countries, these parties might then buy the same digital solutions. I thought this worth mentioning, even though I would of course prefer open source way more (or at least an SRC, which could then very well be exactly the one that I have written about in Section \ref{SRC_instead_of_open_data} above). (11:18)

(17:05) It should also be noted that the elected/hired leaders should continuously write rapports about what they are doing, who they are meeting and why, what agreements they make (unless this is somehow classified (although in a true democracy, one would want to limit classified stuff as much as (reasonably) possible, I'd say)), and also not least about how they intend to carry out the various demands from the members (i.e.\ the questions that are voted on).

(12:35, 30.06.24) I can also add that, if it were up to me to start such a party, I would also even go as far as recording and streaming the majority of the hired leaders' work day, each day. Only meetings with people outside of the party can sometimes be allowed not to be streamed and recorded, if those people really want that. But as time goes on, it might even be regarded as less and less acceptable to insist on a private meeting with the hired party leaders, the way I personally imagine it.\,. (12:39)

%(19.33, 06.07.24) I should also add, if I haven't yet, that people should be able to divide and distribute their voting power out to several other people (for instance people you know personally---which would be the absolute easiest way to vote, espacially for people who don't really use the internet *or follow politics).

%(13.07.24, 14:08) Let me just highlight an important difference between an e/direct-democracy party and a normal party like the ones we have in Denmark: In a normal party, you try to select leaders based on thier opinions (and qualities), and then let it be up to them to make the decisions, whereas in an e-democracy party, the leaders are hired to not to follow their own opinions, but those of the members (and can be fired/reprimanded more easily/frequently).

(16:12, 17.08.24) A good slogan for the direct/e democracy party could be ``a party that listens to and follows the opinion polls.''

(06.11.24, 11:14) `E-democracy' is maybe actually a not-too-good name for it, since it might make people associate the idea with ``a democracy purely/mostly on the internet.'' While opening for more discussions and usage of the internet is also part of the idea, this does not mean that decisions and discussions, etc., has to be done online. The direct democracy (party) can work much like present democracies (parties), only where the members more easily take part in the decision making if/whenever they want to (and have more direct power over the party/government).












%(9:44, 08.12.24) Kom lige på en lille idé helt urelateret til noget andet her: Et lovforslag om, at hver gang man handler med aktier, så tilføjes der en lille smule til den rate, man bliver beskattet med i sidste ende, når man skal betale skat af sine gevinster. Hver handel forøger så denne rate proportionelt med størrelsen af handlen, målt i kroner (eller anden valuta). Og tanken bag dette er selvfølgelig at mindske day trading (or week trading for den sags skyld), da det virker til, som jeg da kan se, essentialt set bare at være gambling i fåreklæder, og uden gavn for samfundet, og til gengæld med en negativ virkning, fordi investorerne så ikke er commitede til at bestyre virksomhederne. (Correct me if I'm wrong..) ..Interessant lille idé, synes jeg selv..:) ..Nå ja, og har også den negative effekt som gambling generelt har, hvilket er, at huset skraber til sig, i dette tilfælde hvor "huset" bare er de store spillere med forbindelserne og ressourcerne til at træffe de gode og hurtige valg.. (9:54)












\section{Robots}

(19:07.24, 9:15) I've been thinking about various things these last couple of days, and also a bit about robots (and modular building) yesterday. And today I actually had a quite interesting idea about.\,. .\,.\,well, about robots.\,:) The idea is a kind of self-assembling robotic system. Specifically, the idea is to have robotic arms on rails, where the arms can assemble these rails themselves, and where we are not at all just talking `rails on the ground,' but more like scaffolding. So a self-assembling scaffolding with robot arms that can move around on the rails of the scaffolding (and which are the ones that assemble/rearrange the scaffolding).

So imagine that you want a house built, as an example. Then you rent such a scaffolding robot. It transported to you (perhaps in self-driving cars, if we want to be really automatic). You place an initial.\,. `scaffolding seed' (;)\textasciicircum\textasciicircum) at the grounds. You direct the robot build itself around the perimeter of the ground. Well, and it could also be around an existing structure that you want to fix and/or rebuild. Once the robot-scaffolding is around the area, scanners of the robot can start scanning the outside. It then builds itself up all around the working ground, getting rails/bars from the truck(s) (which can keep coming if more material is needed) via rails that run from a parking place and to the work area. At the parking place, or the deposit place, let me call it (I don't know the technical term for this), the robot is also assembled into a robot-scaffolding, that can extend itself into trucks and containers, and withdraw and deposit material and tools. The scaffolding keeps scanning its surroundings always, and keep a digital 3D image at all times of the whole the whole thing. And then it gets to work building. Let me already point out that when we think of robot arm, we typically think of pincer grips. But I'm imagining something a lot more advanced, perhaps modeled after human hands, why not. That ensures that the robot can in theory do every specific job that a human can, at least if it can get they space. As you've guessed, the robot scaffolding builds and reassembles itself by screwing on and unscrewing rails. Rails can be of various sizes and weights, and so can the hands, of which there are several, moving around each other on the scaffolding. If for instance the robot has to do work inside of a room, with a floor that shouldn't get marks on it, it can use very light scaffolding, and use smaller, less heavy arms for the jobs. *(And it can use wide feet for the scaffolding with a soft material underneath.)

Now, most of a building process will be programmed in advance. And of course, the individual hand actions to do various jobs are pre-programmed (and will probably take a lot of effort to develop for each specific action). And then if something unexpected happens, maybe the programming can figure out a solution on its own, or it can ask for help (from humans). .\,.\,Hm, what do I actually need to say more?\,.\,. (9:48)

.\,.\,(9:50) Ooh, if it's human-like arms, that might also make it easier for humans to take over from afar, when there are specific tasks that the robot doesn't yet know how to do, or when an unexpected task arises. This could especially be beneficial in the early days, where there will probably be a lot that isn't pre-programmed. Then the constructors can stand in a scanner someplace afar, and do remote controlled tasks (in a VR-like setting) for whichever robot scaffolding (out in the world) needs it at the time.

So that is the idea, and I actually think that it's quite awesome. 'Cause even though it might take some time to become really useful, and more useful than what human crews can do by themselves (with machines but without robots), just imagine, however, how awesome it will be when the robots start to become very functional. It will actually be somewhat of a singularity, since these robots can then also do all the tasks that goes into making the components for themselves. (We can also have whole factories operated fully by scaffolding robots, and much more.) And at that point all is needed are the programmers for programming new actions for the tool belt of the robots, as well as the fallback constructors for the VR setups.

(Of course there might be some social concerns with such a powerful system, and in particular about a loss of jobs, but this is \emph{exactly} why we so desperately need as a society to implement an economic system that is much more safe in terms of securing social fairness and equality (even in a future where most of the present jobs are long since automated).\,:))

These robots will then not just be useful for construction, but can be useful for everything, even down to cleaning. Every apartment complex (or `house complex'.\,.) might have there own scaffolding robotics at the ready (or might share with neighbor complexes). So cleaning might be taken care of by robot scaffolding (in general of the more delicate variety). And it doesn't stop there. Parties might be prepared for and cleaned up after very easily be robot scaffolding, and even larger things like festivals. So even a small town might do a great variety of festivals and parties each week, in principle, when so much of the physical work is automated. And when it comes to planning (and I've wanted to mention this), plans for parties, festivals, etc.\ can be shared among towns/cities---and shared across the world. So once you've planned out the.\,.

Oh, let me actually first talk about the fact that when programming a specific construc-tion---and something similar can apply for other types of jobs---the user of the digital tool first of all simply draws up what they want as the final product (or in terms of cleaning e.g., where and what they want to be cleaned). The program then tries to came of with a plan for how the robot scaffolding does this task, with all the various subprocesses. And the creator might then, and this is probably especially useful in the beginning, try to help the program plan the actions, if the human have an idea of a better plan of how to achieve the goal. And here's the great thing: Once a plan is saved, it can be reused (and further modified, by the way) by other from that point on.\,:)

So if we go back to the paragraph before, people can share their party and festival, etc., plans across the world, so that other might try the same kind of things. How great would that be: A bunch of varied festivals all year around in your own local area, if you want.\,:)

Another thing that I want to mention is the fact that when robots can so easily rearrange the interior of a building (complex), it means that we can also share things and furniture much more, and use rooms for multiple purposes at once. For instance, most people sleep at night, so in the day time, there is really no use in having a bedroom in the house, unless you need to take lie down once in a while. So for the most part, you can just collapse the bedroom, which can be done by having your `house robot (scaffolding)' (in a future after the robot-singularity, where robot scaffolding has become abundant) move the furniture into a deposit and replacing it with something else. And this is just one little example. The possibilities are limitless. (10:26)

And as a last thing, let me also just mention something else, which is that I could also imagine that building blocks might become more modular and thus more reusable in the future---in fact, this might help the robots. Instead of hammering and plastering things together, I could imagine that we might begin to rely more on more modular building blocks (of various types, though) that are connected together via interlocking mechanisms (instead of nails and plaster)---sorta like an advanced real-world version of LEGO *(although not necessarily anything alike in terms of the interlocking mechanisms, of course). This means that the robot scaffolding will just have to assemble or split up these interlocking modular building blocks (of various types) for the most part in the construction process, at least when it comes to things like wall, floors, roofs, and such. And with this modularity, it will be easy to rearrange things after what is needed. .\,.\,Sounds like a pretty awesome future to me.\,.\,:) (10:34)

\ldots\ (12:20) (When thinking of scaffolding, don't think of something quite static: Think of something dynamic that can continuously change and rearrange itself, and which can extend itself everywhere in principle, also throughout the inside of a building, for example.)

It is really important that this project is an open source one, not just for social reasons, but also especially because this would greatly help development, i.e.\ when everyone across the world can help program the various actions that the arms can make (in the ``tool belt'' of actions), as well as the overall programming of how to plan and execute actions. (12:25) .\,.\,(And that's why we absolutely needs to get ``backward payment'' systems implemented across the world as a civilization, and preferable one where there's a whole lot of cooperation between each government in terms of making sure that helpful contributions are rewarded.) (12:28)

(22.07.24, 10:36) I actually think that the idea to specifically using human-like arms for the robot scaffolding could be quite grand, especially in an open source version of the idea. Well, first of all it means that workers can work remotely with the robotic arms, as mentioned, and manage all required work tasks during the operation, either in order to correct faults, or to fully handle more delicate and/or advanced types of work. This could be very useful in the beginning. But what's more, I think it could \emph{greatly} help the training of an AI (using the terms a bit liberally: `machine learning' would probably be a better choice of words) for the arms. For this means that people can contribute to the AI simply by putting on some sensors are doing a specific job several times. They can then send this data to a hub which can then use it as positive examples to train the AI with. This is opposed to when the AI has to train only by trail and error with its own non-human-like robotic arms, where these arms have to be built and experimented with in order to gain the training data. With the human-lie arms (and hands, of course), on the other hand, only the sensors needs to be made, and given out to workers that are experts at doing whatever task in question that you wish to train the robot scaffolding to be able to do.

.\,.\,Well, I guess you also need scanner that films the setting from the POV of the worker (so some googles), and is able to traslate that into a 3D image in real time. I don't know exactly how for such 2D-to-3D-image (real-time) scanning technology is at the moment, though, so this might become a bottleneck: We will surely need such scanning technology for the robot scaffolding to work, regardless of the training, so the idea seems to depend a lot on when (if not already) this technology is developed. (10:55)

\ldots (11:23) It seems that `photogrammetry' is pretty developed already.\,:)

(12:01, 09.09.24) The first thing to implement is of course the ability to assemble and disassemble, and thus also to move itself. And then we should also implement the ability to move things around, such as building materials, first of all, but also at some point to help people get around through the ``robot scaffolding,'' perhaps by carrying them in some kind of gondolas, or perhaps simply by having conveyor belt and elevators help move them around to where they need to be. Thus, at an early stage, before we get the VR-controlled robot arms, the robot scaffolding can be just that: a means to get materials and people quickly to exactly where the need to be for the next task (that the robot arms haven't yet mastered, and then once they master more and more task, the people will be needed less and less, including remote ones). That could be a decent start of the technology.\,. (12:09) .\,.\,Oh, and the people (i.e.\ construction workers) can also possibly help the robot scaffolding build itself at the early stage in cases where the arms have not yet mastered a specific required task for this (e.g.\ if the ground is very uneven, or things like that). (12:11) .\,.\,(Of course the robot scaffolding still has to be at least \emph{almost}-self-assembling and -disassembling in order for it to make any sense.)
















%## Future "church"

%(10.10.24, 10:10) (Haha.) I just had the thought: Instead of churches at sunday, teaching christian morals, which to be honest, doesn't do too much in actually helping us with our moral systems today, we should have a "church" instead (I mean for those who want it) where we meet up and discuss morals and values (although it should still be mainly like "preaching" and not so much a discussion forum, but the participants might also give inputs, who knows..). And then we can make sure to spread good morals and value, and not least everyday tips about how to tackle everyday situations (and of course situations that are not everyday). That's really the main focus, apart from the overall value system, that the commuity wants to follow, but also specifically points about how we ought to behave in various situtation, and not at all just in order to benefit the groups, but very much in order to better the given actor's own happiness, and that of their family and friends. (And of course, bettering the happiness of the community, now and in the future (so child raising is also a topic, e.g.) is also part of it, but a big focus should be on bettering our individual lives.) Just a thought, and of course, it doesn't have to be on sundays, and it doesn't have to be only on sundays, but maybe it could work nicely as a sunday activity, providing an alternative to the regular kind of church..:) (10:20) ..Oh, and of course, life and death and meaning could also be part of it.:) (10:20)









\chapter{Existence theory}


\section{Empathy utilitarianism}

(19.01.23) Jeg tænkte i går (omkring kl.\ et, var det) for sjov på, at man kunne kalde min etiske lovsætning, som jeg har beskrevet i de udkommenterede noter i Chap.\ \ref{notes_from_2022}, for `empatilitarisme.' Og så kom jeg så efterfølgende til at overveje seriøse bud på et navn, og så kom jeg jo hurtigt på, at man kunne kalde det `empatisme.' Umiddelbart et ret flot og passende navn. Nå, men lidt efter fandt jeg så også på, at et oplagt navn jo ellers vil være `empatiutilitarisme.' Jeg har i øvrigt ikke søgt på, om det allerede eksisterer, det kan også godt være. Men hvis ikke denne etik er kendt allerede, så håber jeg altså på, at jeg kan (være med til at) udbrede den. Og så kunne `empatiutilitarisme' (`empathy utilitarianism' på engelsk) altså være et ret passende navn. (12:39)



(26.01.23, 17:53) Princippet om, at ``alt hvad der kan eksistere, eksisterer,'' er vistnok (allerede eksisterende og) kendt under navnet `the principle of plentitude'.\,.

Det virker i øvrigt på den PBS space video (kendte ikke kanalen før), som jeg lige faldt over (hvor jeg lige har hørt om the principle of plentitude), at MUH gør nogle flere antagelser i sin konventionelle udgave, end den egentligt behøver. Men det kan selvfølgelig også bare være kritikernes overfortolkning af det (det virker som en standard ting i filosofi: Kritikere kan altid bare overfortolke et udsagn eller en teori, og kan dermed så nemt finde en måde at erklære sig dybt uenig med det/den.\,.), det ved jeg jo ikke. Men hvis ikke det bare er en overfortolkning, så kan min, mere generelle, udgave af hypotesen altså helt sikkert være gavnlig. Og selv hvis det bare er en overfortolkning, så er jeg stadig ret overbevist om, at jeg kan hjælpe diskussionen på vej en hel del. (Umiddelbart tror jeg også, at de fleste mennesker, selv fagfolk, har en ret specifik forestilling om, hvad matematik er, hvilket jo så gør det let at overfortolke hypotesen, når man så navngiver den `MUH'.\,.) (27.01.23, 12:02) .\,.\,Det skal faktisk også nævnes, at selv hvis den konventionelle udgave har færre antagelser, end at nævnte video antyder, så er det dog stadig helt sikkert, at min teori er meget mere generel end den konventionelle MUH/CUH, for det virker helt klart til, at denne om ikke andet antager hypotesen om, at der ikke er nogen global tid, og (samtidigt) at den regnemæssige kompleksitet af et univers ikke har noget som helst at sige. (Dette er en fornuftig nok hypotese, men man behøver den ikke; man kan sagtens arbejde med en mere generel mængde af muligheder.)

%Hm, lad mig lige endeligt søge på, hvad der er af grene inden for utilitarisme..
(12:47) For at vende tilbage til `empatiutilitarisme,' så har jeg lige søgt på utilitarisme, og det virker til at den eksisterende idé om `preference utilitarianism' på en måde er ret tæt på mine idéer. Dog synes jeg min version med `empatiutilitarisme' er meget mere elegant, og man behøver ikke at tilføje alle de caveats, som præferenceutilitarismen gør. Og tilmed følger der også en god forklaring med til ``empatiutilitarismen,'' hvilket der ikke rigtigt gør for præferenceutilitarismen, ser det ud til. Og hertil skal det siges, at jeg ikke engang synes det er nødvendigt at antage, at vi selv for all intends and purposes kommer til at leve alle mulige liv igen og igen, før at empatiutilitarismen er begrundet; jeg synes også en etisk grundsætning om at man bør leve som om, at man skal leve alle mulige andre liv, giver ogd mening i sig selv, nemlig fordi den bare er ækvivalent med at sige: ``Sæt ikke din egen oplevelse af lykke og smerte --- og andre følelser --- foran andres (når du skal beslutte, hvad er etisk godt og etisk dårligt i princippet).'' 

I øvrigt så er min udgave af utilitarismen (``empatiutilitarisme'') også meget præcis, når det kommer til, hvilke levende væsener, man bør (og ikke bør) begrænse det til, (nemlig fordi man bør antage, at man skal leve deres liv også (så vidt man tror på, at væsnerne kan have en bevidst oplevelse, og så vidt man tror på, at de kan føle diverse følelser)), og hvordan man skal forholde sig til spørgsmålet om, hvad det betyder at noget tilfældigvis fik et vist udfald frem for et andet. Og min teori behøver heller ikke at snakke om, at nogen personer ikke forstår, hvad der er godt for dem, osv., for ``empatiutilitarismen'' fodrer ikke, at individer, der overvejer, hvad der er etisk godt og dårligt, kan blive enige om det --- ja, faktisk så antager min teori ingen gang at der findes noget ultimativt svar på det!\,. Den siger bare, at hvad person, der stiller sig selv spørgsmålet om, hvad der er etisk godt eller dårligt, i princippet skal lede efter svaret ved at forestille sig, at vedkomne skal leve alle mulige liv (og særligt livene af de personer, der er berørt af vedkomnes handlinger (og hvor man jo gerne vil maksimere den samlede lykke (forventet af individet og dennes evne til at leve sig ind i disse andre menneskers sted) ud fra et statistisk synspunkt --- medmindre, i princippet, at man selv forestiller sig, at man i andre menneskers sko heller ikke ville have lyst til at maksimere lykken fra et statistisk synspunkt, men så er vi også virkeligt langt ude.\,.)). (13:10)




\section{An interesting little thing a want to note here *(that is about Physics as well) (22.06.23)}
(18:34) I was just thinking a bit about a video I saw the other day by.\,. 
Sabine Hossenfelder on YouTube. It's a video about entropy, where the main point is: ``Maybe their will be other life forms within a heat-dead universe in the form complex systems, hidden to our eyes. And I thought about afterwards, that it's an interesting almost-counterpoint, or at least comment, to then point out that, well, we actually know that this \emph{will} be true: There \emph{will} actually be complex life forms that, once you apply a certain basis transformation, get the form of being with functioning brains similar to what we know brains to be, whose motion will depend on the rational (and highly intelligent) thinking and decision making that these brains do. You would be able to take your heat-dead universe, look at its quantum state, and then calculate its time-evolution by making said basis change, apply a time-evolution operator with the desired $t$ parameter, during which the mention beings/brains will take rational decision that affects their motion, and then make the reverse basis change and get the right result for the calculation. And what would that basis change be that does this? Well, the basis change defined by a $\psi \to \hat U (-T)\psi$ where $\hat U (-T)$ is a backwards time-evolution operator with a sufficiently large time, $T$.

You could see this both as a sort of counterargument and as a supporting argument, dependent on how you view certain things, but I personally don't find that question very interesting, so just move on to the actual interesting part.

Because the reason why I write this now is that I just thought of something else related to this thing, which is a more interesting than what I've just noted above. The point is to say that we look at any given --- very large --- part of a heat-dead universe. We want to then calculate how our state in this volume, namely a state that we have made spacial cutoffs on, evolves over a given (not very large) time, $t$. Now, we could do what I've just said, provided that the volume is a lot bigger than the $T$ from before times the speed of light, namely since that means that the errors that we get from the boundaries won't propagate far enough into the volume to mess up our calculation. But regardless of how big the volume is, as long as it's just big enough to contain a lot of galaxies like we see in our visible universe, we can also use some other basis changes to achieve the same thing. Since the state is bounded, we can reach a significant volume of all possible configurations in the total configuration space just by using time-evolution operators alone (let alone other kinds of unitary operators), since the bounded universe (part) will be rebirthed from heat deaths a great number of times before it finally reaches the same state that it started in. So we can follow my above thought experiment, not just with the (kind of) $\hat U (-T)$ that I mentioned, but with an unfathomably big number of $\hat U (T)$'s, even if we only use one $T$ for each time order is created/reappears out of chaos. And thus, for each one of these $\hat U (T)$'s, we again have a certain basis change, that transforms our experiment in a heat-dead universe (or any high-entropy universe (with our laws) for that matter) into another problem that has us calculating the motion of intelligent beings/brains instead, whose rational decisions influences that motion. Now \emph{that} is kind of interesting to think about/keep in mind.\,.(!) (19:13) .\,.\,(19:21) It's definitely an important reply to the point/argument presented in that video, anyway.\,:)




\section{Some other notes}

(28.08.23) I have thought a bit about existence in the weekend vacation that I just had. And last evening I got the following idea which I will start off by mentioning: I should make a paper where I just say: Statistically, you can argue that we likely live in a cyclic, i.e. ``Big Crunch''.\,. .\,.\,well, ``Big Bounce'' to be more precise, I guess.\,. cyclic universe. For if you average over all the times that someone asks, will the universe keep expanding or will it crunch and then bounce?, the percentage of answers that say ``it will bounce'' will tend to 100 \%. Then I will follow that up with a section explaining how that means that all life experiences between yours and anyone else, including someone of different gender and even different species! --- and including machines (due to cyborg in-betweens)! Therefore we will not only all live again in the same as well as all slightly altered versions of our lives, but these slight alterations will then lead to alterations upon alterations, and therefore, if you have a somewhat materialistic mindset, it clearly means that we will all live all possible lives there can be in our universe. The last section should then note how this trivializes the qeustion about what the axioms of an ethics system should be: Whether you are an egoist or not, when asking yourself about what the right thing to do is, you should do so by considering what you would want if you had to live the lives of all people on the planet, as well as all people (on or off the planet) in the future that your actions might influence as well.

I should give all this some more thought, but I think I might be able to make a very short paper telling this points (with a main focus on the first point).

.\,.\,Oh, and it's of course not just with a materialistic viewpoint: If you believe in a soul then why should that soul die after your death? It will either find another brain to latch on to an experience that brains thoughts, or it will go back to your god if you believe in such *(or in a ``god'' as a metaphor, not for an actual being, but for the thing that all souls spring from (and return to and become part of)). And even \emph{if} you believe that a soul might die with the brain whose thoughts and feelings it experiences, you can make a similar argument as the one above. The likelihood that when a person/brain ask themselves/itself if there soul will live on and find a new host brain after the death of their current brain, the percentage of yes answers will tend to 100 \% in the multiverse.

So that's the first thing I wanted to mention: That I'm considering writing a paper like that. The next thing is actually also an idea / a thought from last evening, and that is that the multiverse \emph{could} actually consist of purely short experiences without it leading to ``infinite chaos.'' This is because every snapshot experience of a high enough intelligence will easily contain enough sensory inputs in that moment let alone the actual internal thoughts, which will also include a lot of information!, that this information (that defines the snapshot experience precisely) will be much more than what is needed to describe a universe and then a time and a place for where to locate a given brain in that universe in a volume outside of the universe's origo that is big enough that there are a (vast) multitude of brains to choose from! This means that non-chaotic experiences that are part of a combined experience (if you combine all the snapshots) that follows an orderly path might still be much more frequent than those that is built from just a random set of sensory inputs and internal.\,. configurational information. So there we go. I don't personally believe in snapshot experiences, but it is nice to realize that they don't necessarily lead to ``infinite chaos.'' (9:52)

(14:42) I should rather make a little paper about why hardcore materialism doesn't work, giving the example that references that xkcd strip, as well as an argument about a many-world-hypothesis universe that will then lead to infinite chaos for our prior (likelihood). .\,. .\,.\,Yeah. And I want to then end that paper with a list of examples of other hypotheses for existence that is still quite straight-forward and does not require magical-like or mystical assumptions (necessarily).

But after this paper, it does not really make much sense to make the Big Bounce one, since some of those hypotheses will be able to have.\,. ``Big Rip'' universes.\,.

(29.08.23, 8:30) I've thought some more about the topic. I guess I need to think more about it still.\,. I feel like I have some things to mention, but let me just wait until they are more clear to me.\,. (And it might turn out to be nothing new, btw.) But I should say that I can't really say that hardcore materialism doesn't work at this point.\,. What I will do now is to try not to think about the topics in my ``working hours'' and then focus back on my SDB project (where I'll start refactoring as React today). Existence thoughts (including what I could potentially write about) should then be delegated to ``free time.'' (8:37)

(13:26) I just took a midday walk. I think I \emph{can} actually conclude something very nice, and with a not very complicated analysis at all!\,:) The point is: Of course vacuum pair productions can lead to what might look like a new Big Bang in principle, only with a crazy low probability. But such a probability is incredibly large when measured against infinity, so such pseudo Big Bangs will happen an infinite amount of times in any given large enough volume of space. (And even though Wikipedia states so, I think it is a myth that the Big Rip hypothesis actually includes ripping apart galaxies, planets and atoms---although all these will evaporate given enough time.) But that will mean that we will be infinitely more likely to live in a time just after such a pseudo Big Bang rather than the initial one, given an assumption that, 1, we live in a Big Rip universe, and 2, are prior likelihood should only be calculated from that universe.\,. Well, before I even make this point, the initial point is: Whatever the case, we will be reborn in the future, even if we have a soul that only remains in this universe, and who can't travel faster than light to look for a new brain after death (of its brain). But then if you wonder why it looks like an initial Big Bang when you look out at our universe, and you think about the mentioned infinite prior for it actually being a pseudo Big Bang instead, you can just simply conclude instead that we likely live in a Big Bounce universe, or a limited but expanding universe where the Big Bang keeps happening on the edge of the expanding light cone of a universe where your soul is also able to travel much faster than light to look for a new brain after death, or else that souls are simply able to travel across universes in the (or a local part of the) multiverse, and that even if your soul started out in a Big Rip universe where time started simultaneously in every point (and is not continously starting on all point at the edge of a light cone), you are likely in one of the other, more brain populated (in the long run) universes now. (13:44) So even if you fear that your soul is, albeit immortal, bound to this universe, fear not (death); it will still find another brain at some point---and probably quite fast, even if you are the last human to live before the heat death! (13:46) .\,.\,(And if you don't believe in a specific soul that is separate from other souls in the universe, and that experiences aren't differentiated by a ``who'' that experiences it (but is just experienced ``by the multiverse'' in a sense), then Bob's your uncle anyway: you will live forever in all versions of your life, including all versions in between you and all other possible lives in our universe.) (13:50)

.\,.\,And if you believe in mortal souls, you've got to consider the fact that this means that souls can be created and destroyed.\,. Well, maybe not.\,. but if you believe so, it means that a soul can in principle be destroyed and then remade. But how is that different from having a soul be destroyed and then another one made? How is ``destroy then make'' different than ``destroy then remake?'' If there is a difference, it would be that a soul has some internal variables and/or atoms that makes it it. But where do these then go when a soul is destroyed? If they remain in the multiverse somehow, surely that must mean that the new soul that is made (not remade) will include some of those atoms and/or variables. And wouldn't that mean that the new soul created after yours is destroyed would contain parts of your soul? And doesn't that also alleviate the your fear of this happening? (I'm asking that tiny group of people on earth, if any, who believe that a souls exists \emph{but} are at the same time mortal!) Okay, that is about the time I want to spend on this very niche group of hypotheses (and I think I did a god job.\,:)). (14:00)

(14:24) Okay, there are also some other things that are worth considering, for instance whether or not time is subjective.\,. I'll think a bit more about such things.\,.

(14:55) Oh, I should also mention an important point from yesterday, and that is that it would take way less information to describe an experience by starting in some random point in space an then have a mechanism/function to search for a viable brain rather than defining the brain precisely. And that must mean, given the assumption (not at all without basis) that less-information experiences are more frequent in the multiverse, that your soul is very likely to find a brain near itself after death. So even with that little point, you now only need to fear the heat death if you fear death (and fear the few hypotheses where death might be seen as a bad thing in itself). (And if you do fear the heat death, see my point from a little earlier (just above), as they might help with that.) (15:01) .\,.\,Oh, and you could fear that your soul has a sudden expiration time, but come on, who would actually fear that.\,.\,?\,. (15:02)

(01.09.23, 12:30) ``Hardcore materialism'' can work, I guess, 'cause you can for instance assume that the multiverse has some fundamental laws for when ``it'' experiences what a Brain computes. (I personally believe that there can be several of such laws, though, which makes me think that different ``consciousness laws'' can exist for different universes in an ``object-oriented'' multiverse hypothesis.) This could also be consistent with the hypothesis that existence comes about be the multiverse / the fundamental logic ``figures things out about itself.'' And the fact that this would suggest that computations take ``time,'' it fits well with the materialistic hypothesis, because I believe that materialism cannot really work otherwise as a hypothesis: Each universe must have a restricted amount of ``computation power,'' so to speak, for materialism to work. For otherwise we could have Big Bounce universes with the Many-World Hypothesis in them, which would mean, first of all, that these will create much more conscious experiences than ``more normal'' universes, and furthermore, I believe that these would actually yield ``infinite chaos'' in terms of their average experience in them. It would follow (due to the high ``weight'' of a Big Bounce--Many-World universe) that the non-Many-World universes does not influence the total prior much and the multiverse would thus get an ``infinite chaos'' prior for conscious experiences when looking at all universes that looks like our own. (This is especially true since a Many-World universe would probably require \emph{less} information than a non-Many World one!) So if I'm right about a Big Bounce--Many-World universe leading to infinite chaos---which I think because after a finite number of bounces, the so-called ``time arrow'' (when there is no ``measurement mechanism'' to uphold it) will start to get erased, which can only mean total chaos as I see it---then materialism has to include, it seems, some hypothesis about it as well that means that universes have ``restricted computational powers.'' For if the have that, then the (Big Bounce--)Many-World universes would not necessarily ``weigh out'' the other universes in terms of the (consciousness) prior. (12:50)

.\,.\,(12:53) Oh, and note quickly that even though the Many-World Hypothesis also has the inherent problem of why wave functions with higher amplitudes are more likely (proportional to the amplitude squared) to occur, i.e.\ why don't every split of the wave function not just produce a 50/50 \% chance of being one or the other?, this is not really very important for this discussion, since one \emph{can} (I'm sure) find some solution to this (i.e.\ find a hypothesis that includes the Many-World one which does not have this problem). %Btw: Since you need such complicated assumptions to patch the hypothesis, it means that it is no longer more pure than a hypothesis where "measurements" happen quite physically in the universe, I just wanted to mention that little point (here in the comments), not that it has anything at all to do with the discussion (just a quick little jab at the Many-World hypothesis). (12:59)

(04.09.23, 13:21) In my discussion above, I forgot the fact that universes can also be created continuously. We could thus have a local multiverse where universes like ours are created continuously and then stops after some time. That can also give a reasonable prior where most experiences can be set in the time (on average) just after the original Big Bang.

%(For some reason I've been taking a thinking day, instead of continuing working on the SDB project. I will most likely get back with some more thoughts from today: I feel like I've had some very good thoughts today, but there is something I have to think more about.. ..So I'll probably get back to writing about all that (maybe there won't be too much to note, but we'll see) later today, or in the evening. (13:28))
%(06.09.23, 9:43) Met up with someone the day before yesterday (fore-yesterday.. foreday.. ..foreday and "overmorrow" could be nice choices..) for a walk and ended up spending the rest of the day with them and others. Then yesterday I took a whole thinking (and walking outside) day as well, partly because my kitchen stank because of a (removed) burned down lamp (and it was also very good weather..).

(06.09.23, 9:49) I should first of all mention, if I haven't done before (which I think I might not have.\,.), that my argument about likely being one of the very long-lived souls if there are such unfortunately isn't as water tight after all: You can easily make some assumptions about how to calculate the prior that ruins this result.\,.

And apart from the other very good hypothesis, there still is this one hypothesis that haunts the group of possibilities (that I can think of and which sounds good to me), and that is where, not just the concept of sensing an experience exists in the multiverse, but also the concept of a Subject that senses experiences. (And this would be in the ``all things exists, 'cause what else should things do other than that?\,.'' group of hypothesis (not the ``the fundamental logic finds out things about itself'' hypothesis).)

Now, if we think about such Subjects/souls, I cannot see why the soul would also carry a memory and some mechanism to produce thought (or ability to think in other words) with them, when the brain already has that. The only reasonable thing, the way I see it, is therefore to assume that ``souls'' are pure things without memories, thoughts and personalities themselves, but are simply what we could call ``Sensors'' in the universe; they \emph{sense} whatever the given Brain senses, including the thoughts (i.e.\ it senses the experience of having thoughts). You can almost view it as just a ``media player for experiences,'' in a sense.

If we assume this (troublesome) hypothesis, then the nice message that ``we are all a part of the same; our consciousnesses not fundamentally separate (and therefore we will all live all versions of ourselves and all others)'' doesn't necessarily follow. It \emph{might} very well be true still: Just because you have separate ``media players,'' i.e.\ Sensors, doesn't mean that they aren't interchangeable. But you might interpret them as not interchangeable, and I haven't found any arguments against this particular hypothesis.

.\,.\,Well, I'm still looking a bit into it (but I must get back to the other work soon).\,. But otherwise, the next thing is to go into the matters of, what is the likelihood from there that a given soul/Sensor will still live forever and experience all kinds of things. I think you can actually quite easily conclude that it very likely sticks around after the brain's death and finds a new brain. So in any Big Bounce universe or.\,. let's call it a universe with ``light cone time,'' we would get the same nice conclusion. They same applies if our laws of physics allows any single lepton or photon to cause lasting pair productions in empty space, even with the most minuscule (but finite probability). I, by the way, think that this is true, but it's hard for me to say for sure, 'cause it also depends on the physics of the wave function collapse. But if you could prove/derive that such pair productions can happen for a single particle in vacuum.\,. (10:25) .\,.\,And I know that most physicists would say ``no'' due to ``energy and momentum conservation'' but you \emph{can} have a series of transitions that ends up conserving quantum mechanical energy and momentum (energy and momentum conservation is a nice rule of thumb, but if you are treating it the same way as you are classical energy and momentum conservation, you are not understanding quantum mechanics fully).\,. (10:29) .\,.\,(10:33) If you could derive that, you would get the same nice message. But maybe you can't. And whether you can or not, at the end of the day, the true answer still relies on the physics of the wave function collapse, which we don't know. (And I'm assuming that wave function collapses are real and physical, since I don't believe in the Many-World Hypothesis.)

There is also another another kind of universe that we could live in which gives the same good result of us living forever and in all versions of the possible lives in the universe, and that is if the Big Bang is just an exploding black hole from a larger universe beforehand. Theorists of general relativity tend to take the singularities and material passing the event horizon seriously, even though it can only happen in a toy theory where you allow yourself the continuously shift the parameters of your manifold such that you ``follow the observer.'' I know I'm not competent enough in GR to state this, but I really believe that the actual (mathematical) theory can't just deal with such an event in reality; I bet that you can't define a manifold where a physical object ever crosses the event horizon, when you apply Einstein's equation to get the EOM (of the manifold plus the matter). And even if you can, we are still taking about something that takes place more after \emph{more than an infinite} amount of time in the future, so how can you just state that ``things crossing the event horizon is a reality'' with a straight face?! If you do, you are forgetting what a physical theory is in the first place: It's something to describe what we observe, not a gospel blueprint of the universe and how it works. Anyway, this is besides the point, because even though that GR \emph{might} allow such things, we know that GR (with all likelihood) need to be altered in a theory of everything when having to make it compatible with quantum mechanics. And quantum mechanics is hard to escape: If you have to combine a classical theory and a quantum mechanical one, it is all but expected that the result will also be a quantum mechanical theory. Now, in quantum mechanics you cannot have something like a black hole that is a low-energy state that is also one-way, inescapable. No, if black holes can be made to follow the laws of quantum mechanics, the will \emph{not} be one way. With all likelihood, they will instead be some low-energy states that \emph{almost} behave exactly like the classical black hole, but only almost! For even though matter might be compressed and almost-trapped near the event horizon for an extraordinary amount of time, due to energy (and information) conservation, at some point the inward movement closer and closer to the ``event horizon'' (which by the way won't be a sphere but a ball of compressed matter that is everywhere almost at the threshold of reaching the ``event horizon'' (which is then just an upper bound on the compression)) will transition into an outward movement, as the low-energy state.\,. casts out (I'm forgetting the word that start with `ex-') a big part of the matter again, due to the fact that it had too much energy to remain in the low-energy state. (11:03)
This theory can also explain Hubble's law without having an expanding space, since the Doppler shift can simply be due to time moving faster and faster again, at our visible universe escapes the black hole.\,. Oh, and I guess that doesn't explain the background radiation, but that could also simply be the radiation from the ``outside'' universe that was there before the black hole started expanding, i.e.\ all the light of the previous universe that has not yet been captured by a black hole (or maybe has in the long-ago past but has since then been ex.\,. He, I cannot remember the word.\,. has not since then been.\,. expulsed! (.\,.\,I think that was the word I was looking for, but maybe there is another.\,.) has not since then been expulsed by another black hole). This theory can mean that we live in a non-expanding universe, which then further means that such black-hole implosion--explosions will happen forever and ever in that universe (and has probably happens an uncountable amount of times already, if we live in one now). Thus you would also get the same result with that kind of universe. (11:14)

.\,.\,(11:21) Oh, and by the way, isn't it a bit weird that the entire universe was compressed to a small point, but matter didn't collect in black holes? How did the laws of gravity---i.e. the rest of GR!---only come into being after matter was far enough apart to not cause ``event horizons'' everywhere? I find that part of the theory questionable.\,.

*(11.01.24) Never mind about these theories about originating from an earlier black hole; I'm sure cosmologists have thought of this hypothesis and would have talked more about it if it was a possibility.\,. (17:17)

.\,.\,But to get back to the topic of existence, all in all, even though souls/Sensors/Subjects might exist not be interchangeable at the same time, the Subject/Sensor that experiences your life now will at least likely keep on living until the heat death followed be a Big Rip that stops any recreation of the universe from happening. But there is still a good change, that we do not live in such a Big Rip universe, which means that we will live all possible lives forever, even with such an hypothesis.

And you happen to be a Subject/Sensor of an early alive-but-mortal universe, then you still know that there will exists Big Bounce universes with exact copies of yourself, with the same exact live as you from birth to death, and who \emph{will} live forever like that, which might also be comforting, even if you fear that hypothesis. (11:34)

.\,.\,Hm, but isn't it also kind of weird to begin with to have these Sensors in the multiverse, that are not interchangeable.\,. Well.\,. Maybe, maybe not.\,.

.\,.\,But let me underline that you could also just see these Sensors as giving life to an experience, and that ``you'' are the experience itself more than the Sensor. .\,.

.\,.\,I'm by the way still thinking about whether I could make a neat little paper. I could either just propose one or both of my favorite hypothesis, namely as an alternative to materialism, which most atheists sadly.\,. well, not so much materialism, but they seem to believe that it's just ``lights out'' after you are dead, but you could instead make a good argument that we might live forever.\,. Oh, that's the point I should make: We might very well live forever. Not we \emph{will}, but it \emph{might} be the case that we will live forever as all possible lives. And that is a good message as well: It does not need to be confirmed. Okay, so maybe I \emph{should} actually write a small paper where I point this out.\,. (11:47)

.\,.\,Hm, or maybe not, I'm not completely sure.\,. (11:52)

(11:59) Let me mention a(n important) point that I thought of yesterday: If Sensors are described and governed by the fundamental logic, the mechanism that allows a Sensor to sense is also governed by the fundamental logic, and the experiences that are sensed are also continuously governed by the fundamental logic. The fundamental logic handles everything, what is sensed and how each sub-experience feels for the Sensor. So isn't the Sensor then just an arm of the fundamental logic. Well it \emph{is} that, and doesn't that mean, that the experience is really experienced by the fundamental logic after all? I believe so (but I wish I could just bolster that argument a tad more).\,. (12:05) .\,.\,It also just seems intuitively right, and even though I know that that's not something you should really go by, I think we all have the thought that we are in a sense not alone with our thoughts. The fundamental substance/entity that makes things exists shares our thought, and are just as much a part of us as whatever the rest of us is, assuming that we are not just that underlying thing completely (which I believe that we are). (12:08)

%(12:47) No, I'm satisfied with where I'm at right now with this topic. I will not think more about writing a paper on it for now.

(08.09.23, 8:51) If souls have some information about them that makes them different from others, i.e.\ a kind of ``soul atoms,'' then if everything that can exist exists, there will just be a different universe where the same soul experiences different things. But one might still hypothesize, that souls do not differ but are nevertheless not interchangeable, unfortunately. But since that is kind of a weird statement, some might not like that that hypothesis for that reason.

Continuing on the previous paragraph before this above one that I just wrote, let me try to underline/bolster that argument.\,. (8:59) .\,.\,(9:09) Well, essentially it boils down to this: If a feeling/experience of a soul/Sensor/Subject is governed by a more fundamental logic, and it thus springs from that in reality, how can it then not be just as much a part of the fundamental logic as any other existing feeling/experience?\,.\,. .\,.\,How can one claim that the experience can spring from the soul/Sensor alone, when everything about the soul/Sensor springs from something more fundamental. And in that way, how can anything really spring from anything \emph{but} the most fundamental logic of the multiverse?\,.

So that's really what my thoughts boil down to.\,. .\,.\,I think that these thoughts are especially reasonable when it comes to hypotheses where each thing/universe in the multiverse has a starting point, and then the rest is derived/calculated from there. For what should calculate them then other than the fundamental logic, so to speak?\,.\,. (Hm, note that I'm brainstorming a bit here.\,.) (9:18) .\,.\,This is as opposed to hypotheses where everything is just known from the very start and all things/universes just exist with their entire temporal dimension mapped out from the beginning. .\,.\,Hm.\,. (9:21) .\,.\,Hm, but that doesn't really change much for the ``everything must spring from the source of everything, ultimately, and therefore our feelings/experiences must be a part of the fundamental thing/logic/entity in the multiverse'' argument.\,. (9:26)

Okay, I'm actually really satisfied with this, like I have said before (two days ago). And as a last point, even if you think that there is a real change that souls/Sensors are not interchangeable, then what does it really matter anyway? If we ask the question from an objective standpoint, looking at it from the multiverse's point of view, is it good or is it bad, then, that souls are not interchangeable. And there the answer must just be: it's neither or. And if that is the objective truth, where care about some subjective version of the same question?\,. And to take it more down to earth: If other souls live in what could have been your place after you are dead, why not just be happy for those souls?\,. (9:32) I actually think that a great majority of people might feel the same way as I do on this point.
%%
%\\\\
%%
%(Now, about the paper, I am actually sort of considering a very short paper, that basically just says: The multiverse must have a great symmetry to it; whenever a specific choice is made over another, the converse must just be true in a different part of the multiverse. And let me propose a couple of multiverse hypotheses, namely one where the multiverse is ``thought up/derived'' from the fundamental logic, and.\,. Oh, I should mention first that conscious experiences must exist (we know that). So what the fundamental logic derives also includes experiences, and by calculating each experience, the fundamental logic might essentially feel those experiences as it calculates and understands them. Another one could be that all things just exists---including universes with a time attached to it---for what else is there for things to do, other than to exist, that is?\,. This then includes things which includes experiences (which we know are something that can and does exist). Hm, but then I should also talk about the problems with plain materialism, right.\,.\,? Okay, maybe this is actually the long version of the paper that I'm giving a resumè for here.\,. The very short version could just be: There must be a great symmetry to it all, yarder yarder, and therefore there must also exist e.g.\ universes like ours that Big-Bounces. So even if our universe will heat die and then never recover from that due to a ``Big Rip'' of some kind, there will just be other universes where being will live on forever (and I could mention that with some assumptions about how to construct the prior, this would mean that we would already likely \emph{live} in a Big Bounce universe right now!). And this is then where I'll point out that this means that there will live all kinds of versions of ourselves, and that's not all, even all kind of lives in between you and anyone else will at some point be lived, even though it might be very infrequently. So in that sense, all our lives are just part of a great spectrum of lives, that will all be lived forever and ever. So no need to worry about the heat death and/or the Big Rip. (And maybe I should also mention that physical wave function collapses + Big Bounce means that each bounce will be like a new, fresh Big Bang, despite the law of entropy.) .\,.\,But I'll probably not do that just yet, anyway.\,. I will, however, think a bit about it.\,. (9:56) .\,.\,Oh, and I would also like to tie ``Empathy Utilitarianism'' on at the end, which makes the project larger, and thus probably not something that I can just do---'cause it will probably take a long time to formulate it so that it doesn't sound quite as long-haired.\,. So I'll probably forget the thoughts about writing a paper about the topic for now, actually.\,. (10:00))


%(10.09.23, 10:37) Apropos physics, I had the thought last night that if space expands, whouldn't stars orbits in a galaxy be drawn further and further out? Wouldn't an expanding space thus not also expand galaxies over time? Shouldn't we therefore be able to look out and back in time and see that galaxies tend to be more compressed the earlier we see them (i.e. farther away from us)? A quite interesting question, if I may say so myself..!


%(15.09.23, 9:22) I watched a YouTube video from "Kurtzgesagt" yesterday about "being the dream of the universe." The video turned out to be about the thing about how all, even completely unlikely events occur in a "dead" universe as long as there will be matter left (with a non-vanishing density) in that "dead" universe, and that statistically this leads to the sorta paradoxical conclusion that we are all most likely floating brains in a dead universe. First of all let me say that I'm generally impressed with the video. I think it's very cool that this topic is brought up by such a channel (not that I really know much of it; I have probably seen one or two videos of them beforehand, not really more than that). And they did a very good job of it. Cool.
%Now, a counterpoint to the hypothesis (of us being floating brains) is this: We are likely not the only universe in existence: Why on earth would we be that? And among all the universes (at least that are sort of like ours), there will likely be some that has some mechanism for which something that looks like the Big Bang can happen again and again. (I've mentioned a lot of other hypotheses that will lead to this, but the obvious candidates are Big Bounce universes, which might appear exactly like ours do just after the/a Big Bang, with an expanding space, but where that expansion then turs again at some point.) And even if those types of universes are very infrequent, say like one in a million, they would still end up domination the others statistically in terms of how many brains they have over a long period. Therefore, we are most likely actually living in such a universe ourselves right now.
%This hypothesis is then in fact even bolstered by that very same "paradox" from before, for if the opposite hypothesis, namely that there are only Big Rip universes, is true, well then we \emph{are} likely floating brains for all we know---or it could be that it will still be more likely that whole star systems form, but in that case it would be incredible unlikely that when we look out, we see other star systems, let alone galaxeis, let alone this Big Bang-like universe around us (then it would probably even be more likely, that the light we see coming from outer space is just randomly organized to produce what we are seeing). Well, in fact, if this is not the case, we then \emph{do} live in a universe that has a mechanism where Big Bang-like universes can be created again (with much higher likelihood than floating brains), which contradicts the hypothesis that this does not exist in the multiverse. This means then that we can actually rule out said hypothesis: There \emph{must} be universes where Big Bang-like universes can keep on forming repeatedly with higher probablities higher than floating brains. For else, we \emph{are} floating brains right now. But that means that anything we see is completely random: The cup on my desk right now might just as well turn into a rabbit in a moment than it might stay a cup. So anyone wh is in favor of the pure Big Rip universes hypothesis will have to deem the reverse hypothesis to have a lower (prior) likelihood of occuring, than what they actually deem that whatever object the lay their eyes on can actually turn into a rabbit at any moment, and will likely do so.. Okay, well I guess that a Matrix (the movie (and book)) -like simulation chamber could appear out of nothing, i.e. where the brain is then hooked up to a simulation, in which case that simulation might very well have some laws to it.. ..Yeah, but anyway: We still likely live in a repeating (some way or another) universe.
%Now, I also want to talk about something else about the video, which is not really very cool, and that is that event-horizon--Hawking-radiation bullshit.. ..I mean, let us think about it. In a universe old enough so that some galaxies will move away from us faster than the speed of light due to the expansion alone, yes there will then be an event horizon. In fact, at any point in time, the event horizons from observers exactly on the sphere where things move away with the speed of light (due to the expansion alone) will sweep past us (in every directions), each at exactly the speed of light. Now.. OF COURSE this will not mean that pair-produced vacuum particles will all of a sudden be ripped apart and pushed forward at the speed of light just on the edges of the (invisible) event horizons that passes through us. And it is a bit disappointing that non of the physisict that has entertained this (bullshit, sorry to say it) hypothesis has been able to try this simple line of reasoning before sending the hypothesis on to the public/jounalist (like Kurtzgesagt). That is embarrassing. But I guess maybe there is a kind of selection principle involved at least, where the reasonable physisists that are able to see that kind of reason, and therefore not go on and spout a lot of weird stuff to journalist, well they are not the ones that the journalists get to hear (and maybe the jounalists also play their own role in this selection principle).. So yeah, not too cool..
%And to something a bit urelated.. well it is related somehow, 'casue I thought about it---oh 2 seconds.. (10:03) ...(10:28) and I'm back---I thought about it in the shower this morning, thinking about this topic, but I can't remember what the thread was. Anyway, the thing is: I do actually beleive in a physical wave function collapse, but it might only happen at quite large scales, perhaps such that gravity comes into play (so that if we build a planet-sized quantum computer, there is a chance that collapses will happen, and in that case we would then be able to observed it with such a computer).. ..Okay, I think I've caught the thread: I had an intersting thought about the fact that, \emph{if} the universe is completely quantum mechanical---which would be the most elegant thing for it to be, but on the other hand, I personly don't mind if GR and QM can't be joined together quite as seamlessly, since I don't mind at all the thought of collapsing wave functions (which then means that the space can then know where objects are in space (roughly) and thus know how to bend around them)---if that is the case, and the now quantum-mechanical manifold we live on is also albe to expand and shrink (which it seems to be), then wouldn't it make sense that there is a higher energy associated with more space, and that the expansion that we observe is thus actually part of a process where the photon energy is converted (elongating the wave lengths of the photons) into more space. I know that this idea is not new, but in the light of a unified GR and QM, it just seems like something that is quite likely, doesn't it?.. I think so..
%Okay, that was all.. almost all that I wnated to mention here: I also wanted to mention one more thing, and that is that, if we assume that we don't have a Many-World universe, and if we do in fact see large quantum computers work without any detectable wave function collapse, then we could in principle simulate a true AI on a quantum computer, making it go though $2^n$ thought processes at once. But with said assumption, it would mean that we should only count that as one thought process in when looking at it e.g. from a utilitarian perspective (where one might want to meassure total happiness (and the true AI might tell us that it is happy thinking about certain things, or whatever)). For with that assumption, the "soul" of the true AI must choose \emph{one} path to go by, and the 2^n - 1 other thought will then not be lived by that particular "soul." I thought this was an interesting thought, interesting enough to also mention it here. (10:47)


\subsection{Continuation}

(09.12.23, 9:36) I was considering writing a small paper about this topic before, but I came to a halt, as can be seen in the source comments above. However, now I'm actually considering writing a small paper again. This time I'm thinking about letting the proposal of ``maybe the universes simply have different laws for how consciousness/conscious experiences appears, sorta ``next to'' the laws of their physics.'' And I could start out be talking about how the concept of experiences something consciously probably can't be fully explained/understood with the (``mortal''/intra-universal) logic that is available to us---similar to how we probably can't (even) explain how the experience of seeing something blue differs from that of seeing something yellow. But, I could go on, the fundamental logic underneath all existence \emph{would} obviously include these concepts. Now, a compelling hypothesis (related to MUH/CUH) is that there is a perfect symmetry when it comes to the set of all things that exist. For what would be able to cause any asymmetry? Only the fundamental logic of everything would be able to hold the reason why there would be an (arbitrary! (i.e. with no reason for being there)) asymmetry, but then the fundamental logic of everything would include an asymmetry.. Yeah, this get's a little long-haired, but I believe that I can formulate this point briefly and concisely. And then, since the fundamental logic includes the concept of what conscious experiences mean, there is nothing from preventing the universes in existence to simply contain / spring out of laws about when conscious thoughts/experiences appear within them. And therefore there doesn't need to be an overall law for when something becomes sentient in the entire existence. This law could simply be universe-dependent instead.

I think this could make for a neat little paper, and then I'm also considering including a section or two about my point that with an infinite (or at least virtually infinite) number possible universes, and therefore an infinite \emph{actual} number of universes, if this hypothesis of a grand symmetry of everything is true, then there is an infinite possible lives between your and any other sapient being. And with any kind of hypothesis where the ``I'' in ``I experience something,'' i.e. the subject of an experience, is either something that is spread out and/or is something that is part of grander whole, then each individual can therefore effectively say to themselves, that they will live all possible lives in the grand multiverse (and possibly an infinite number of times as well). And this gives a very nice theory of ethics, don't you think: Try to maximize happiness and content as if you would live out each possible live in the universe (with frequencies matching the probabilities of thing occurring, by the way). For given that these hypotheses are true, you effectively will. (10:06)


(10:00, 21.12.23) No, let me do something even simpler than this. Let me write a paper, where I briefly talk about the potential symmetry of the multiverse (the entirety of existence), and mention CUH, but then quickly get to talking about what such as infinite multiverse might mean, and how this might affect the moral question, at least for a (broad) set of.\,. sets of assumptions (sets of sets, i.e.). I could call the paper \emph{Symmetry of the multiverse and empathy utilitarianism}.\,. .\,.\,And the main point of the paper should then simply focus around that realization that having infinite universes (or even if our own universe is infinite) means that there is lives that follow all possible paths in between one person and any other. .\,.\,Let me give this potential paper some more thought\ldots

\ldots (10:19) Hm, I wonder if there is enough meat on that paper, then.\,. .\,.\,I would probably need to dive some more into the symmetry hypothesis.\,. .\,.\,And maybe I \emph{should} also talk about the possibility for separate consciousness laws.\,.

(11:22) Maybe I should make a summery of a list of hypotheses which a lot of people might find reasonable, and then conclude by talking about empathy utilitarianism.\,.


(23.12.23, 14:57) Or maybe I should focus more on my points about consciousness.\,.

%Jeg overvejer lidt (og tænker lige at brainstorme lidt), at man måske kunne nævne A Bunch of Rocks, xkcd, og så nævne paradokset med, jamen hvad hvis man går det to gange? Hvad hvis man gør det to gange, men over lapper i tid? Hvad hvis man gør de to gange samtidigt? Hvad hvis man flytter rundt på bunker af sand i stedet? ..Og inden da kunne jeg nævne, at jamen, vores hjerner er jo også bare, ikke sten, men atomer, som flyttes rundt på.. (15:01) ..Hm, og jeg kan så se to løsninger: Man kunne enten simpelthen have bevidstheds\emph{love}, ved siden af de fysiske love, så at sige, tilknyttet et hvert univers. Og ellers kunne det også være, at på et fundemantalt plan, jamen så det der eksisterer i det samlede multivers, det er simpelthen bare bevidstheder. Så med andre ord er dr \emph{kun} bevidsthedslove---og 'oplevelseslove' skulle man så næsten også sige---for hvert univers, og de "fysiske love" kommer så bare i kraft af, at bevidsthederne skal have noget at opleve. (Og her burde man så næsten også nævne, eller have nævnt, at der jo nok sagtens kunne være et princip, der favoriserer mindre information i de individuelle universers beskrivelser, dog..) ..Ja, så måske bør man netop først snakke lidt om, at multiverset jo kunne være ret symmetrisk og omfavnende, og at dette så ikke leder til komplet kaos---i hvert fald ikke nødvendigvis---fordi man så kunne have en højere frekvens, fra individets perspektiv, af universer med relativ lav information. (15:08) ..Ja, så lad mig faktisk stadig prøve at indlede med den pointe (og refere så til MUH/CUH)..

%(12:40, 28.12.23) Jeg har været lidt frem og tilbage, for jeg synes lidt det er et problem, at der enten ikke virker til at være nok kød på det, hvis jeg udelader nogle ting, men at det til gengæld bliver for langhåret, hvis jeg prøver at inkludere dem.
%Men nu har jeg faktisk muligvis en god idé til, hvordan man kunne opbygge en rimelig simpel artikel, som kommer med nogle interessante pointer uden at blive for langhåret..
%Denne opbygning handler så stadig om kort at nævne lidt om, at den samlede eksistens nok er ret bred, og så vil jeg nævne det med kontinuumet af livs-stier allerede her. Og så vil jeg nævne, at så med en materialistisk antagelse, så er dette jo allerede en meget behagelig og glædelig anskuelse.. Jeg vil for øvrigt sørge for at fokusere en del på antagelsen om en monoteistisk, og særligt den kristne, gud, og dette vil jeg så også behandle kort i denne første sektion også.. Nå, og så vil jeg så fortsætte med en sektion om bevidsthed, hvor jeg starter med A Bunch of Rocks -eksemplet, og alle dens udvidelser, og så kommer med pointen om, at det ikke gør noget, at bevidsthed ikke kan defineres ud fra.. rene.. principper, for hvis der er arbitrære valg, så må de alle sammen nok bare gælde i forskellige universer. Og man kan så sige det samme om antagelsen, hvor der er en skabergud: Så kan denne jo bare tage nogle forskellige valg for forskellige universer, hvis det er, altså om hvad "sjælene" skal hægte sig på (af hjerner). Herefter kunne man snakke om, at "sjæle" jo nok ikke er aktører, men bare er nogen der \emph{oplever} ting, og at dette om ikke andet kan eftervises, når man for (deterministiske!) maskiner, der kan gøre sig de samme tanker (også ekstentielle!) tannker som vi kan (for så kan man se at vores hjerner godt kan være deterministiske og ikke behøver en sjæl til at give input til den for at give den den karakter, vi kender).. Men det vil jeg springe over.. I stedet tænker jeg, at tredje sektion så bare skal handle om, at selv hvis sjæle eksisterer, og hvad end de er atomare eller sammensatte af mindre dele---og hvad en de udspringer af en skaber-gud eller ej!---så kommer man altså ret nemt frem til, at.. "vi alle er ét," så at sige.. Og så kan jeg herefter slutte af med en sektion, der lige understreger den etik, dette klart leder op til (selv for "egoister"), nemlig: "Empati-utilitarismen." (12:54)

%(29.12.23, 11:49) Jeg overvejer også at nævne i første sektion, at sandsynligheden for at bo i et Big Bounce-univers går mod 1 med visse antagelser, interessant nok.


%(04.01.24, 15:56) Jeg var egentligt kommet frem til en ny disposition her efter nytår. Den gik på først at snakke kort om, at det samlede multivers nok er rimeligt bredt, og særligt bredt nok til at indeholde Big Bounce-universer og andre universer, som varer/lever for evigt. Derfor er der ingen grund til at frygte vores eget univers' død, ville jeg så nok konkludere denne første sektion med. Så ville jeg snakke om, at en dt grundlæggende multivers jo nok faktisk burde være \emph{fuldstændigt} bredt: "Alt hvad der kan eksisterer, eksisterer." Men så kan man så spørge: "hvad kan eksistere," og det er det jo så ikke et klart svar på. Derfor er denne hypotese ikke nødvendigvis i modstrid med tanken om, at der skal være en skaber-Gud, enten for hvert enkelte univers i eksistens, eller for den sags skyld for det samlede hele. ..Hm, og hvordan ville jeg så fortsætte derfra, var det ikke noget med at snakke om bevidsthed (noget a la mit A Bunch of Rocks-eksempel (som jeg dog nok ville ændre til at tale om to neurale netværk med samme ydre sanseinput, som man så langsomt sætter sammen i tid og sted og kobler til hinanden, først svagt og så mere og mere, til det sidst bliver til en og samme hejrne))..? ..Jo, og så pointere, at "alt eksisterer"-hypotesen så ikke har noget problemer med denne arbitr..ærhed..aritet... ..Og så ville jeg også sige det tilsvarnede om forkellen på at se farven rød som rød eller se den som blå.. ..Så ville jeg vist snakke om det her med, at vi alle er forbundne af et kontinuum af mulige livs stier, og at man på den måde kan sige, at vi alle skal leve alle liv for evigt. ..Og så ville jeg vist gerne dykke længere ned og se på: jamen, er dette en gyldig fortolkning med alle de forskellige muligheder ift., om der er en Gud (eller flere) eller ej, og om der er en sjæl for hver oplevelse eller ej.. Hm.. (16:09) .. ..(16:13) Ja, jo, pointen var, at hvis vi ikke har en unik sjæl hver især, jamen så holder fortolkningen, men hvis vi har en sjæl, der gør forskel i kosmos på, om det f.eks. er min sjæl, der sidder i min hjerne, og din i din, eller om man bytter rundt på disse sjæle, jamen så må hver sjæl have nogle kvaliteter, der gør den distinkte. ..Hm, og disse sjæle må komme et sted fra.. Hm, eller hvad.. ..Hvordan var det nu.. ..Hm, man kunne måske have sjæle uden gud, men mon ikke de fleste.. Eller om ikke andet: De fleste, der tror på en sjæl, vil nok mene, at denne sjæl enten med det samme (eller næsten) hopper videre til en ny hjerne, eller at den vil returnere til en højere enhed, nemlig til det samlede kosmos eller Gud. Hvis kosmos/multiverses så varer ved evigt, så..---Ah, jeg manglede forresten at nævne ovenfor, at en Gud jo også nok ville skabe mere end ét meget specifikt unvivers (som dør efter en kort tid); hvis Gud hvilede på syvendedagen, jamen hvad fortog han så så næste dag af arbejde udover at skabe en ny verden, selvfølgelig; hvilket arbejde ville ellers kunne måle sig med at skabe vores verden. Og en evig skaber-Gud ville vel ikke dovne den efter sin hviledag, men ville forsætte med samme energi som i den forgange uge. Nå men for at vende tilbage: Hvis kosmos/multiverses så varer ved evigt, så vil hver sjæl så sendes tilbage til en hjerne igen. Hvis hver sjæl er atomar, så for vi så den samme fortolkning igen, med at vi skal leve alle liv for evigt, og hvis sjæle er kompositte, jamen så kan det være, at de skilles ad og samles anderledes, inden de sendes tilbage igen i live, men hvis muliverses varer evigt, jamen så vil kosmos på et tidspunkt samle en tilsvarende sjæl igen, og så for vi samme fortolkning endnu engang. Så det er jo dejligt at tænke på. Og så kunne man spørge, jamen hvad hvis der også er en oplevelse forbundet med at blive en del af det store hele (kosmos/gud) imellem at ens sjæl finder tilbage til en hjerne? Jo, altså enten er der ikke nogen oplevelse forbundet med det, og så er det jo fint. Men hvis der er, så må man sige, at givet at vi her i livet føler noget af den allerstørste lykke, når vi føler os forbundet til andre, og til et større hele, så er det meget svært at forestille sig, at oplevelsen af at blive en del af det store hele (kosmos/Gud) ikke vil være positivt, hvis ikke ligefrem virkeligt lykkelig. (16:30) Og med det sagt, så kunne jeg så nævne "empati-utilitarismen," og her ville jeg så i øvrigt også gerne nævne denne pointe: Empati-utilitarismen siger ikke bare, at man skal være god mod andre mennesker, men den siger jo også modsat, at man ikke skal ofre sig for meget for andre mennesker, for de skal jo også leve ens eget liv i ligeså høj grad som, at du skal leve deres. Derfor er ekstrem selvopofrelse også dårligt, altså hvis det samlet set skaber mere ulykke end lykke. Og det samme kan man i øvrigt også sige med at blive ulykkelig på nadres vegne ved at have medfølelse med dem: Medfølelse er generelt godt, men husk at de andre også skal leve igennem den smerte, som deres egen smerte indirekte forvolder dig. Jeg synes selv, dette kunne være en meget god pointe, som det ville være godt for nogen at huske. Nå, og havde jeg så mere?.. Nej, det var vist det.
%
%(16:36) Men som sagt så tror jeg altså ikke, jeg vil skrive artikel over dette alligevel.. I går og i dag har jeg læst lidt Tegmark og lidt andet også, bl.a. lidt Paul Davies, The Goldilocks Enigma. Og her for noget tid siden kom jeg hjem fra en god gåtur (i sneen), hvor jeg i første omgang kom frem til, at jeg ikke ville skrive den artikel alligevel (men måske bare ville opsummere den her, hvad jeg så har gjort nu), og senere kom jeg så på, at jeg måske alligevel vil skrive en anden en, hvor jeg i stedet for at sigte mod "vi skal leve alle liv" og "empati-ultilitarismen" i stedet bare redegører for "alt eksisterer"-hypotesen, som er lidt anderledes en Tegmarks, umiddelbart, og så særligt fokusrer på den gren af den, hvor vi ligesom alle er "udregningerne" fra den Grundlæggende Logik som en slags stor \emph{forstående} kraft.. Hm.. (16:42)

%(05.01.24, 11:39) Okay, jeg vil prøve at skrive denne artikel, og jeg går i gang nu her...


%(10.01.24, 11:22) Jeg tænkte lidt i sengen her til morges (jeg sover ret længe her for tiden (efter nytår)---og bliver også træt rigtig tidligt) over, om det nu også løser problemet med uendeligt kaos, hvis man gør 'oplevelser' til, hvad der er genstand for eksistens. Og jo, måske er der en vej, hvis man lægger vægt på '\emph{singulær} kohærent oplevelse,' men det lyder nu lidt arbitrært, synes jeg nu, og hvad værre er, jeg er ingen gang sikker på, at det vil løse problemet alligevel. Men dette er faktisk ret spændende, for det får mig jo så bare pludselig til at tro meget mere på lige præcis den her antagelse om, at multiverses ligesom deduceres (og udregnes (ét eller bare endeligt mange skridt ad gangen)) af the Logic of Everything (LoE).:) Og det føles bare virkeligt dejligt, at være blevet endnu mere sporet ind på den hypotese.:) Der er også stadig den mulighed om, at gentandene for eksistens er 'intelligente væsner' (eller, lidt tilsvarende, (rimeligt) 'fundamentale logikker' selv), hvilket jo også løser problemet på samme måde, fordi vi så igen kan få en løbende udregning af alting (alle sandheder, og specielt alle sandheder om 'oplevelser'). Dette kan jo jo enten nævne i et af det sidste afsnit eller i et appendix. (11:34)



%(14:00, 03.10.24) Jeg tror jeg skrev et sted, at der er en fejl i min eksistens-artikel omkring at universet looper. Men det er en fejl, at der var den fejl, for det gør det jo netop når man laver cutoffs. Så hvis jeg har skrevet det et sted, så never mind, den er god nok. (Så er der bare lige den der mikro-ting med, at dinosaur-fosiler måske ikke carbon-dateres (men måske uran-dateres), men det gør jo ikke noget for pointen..)









\chapter{Physics}

(12.09.23, 10:28) I already have a few physics-related notes in the Existence chapter above (including out in the source code comments), but now I have an update on my quest of showing self-adjointness of certain unbounded operators, so this deserves it own section/chapter.

%(10:32) Jeg har fået læst en lille smule op på mine noter om aftenen og prøvet så småt at sætte tankerne lidt ind i problemet igen (for jeg sidder alligevel for tiden og slapper meget af om aftenen med logiske puslespil---Linux har en hel pakke, bl.a. "Galaxies," "Palisade" og "Black box" (som faktisk ikke er strengt taget logisk, men som er meget sjov alligevel med de rigtige indstillinger (e.g. 18, 18, 18 *(20, 20, 22)))---samtidigt med at jeg hører musik, og tanken var så: Hvorfor ikke læse op på dette problem og så slappe af ved at prøve at løse det i stedet?.), og i går på vej til tandlægen kom jeg så på, at bare bør sørge for at foholdet mellem halens norm-størrelse og størrelsen på den del-vektor, den kvæler, bør gå hastigt mod nul, nemlig for at løse det problem, som jeg kan huske jeg lidt gik i stå i, da jeg gik i stå sidst i problemmet (og i fysikken i det hele taget). Det problem handler om, at A^- jo også stadig vil opererer på de lavere k i \psi(k_1, k_2, ...), og for halers haler vil de "nedre haler" altså skabe store vektorer, potentielt set. Men hvis halerne bare bliver hastigt mindre og mindre pr. niveau, så vil de, som jeg kan se, godt kunne bare, at de "nedre haler" producerer vektorer i det samlede billede, for disse kan så også bare gøres (hastigt) mindre og mindre for hver \psi_{2n (+ 1)}. (10:45)
%Og i går aftes fik jeg så lige kigget lidt på eq. (3.88) i qed.tex/pdf, ikke fordi min hjerne var til særligt meget der---pudsigt nok, for jeg havde ikke brugt den så forfærdeligt meget, ellers (mest bare til at tænke over, hvordan jeg skal om-implementere SetDisplays nu her i dag, hvilket dog også kræver lidt..)---men jeg fik lige akkurat overbevist mig selv om, at, ja, når vi har en \phi_n, der kvæler et "forbudt område" under sig, så kan gøre det sådan, at vi kan vælge en lille \psi_{n+1}, der stadig giver et ret stort \Braket{\phi_n | A^- \psi_{n+1}}. (For man skal nemlig give store *(nej små) hale-områder til sådanne næsten-basisvektorer, der har nedad-produktioner til et "forbudt område" under sig.) Og pointen er så for det første, at selv hvis \phi_n "snyder" og kvæler produktioner til området, der støtter \psi_{n+1}, hvad det nemlig egentligt ikke må umiddelbart for "V," for der er nemlig i det forbudte område over \phi_n, at vi har placeret \psi_{n+1}, jamen så vil dette (store) bidrag til \Braket{\phi | A \psi} stadig bestå, for her vil tredje led i første ligning i formular (3.88) nemlig altid udligne andet led eksakt, og så videre for hvert tilsvarende par i denne række. Men! Hvis phi har et \phi_{n+2}, der kvæler produktioner til \psi_{n+1}-området, så vil vi faktisk kunne forøge \Braket{\phi | A \psi} endnu mere (som jeg kan se det), for så vil vi nemlig kunne gøre samme trick og tilføje vektorer til \psi_{n+3}, der tilføjer til \Braket{\phi_{n+2} | A^- \psi_{n+3}}'s bidrag til \Braket{\phi | A \psi}, som \phi så igen ikke har nogen chance for at udligne fra \Braket{\phi | A \psi} igen. Og hermed mener jeg altså så, at man kan opnå, at hvis \Braket{\phi | A \psi} skal være bundet, så må \phi altså nødvendigvis også skulle lade sine "forbudte områder" være på et tidspunkt, og nok til at man så kan omdanne \phi til en sum af vektorer, der lader sine "forbudte områder" være helt, hver især, og som stadig har et endeligt billede "under A," hvilket pr. definition gør den til en del af mit Dom(A). (11:04)
%Det skal også lige siges, og dette var vist en idé fra i forgårs, at jeg nu holder mere af at bruge sfæriske skaller i stedet for kulger til "V." Disse skaller skal så have en skaltykkelse, der vokser proportionelt med raduis (hvilket vil sige med |\Delta k|), men hvor skallen er relativt tynd ift. radius, således at vi bare kan regne med en fast faktor på 1/\sqrt{k}, når vi tænker på problemet.
%Og ellers vil jeg bare lige sige, at jeg stadig skal læse mit gamle løsningsforslag igennem, for jeg er faktisk kun nået til først på side 133, nu hvor jeg har læst i mine gamle noter om dette problem. Men jeg er altså rimeligt fortrøstningsfuld over for, at jeg har en løsning, 7, 9, 13.. Det ville jo være lidt vildt.. Det ville også betyde, at jeg ville kunne tilføje "har bevist en (vigtig) matematisk sætning" til listen over ting, jeg har opnået, hvilket faktisk ville være ret stort for mig.. Nå, men hvis det virkeligt er så nemt, og jeg tror, som jeg har skrevet før, faktisk ikke problemet er løst, for jeg har kun set matematiske papers, der laver et cutoff, for at kunne håndtere sådanne formler og gøre dem til selvadjungerede operatorer, så \emph{bør} jeg faktisk bruge noget af min tid på at skrive en artikel om det. For i modsætning til mange af mine andre ting, så kunne jeg forestille mig, at dette ville være en ting, der faktisk kunne udbrede sig ret hurtigt.. ..Måske.:) (11:13)
%*(11:30) Nå ja, og jeg kan altså så forresten ikke se, hvorfor jeg skulle behøve at vise Dom(A^*) \subset Dom(B) direkte nu..:)

(11:13) I've just written some notes (in Danish) out in the source code comments about the update. I think I'm gonna leave it at this for now, and then get back here when I want to continue more thoroughly on the problem.\,.

(20:31) Hm, about ``Task 2'' / showing that eq.\ (124) in my QED paper holds, won't almost eigenvectors in $\mathbf{H}_{CL}$ be sent into almost eigenvectors as well in $\mathbf{H}_{red}(j)$ when $j$ is let tend to infinity?\,.\,. .\,.\,Well, I would think so, and if they do, don't that show the equation.\,.\,? .\,.\,Yes, it does.\,.\,!\,.\,. (20:38)
%(21:05) Ja, for billedet af en(hver) næsten-egenvektor (samt enhver anden vektor) må konvergere punktvist, når cuutoff'et sendes mod uendeligt. Så når cutoff'et sendes mod uendeligt kan vi tage en vilkårligt stor del af parameterrummet (altså Fock-k-rummet) og se at både vektorens billede konvergerer mod kontinuumsgrænsens billede i dette område, og da vektoren altså også selv konvergerer mod kontinuum-modparten, så vil vektoren minus dens billede divideret med en faktor konvergere til en forsvindende vektor, når vi lader området og cutoffet gå mod uendeligt.. Okay, jeg føler ikke, at jeg fik gjort begrundelsen meget klarere her, men hovedpointen er bare, at billede konvergerer punktvist, når man lader cutoffet blive større og større, og derfor er det rimeligt nemt at se, at approksimationen af næsten-egenvektoren selv må konvergere til en næsten-egenvektor, når cutoff'et løftes.. (21:14)


%(13.09.23, 12:54) Fantastiske nyheder! Jeg er lige kommet hjem fra en lille gåtur, som jeg gik med målet om at finde ud af, hvordan jeg helt præcist skal lave min (nye) InstanceSetDisplay. Det fik fandt jeg så endeligt ud af her ret tidligt på turen (og skal have implementeret det nu her efter, jeg får skrevet dette her), og så begyndte jeg jo at tænke lidt over fysikken igen. Jeg startede nemlig dagen med lige at catche op på, hvad problematikken omkring Dirac-havet i kontinuumsgrænsen var. Og midt på turen slog det mig så! Hvem siger, at 0-impuls-bølgefunktionen skal være invariant (i praksis) under Lorentz-transformationer?! ..Hov, lad mig forresten skrive dette i den renderede tekst:
(13.09.23, 12:59) I have great news! I thought about ``Task 5'' (about the Dirac sea) just now on a little walk, and I realized: Who says that we have to have a solution to the empty vacuum that is invariant (physically) under Lorentz transforms?! Sure, we need to show this if we want this solution to be our \emph{vacuum} solution, but who says that the 0-momentum part of the wave function have to be vacuum-like?! Why can't this state just be part of the physical state of the universe?! For when you think about it, the universe already seems, from where we stand at least, to have a preference towards a certain inertial system, namely the one we arive at if we take the mean momentum (I guess when accounting for the expending space also) of all the galaxies we observe. But this does not makes us say that the universe isn't Lorentz-covariant.\,! And similarly, if we find that the 0-momentum state has a skewness to it, that makes it seem to point to one inertial system over others---for instance if it is (close to) the ground state in that inertial system but not in any others---we would also not say that our universe isn't Lorentz-covariant then! The 0-momentum state would in that case just not be an actual \emph{vacuum} state; you wouldn't quite be able to call it that. It would be a \emph{physical} state instead, carrying actual information about the current state of the universe.

And with that.\,. Oh, first I should then also say, that if I can indeed show self-adjointness of Dirac-like Hamiltonians without the vacuum-perturbing terms, I'm actually pretty confident (7, 9, 13) that I can also show that the Dirac Hamiltonian \emph{with} the vacuum-perturbing terms are self-adjoint.\,! It wouldn't surprise me if I could, at least.\,:) For in order to do this, we could just remove all the 0-momentum k-states in the reduced Fock space, $\mathbf{H}_{red}(j)$, and separate it out as its own Hilbert space (multiplied by a direct product to the other part of $\mathbf{H}_{red}(j)$). And we can then do the same for $\mathbf{H}_{CL}$, only where the 0-momentum sub-Hilbert space would in this case be a Fock space over a k-space with one less k for each $\mathbf{H}_n$. And with that done, it wouldn't surprise me, if it would be almost just as easy to do ``Task 1'' and ``Task 2'' from here, given that you have already solved the same tasks for the Hamiltonian without the vacuum-perturbing terms (and without the separation just mentioned). And that would then yield that the actual, full Dirac Hamiltonian is self-adjoint (and that it can be turned into path integrals, which follows from ``Task 2'')!! (13:21)

And with that, I actually think that I would be able to do all of the five ``Tasks'' that I mention in the `Future work' section of my QED paper! For if I remember correctly, I believed that I had already solved ``Task 3'' when I finished the paper, i.e.\ in terms of sketching a proof. So I think that I have that under control, broadly speaking---or at least I think I thought so when I finished the paper. And ``Task 4'' really isn't that serious anyway: Even if I can't solve it, I'm pretty sure that you can make a convincing argument why it actually doesn't matter ('cause we can't physically tell the difference if we are pure states or not!\,.\,.).\,.\,! *(Oh, and I also thought/think that I have a solution to that problem, anyway, as I write in the paper.) And I've just argued why ``Task 5'' also probably \emph{doesn't} need solving, which, let me reiterate, is really just fantastic news in that case!\,.\,.\,!! I went from viewing this problem as something we might never really be able to solve, potentially: It could very well be a truly Hard problem for all we know. So now that it seems to me, that we probably \emph{don't need} to solve it in order to show that the Dirac Hamiltonian is (self-adjoint and) Lorentz-covariant, it is really just.\,. yeah, I've said it: Fantastic.\,.\,! (13:32)

%(17:26) Hov, det kan jo godt nok være, at 0-impuls-delen af H_{red}(j) ligesom exploderer oven på dets (ud-separerede) Hilbertrum, det kan jeg ikke huske, om det vil gøre..
%(17:39) Oh, but on the other hand!: If it blows up, can't we then not just (which would atually be much better) argue that this would tell us, that this 0-momentum part of the Hilbert space will be more and more decoupled from the rest of the Hilbert space in the continuum limit?.!!!..:D It would seem like it, wouldn't it!..:D.. (17:42)
%(18:08) It seems to do the opposite.. It seems that the (vacuum-)perturbing terms vanishes when compared to the free energy.. ..Now, wouldn't that mean, that the ground state solution would approach (becoming more and more parallel to) the bare ground state solution (which is the one where the vacuum is just empty)..? ..Well there would probably still be an infinite number of particles in the C.L. but the expectation value of each individual k-vector would go to zero.. ..Ah, but the problem is that the term work on all states, not just the ground state. So that's why they might still be able to cause a "vacuum" that is much different from the bare one.. (18:16) ..(18:18) But still, if we then look at the 0-momentum Fock space and this Hamiltonian that tends toward its C.L., and we then try a solution where all k-p-p-states are slightly excited into coherent oscillators (with very small amplitudes, smaller and smaller as \delta k \to 0) in a separable way---and note that we get more and more states in thi solution as \delta k \to 0, but that's okay for this argument---and see what happens when we apply \hat H_{vacuum-perturbing} + \hat H_0 *(i.e. \hat H_{free}), won't we then see that the resulting state will be more and more parallel to the initial state (before applying \hat H), and that this solution (of a lot of separable coherent states with smaller and smaller coherent-wave amplitude) as a function of \delta k thus we become closer and closer to an eigenstate when \delta k \to 0? Interesting thoughts.. (18:27) ..(!) ..Hm, well if that is all we need for the argument, why even bother with the coherent states: Why not just look at (1 + \epsilon a^\dagger)-states, or even better, why not just look at the bare ground state? Wouldn't that perhaps also become closer and closer to an eigenstate in the C.L.?(..!) (18:31) ..Well, yeah, it would!, at least if I'm right about how the factor in front of the v.p. terms will become less and less potent in the C.L...!(!).. (18:34) ..Oh, and that would then completely justify the conventional approach for path integrals of just assuming that the vacuum starts out in the bare ground state! Interesting!.. (18:36) ..This would actually be incredible..!

(18:38) I also have some very exciting additional notes out in the source code comments just above this paragraph that I've just added.\,.\,!\,\texttt{:D}

%(23:25) Ah, but if there is a degeneracy, we don't know that the bare ground state will remain in that state in the C.L.; it could then go to other states with.. wait.. no?.. Oh no, never mind, the bare ground state will still remain in the same state. .. 

%(23:50) Oh, I think you can actually argue that you can remove the vacuum-perturbing terms from \hat H, then..! ..(And you are also free to put them back if you want when deriving the path integral (if that makes it nicer).)

%(14.09.23, 9:28) Hm, jeg kan se at mit argument for at ændre C.L.-sektionen og indføre "Task 2" som en fremtidsarbejde-opgave i stedet var, at jeg ikke kunne argumentere for, at følgen af mere og mere almost-egenvektorer (for jeg havde nemlig samme løsningstanke dengang forinden) vil tilhøre domænet af \hat H_{red}(j) hele vejen, når j \to \infty, og at deres billede altså ikke divergerer. Jeg har ikke lyst til at tænke over, om dette nu var fornuftigt, for jeg kan jo bare have ment: givet at jeg/vi ikke står og kender selve Dom(\hat H_{CL}).. Men ja, jeg kan altså ikke se nu, hvorfor det ikke skulle kunne lade sig gøre at lave sådan en følge, givet den Dom(\hat H_{CL}) jeg formoder.. Måske overser jeg noget, men.. (9:33) ..(9:35) Ah, nu var der lige en klokke der ringede og sagde, at det handlede om, at produktionerne til det sidste niveau, for jeg må, pr. min hukommelse og omtalte klokke, altså også have haft laet et cutoff på partikelantallet der.. Hm.. ..Hm, nå, det er også ligegyldigt, for jeg skal jo bruge det i argumentet, at man kan lave den følge, så enten så kan man det, eller også kan man ikke, og hvis man kan (hvad jeg da stærkt regner med), så kan man altså (er jeg næsten helt sikker på) udlede eq. 124 ("Task 2") heraf. (9:40)

%(9:41) Jeg overvejer så nu, når jeg altså får tid til det for mit SDB-projekt, først at lave en artikel omkring min helt-vildt-spændende konklusion fra i går om vakuumet, og hvor jeg så i den artikel starter fra den diskretiserede \hat H (i.e. "\hat H_{red}"), og så både referere til min 2022-artikel, men også udleder den startende fra dens formelle kontinuumsgrænse (med de divergente vakuumperturberende termer), nemlig ved at gentage C.L.-sektionen fra 2022-artiklen lidt. Og i den forbindelse kan jeg så også lige lappe denne argumentation ved altså denne gang at argumentere for eq. (124). Når det så er gjort, så kan jeg så endelig gå videre og argumetere for min vakuum-konklusion og slutte den artikel af med denne. Efterfølgende kan jeg så lave en artikel om selv-adjungerethed, hvor jeg kan referere til denne forrige artikel som argument for, at vi kan fjerne de vakuumperturberende termer fra \hat H i dens kontinuumsgrænse. Og dette gøres så altså, når jeg slutter den artikel af og "perspektiverer til," at det fra sætningen så bl.a. følger, at QED-Hamilton-operatoren så vil være selv-adjungeret, og at (den konventionelle!) QED så er konsistent---og at man faktisk ikke behøver nogen renormalisering i stiintegralerne!! (9:52)

%..Og efterfølgende kan jeg så også altid lige lave et samlet værk, der sammensætter alle disse artikler, og så kan jeg i øvrigt også overveje her, om jeg skal prøve at få klaret "Task 3" (og også lige "Task 4") i dette værk også (men man kunne også sagtens bare udsætte dette yderligere..).

%..(9:58) Åh, den pointe der om, at man ikke behøver renormalisering i teorien, den gør faktisk lige det hele en \emph{væsentlig} tand mere interessant...!!

(14.09.23, 14:07) Okay, despite my notes above in the source code comments, the vacuum is not solved yet. I thought that the vacuum perturbing terms would get a $1/\sqrt{\mathcal{V}}^3$ in front, but if course it is just $1/\sqrt{\mathcal{V}}$. That means that the 0-momentum part of the Hamiltonian on the 0-momentum part of the Hilbert space will have ``perturbing terms'' that drowns out the free energy in the continuum limit. You \emph{could} probably then introduce another factor of $1/\sqrt{\mathcal{V}}$ and show that that has a self-adjoint C.L., and then from there argue that the original 0-momentum $\hat H$ would have similar dynamics as that one going to the CL, only with a growing phase factor as the difference when approaching said limit. Now, \emph{if} you could then show that the self-adjoint CL (when having the extra $1/\sqrt{\mathcal{V}}$ factor in front) of the 0-momentum $\hat H$ will have a true, non-degenerate ground state, or at least just one part of the energy spectrum with no degeneracy, you could argue that the 0-momentum part of the wave function will stay in that state forever, not interacting with the rest of the wave function. At least you might be able to argue this. But since I don't know how to show such the existence of such a non-degenerate state, I can't necessarily show that we can remove the vacuum-perturbing terms this way.\,. (14:20)

.\,.\,(14:24) You could then try to return to my argument about: What does it matter that the vacuum state / 0-momentum state is part of the physical state? And sure, but that does mean that we can't rely 100 \% on path integral calculations, 'cause the state of the ``vacuum'' might then play a role.\,. Well, if that's how it is, so be it: Nature could be like that, and then there's nothing to do about that, other than settle for a theory that include this.\,. Hm.\,. (14:27) .\,.\,Yeah, and the resulting theory will then just have a Hamiltonian, where there is a vacuum / 0-momentum part of the Hilbert space that only includes the space spanned by a set of degenerate states with the same energy, and where that (diverging) energy is then just subtracted (at every step going to the CL). So that kind of theory would/could be a consistent theory indeed.\,. (14.32)

.\,.\,But this prospect is definitely not \emph{as} exciting.\,. (14:33)

.\,.\,(14:35) Oh, bt maybe you could go the route of trying to argue that since the.\,. let's just call it the ``vacuum Hamiltonian'' for now for brevity.\,. since this is self-adjoint (if it can indeed be shown to be this), it will, remarkably, mean that the expectation value of the combined particle number operator will be finite. Intuitively this might be because the eigenstates of the ``vacuum Hamiltonian'' will all get a kind of symmetry that means that transitions up to states with higher numbers will be canceled by other similar transitions from below (but from different k-states) with an opposite sign in the transition amplitude. So that's how this might be possible (and why the self-adjointness of the vacuum Hamiltonian will not constitute a paradox this way). And the idea from there is then to look at.\,. Hm, let me think ('cause I haven't actually thought beyond this point yet (this time around; I've certainly had similar thought before in the past)).\,. (14:44) .\,.\,Hm, well heuristically you could say: If there is a finite number of particles an average in the total space despite that space, $\mathcal{V}$, tending to infinity, then we shouldn't care about those particles' interactions with our experimental/physical state, that's the general idea of the argument.\,. (14:47) .\,.\,Hm, this \emph{does} actually sound like exactly the right route to go.\,.\,:) (14:48)

.\,.\,Yeah, and mathematically, this would then be to show that, due to the.\,. oh, wait, how can you know that this limited amount of particles are spread out.\,. because of translational symmetry.\,.\,?\,.\,. (14:52) .\,.\,(14:53) Hm, doesn't this lead to a paradox, 'cause it would mean that.\,. no.\,. It wouldn't mean that the eigenstates would be close-to-parallel to the ground state, right?\,.\,. .\,.\,No, of course not. The expectation value for the particle number can be quite large.\,. (14:55)

.\,.\,(14:56) Ah, you only need to look at the non-vacuum part of the end state of the vacuum-interaction transitions.\,! I'm almost sure that you would thus be able to argue (and this is btw definitely something that I've thought about before---and so is most of all this, I would say) that the amplitude of the ``physical'' part of the end state of such a transition would be vanishing, meaning that the transition as a whole would be vanishing.\,! (14:59)

And in that case, you would be able to remove the ``vacuum'' / ``0-momentum'' part of the Hamiltonian---despite whatever the state the vacuum is in initial!\,:) In other words, we would be able to indeed remove the vacuum-perturbing part of the QED Hamiltonian.\,:) (15:02)

%(15:13) Hvis dette holder, så gør det faktisk bare det hele \emph{endnu} en tand mere interessant (end da jeg skrev sidst om, at det hele var blevet en tand mere interessant, her tidligere i dag).! For så vil jeg kunne nå det samme glædelige resultater, som jeg tænkte da, men nu hvor disse resultater vil afhænge endnu mere af min teknik (hvis den holder; 7, 9, 13) til at bevise selvadjungerethed for sådanne ubegrænsede Hamilton-operatorer.!:) (15:16)

%(16:07) Men så bør jeg jo så faktisk lave selvadjungeretheds-artiklen først, og så bagefter lave en om at håndtere vakuumet. Det tror jeg, jeg vil. Og så vil jeg bare vise det for en halv-generel mængde af ubegrænsede Hamilton-operatorer, og bare lige nævne det specialle eksempel med \hat H_{QED}, hvor vakuum-termerne altså er fjernet (men hvor jeg ikke argumentere for, at de også \emph{kan} fjernes). Jeg tror så, at jeg bare vil arbejde på næste artikel i forlængelse af denne, så jeg bare kan udgive dem stort set lige efter hinanden. Jeg tror så faktisk ikke, at jeg vil gøre at stort nummer ud af at retfærdiggøre mine \hat H'er i den (nr. 2) artikel. Måske vil jeg endda ingen gang gennemgå CL-sektionen igen, som jeg ellers snakkede om, men muligvis bare referere til den. Og hvad angår "Task 2," så kunne jeg i såfald bare tilføje et appendix til den artikel, hvor jeg lige lapper dette manglende skridt i min reference (altså i CL-sektionen af mit 2022-paper). (16:13)

(15.09.23, 9:15) It was wrong that the expectation value for the number operator will be finite: We won't know that. But luckily this should not ruin the actual, mathematical argument for why the transition amplitudes must vanish (given that the vacuum Hamiltonian is self-adjoint). (And it's by the way interesting enough to note, that there will be some growing $f(n)$, where the $f(n)$-operator (e.g.\ the $n^{-10}$ or the $e^{-n^{10}}$-operator) will have a finite expectation value.)

(18.09.23, 6:26) I realized in the bed earlier that my method of regularization actually does change the path integral. For in principle, if you have the bare vacuum as both the initial and final vacuum state (around the experiment), then you might get lower transition amplitudes than the actual ones since you would suspect that the bare vacuum only remains partially as that (and partially turns into another vacuum state). So my results might actually alter how we do path integrals!

Now, I think I might actually use part of today working on making the self-adjointness proof 'cause it could be really nice to know if I do have this great, great result---or result\emph{s}, rather---or not. (Because if I do have this/these result(s), and if it also seems like a short proof to write a paper over, then I think it might be worth my time to do now, due to the exposure that would very likely give.) (6:35)



\section{Notes about/while proving self-adjointness}

(18.09.23, 6:37) I'll stick to using shells instead of balls because it means that we can make it so that the ``cancelation tails'' always have higher $k$ than the previously highest $k$ (i.e.\ of all the photons involved). \ldots But they shouldn't be thin, I don't think, since the tails of the tails etc.\ has to be smaller and smaller.\,. (7:12) .\,.\,Ah, no, they could be thin and then just start out a long distance from $k_{n-1}$, the previously maximal $k$.\,.

(7:57) Ah, I will need to show $\mathrm{Dom}(A^*)\subset\mathrm{Dom}(B)$ as well, and I have not done that yet.\,. .\,.\,(8:06) Oh no, wasn't that actually kind of trivial, since we don't need anything about having a converging $A\psi$ for a sequence of $\psi$'s that produces a larger and larger $\braket{\phi |A\psi}$.\,! .\,.\,Yes, that will be almost trivial to show.\,! (8:12) .\,.\,I guess.\,. Hm.\,. .\,.\,Except that we also need to take care that the difference in
\begin{align}
\begin{aligned}
	\braket{\phi'| A \psi} =&\, 
		\braket{\phi'_{n}   | A^- \psi_{n+1}} + \big[
		\braket{\phi'_{n+2} | A^+ \psi_{n+1}} + 
		\braket{\phi'_{n+2} | A^- \psi_{n+3}} \big] + \big[
		\braket{\phi'_{n+4} | A^+ \psi_{n+3}} + 
		\ldots\\
	\braket{A \phi'| \psi} =&\, 
		\big[
		\braket{A^+ \phi'_{n}   | \psi_{n+1}} + 
		\braket{A^- \phi'_{n+2} | \psi_{n+1}} \big] + \big[ 
		\braket{A^+ \phi'_{n+2} | \psi_{n+3}} + 
		\braket{A^- \phi'_{n+4} | \psi_{n+3}} \big] + 
		\ldots
%	\label{symmetry_considerations_ldots}
\end{aligned}
\end{align}
doesn't ruin it.\,.

\ldots Ah, but I think you don't have to show $\mathrm{Dom}(A^*)\subset\mathrm{Dom}(B)$ separately; I think that you can instead just let it be part of the argument where you see what happens if $\phi\notin\mathrm{Dom}(A)$, where you then use that $\phi$ must keep canceling its production, 'cause otherwise you could just exploit these infinite productions with a $\psi_{n+1}$ that aligns itself more and more to the formula of $A\phi_{n}$ for the given $\phi_{n}$ that does not have its productions canceled.\,. (8:43) .\,.\,(And those productions have to be canceled from above.\,.) .\,.\,Hm, but $\phi_{n+2}, \phi_{n+4}, \ldots$ could still cause trouble.\,. .\,.\,Hm, oh wait, couldn't you just always choose a $\phi\notin\mathrm{Dom}(A)$ by taking a $\psi\in\mathrm{Dom}(A)$ and then removing any *(particular) inner $\psi_n$ from $\psi$.\,.\,!? (8:53) .\,.\,(8:56) No, perhaps not.\,.
.\,.\,(9:01) Hm, it might actually be true that the argument (analyzing a $\phi\notin\mathrm{Dom}(A)$) does not need to change, and that we don't need to have proven $\mathrm{Dom}(A^*)\subset\mathrm{Dom}(B)$ beforehand to complete that argument.\,.

\ldots (9:19) I think that you can argue that $\phi_{n+2}, \phi_{n+4}, \ldots$ can't achieve much: If we look at e.g.\ $\braket{\phi_{n+2} |A^-\psi_{n+3}}$, this matrix element divided with $\|\phi_{n+2}\|$ is very limited (and increasingly so for higher and higher $n$). And if you look at e.g.\ $\braket{\phi_{n+4} |A^+\psi_{n+3}}$, that will, at some $n$ and afterwards, just keep being canceled by $\braket{\phi_{n+4} |A^-\psi_{n+5}}$.
%..I type so slowly.. I have never gotten used to this keyboard; it's so clonky.. (9:27)
.\,.\,And that's why $\phi$ can't allow itself not to cancel its own productions enough that they become effectively finite, for otherwise there's a $\psi$ exploits this while $\phi$ also cannot do anything to counteract this divergence for this $\braket{\phi |A\psi}$. (9:30) .\,.\,Yes.\,.\,:)\,.\,.

(11:30) Oh yeah, I probably should show $\mathrm{Dom}(A^*)\subset\mathrm{Dom}(B)$ first, but I can do that with said argument (where you also argue that an infinite image that is finite at every layer will also yield an unbounded $\braket{\phi |A\psi}$ functional, for the possible $\braket{\phi |A\psi_{n}}$/$\|\psi_n\|$ will only grow with $n$). And then I'll use an argument when assuming that $\psi\mapsto\braket{\phi |A\psi}$ is bounded that defines a process to rewrite $\phi$ repeatedly so that the end result will be a vector that can be shown to be in $\mathrm{Dom}(A)$ given that we already have $\mathrm{Dom}(A^*)\subset\mathrm{Dom}(B)$. (11:36) .\,.\,And I believe that this will work.\,.\,!\,!

(13:43) My $\mathrm{Dom}(A^*)\subset\mathrm{Dom}(B)$ argument here doesn't hold, sadly, 'cause there is a very big area where $\phi_{n+2}$ can lie that $A^-\psi_{n+3}$ can't touch. \ldots (14:04) Oh, wait a minute.\,. .\,.\,If you let $\phi_{n+2}$ be so that $\braket{\phi_{n+2} |A^+\psi_{n+1}}$ cancels $\braket{\phi_{n} |A^-\psi_{n+1}}$, aren't then just on your way to building a new vector in $\mathrm{Dom}(A)$?\,.\,.(!) .\,.\,(When you locate $\phi_{n+2}$ in that very area that is designated for $\mathrm{Dom}(A)$.\,.) .\,.\,Haha! Yes! (14:12)
%(Lad mig btw lige nævne, at det jo må være Reitz' (eller hvad han nu hedder) sætning, man må bruge her.)

(15:09) Well, I can use that argument to show that $\phi$ cannot have any infinite $A\phi_n$, which should be sufficient for the next argument, I think.\,. .\,.\,Well, maybe not, but let's see.\,.
%(15:17) For dette næsten-\mathrm{Dom}(A^*)\subset\mathrm{Dom}(B)-argument kan man også dele \phi op i en Dom(A)-del og en ikke-Dom(A)-del, sidstnævnte med uendeligt billede, hvor denne ikke-Dom(A)-delen så kan ses altid at kunne udnyttes.. ..Tja, nej, men man kan dele \emph{halen} op i to dele efter samme princip. ..Tja, det ændrer nu ikke argumentet; det var nok fint, som det var.. ..(15:24) Ah, men kunne faktisk ikke næsten lave begge argumneter på én gang, hvis man altså deler halen op i to således?.. ..Jo, måske!..! (15:26) ..Så at argumentet altså kommer til at starte med, man kan udnytte så meget af A^+\phi_n, som lige nøjagtig ikke udlignes af A^-\phi_{n+2} fra det designerede område..! (15:28) ..Ja! Nu bliver det virkeligt elegant!
(15:30) I think that I might have an elegant solution to showing.\,. Well, to showing $\mathrm{Dom}(A^*)\subset\mathrm{Dom}(A)$ at once (after having shown that $\mathrm{Dom}(A)$ is dense and that $A$ is symmetric)! I've written shortly about this solution out in the source code comments.

(15:43) Yes, I really think it works.\,.\,!! The argument is that, first of all, all of $A^+\phi_n$ that is not canceled by a legal $A^-\phi_{n+2}$ can be exploited to let $\braket{\phi |A\psi}$ grow. Let us then factor out the finite part of the tail that is canceled by an illegal part of $\phi_{n+2}$ and first of all note that the part of its image in the $n-1$-layer is finite as well. Then we again ask, how much can we exploit of the out-factored part, and again we see that is must for the most part have its productions canceled på a legal $\phi_{n+4}$. We can continue such factorizations indefinitely to obtain a set of vectors that are all in Dom($A$) plus something else in principle. But that part will vanish at every level (point-wisely), when subtract the sum of this set of vectors, so we are left with nothing.\,. (15:53) .\,.\,By.\,. Riesz' theorem, we should then see that all these vectors have a finite image.\,. And we should also see that their sum has a finite image, right.\,.\,? .\,.\,Uhm.\,. .\,.\,Oh, they are all orthogonal, by the way, which is quite nice!\,.\,. (16:01) .\,.\,Hm, or maybe not.\,. (16:06) .\,.\,Well okay, we can argue, I'm sure, that they have finite images when we factor them out.\,. .\,.\,Yeah.\,. So by the nature of this out-factoring argument, we'll now have a sequence of finite, Dom($A$)-like vectors with finite images under $A$ (as I tend to say), which makes them part of Dom($A$), exactly. .\,.\,And what about their combined image, then?\,.\,. (16:13) .\,.\,Can we use that we know that there exist that $\chi$.\,.\,? .\,.\,(16:18) Ah, we know that the sum of these vector formulas will converge at each level! .\,.\,Yeah, each level of the combined image will only have contributions from a finite number of $L^2$-functions! (16:20) .\,.\,That will make each level an $L^2$-function, which then must be equal to $\chi_n$ of a finite ($L^2$) vector $\chi$. And therefore the sum of all the vectors' formulas must be an $L^2$-function itself (equal to that $\chi$). !! (16:23) (And this therefore shows that $\phi\in\mathrm{Dom}(A)$.)

%(25.09.23, 10:20) Jeg kom i tanke om, da jeg lagde mig i seng, at mit argument om at \hat A er symmetrisk på domænet ikke helt holdt, men i sengen i nat kom jeg så frem til, at det stadig holder med mit nuværende domæne, for \braket{\psi_n | \hat A^- \psi_{n+1}} kan kun blive ved med at holdes stor (for voksende n), hvis man breder \psi_{n+1} mere og mere ud, men så får man også et større og større billede pga. den første betingele for Dom(\hat A). Og her kan man altså mere specifikt argumetere ved at sige, at der bliver en begrænsing på \braket{\psi_n | \hat A^- \psi_{n+1}}, hvis \psi_{n+1} ikke må overstige en vis norm. (10:25)

%(27.09.23, 9:05) Jeg har lige startet dagen med at tænke lidt over SA-argumentet (det sluttelige; det jeg har skrevet om ovenfor i den sidste renderede paragraf). Hvis nu jeg kan vise, at man kan danne \psi-sekvenser, hvis billede vokser og vokser i \psi'_n og i \psi'_{n+2}, men forbliver begrænset i alle højere niveauer, jamen så er det pludeselig rigtig nemt at sige, at \phi_{n+2} \emph{må} skulle modsvare \psi'_{n+2} \equiv A^+\psi_{n+1}..!.. (9:10) ..Hm, og det burde være til at vise ok nemt, for hvis man har et \psi_{n+1} der er understøttet af større k'er, så må dette kun gøre, at de "grimme" termer fra A^-_j\chi_j må blive mindre (norm-mæssigt, selvfølgelig).. ..Ja, så når jeg viser, at de.. åh, deja vu.. nå.. når jeg viser, at de konvergerer, så bør jeg også lige vise/bemærke, at normen ikke vokser, når \chi_m understøttes af større k'er.. (9:25) ..Ja, dette må virke, og så jeg jeg frem til, i første omgang herved, at \phi_{n+2} kun må stå for at kvæle en endelig del af A^+_n\phi_n.. ..Og denne del kan man så separere ud.. ..Så får vi to nye \phi'er, hvor den ene indtil videre følger V, og hvor den anden er på sit første niveau (n=m).. (9:29) ..For sidstnævnte kan man så lave samme argument, og herved kan man splitte \phi i undeligt mange, der alle følger V i n=(m+2)-niveauet.. ..Og hvis vi så ser på hver af deres n=(m+4)-niveau.. ..Noget af hver af disse kan så følge V.. Hov, nej, følge Dom(A).. og noget af dem kan bryde Dom(A)-områderne.. ..Hm, og her har vi allerede at sidtnævnte del må være endelig, men vi ved dog ikke at denne dels billede nedadtil må være endeligt, hvilket vi skal bruge for at kunne separere det ud som et muligt.. Hov, nej, jeg mente faktisk V, ups.. et muligt V-hoved.. (9:38) ..Hm, men vi kan ikke bare bruge helt det samme argument som før?.. (til at konkludere at den del kun kan kvæle en endelig del nedadtil).. ..Jo, det må vi jo kunne.. Og så ruller det; så kan vi forgrene \phi i undeligt mange dele, der alle følger V for altid. ..Hov vent, kan vi så bruge Riesz, for vil dette ikke kræve, at A er surjektiv.. Hm.. (9:44) ...(9:59) Ah nej, jeg bruger jo bare, at der findes et \chi, så \braket{\chi | \psi} = \braket{\phi | A \psi}.. ..Hm, og kan jeg så konkludere, at.. Hm, men hvordan i alverden giver det mig, at \phi.. ..Hov, jeg har allerede, at normen af \phi er endelig. Hm, så har jeg ikke allerede her, at \phi må være i Dom(A), det virker da sådan.. (10:02) ..Hov, ups, jeg skal jo netop vise, at A \phi konvergerer. Så spørgsmålet er rigtigt nok, om man så kan konkludere, at A\phi = \chi.. vent, er det ikke givet? ..Nej, vi ved at A^*\phi = \chi, ikke at A\phi=\chi.. ..Hm, gad vide om det så alligevel er her, at man så skal vise at A\phi er endelig først (altså at \phi \in Dom(B))..? (10:07) ..(10:10) Hov, jeg glemte, at jeg jo nok kan vise, at alle \phi_j er endelige! ..(10:15) Hov, men jeg skal jo bruge, at alle (A\phi)_j er endelige, ikke..? ..(10:19) Hov, hvis man nu lavede Dom(A) om til at bruge kulgeskaller i stedet for kulger, får man så ikke, at alle disse \phi-dele vil være ortogonale? ..Hm nej, alle hovederne i et vist niveau behøver ikke at være ortogonale. ..Men til gengæld er alle disse hoveder endelge, og da der er et endeligt antal af dem, kan man også sætte dem sammen til én. Og så får man \phi delt op et sæt a V-vektorer, der alle er ortogonale indbyrdes.:) (10:23) .. ..Og det er klart at alle disse \phi-dele selv må være i Dom(A^*) også.. ..Det er de, ja, når de er vist at være ortogonale.. ..Hov, og jeg har jo også, at deres produktioner er endelige i hvert lag! Ah, det er det, jeg skal bruge! ..Altså at den "gode del" af hvert \phi_n's hale to niveauer oppe sørger for at kvæle alt andet end en endelig del a A^+_n\phi_n. Og så behøver vi ingen gang noget om, at de kan gøres ortogonale (virker det til). For så vil hvert niveau af A\phi være endeligt. ..Hm, og kan vi så ikke argumentere for, at \chi = A\phi overalt (\chi som vi ved har en endelig norm)..? ..Hm, men hvis ikke, så kan jeg muligvis også lave et bevis for, at A\phi også må være endeligt i sig selv, når \braket{\phi | A \cdot} skal være begrænset.. ..Ja, det må jeg også kunne.:) (10:37) ..Ja, så jeg kan komme helskinnet igennem uanset hvad---7, 9, 13!---:).. (10:43) ..Ja, og måske gør dette ikke beviset meget sværere, for jeg skal sikkert alligevel argumentere for, hvor meget vi kan "udnytte" hver del af \phi, og så er det let lige at sørge for, at man også kan argumetere for, at \|A\phi\| må skulle konvergere, når man summer over n.(!):) (10:50)
%(11:05) Ah, jeg tænkte i går aftes, at det måske slet ikke gjorde noget med de "grimme termer" til A^-\chi, får de forsvinder måske alligevel for større og større D. Men så fik jeg alligevel tænkt, at det nok var omvendt. Men nu tænker jeg igen: Det er da sådan, er det ikke?!.. ..Jo, selvfølgelig! (11:07) ..Ork, jamen så bliver det jo SA-argumentet jo pludselig en hel del pænere og nemmere!^^ (11:08) ..Åh, hvor er det altså dejligt, at det stadig virker til at ville lykkes, og også at det ovenikøbet ikke virker alt for besværligt med det sidste nu. ..! (11:10)
%(12:21) Åh vent, måske kommer det ikke til at løse alle problemer, det med at sende D'ernes "radius"/afstand, hvad jeg nu kalder d_{n-1} for hver D_{n-1}, mod uendeligt, for så jeg får jo i hvert fald så ikke herved, at A\psi-sekvensen bliver større og større kun i \psi'_n og i \psi'_{n+2}.. ..Hm, men der er så også en chance for, at jeg ikke længere behøver dette, når nu jeg er fri til at lade de "grimme termer" (kalder jeg dem bare; så grimme er de heller ikke for mig, men hvad ellers skal jeg kalde dem?.) gå mod 0.. (12:26) ..Hm, hvad sker der, når det specifikt er d_{m+1}, men gør større og større..? ..Hm, så kan \phi modsvare alt det.. hm.. (12:32) ..Ja, det hjælper os ikke at lade d_{m+1} være andet end det minimale.. ..(12:37) Hm, nu virker det som om, at det med at gøre D'erne større slet ikke hjæler os, for selvom det reducere de "grimme" termer fra A-\psi, så skaber det bare samtidigt også produktioner til den anden side, som \phi så kan "udnytte imod os," og det gør det jo bare værre.. (12:39) ..Okay, der er vist ingen vej udenom det. Så bliver beviset altså bare lige noget mere kompliceret, fordi man så skal trække rundt på de "grimme termer".. (12:42) ...(12:55) Okay, det kommer nok til at gå fint alligevel, tror jeg. Jeg tror stadig, at argumentet bliver klart nok, og ikke så vildt kompliceret igen..:)..

%"
%(08.10.23, 9:24) Som sagt fik jeg nogle gode idéer i går aftes. Jeg skriver dem ind her, og så kopierer jeg dem også lige til mine 23--xx-noter.
%Det var en rigtig god idé, den der med at jeg kan tage en kompakt basisvektor i et område af m-niveauet, og så kan jeg opløse først m-laget.. Ja, eller nu forklarer jeg faktisk den videreudviklede version af idéen, som jeg kom frem til i går.. man kan så opløse m-laget ved også at dele det op i områder og så starte med 1_S_{m+1,j}A^+\phi_{m,i}, hvor S_{m+1,j} er det område i m+1-laget, man ser på, og \phi_{m, i} er den basis-vektor, man startede med. Herefter kan Graham-Schmidt-producere et set af andre basis-vektorer i dette S_{m+1,j}-område, som sammen med 1_S_{m+1,j}A^+\phi_{m,i} skaber et ortogonalt set (efter at man har normaliseret nævnte vektor også). Jeg mener så, at man kan gøre dette.. eller det må man kunne: Man kan lave denne procedure på samme måde for.. Hm, eller lad mig lige tænke mig om.. (9:34) ..Hm nej, vi kan ikke lave proceduren på samme måde for hver delta-funktion i \phi_{m,i}.. ..Men vi kan lave den på en vis fast måde for hver delta-funktion, som så også kan bruges for alle andre \phi_{m,l}, der også er i samme område, lad os sige at dette område hedder S_{m+1,i} (også selvom jeg burde kalde det S_{m+1,l} så l'et og i'et her ikke clasher..).. (9:39) ..Ja, og er det ikke også bare det, vi skal gøre for at opnå, at alle disse vektorer bliver ortogonale i m+1-laget?.. ..Jo.:) (9:40) Og de bliver altså så ortogonale.. Hm, lad mig kalde det S_{m+1,l} i stedet.. de bliver så ortogonale for alle i og .. k.. Ah, lad mig kalde S_{m+1,j} for S_{m+1,k} i stedet, og lad så j betegne den j'de funktion i Graham-Schmidt.. ..Gram-Schmidt, rettere.. den j'de funktion fra Gram-Schmidt-processen. Så vil alle \phi_{m+1, i, j} være ortogonale, altså for hvert (i, j) \neq (i', j') vil \phi_{m+1, i, j} og \phi_{m+1, i', j'} være ortogonale. Og! Vi kan så fortsætte denne proces hele vejen op for alle lag, således at hele Fock-rummet nu bliver ortogonaliseret på denne måde, ikke mindst hvis vi starter med m=0, altså.:) Og det fede ved denne basis er, at det altid kun er den første j, som man starter med i Gram-Schmidt-processen, altså den man fik på forhånd fra 1_S_{n+1,j}A^+\phi_{n,i}, så vil bidrage til \braket{\phi_{m,i} | \hat A^- \phi_{m+1, i, j}}! For alle andre j vil dette indre produkt (/matrixelement) være nul! Og for i \neq i' vil \braket{\phi_{m,i} | \hat A^- \phi_{m+1, i', j}} også være 0 for alle j, fordi.. (9:50) ..fordi hvert \braket{\hat A^+ \delta{m} | \phi_{m+1, i', j}} vil give det samme bidrag for hver \phi_{m,i}, men.. ..Hm nej, hvordan skal jeg formulere dette..? (9:53) ..(9:56) Nå, det skal jeg faktisk lige tænke noget mere over. Lad mig vende tilbage og indsætte tekst her, efter denne sætning, for jeg har lyst til at fortsætte og skrive om nogle andre ting. (9:58)
%*[(10:19) Nej, jeg skriver det bare i forlængelse nedenfor, når jeg for tænkt over dette.]
%(9:58) Det er ret fedt, hvis jeg kan forsimple alting sådan her, men det er nu ikke sikkert, at jeg vil bruge det i mit paper her, selv hvis jeg kan. For det gør ikke noget, hvis mit domæne også ser alle de "ubrugelige" \phi_{m+1, i, j}'er (j \neq 1) som "gyldige" vektorer alligevel (i modsætning til hvis man gik ud fra disse basisvektorer i stedet for bare områder i prarameterrummet, og at man så direkte gik ind og sagde, at alle andre \phi_{m+1, i, j} end den første (j = 1) er ugyldige \emph{vektorer} ift. V). Nå, men noget andet fedt er, at jeg kom frem til (det var forresten det sidste, jeg kom frem til der kl. halv ti i går), at for SA-argumentet, der kan.. Lad mig lige skifte linje..
%(10:04) For SA-argumentet, der kommer jeg sikkert til at kunne argumetere for, at f.eks. den første vektor, man udseparerer ved hele tiden kun at tage de "gode områder" fra alle lag, at den vektor også må skulle være endelig (norm-mæssigt) for ikke at få et ubegrænset indre produkt-funktionale. Og så kom jeg på, at resten af argementet derfra vel bare må være, at.. ja, eller først skal man lige sige, at dette kan vi gøre for alle andre også, og nu har vi så en (muligvis uendelig) sum af endelige vektorer i V. Og så er det derfra bare at sige, jaman for hver vektor i V, der kan vi mindst "udnytte" så og så meget fra hvert hovede. Så hvis ikke det skal være ubegrænset samlet set, så må amplituderne på et tidspunkt aftage ret drastisk, klart drastisk nok til, at den samlede norm kan blive endelig. Det må kunne lade sig gøre at argumetere sådan, i hvert fald, og det er den eneste lille hage---eller det skal jeg selvfølgelig ikke sige; 7, 9, 13---at i "hvert fald" ikke hvis man også lige kan argumetere for, at.. Ja, at det ligesom er ortogonale, de her vektorer i denne sum.. ..Og/eller at deres billeder er det. Men det føler jeg nu også, at jeg når frem til med, når man betragter disse basisvektorer, jeg lige har snakket om, men som jeg dog pt. mangler at snakke færdigt om.. (10:12)
%Nå, så det vil jeg altså også vende tilbage til. Men en rigtig stor idé fra i går, det er, at jeg fik tænkt over argumentet for, at man kan diskretisere \hat A, og jeg blev lige lidt bekymret, for jeg glemte, at selve k-cutoffet også potentielt set kunne være problematisk. Men så kom jeg på dette argument: Med et stort k-cutoff (k_max) så er det ikke sikkert, at alle produktionerne.. Hov.. Åh, jeg skal faktisk tænke noget mere over det, for det er jo ikke kun produktioner fra områder med store k, hvor det ikke er sikkert, at man får kvælt alle.. Hm.. Nå, jeg skal tænke noget mere over det.
%Øv, sikke meget af det, som jeg alligevel skal tænke mere over. Det kan være, at i dag bliver en tænkedag, så.. Jeg havde ellers håbet på en god skrive/regne-dag.. Jeg tror heller ikke det giver mening nu, at indsætte tekst ovenfor; lad mig i stedet bare indsætte det herunder, når jeg får udtænkt det... (10:19)
%..(10:21) Hm, hvis jeg vender tilbage til \braket{\phi_{m,i} | \hat A^- \phi_{m+1, i', j}}, så er min intuition altså, at når hvert punkt i S_{m, l} fører til den samme delvektor for \phi_{m+1, i', j}, som altså kan delvist separeres til.. Ah, lad mig se på den vinkel: \phi_{m+1, i, j} kan delvist separeres til
%\phi_{m+1, i, j}(k_1,...,k_{m+1}) = \phi_{m, i}(k_1,...,k_m)\Phi_{j}(k_{m+1}).. ..Hm, nu ligner det da en fuldt separabel funktion.. (10:28) ..For en mere kompliceret \hat A kunne den også afhænge af p, som så ville optræde begge steder, men det behøver jeg ikke at tænke på.. .Hov nej, \Phi skal nok afhænge af de andre.. Hm, lad mig lige prøve at opskrive:
%\phi_{m+1, i, j}(k_1,...,k_{m+1}) = \phi_{m, i}(k_1,...,k_m)\Phi_{j}(k_1,...,k_{m+1}).. ..Og når man integrerer over k_{m+1} i \braket{ \phi_{m+1, i, j} | \phi_{m+1, i', j'} }, så må man i hver delta-funktion over (k_1,...,k_m) få nul, hvis j \neq j', og hvis j = j', så må man få det samme, men når man så integrerer over resten, så vil \phi_{m, i} og \phi_{m, i'}'s ortogonalitet medfører, at integralet samlet set bliver nul også. Ja, jeg tror, jeg har ret.
%Og dette gør jo problemet meget mere simpelt---og det vil helt sikkert i så fald blive en del af et mere elegant bevis for den proposition, som jeg prøver at vise nu i det her paper, samt også for mere generelle operatorer. (10:36) Men som sagt, så er det ikke sikkert, at jeg når at gøre brug af det..
%*[(20:07) Jeg kunne også lige nævne, at det at lade r_{m+1} vokse (for D_{m+1}), det vil så i denne version af domænet bare skulle erstattes med, at man fjerner flere og flere basisvektorer fra m+1-laget, samt de tilhørende i alle øvre lag, fra \chi.]
%(10:37) Angående SA-argumentet, og om de forskellige vektorer, jeg får vil være ortogonale.. ..Hm, jeg bør næsten gå-tænke over dette i stedet, og hvis jeg så når i mål i god tid, så kan jeg bare fortsætte med diskretiseringsproblemet, som nemlig også er et rigtigt godt gå-tænke problem (altså i sådan en grad, at jeg tror gå-tænkeri, vil være det mest effektive, og det gælder for begge disse problemer)... (10:41)
%(13:02) Okay, ift. SA-argumentet, så er det rimeligt trivielt, at de bliver ortogonale med min nye måde at lave de gyldige områder på, hvor rækkefølgen af k'erne altså kan udledes fra deres størrelser. Og så kommer jeg altså i mål sådan med det.:)
%Noget andet er dog det der med at diskretisere \hat A og Hilbertrummet. Jeg fik den tanke på gåturen, at man måske kan "snyde" og kvæle nogle områder i sin tilnærmede vektor med nogle andre områder to niveauer oppe (som så \emph{er} i Hilbert-rummet, i modsætning til de områder som den rene egenvektor selv bruger).. Men jeg kan desværre ikke lige fuldføre det argument, især ikke fordi n-begrænsningen, man kan sætte, vil afhænge af k(_max)-begrænsningen.. Hm, nu kom jeg så godt nok på, lige her for kort tid siden, at man måske kunne begrænse n delvist ved også at diskretisere og begrænse p(_max) for fermionerne.. Hm, var det ikke en idé, der er værd at tænke mere over?.. (13:08) ..(13:14) Hm, det kan også være, at det her bare skal være aften-tænkeri i stedet.. Lad mig sige det.. ..(/ pause-tænkeri).. Ok.. (13:15)
%"
%
%(17:22, 08.10.23) Ah, men diskretiseringsproblemet hænger jo kun sammen med mit (samlede) Loretnz-invarians-argument; det er jo ikke nødvendingt for at kunne udlede feltintegraler, er det!? ..Nej, det tror jeg ikke(!), men lad mig lige tænke over vakuum-delen igen.. Her er det jo tanken, at.. Ja, der må man kunne lave samme slags begrænsninger på k'erne, nemligt hvor man tillader, at k_max kan være forskelligt for hvert n, altså hvert niveau i Fock-rummet. (Eller man kan forresten også lave andre regulariseringer, f.eks. kan definere en form for (meget langsomt) eksponentielt aftagende faktor over hele Fock-parameterrummet, og så kan man ændre \hat A således at hver del får ganget en faktor svarende til brøken af, hvad den faktor er i henholdsvis start- og sluttilstanden for den givne transition.) Så det jeg gør, er jo at sige, at hvis vi kan.. Nå, ja, og det med at håndtere vakuumfluktuationerne handler bare om, at man skal diskretisere k-rummene med en spacing (svarende til et volumen-cutoff (i positionsrummet)), og det kan man jo sagtens. Og når man så gør det, så skal jeg bruge, at der eksistere en almost-egenvektor for vakuum-delen, og at dennes transitioner med ikke-vakuum-delen af Hilbert-rummet vil være forsvindende. Så vakuum(-perturberende)-termerne vil altså bare give en konstant energi, der afhænger af k-spacingen/positions-volumen-begrænsningen, og ellers vil vakuum-delen ikke interagere med resten. Så den kan man bare fjerne for felt-integralet (ved at man så også trækker denne konstante energi fra, hvilket er trivielt, givet at man også har vist, at \hat A er self-adjoint uden vakuum(-perturberende)-termerne) sammen med vakuum(-perturberende)-termerne, og så får man et felt-integrale, der konvergerer, uden nogen yderligere regularisering eller normalisering påkrævet! Men bemærk dog, at der \emph{ligger} en ikke-triviel regularisering, for jeg får disse felt-integraler, for, foruden at man jo skal fjerne vakuum-termerne (hvilket jo er den lykkelige del), så kræver det dog lige en af det omtalte regulariseringer af k-rummet. Men man ved dog i det mindste altså, at jo mere man "ophæver" denne regularisering (hvis man har konstrueret den på en sikker måde), så vil felt-integralet konvergere imod et resultat, og det vil være resultatet af \exp(-i \hat A) (eller \hat H kunne jeg også skrive her). Hvor er det fedt! For selvom at Lorentz-kovarians-beviset er noget, der betyder noget for mig i princippet, så er jeg ret sikker på, at det ikke betyder så meget for så mange andre; folk er allerede rimeligt overbeviste om, at vi har fat i den rigtige \hat H_{Dirac} (som er den samme, som jeg udledte; de viste sig jo at være ens alligevel), så det eneste folk bekymrer sig om derfra, er netop at få feltintegralet til at konvergere (og til det rigtige). Fedt! :) (17:42)

(19:43, 08.10.23) As written out in the source-code comments (in Danish) above this paragraph, I've discovered an error in my argument for how you can discretize the Hilbert space and the operator, and then go on to develop the path integrals from that. But as I've realized a few hours ago, this doesn't stop me from being able to derive path integrals (/ field integrals) in k-space! Because you can discretize and regularize that, so that you get almost-eigenvectors that approaches the real (non-discretized) ones. And while you cannot then immediately go to position space from here, I only need to do that for the Lorentz-covariance part of it all. If we just assume that $\hat H_{QED}$ is Lorentz-covariant, which most people already believe, then there's no trouble. We can introduce a spacing, as well as a kind of skew k-cutoff (or some other kinds of regularizations, see my source-code-comment notes), and this then allows for my argument of how we can get rid of the vacuum-perturbing terms. And then we get our path/field integrals. These path/field integrals then do have a regularization in them, but we can make this regularization such that when we lift it gradually, the result will converge to the true $\hat U_{QED} = \exp(-i \hat H_{QED})$.\,:)

%(8:41, 10.10.23) Hov nej, angående SA-argumentet (self-adjointness-argumentet), så kan jeg ikke bruge, der til sidst, at deres billeder skal være endelige, ikke på samme måde i hvert fald som i (Hermitisk-)symmetri-argumentet, for har ved vi ikke umiddelbart, at deres billeder skal være endelige. Så hvis jeg skal have den del til at virke, så skal jeg jo først argumentere for, at man kan udnytte.. Ja, udnytte D-billede som minimum.. ..Hm, men så er det lige før, at man bare skal omskrive \phi med det samme til V-funktioner, for det må man jo kunne.. (8:48) ..Hm.. ..Nå ja, det gør jeg jo på en eller anden måde også i forvejen; det er bare et spørgsmål om, hvornår jeg skal bruge et ortogonalitetsargument.. Hm.. (8:50) ..Tror måske, jeg vil arbejde på skriveriet, og så kan jeg tænke over dette i mellemtiden.. ..Nå nej, for det betyder noget ift. båndene, for der er en lille change for, at jeg skal ændre dem.. ..(9:00) Ah okay, det er faktisk simpelt nok: Når man deler \phi op i V-funktioner, så laver man så et argument for, at.. ..at man "udnytte" dem propertionelt med, hvor stort et billede, de hver især efterlader. ..Derfor skal deres samlede billede summe til noget endeligt, og bum, så er man i mål.. (9:03)

(9:05, 10.10.23) I think a good regularization wold be to do as I've written (briefly) about in the source code comments, which is to introduce a factor over all parameter space, that could go something like $\exp(-\alpha \sum_i \mathbf{k}_i^2)$, where $\alpha$ is then extremely small. And then you change $\hat A$ (/ $\hat H$) such that all transitions get a fraction that is the factor at the end-ket (where these kets are generalized momentum eigenvectors) divided by that of the in-ket. I think you might then be able to argue, that you can then go on to also make a cutoff on the k-space that is then the same for all $n$ (levels in the Fock space). And that would be desirable, I think. I need to think a bit more about it, I guess, but I think this two-fold regularization could work such that you get a desired (if desired) cutoff on the k-space that is the same for all $n$. (9:12)

%(9:13) Nå, tilbage til SA-argumentet, for lad mig så lige sikre mig, at mine bounds så er gode nok, som de er.. ..Og her har jeg altså tænkt mig at bruge, at \hat A^+ bevarer ortogonaliteten imellem lokalt bundne tilstande, i hvert fald ligesom når sluttilstandende også skæres ud og begrænses til at lokalt område.. ..For så kan jeg nemlig få et begrænset C^2 for hver komposant af \phi, som jeg prøver at udnytte med et \psi. .. ..Ah, og jeg skal nemlig lige præcis bruge \psi'er, der er en cutoff (begrænset i k-rummet, i.e.) version af \hat A^+\phi^i, hvor \phi^i er en komposant af \phi.. (9:20) ..Nå ja, en cutoff og normaliseret version.. ..Og det gode er nemlig, at her kan jeg få en C, der så går som normaliseringsfaktoren! ..ikke?.. ..Jo, lige netop!.. ..Og når den således bare bliver mindre og mindre, så er der jo ingen ko på isen.. er der vel?.. (9:24) ..Nej, det er der så netop ikke; jeg kan snildt vælge f.eks. aldrig at lade C overstige 1, hvis jeg vil (er jeg ret sikker på). :) (9:26) ..Ja, så alt hvad \phi^i \in V efterlader af sit billede, det kan jeg udnytte en vis andel af som minimum (altså ganget med en faktor (\leq 1)). For hvis andre dele af \phi skal forstyrre min "udnyttelse," så skal de gøre det i laget over min specifikke del af \psi, som vi ser på. Men her kan man så vise, at \phi faktisk ikke får noget ud af at gøre dette, for \psi's hale kan nemt sørge for, at det meste af \phi's modsvar hele tiden bare bliver modsvaret ét lag oppe igen, og hvor det samlede resultat altså konvergerer til, at \phi ikke får særligt meget ud af det, slet ikke nok til, at det betyder noget (for tilstrækkeligt store a (aka. \alpha)). Og dermed kommer jeg i mål. (9:32) ..(9:41) Ah, og jeg skal også lige bruge, at \phi ét lag over min \psi kun kan "forsøge" at reparere én komposant af sig selv ad gangen! Og dette er altså igen fordi, at \hat A^+, og dermed også \hat A^-, ligesom bevarer ortogonaliten mellem begrænsede start og sluttilstande. (9:43)

%... (13:15) Ah, til SA-argumentet, der skal jeg faktisk starte med at opløse Hilbert-rummet i de der vektorer, som hver især er lokale, og som bygger oven på (lokale) vektorer fra laget under sig, således at.. Hm.. ..Hm.. ..Hm.. (13:19) ..Ah jo, måske går det netop hvis jeg gør mine \chi'er symmetriske.. (13:21) ..Men gælder det med ortogonaitetsbevarelsen så stadigvæk (når den producerede k ikke behøver at være stor ift. resten)..? ..Nej, nok ikke, faktisk.. (Øv..) ..Hm, heller ikke hvis \chi_m er symmetrisk..?!.. ..Jo, gør det ikke!?. (13:27) ..Jo, det tror jeg!.. (13:30) ..Tja, eller.. ..(13:38) Hm, men måske er det også i virkeligheden bare \psi_{m+2} osv., vi skal opløse, lad mig nu se.. ..Hm, jeg kunne nu forresten nok godt lave en S_{n,m}, hvor man kan identificere rækkefølgen af alle k'erne unikt.. (13:40) ..Hm, og så kunne jeg nemlig måske faktisk gøre G_{n,m} til den, der definerer V i stedet.. Men lad mig lige tænke over det med \psi_{m+2} først.. ..Hov, hvad taler jeg om?.. \psi_{m+1}..? (13:43) ..Hov, måske var den god nok, den der med ortogonaitetsbevarelsen når \chi_m er symmetrisk, lad mig nu se.. ..Tja, måske ikke.. ..Hm, men måske kan vi opløse Hilbert-rummet, så alle områderne (på nær en uendelig lille del) har distinkte k, også hvis vi kigger på det løsninger, der bygger oven på.. ..Ja..! Og vil \hat A^+ så ikke være ortogonalitetsbevarende?..!.. (13:50) ..Nej, ikke når to områder i laget under producerer til det samme område i laget over.. (13:52) ..(14:00) Nu fik jeg lige en idé om alligevel kun at lade E_n afhænge af k_{n+1}, og så bare hive D_{m+1}^\complement-faktoren ind under integralet, når jeg skal integrere over k_j, j \leq m.. ..Hm, jeg har det bare som om, at jeg kom frem til, at det ikke kunne lade sig gøre; at k_n skal vokse-vokse-vokse.. (14:04) ..Hm, ser umiddelbart lovende ud, men så skal jeg nu nok bare fjerne k_j'erne for j \leq m i hvert E_n, således at hvert E_n altså stadig afhænger af alle k_j, j > m. (14:14) ...(14:30) Ja, det er bare det, jeg gør---simpelthen så simpelt.. Virkelig fedt.. Og så kommer jeg nemlig til at kunne bruge, at \hat A^-_m \chi_m her endelig norm, og så får jeg ikke længere det der C i mit bound. Og det er rigtig godt, for så behøver jeg ikke længere at opløse \phi. For man kan nemlig også sagtens vise, at \hat A^-_m \phi_m skal være begrænset, og så kommer mit \psi_{m+1} også nemt til at opfylde det. :) (14:33) ..Hm, og nu har jeg ingen grund til at symmetrisere domænet, har jeg..? ..Nå jo, for det skal jo som minimum symmetriseres lidt, for at mine \chi'er kan være der.. ..Men vigtigere: Har jeg nogen som helst grund til at give G_{n,m} den der egenskab? (14:36) ..(14:40) Nej, for det begyndte jeg kun på, mener jeg, fordi jeg blev nødt til at opløse \phi pga. den der C-begrænsning på \chi_m.. ..(14:44) Ja, og hvis jeg ser på symmetri-argumentet, så havde jeg faktisk tænkt mig nu her, at jeg også ville opløse Hilbert-rummet før dette, men det behøver jeg faktisk slet ikke, og selv hvis jeg vælger at gøre det alligevel, så behøver der ikke at gælde noget særligt om den opløsning rigtigt alligevel (andet end at vektorerne er lokale (igen: ikke at jeg behøver det *[Jo, måske er det faktisk meget fornuftigt alligevel.. (14:58)])). (14:46) Og i SA-argumentet skal jeg ikke opløse \phi andet end at jeg selvfølgelig skal dele den op i V-funktioner. ..Og så argumenterer jeg for, at det man kan "udnytte" af hvert \phi_m, det vil være propertionelt med, hvor meget dens egen V-følgende hale lader dens billede fra \hat A^+_m være, nå ja, og det samme med alle n over m. Og så kan jeg så argumentere for, at \hat A_{form} \ket{\phi} må være endelig. (14:50)
%(15:06) Hm, SA-argumentet kræver så nok også lige, at man argumeterer for, at man kan udnytte \hat A^-_m \phi_m, for så kan man nemlig måle alle \|\chi^{1-}_{n-1,j}\|, j \leq m, op imod denne.. ..Og kan man udnytte \hat A^-_m \phi_m..? ..Ah, er det ikke gratis, så at sige? (15:11) ..Jo..!:)

%(18:15) Nå ja, jeg fik aldrig skrevet:
(10.10.23, 18:15) Yes, that two-fold regularization would work, I'm pretty sure. The point is that once you've done the first one, then there will be some large enough $k$, for any given $\epsilon$, such that the combined image of all those states that you cat away from the state (that has only undergone the first regularization), even if we don't cancel any of $\hat A^+ \psi$, where $\psi$ is the state that we cutoff (i.e.\ with $k$'s above a certain size), it would still only yield a state with norm less than $\epsilon$. Because once the first regularization is in place $\hat A^+$ will than only produce a (very large but) finite vector. So if you just choose a $k$-cutoff that is large enough that $\psi$ will be smaller than $\epsilon$ divided with the maximum norm that $\hat A^+$ can produce (or something like that), then the image will only be off the one that was before this second regularization with a vector that has smaller norm than $\epsilon$. (And the image before the second regularization can also be made arbitrarily close to the one before the first regularization, i.e.\ the original one.) So that's the general idea. And I don't really see that $\hat A^-$ could cause any trouble that ruins this trick either.\,. So it seems to me that it would indeed work.\,:) (18:27)


%(11.10.23, 10:43) Jeg kopierer lige de her tilfælde noter ind fra min SA paper draft, bare for fordi.. (jeg ved ikke hvorfor..):
%"
%*(14:50) Jeg har fundet ud af, at jeg ikke behøver den der egenskab omkring G_{n,m} alligevel.:) Så lad mig gå i gang med at ændre det. Nå ja, og så har jeg også fundet ud af, at mit R (eller måske vil jeg begynde at kalde det lille r, og rettere r_{n,m}) kun skal afhænge af k_{m+1},...,k_{n-1}.:) (14:52)
%(16:03) Hov, måske er det ikke så nemt at få E_{n} til kun at afhænge af de sidste n-m k'er. Men måske kan jeg få den til kun at afhænge af k_{n-1}, men så ved jeg dog ikke, om det kommer til at gå længere nede.. ..Hm, jo måske gør det.. (16:07) ..Tja, nu fik jeg godt nok et ikke så godt *(nærmest-)deja vu.. ..(16:10) Jo, det kommer da til at gå snildt, gør det ikke!?.. ..Det tror jeg! (16:12) ...(16:36) Til norm-udregningen skal jeg så beholde r'erne, som jeg før smed væk sammen med T, men så går den udregning ellers også.:) .. ..Og det ser altså også d til at gå med de j'er mellem m+1 og n for \chi^{1-}_{n-1, j}..!:).. (16:40) ..Hm, hvorfor var lige, at jeg satte \beta til 3/4 og ikke bare til 1..??.. ..Hm, det forstår jeg faktisk ikke lige---var det virkeligt bare helt skørt af mig.. Ah, nå nej, det var for at få \beta der i eksponenten.. ..Hm, men kommer jeg så til at behøve det nu?.. ..Nej, for nu er der slet ikke andre k_i'er end k_n i de grænser!:) (16:46) ..Så skulle kun lige være for ikke at nappe hele \exp(-2\alpha n), men måske kan jeg godt det, det kan jeg lige se på.. ..Nej, det er ikke sikkert, jeg kan det, men så må jeg jo bare lade \beta=3/4 i så fald.. (16:50) ..Ah, og nu kommer jeg til at kunne sætte summen af \chi^{1-}_{n-1, j}'erne med j \leq m sammen, så jeg får \hat A^-_m \chi_m direkte i udtrykket.:) (16:58) ..Ah, hvorfor fik jeg ikke gjort dette noget før, jeg overvejde det jo klart!x)x) ..(Men af en eller anden grund kom jeg frem til, desværre, at det ikke holdt.) ..x) (17:00) ...(17:32) Ah, B_n skal være symmetrisk.. Og det skal D_{m+1} så også, og så kommer vi til at få noget for n = m+2, som er endeligt givet min antagelse om, at \hat A^+_m \chi_m 1_{D} skal være endelig.. ..Ok, nice.. (17:34) ..(Hm, har lidt svært ved at finde enrgien til at gå i gang med at lave det om; er ret træt i hovedet, så kan godt være, at det først rigtigt bliver i morgen.. ..(17:41) Puh, blev faktisk virkeligt træt lige pludseligt..!..)
%(11.10.23, 10:35) Har sovet længe i dag. (Vågner mærkeligt nok altid ved fire-fem-tiden om natten lige for tiden (plejer jeg ikke), men faldt heldigvis i søvn igen nogenlunde hurtigt.) Nu hvor jeg sidder og kigger på det, så kom jeg til at se, nu her, at jeg måske kan bruge lutter rekursive argumenter med den her ændring. Lad mig lige tjekke.. ..Ah, men det passer jo. Hvor er det mærkeligt, at jeg kom væk fra denne idé/mulghed, har har jo overvejet den mere en én gang.. Kan ikke huske, hvorfor jeg gik væk fra den igen, men enten var dt noget med, at jeg troede, at det ville gøre udtrykkene meget komplicerede, eller også kom jeg på et tidspunkt frem til, at det ikke duede. Nå, men nu har jeg endeligt set den mulighed, og set at den er overlegen til mine andre muligheder, så det er jo godt. ..Kunne sikkert have sparet en del dage *[måske en hel uge, måske endda lidt mere.. (11:19)], men jeg er da på den anden side også kommet på en del andre gode idéer i disse dage, så ikke græde over spildt mælk; det er nok heller ikke så skidt alligevel. (10:41)
%"
%
%(11:49) Hov, det er ikke fordi, G_{n,m} så bliver mærkelig at definere..? ..Ah, det må næsten være derfor..!.. (derfor at jeg valgte et E_n, der ver symmetrisk mht. de andre k'er..).. ..Hm, men kan jeg ikke bare lade E_n (omdøbt), som jeg bruger til G's definition være en symmetriseret udgave af den E_n, jeg bruger til \chi'erne?!.. (11:53) ..Jo, det er jo det, jeg skal gøre!.. (11:54) ...(12:21) Hm, men måske bliver det så lidt svært at skære B_{m+1} ud, det skal jeg lige tænke over.. ..Hm, men jeg kunne måske bare ændre i mit nuværende H_n og lave det til et H_{n,m}.. (12:23) ..Hm, det her gør det helt klart meget mere mudret, hvordan man lige skal identificere m-laget, så jeg kan nu faktisk godt forstå, at jeg har undgået dette asymmetriske E_n førhen. (12:27) ..(Hvilket faktisk er ret rart; så har jeg ikke været en klovn at overse denne mulighed---hvis det altså overhovedet er en mulighed!..) ..(12:29) Ja, og det bliver nemlig lige netop ret problematisk, hvis jeg ikke kan få lov at kanselere en masse produktioner, fordi de kommer til at ligge i B_{m+1} \times ..., jeg ved, hvad jeg mener.. ..(12:32) Hm, ja, så jeg tror faktisk ikke den går alligvel, jeg bliver nødt til at beholde det symmetriske E_n.. ..Hm, men kan jeg ikke got alligevel bruge rekursive argumenter?..!.. (12:34) ..Jo, det virker da sådan!:) (12:37) ..(Ikke at det gør mit arbejde meget nemmere, men det kommer til at gøre læserens arbejde nemmere..)
%(12:52) Hm, jeg kunne stadig godt have haft set det rekursive argument for de to sidste bånd noget tidligere, men igen: ikke græde over spildt mælk. ...(13:33) Hov, måske kan jeg ikke lave det rekursive argument alligevel, når nu E_n, altså den jeg skal bruge til k_n-integrationen, er så kompliceret.. Hm.. ..(13:37) Ja, det er jo det.. Nu overvejer jeg dog lige, om man mon bare altid kan lade en radius af \max(|k_1|,...,|k_m|) være af produktionerne, nemlig hvis man så bare sørger for at E_n-radiussen vokser hurtigt nok.. ..Hm, det bliver nu nok lidt mærkeligt, så.. (13:40) ..Ja, jeg tror altså ikke, jeg kan lave om på det.. (13:43)
%... (15:50) Det tog lidt tid, men på en regnfuld gåtur nu her kom jeg langt om længe frem til, at jeg jo faktisk gerne vil tilbage til mit R_n med det der L_n, for jeg vil jo gerne have til symmetri-argumentet, at halerne lader hinanden være helt. Men så bør jeg bare bruge (k_i^2 + 1) i stedet for k_i^2 i eksponenterne, i stedet for at komme med den der grimme (viste det sig) ekstra-begrænsning til E_n. (15:52) ...(16:33) Vent, er jeg nu også helt sikker på, at jeg behøver dette?.. ..Ah, som om min hjerne lidt hellere vil arbejde med formler i dag, end den vil tænke over delmængder af 3n-dimensionelle parameterrum.. (16:35) ..Hm, jeg behøver det nok.. (16:37) ..Ja.. (16:42) ..(16:43) Ah, jeg kunne måske dog lave r_n mere symmetrisk.. Hm.. ..Sure.. ..Hm, tænke nemlig at sætte en g_n(k_1,...,k_{n-1};k_i) ind på alle k_i'er, men hvad med i stedet, at gange g_n(...) på n..? ..Ja, eller bare plusse den på i eksponenten.! (16:48) ..Ja, det er det sidste, jeg gør..

(12.10.23, 9:49) It just occurred to me that I never even go to position space in my planned Lorentz-covariance argument, and maybe that exponentially decaying regularization is actually just as good if not better than a $k$-cutoff.\,! I don't know at all, 'cause it's been ages since I looked at that argument. And I by the way don't think I will look into it again any time soon. But it's still so nice to know that there is a good chance that.\,. well, that the double regularization, where you first use the exponential decay and then cut off the $k$-space, might work for the Lorentz-covariance argument still.\,:)

(11:20) Yeah, the exponential decaying regularization should actually just give you much more power, in principle, to show that the two integrals, or rather sums, in $(\omega, k)$-space converge to the same value! .\,.\,At least I think so!\,.\,:)

%(13.10.23, 10:16) Okay, jeg har lige fået tænkt noget mere over mit domæne. Det kræver lige lidt argumentation, lidt mere, end jeg regnede med endda, men jeg kommer helskinnet igennem med min g-funktion. Jeg er så dog også kommet lidt frem til, at man nok godt ville kunne komme igennem med et E_n, der kun afhang af k_{n-1}, men så ville symmetri argumentet til gengæld kræve, at man udledte tilsvarende bånd for hele V, som jeg har gjort for W. Og det er jeg personligt ikke lige klar til, selvom det på sigt måske kunne føre til et mere elegant bevis (når først man har regnet den ud og udført arbejdet).
%..(10:23) Hm, nu overvejer jeg lige, hvad der ville ske, hvis man gjorde g afhængig af m's paritet.. ..Hm, det gør nok ikke noget, nej, så never mind..


(15.10.23, 8:54) Even if my argument about how the vacuum particle--physical particle interaction transitions will vanish in the continuum limit does not hold for some reason, the perturbative argument that the kernel of the vacuum-perturbing part of the operator will be.\,. point-like, so to speak.\,. is actually a pretty good intuitive argument still for why you must be able to remove that part of the operator (the Hamiltonian, i.e.). All intuition tells us, that the functions with exactly 0 energy will be these non-normalizable functions, and that.\,. Hm, or I guess maybe there could be a space of them, that could yield normalizable function when you integrate over just that space.\,. .\,.\,Hm.\,. .\,.\,I don't know, my intuition says that there won't be.\,. .\,.\,Well, but I hope my other argument works (and I think that it does).\,.


%(9:50) Jeg kopierer lige følgende ind fra mit SA paper draft:
%"
%We recall that
%\begin{equation}
%\begin{aligned}
%	G_{n,m} =
%		\bigcap_{j = 2, 4, 6, \ldots}^{n-m} \big(
%			E_{m+j} \times \mathbb{R}^{3(n-m-j)}
%		\big)
%		\,\cap\, \big(
%			(\mathcal{P} E_{m+1} )^\complement \times \mathbb{R}^{3(n-m-1)}
%		\big),
%\end{aligned}
%\end{equation}
%and
%\begin{equation}
%\begin{aligned}
%	E_n = \big\{
%		(\mathbf{k}_{1}, \ldots, \mathbf{k}_{n})\in \mathbb{R}^{3n} \;\big|\;
%			e^{ \alpha n + \sum_{i=1}^{n-1}\mathbf{k}_i^2 } - 1
%			<
%			|\mathbf{k}_{n}|
%			< 
%			e^{ \alpha n + \sum_{i=1}^{n-1}\mathbf{k}_i^2 }
%	\big\}.
%\end{aligned}
%\end{equation}
%..
%
%(9:29) Åh! Jeg har snakket om at bruge, at $\hat A \chi$ mest ``befinder sig'' på niveau $m-1$ og $m+1$. Men jeg kan gøre meget bedre! Jeg kan bruge, at på lag $m+1$, der befinder $(\hat A \chi)_{m+1}$ sig mest kun på $D_{m+1}$, hvilken jeg sætter lig $\mathcal{P} E_{m+1}$. !! (9:42) .\,.\,Så allerede der får jeg jo, at udligningen af $\chi_m$'s ``udnyttelse'' af $\phi_{m-1}$ må skulle modsvares på.\,. .\,.\,Hov, nu blev jeg lige forvirret, 2 sek.\,. .\,.\,Ja, jo, så jeg skal gerne få herved, at $\phi_{m+1}$ derfor er tvunget til at arbejde på $\mathcal{P} E_{m+1}$, hvilket så gerne skal gøre det på vej til at følge $G$ (og dermed på vej til at følge $V$ også).\,. :)\,.\,. (9:49)
%"



\ 

(16.10.23, 11:36) Copied from SA paper draft:

``

(16.10.23, 10:33) I felt a bit dumb yesterday, and a bit disappointed in my self: Why did I think that I could just remove that lower part of $G$ (what I have called $V$ in my old qed.tex notes.\,.)?\,. But having thought some more about it all this night and this morning, I actually don't feel quite so stupid after all, I mean, I did get actually get remarkable far with this strategy.\,. And it is remarkable how this idea of of cutting out that $(\mathcal{P} E_{m+1} )^\complement \times \mathbb{R}^{3(n-m-1)}$ part not only seems to lead us towards (Hermitian) symmetry (although I couldn't quite complete the argument that it leads to symmetry; it might not), but it also restricts Dom($\hat A^*$) so that any $\phi$ in Dom($\hat A^*$) has to cancel itself enough that $\hat A_{form} \phi$ get a finite norm, and it has to cancel itself using
$
\bigcap_{j = 2, 4, 6, \ldots}^{n-m} (
	E_{m+j} \times \mathbb{R}^{3(n-m-j)}
)
$! (At least that's what it seems.\,.) So the idea actually really seems quite promising, in a sense.\,!\,.\,. Today (this morning and noon) I've thought about some more options, and it is also enticing to think about, what happens when we \emph{reduce} that the $E_{m+1}$-/$B_{m+1}$-image of some $\psi\in\mathrm{Dom}(\hat A)$-heads: This has the influence.\,. Hm, let me think again.\,. (10:46) %..(10:56) Nå, min hjerne kan ikke lige fokusere, åbenbart.. ..så tager en pause..
.\,.\,.(11:02) Oh yeah, that has the effect that $\phi$'s in the same area will now always contribute some to the braket functional, and if we remove the $E_{m+1}$-/$B_{m+1}$-image completely, it should just make the functional unbounded.\,. right?\,.\,. .\,.\,Yeah. So there is this tug and pull between $\mathrm{Dom}(\hat A)$ and $\mathrm{Dom}(\hat A^*)$ that seems really interesting and almost promising. And the fact that $\mathrm{Dom}(\hat A^*)$ already consists of $\phi$'s where $\hat A_{\text{form}} \phi$ has a finite norm, I mean, I really seem to be so close.\,!\,.\,. .\,.\,But as things stand, I've actually lost the hope that I will be able to complete this strategy (and I only have a very small hope that it is even possible).\,. I guess I should think more on it, but it does seem now that the task is actually much more difficult.\,.

There are, however, also a lot of other things, that one could try, i.e.\ to search for a way to find a ``symmetric domain'' that can also ``tug'' $\mathrm{Dom}(\hat A^*)$ closer and closer to it, such that hopefully, it can pull it all the way into itself. So unless someone knows a theorem that tells us that this will not be possible, I guess there is still hope. (11:12)

And I don't leave this period of work (a month now of working as much as I could each day, save for that single one-day vacation.\,.) completely empty-handed. Well, first of all, I am a quite a bit wiser on the problem now. And importantly I also realized that I could actually solve the other tasks *(that I left after my QED paper), it seems (especially if we don't count the Lorentz-covariance task(s), which are not so important when my operator is equal to the conventional one, although I don't see why I wouldn't be able to complete that task if I was right about thinking I would be able do so before). And the most exciting part of this is that I have renewed my believe in my old solution to getting rid of the vacuum fluctuations / the vacuum-perturbing terms of the Hamiltonian. Unfortunately, this argument requires that the (renormalized) vacuum-perturbing part of the Hamiltonian will be self-adjoint on its own in whatever theory of everything for which the full Hamiltonian is self-adjoint. So unfortunately we can't conclude exactly that we \emph{will} (100 \%) be able to remove the vacuum-perturbing terms. But, it is nevertheless an important point still: If it turns out that the renormalized vacuum-perturbing Hamiltonian is self-adjoint, and that it thus has generalized eigenvectors, then we should be able to get rid of the vacuum fluctuations, just like that, without changing the physics, and therefore without breaking any symmetries such as the Lorentz covariance. It's not the paper (and I'm talking about what I thought would be a follow-up paper to this one, but now I don't think I will finish this one, not at this point in time at least) that I hoped for, but it's still something. And it's actually enough that I think that it justifies this month of work (taking away from my Semantic Database project), which is really comforting.\,. 

.\,.\,Yeah, so while I did really question if this month of work had been worth my time yesterday, I now think that indeed it has been. (11:29) .\,.\,I think I ought to try to get that other paper done, or at least close to done, this week, but I might also want a little break from the physics first.\,. I'll see. (11:31)

''


(11:37) As can be seen here in this copied text, I think I've given up on my strategy for now. I do still have to think a bit more on it, but it seems like a much harder problem than I thought (this time around), and I'm not sure that it's even possible, at least not with my strategy exactly. So I think I will most likely leave this self-adjointness problem for now, and thus put it on the shelf, so to speak.\,. (11:40)

(12:09) Oh, I just had the thought/realization a moment ago that the vacuum-perturbing part of the Hamiltonian is just what we are effectively left with.\,. hm, when there is no physical particles in the system.\,.(?\,.\,.) .\,.\,Yeah, exactly (I think).\,. So maybe we can argue that this ``part'' of the Hamiltonian must be self-adjoint for any full theory of everything after all.\,.\,!! (12:12) .\,.\,This would great!\,.\,.


(17.10.23, 11:08) Wow!! Maybe I'm actually a lot closer than I thought!! I have been so close to putting the problem on the shelf, but I've just thought some more about it this noon, and now I get that we can actually already conclude that $\phi \in \mathrm{Dom}(\hat A^*)$ needs to use $E_{m+2}$ to cancel its productions at $m+1$, \emph{and} that it however cannot do anything about its productions at $E_{m+1}$ in terms of their contribution to the maximum of the braket functional! It's.\,. I almost can't believe it---and in fact I am hesitant to believe it.\,. .\,.\,I have to mention, then, that the restriction that $\psi \in \mathrm{Dom}(\hat A)$ should ``leave its productions to $E_{m+1}$ be'' should not count when $\psi$ is already in $E_{m}$.\,. (11:15) .\,.\,If I'm not mistaken about this, I actually feel that at this point, I should publish the results, even if I can't quite complete the proof (myself) at this time.\,.\,!\,.\,. (11:18)

.\,.\,The argument, by the way, for why $\phi$ can't do anything about its ``productions'' (as I like to call it) to $E_{m+1}$ is that we can add a state with its head in the $(m-1)$-layer, and then give that state a tail that ``overcorrects,'' i.e.\ one that ``cancels'' much more of its ``productions'' to the area where $\phi$'s head is located/supported, such that in the end, $\hat A \psi$, where $\psi$ is this $(m-1)$-head-state, is approximately parallel to $\phi$'s head at $m$. (11:24)

.\,.\,Right now I'm also hopeful, that this argument will collapse if we look at the layers above from $m$, i.e.\ where $\phi$'s ``head'' is located---or \emph{one} of its ``heads,'' rather. This would be quite great, if this is so, wouldn't it.\,. (11:27)

(11:41) Oh, but this seems to indicate that I've made a mistake in my (partial) symmetry argument, doesn't it?\,.\,. .\,.\,(11:48) Or maybe not.\,.(?\,.\,.)

.\,.\,(11:52) Ah, it's not $\psi$'s located in $E_{m}$ that doesn't need to leave its productions be, it's $\psi$'s in $G_{m}$/$S_m$.\,. I think.\,. %..(11:56) I can feel my brain grinding to a halt; this is also a bit much to take in. I should take a break and then "hum" (in my mind) over it. ..(Jeg tror, at det danske begreb bare betyder, at man snakker (lavmælt) sammen om en ting i grupper, men af en eller anden grund tænker jeg på det og bruger det, som om det betyder at summe over en ting i hovedet (at tankerne ligesom summer lidt i baggrunden af hovedet)..)

%...(16:07) Åh, måske er det lige præcis det, at jeg ikke begrænser $\psi$'er, der selv ligger i $E_n$, til at skulle lade deres egen $E_{n+1}$-produktioner være, der gør at det går op..!(!).. Men lad mig se videre.. %..Tog forresten en lang gåtur lidt tidligere, hvor jeg summede lidt videre over det, men på den sidste del af turen lod jeg det egentligt være (til jeg kom hjem til noget papir). Nu har jeg tegnet lidt på et papir, og er altså kommet frem til dette 'måske,' jeg lige skrev om. Men ja, jeg kunne sikkert godt have arbejdet hurtigere/mere i dag, men det har nu også været rart ikke at skynde mig med det---og bare være glad for, at jeg i det mindste ser ud til at kunne komme en millimeter tæt på en løsning, hvis altså jeg ikke ligefrem \emph{kan} (fucking) løse det.. ..Og jeg vil nemlig være stolt, selv hvis bare min løsningsstrategi/idé kommer så tæt på..
%
%..(16:18) Og jeg har vist ikke nævnt, så lad mig lige gøre det, at det med, at jeg nu ser ud til at kunne få, at $\phi$ ikke må have et uendeligt stort samlet $E_{m+1}$-billede, når man summer dem sammen, det er jo så netop det, der kan gøre, at jeg kan antage det for Dom($\hat A$) og så sikre mig, at $\hat A$ bliver symmetrisk (Hermitisk).
%
%..(16:25) Tja, måske er den ikke-begrænsning faktisk lige meget, men princippet i den/det kan altså stadig være det, der får det til at gå op, så at sige..
%..Udkommenterer lige dette her, jeg lige har skrevet, for bedre at have det stående herude i kommentarerne.. ..Jeg kan ikke samle mig om at fokusere på problemet head-on, så tror lige, jeg går mig en aftentur (for tror stadig benene er friske nok)... (16:32) ..(eftermiddag, ikke aften---føles bare som aften allerede, når jeg ikke har arbejdet så meget/intenst i løbet af dagen (havde det på samme måde i går ved denne tid).)

%(18:27) Nej, men skal ikke lave den "ikke-begrænsning"/begrænsning, eller om ikke andet så behøver man den altså ikke. Puh, jeg tror altså faktisk, at min løsning, hvor man altså bare skærer $E_{m+1}$ fra for G_{n,m} (og dermed for S_{n,m}), holder alligvel, eller sådan ser det altså lidt ud (7, 9, 13)..!.. ..Det ser sådan ud..!.. (18:31)

%(18.10.23, 9:41) Årh, det ser altså ud til at holde..!.. Jeg har en ret stor følelse af ærefrygt/awe ligenu---en følelse som jeg sjovt nok ikke rigtigt havde så meget i går, måske fordi jeg et eller andet sted ikke rigtigt turre tro på det (efter at jeg først mistede håbet (næsten helt)).. ..Men nu har jeg altså lige kigget på det igen, og det ser altså umiddelbart ud til, at det holder. Og nu snurrer det altså lidt i min krop (nakke, ryg), hvilket også er fortåeligt..

(18.10.23, 9:46) I think my solution holds.\,.\,(!!!\,.\,.)

%..(9:48) Jeg fik også lige kigget lidt i en QFT bog i går morges, og hvor er der bare meget underligt fysikteori, som vi nu bør kunne skære fra (det tror jeg). Så det er jo en helt vildt, fantastisk stor opdagelse..!..
%..Det føles måske også bare ekstra vildt, fordi de sidste brikker her faldt på plads af sig selv for mig, så at sige (selvom jeg da lige skulle indse det---og hvor var det utroligt godt, at jeg lige akkurat ikke nået at lægge det helt på hylden..!!).. ..Det for det hele til at virke endnu mere fantastisk.. (9:53) ..(Jeg har godt nok, i øvrigt, haft en intuition om, at \phi'erne ikke kunne ende med at bryde symmetrien for \psi'erne, altså at \phi ikke kunne være assymetrisk med \psi, altså at matrix-elementet ikke måtte give noet andet, når man vendte det om, men jeg kan ikke helt huske, hvad argumentet var, og jeg er bestemt ikke sikker på, at det er relateret til det faktiske argument, jeg står med nu. Og jeg tror altså ikke rigtigt, at den argumentation var rigtig eller tilstrækkelig i sig selv, men lidt svært at sige, når jeg ikke kan huske præcis, hvordan de tanker gik. Hvis jeg husker det, må jeg lige nævne det her..)
%..(9:58) Ej, den der følelse af awe/ærefrygt er altså ret stor.. ..Igen: totalt forståeligt også.. ..(I nakken især og i skuldrene, og lidt i rygraden i det hele taget (fik lige lyst til at beskrive følelsen).) ..Ah, og hvor er det også bare vildt dejligt at tænke på. ..Det er svært at beskrive, hvor meget det betyder for mig, og hvor meget jeg tror, det kommer til at betyde for mig (men på den anden side giver det på en måde også lidt sig selv).:) (10:05)

%(10:33) Jeg har lige taget en stund, hvor jeg bare har tilladt mig selv at nyde denne følelse. Og i stedet for at gå i gang med arbejdet om at finde frem til, hvordan SA-argumentet så skal udformes færdigt, så tror jeg faktisk, at jeg lige vil tage det stille og roligt, og så lige skrive om nogle andre ting, som jeg har på todo-listen, noget fra i går og så også en del ting, som jeg har haft på den i lang tid (i hovedet, altså).

%(11:55) (Sådan.) Ah, det var meget også meget rart lige endeligt at få noteret de småting. Selvom det bare var nogle småting som sådan, så var det altså stadigt ret rart. ..Har stadig den der awe-følelse i rygraden, btw. ..Den sidder længe.. ...(12:17) Ej, jeg er helt sat ud af spillet..xD (^^)
%... (14:13) Ej, jeg har ikke følt mig så lykkelig i lang tid.! Jeg føler mig virkeligt lykkelig nu.. ..Det validerer også bare ligesom alt mit arbejde, og hele mit liv, faktisk, indtil nu. Og samtidigt så får det også bare min nære fremtid til at se så lys ud, så ja, på et personligt plan betyder det bare så meget, hvis altså jeg ikke tager totalt fejl med mit bevis (og særligt den nye sidste del her af det) (7, 9, 13).:) ..Det er bare virkeligt fantastisk..
%(14:48) Puh, det er ret hæftigt: Den lykkefølelse bliver bare ved og ved. Det er lige før jeg bliver i tvivl om, om jeg overhovedet når at samle mig om at arbejde igen i dag.xD ..(Det er nemlig sådan en døsende lykkefornemmelse.) ..Samtidigt føler jeg også et eller andet sted, at jeg bør nyde det.. ..Nyde det ordentligt, at jeg har gjort denne opdagelse, og nyde denne særlige lykkefølelse, der følger med.. (14:57)
%... (16:19) Det var faktisk nok noget lidt tilsvarende til den her sidste del af beviset, jeg havde tænkt omkring, hvorfor \phi ikke kan bryde symmetrien, som jeg nævnte tidligere i dag. *(Men jeg er ikke 100 % sikker *[er faktisk langt fra sikker... *(Men det var nu nok noget af det samme, jeg har været inde på der.. *[..Måske..])], og det ver nok ikke helt så gennemtænkt under alle omstændigheder..) Så på den måde er det ikke dumpet totalt ned i turbanen, at min løsning holdt hele vejen (som den ser ud til nu, 7, 9, 13). Men det synes jeg nu alligevel ikke, gør lykkefølelsen mindre.:) ..Så er det bare dejligt at have genfundet (og sikkert i en meget mere velovervejet udgave) det argument, især nu hvor jeg nærmest var en hårsbredde fra at give op for denne gang; det virkede nemlig som om, at jeg var det.:)xD (16:24)

%(16:43) Ah, man skal vist alligevel have den der "ikke-begrænsning" med, som jeg har talt om.:)

%(17:43) Og lad mig lige få det på det rene: Jeg tror altså, at dette kan revolutionere QFT fuldstændigt.:)^^


%(20.10.23, 10:57) Nå, jeg har sovet lidt dårligt i nat, bl.a. fordi jeg kom til at tænke på, hvordan man kunne vise SA, hvis man tilføjer den frie energi til operatoren, og så indså jeg, at mine \chi-løsninger jo tilsyneladende ikke vil sendes til noget endeligt af \hat H_0. Og så har jeg tænkt lidt i, hvad man mon kunne gøre for at reparere dette. Jeg har nogle få idéer, men min hjerne er lidt langsom.. ..Jeg skal lige have den op i gear (og det tager lidt tid..).. ..(11:06) Okay, det virker som om, at det kunne være en løsning at gange med \sqrt{\mathbf{k}_n} i stedet for at dividere med det, og så dividere med r_n^2 i stedet for med r_n, men lad mig lige se på resten så.. ..(Men virkeligt fedt at jeg allerede har en ret god kandidat til en løsning.!..) ..Hm, det virker nu lidt hacket, godt nok.. (er lidt skeptisk).. ..Ah, jeg glemte også at kvadrere i hovedet et sted, så never mind; tror ikke, at det løser det.. ..Hm, nu overvejer jeg, at gå den anden vej.. (11:13) ..Ja, nej.. ...(11:29) Hov vent, skulle jeg også kvadrere der?.. ..Ja.. ...(11:47) Uh, måske kan jeg godt komme igennem, også ift. \chi^{1-}_{n-1, j}, m < j < n, -båndet, hvis jeg bruger en kugleskal-tykkelse, der går som r_n også!.. ..For så får jeg godt nok et udtryk i lign. (\ref{bound_for_limited_E_n_over_k_j_integral}), hvor der kommer til at stå 2b i stedet for 4b i potensen (i nævneren), men så kan jeg til gengæld få lov at bruge alle b < 2 (i stedet for b < 1).:).. ..Hm, men kan det hjælpe med at løse problemet med at tilføje den frie energi..? (11:54) ..Jeg får vil så noget, hvor faktorerne udligner hinanden, og det er vel desværre ikke helt godt nok.. ..(12:05) Jeg fik idéen i morges/nat (kan ikke lige huske om det var efter jeg stod op, eller om det var i sengen i nat) om at lade tykkelsen gå som \exp(\sum_i |k_i|) (ikke \exp(\sum_i k_i^2)), og nu ser den idé pludselig endnu mere frsitende at gå i gang med at undersøge.. ...(12:18) Eller måske som \exp(\sum_i(k_i^2 + |k_i|)).. ..Eller rettere, at den øverste grænse går som det..
%(12:22) Hm, jeg bør egentligt også overveje at lave prefaktoren om til, hvad jeg får fra lign. (\ref{E_n_over_k_n_integral}) eksakt, nemlig for så måske at space \chi^{1-}_{n-1,n,2}. Det må jeg lige huske at overveje..

%(12:37) Nå, det med \exp(\sum_i(k_i^2 + |k_i|)) hjælper mig vist ikke. Lad mig prøve i stedet at tænke i noget, der går som.. ..k_n^{-5/2}, og så med et tilstrækkeligt stort volumen, for det er det, der bør løse det, ligesom.. ..(Eller k_n^{-3}..) ..Hov!.. ..Ah, k_n^{-5/2} er jo den grænse, hvor både \hat A^{-}-integralet og energien bliver uendeligt; jeg er ret sikker på, at jeg har gået og troet, at det var forskellige grænser!.. Hm, lad mig lige se nærmere.. (12:48) ..Hm ja, så der er faktisk ikke længere noget der fortæller mig, at dette problem \emph{kan} løses.. Hm.. ..Ej, det er virkeligt ærgerligt, for selvom jeg har det her store resultat, så betyder det bare ret meget, om det også ser ud til, at man nemt kan tilføje den frie energi (som jeg har gået at troet.:\), eller om dette vil blive et nyt avanceret problem (hvor man så muligvis kan prøve at få \psi'erne i Dom(\hat A) til også at udligne deres egen frie energi via \hat A^- \psi..)..:\..

%(13:02) Okay, det virker faktisk til, at jeg kan løse det, måske, hvis jeg kan få
%(r_2^2 - r_1^2)/(r_2^2 + r_1^2) til at aftage nok!.. ..Igen: Måske!.. ..Men så burde min (d=1)-løsning næsten allerede virke..(?) ..(13:09) Hm, men det kan da være, at den alligevel gør det, så!..(?..) ..For min tanke er nemlig, at jeg får en prefaktor (nu hvor jeg laver denne mere pragmatisk) på 1/(r_2^2 - r_1^2). Og for den frie energi får jeg så en faktor, der går som (r_2^4 - r_1^4), når man integrerer over k_n. Og vi har jo---for vi har nemlig kvadreret prefaktoren i mellemtiden---(r_2^4 - r_1^4)/(r_2^2 - r_1^2)^2.. Hov, måske har jeg bare lavet en regnefejl i hovedet.. Vi har 
%(r_2^4 - r_1^4)/(r_2^2 - r_1^2)^2 =
%	(r_2^2 - r_1^2)(r_2^2 + r_1^2) / (r_2^2 - r_1^2)(r_2^2 - r_1^2)
%=
%	(r_2^2 + r_1^2) / (r_2^2 - r_1^2)..
%Ja, så vi ville skulle få førnævnte brøk (den inverse af denne) til at vokse i stedet.. (13:16) ..Hm, og det kan man ikke, kan man vel?..:\ ..Nej, det kan man jo ikke..:\ (13:17)

%(13:25) Hm, og jeg skal ikke bare sigte efter en k_n potens, der lige er lidt "mindre" end k_n^{-5/2}..?
%...(13:53) Nu tænker jeg på at bruge k_n^{-5/2} præcist, og så med \exp(\sum_i k_i^4)).. ..For så vil prefaktoren jo blive 1/(p_2 - p_1), hvor p'erne er potenserne.. eller lad mig se.. ..Det kunne blive 1/((1-a)p).. nej.. Hm.. (13:57) ..Lad mig prøve at gange potensen med a, så det rigtignok bliver 1/((1-a)p). ..Og for den frie energi må man så få ((1-a)p)/((1-a)p)^2 = 1/((1-a)p), right?!.. (13:59) ..Vældig interessant.. Jeg tager lige en lille pause, så jeg kan komme tilbage og se på det med friske øjne.. (14:00) ...(14:15) Tror jeg vil prøve at gå en lille tur i ruskvejret, måske blive mere frisk på den måde.. ... (15:03) Jeg skulle lige til at skrive, at det hjalp at blive blæst lidt igennem, men nu føler jeg mig faktisk lidt træt igen. Men hvad meget vigtigere er, jeg kan altså stadig ikke se, hvorfor denne k_n^{-5/2}-løsning ikke fungere, ift.\ også at få den frie energi til at blive endelig. Men godt nok mangler jeg lige, at gå \chi^{1-}_{n-1,j}, m < j n,-argumentet igennem først, så det skal jeg lige gøre. Men lad mig lige nævne, at det gør mig forhåbningsfuld over for denne løsning, at selvom jeg ikke alligevel har et argument for at \hat A^- \psi \emph{bør} vinde over \hat H_0 \psi, så at sige, så har jeg stadig et intuitivt argument, ligesom, for hvorfor den/det (altså \hat A^- \psi) \emph{kunne} dette. (15:08) ..(Nemlig fordi min intuition siger mig, at \hat A^- \psi er lidt mere "kraftfuld," når vi når ned under k_n^{-5/2}, potensmæssigt.) (15:09)
%(15:27) Hm, det kunne måske være bedre, så, hvis eksponenten var et produkt i stedet for en sum.. ..Hm, selvom det gør \chi^{1-}_{n-1,j}, m < j n,-delen svære at regne på, gør det ikke..? ..Ellers kan det måske også gå fint, hvis jeg beholder det som en sum.. ..Men jeg kunne forestille mig, at jeg så hellere vil gøre det i en artikel nr. 2, således at jeg nemlig tager en lidt nemmere løsning her i denne artikel, hvor den frie energi ikke er med..
%..(15:36) Hov, er det mig, der er helt væk, eller.. ..behøver vi overhovedet det med at invertere E_n?.. Jeg ville blive overrasket, hvis man ikke behøvede det, og nok synes, at det ville være lidt mistænkeligt, men nu må jeg se.. ..Ah, bare jeg var lidt friskere---kunne nok have været godt med en middagslur, hvis jeg havde evnen.. (15:40) ..Hm, jeg burde prøve at gå det igennem i hånden.. (15:42) ..Eller prøve at lægge mig lidt først.. ...(15:56) Man skal bruge E_n til den del, så det lyder meget godt.. ...(16:22) Hov, jeg retter lige k_n^{-2/5} til k_n^{-5/2} ovenfor.. ..Hm, \emph{er} det faktisk ikke ligefør, jeg ikke behøver at invertere E_n alligevel.. For vi får jo k_n^{-5}, når vi kvadrerer.. ..Hm, det virker lidt vildt, men.. ..Hm, det virker altså lidt utroligt, hvis alt det her virker, men det ser det da umiddelbart ud til at gøre..!!.. (Har forresten ikke fået regnet noget i hånden; har kun regnet på det i hovedet, men stadigvæk..) (16:27) ..(Årh, egentligt vildt, at klokken allerede er halv fem..)
%(16:47) Hm, det virker altså umiddelbart til, at det er rigtigt nok.. Og hvis det er, jamen så bør jeg jo bruge den løsning, selv til dette paper, hvor jeg altså nok stadigvæk ikke har i sinde at tilføje den frie energi.. ..Hvor er det bare overdrevet fedt, hvis jeg har ret, men jeg er lidt for træt til at føle mig særligt sikker.. ..(16:52) Er det virker altså faktisk til, at resten af beviset, både symmetri- og SA-argumentet, kommer til at forløbe på samme måde, bare hvor det så er produktionerne + den frie energi gange \phi i samme område, som man \phi \in Dom(\hat A^*) bliver nødt til at sørge for, bliver endeligt. ..!!.. (16:54)
%(18:11) Arh, og det skal også være.. Hm, jeg forestiller mig, at \|\mathcal{J}\psi\|-restriktionen også lige skal ændres, lad mig se.. ..(18:15) Ah ja, vi skal ændre den, så at vi i stedet sætter begrænsningen på $\| \sum_i \mathcal{J}(\phi^i) \|^2$ (i stedet for på $\sum_i \| \mathcal{J}(\phi^i) \|^2$)! Og så skal dette så yderligere også ændres til, at man begrænser $\| \sum_i \mathcal{J}(\phi^i) + \hat H_0 \phi^i \|^2$ i stedet! (18:18) ..Og så skal symmetri-argumentet nemlig bare lige justeres, så man også lige sørger for at argumentere for, at for store nok N.. Hm, jeg skulle til at sige, at så vil \hat H_0 \phi^i-delen blive mindre og mindre betydende, men det kan man vist ikke bare slå fast så nemt alligevel.. (18:21) ..(18:27) Ah vent, måske behøver man slet ikke at bruge $\sum_i \mathcal{J}(\phi^i) + \hat H_0 \phi^i$ i stedet for $\sum_i \mathcal{J}(\phi^i)$; måske får vi allerede vist, at bidraget til billede fra \hat H_0 vil blive mindre og mindre betydende... ..Hm, eller måske ikke.. (18:31) ..(18:34) Hm, men vi må næsten kunne argumentere for, at den der $\hat H_0 \phi^i$-del vil blive svagere og svagere.. ..(18:43) Hov, det kan da være, at det faktisk lige netop bliver nemt at argumentere for, at \hat H_0 \phi^i ikke kan gøre en forskel her.. ..Ja, så måske kommer det ingen gang til at ændre symmetri-argumentet særligt meget, at man nu har $\sum_i \mathcal{J}(\phi^i) + \hat H_0 \phi^i$ i stedet for bare $\sum_i \mathcal{J}(\phi^i)$!:D (7, 9, 13.) ..^^ (18:46)




(25.10.23, 12:11) Until yesterday my plan was to use that $\chi_n$ is either non-negative or non-positive everywhere for all $n$ in the part of the proof that shows that $\|\hat A^-_{n,l} \chi_{n,j} \|$ for all $l \neq j, n$. But I then got a bit worried that this argument would not be generalizable to coupling factors that might depend on $\mathbf{k}$ (and be complex). So I considered showing a bound for all
\begin{equation}
\begin{aligned}
	\hat A^{-(q)}_{n,l} \psi_{n}(
		\mathbf{k}_1, \ldots, \widehat{\mathbf{k}_{l\,}}, \ldots, \mathbf{k}_{n-1};
		\mathbf{p}
	) =
		\frac{1}{\sqrt{n}} \int
		\frac{1}{(\mathbf{k}_l^2 + 1)^{2q} \sqrt{|\mathbf{k}_{l}|}}
		\psi_{n}(
			\mathbf{k}_1, \ldots, \mathbf{k}_{n-1}; \mathbf{p} - \mathbf{k}_l
		)
		\,d\mathbf{k}_{l}
\end{aligned}
\end{equation}
instead. But this then limits $\phi$ in the SA argument, as I've just realized. But luckily, I just had the realization, that my argument using that $\chi_n$ is either non-negative or non-positive everywhere might actually very well be generalizable after all. In fact, I'm pretty sure that it is. For you can just redefine $E_n$ such that it only includes a volume where the phase of the coupling factor, call it $\alpha(\mathbf{k})$, is almost constant. And then you just make sure that the deviation from this constant decreases exponentially (or more) as $n$ increases, and Bob's your uncle. So I \emph{will} therefore just use my argument, that uses that $\chi_n$ is either non-negative or non-positive everywhere, as was the plan beforehand. (12:20)

(16:54) Oh no, you don't even need to make the deviation decrease, actually. You can just define an $E_n$ that spans a solid angle where $\alpha(\mathbf{k})$ does not vary too much, and then you can just make sure that the deviation to both sides (of the phase factor that you're going for) cancel each other, pretty much.






\section{Perpetual motion machines and GR}

(18.12.23, 10:08) For some reason, I just thought a bit about the 2nd law and all that, and last night in bed, I recalled an old idea of mine about letting two objects exchange heat (``vekselvirke'' in Danish) via heat radiation where one object is high up enough that it experiences a time dilation due to GR. I just looked at my notes from '17 this morning and recalled that if we filter out all radiation except some which has an angle that it close to the radial vector (between the heavy object, e.g.\ a planet, and the outer object, and also the inner object, since we can assume that they are in line), then the light beams should not be distorted to 1st order (only to higher orders) of the beam angle. And I have several times before calculated that the Planck curve when red/blue-shifted does not turn into a new Planck curve, as the amplitudes are wrong. When distant galaxies are red-shifted, the beams are also expanded in the transverse directions on velocity vector, which means that the Planck curves \emph{does} turn into other Planck curves. But without that transverse distortion, this does not happen, and as I said, I don't believe that the light will be distorted this way between two object like I have described here (one could be on the ground and the other could be on a theoretically high and solid tower on one of the poles of the planet (or the planet could have zero rotation)).

Now, I have abandoned thinking about perpetual motion machines (PMMs) in the past, since one can quite easily (it seems) argue from quantum mechanics that they cannot exist: For any quantum mechanical system that we might enclose in a large (perhaps extremely so) container, then having a PMM that, say, sorts the gas molecules of a giant box into one partition of that box, without changing the temperature (as a PMM of the 2nd kind would be able to), would lead to a contradiction: Even if we take a many-world interpretation of QM, if the box is large enough, this situation will always result in us having less entropy than what we started with, giving that the box was an ensemble (in thermodynamic equilibrium) to begin with. So PMMs an QM does not mix! (And maybe one can make a similar argument for classical physics as well, but I'm not sure that it is quite as easy (for how do you really define the combinations of possible states in a meaningful way to achieve this, even if you simulate the system via a discretization?\,.).)

But of course, we don't know for sure that GR can be made into a quantum mechanical theory. And, I've just realized, if we consider a mixed theory where we have QM on a curved space-time, then I don't know that you can necessarily use the same arguments.

This was also my conclusion back then, except that I maybe didn't think about the last part here about QM on a curved space-time maybe does not lead to a situation, where you can describe the system in terms of energy eigenstates.

.\,.\,Of course, the 2nd law is one of the most---I would personally even say \emph{the} most---fundamental symmetries of nature, since it only relies on very light assumptions, namely that the laws of physics can be reverted so that time can run backwards (and with the second law, it does not matter that the physical laws might be time-dependent, as opposed to the 1st law (of energy conservation); as long as you can just reverse the time-dependence of those laws). In other words, as long as information about the universe isn't destroyed, then you have the 2nd law. But QM does give a potential for information destruction if we look at the standard interpretation where measurements causes wave function collapses.\,. Now, this normally does not ruin/hinder the 2nd law, as per the argument that I just gave. But when we include GR.\,. Who says that the combination of GR and QM could not lead to irreversible physics in a way where the 2nd law becomes broken?\,.\,. (10:49)

When I abandoned PMMs, I probably did so partly because I believed (and probably still believe) in a theory of everything that is quantum mechanical. And even though I'm sure that I also considered the potential for GR not.\,. Wait a minute, now I just recall that I actually kind of believed back then (and maybe I actually do still, come to think of it) that the union between QM and GR would include a true, physical measurement process where states with enough decoherence would undergo a wave function collapse such that only of of those states goes forward in time from there. So why did I abandon these thoughts?\,.\,. Maybe just because I had better and more interesting things to do at the time (.\,.\,reinventing QED\ldots), but whatever.\,.

Now I'm actually a little bit excited about this topic once again. .\,.\,It would unfortunately probably take a while for us to confirm experimentally that this kind of PMM that i described above (with the two objects, etc.) \emph{might} have a potential (never mind experiments to \emph{confirm} it). Oh, and of course, I might also simply have overlooked something or be wrong in my calculations. But imagine if we could someday make an experiment that would conf.\,. or rather which would \emph{suggest} that such PMMs might be possible.\,.\,! (Oh, and I should mention, that the PMM part then comes from shifting between filtering out different parts of the spectrum, such that the two objects will reach different temperature differences when they are in equilibrium, given the current filter (and the point is that this temperature difference is dependent on the filter).)
It would then give us some very important clues about how we can expect QM and GR to mix, and more precisely it will give of some clues about how they probably does \emph{not} mix. (11:04)

%(11:05) Let me also just mention a thouhgt out here in the source comments: Imagine if the mix between QM and GR would be non-linear. Then we could in theory (at least until some other aspects of that theory is discovered which might ruin this) make QM--GR computers that can effectively just kill all computations which does not yield an interesting result, and then P would be equal to NP. Now, that kind of computer would probably be enormously difficult to make, and would not be worth the effort. And to begin with, I really don't believe that we will find non-linearity. But it is an interesting and funny the thought, still. And given how much people are excited by things like warp drives and tacheons, etc., I'm sure a lot of people would find these thoughts interesting as well. (11:11)


%(19.12.23, 13:46) Hm, I'm trying to calculate on the problem today, but I get a bit stuck. So it's very much possible that I was simply incorrect and that there is a lensing effect that is proportional to the time dialation. I'll calculate some more on it, though.. By the way, my vacuum paper was released on arXiv today! Unfortunately, the moderator(s) "degraded" it to "general physics," so they must not think much of it. (And weirdly enough they didn't include the CC BY 4.0 licence! ..Oh well..) So yeah, that's not good, but I hope somebody will read it and get the point. ..Or else I kinda rely on my SA paper to break through..

%(14:43) Okay, har lige læst mig frem til, at et sort huls radius ser \sqrt{3} gange større ud end foton-sfæren. Og time dialation ved foton-sfæren er også \sqrt{3}, så hvis det indre sorte legme erstattes med en sfære lige over foton-sfæren, så passer pengene altså; man vil få en samlet intensitetskurve der passer med en Planck-kurve. Kan være, jeg bare holder her, men det kan også være, at jeg lige vil tænke lidt mere over, hvis man nu kun ser på stråler, der er tættere på den radiale akse..
%..Hm vent, hvad hvis man sætter den indre sfære lige under foton-sfæren?.. (14:50) ..Så vil intensiteten stadig stige, for så vil keglen, hvormed man kan se ud, blive mindre..
%..Hm, det kan være, at jeg bare holder for nu med dette problem, for jeg regner ikke med, at det vil lede mig nogen vegne, eller rettere lede mig til andet end, at EM-idéen ikke holder. For som sagt, den 2. lov er \emph{virkeligt} bare en fundamental lov; der skal nogle meget særlige omstændigheder til for at et univers kan bryde den.

(19.12.23) I've looked some more at the problem, and I can't reproduce my result, so it's probably nothing after all: The 2nd law most likely holds.

%(15:03) Wait a minute!.. If the total magnification is \sqrt 3, but the surface that is visible is far greater than just the semi-sphere (and you will even see the same surface repeated several times at the edge of the \sqrt 3 \times r_\text{photon sphere} radius circle), will that not mean that the magnification at each local point is \emph{less} than \sqrt{3}!?

\ldots (15:10) Okay, now I actually just had a thought---see the source comments---that has reignited my interest quite a bit.

%...(15:42) Oh, but this isn't enough.. My argument here is not complete. (So it probably doens't work..) ..Yeah, that argument does not work, but let me just think about it a bit more..

\ldots (15:57) Yeah, no, it probably doesn't work.\,.

%(16:01) Oh, now I recalled how I probably want to do the calculation (the way I did back then in '17 (..or at least at some point)), so let me just do that.. ..Hm, or maybe it's not that simple, let me see..

(16:49) Oh, I actually just managed to (finally!) do the intensity factor calculation, and once again found that it ought to be 1 (intensity is the same as in flat space) when the beams are very close to following the radial axis.\,. (.\,.\,!\,\ldots)


%(20.12.23) Instead of a tower, you could also have a Dyson sphere around a dead star, and then put the parabola on the sphere, together with the reservoir, the temperature of which you want to change, and the people and/or machines who needs the energy from the PMM.


\subsection{Considerations about the 2nd law in general}

(20.12.23, 16:16) I was very excited earlier, out on a walk, since I kinda reached the conclusion that classical physics, and GR in particular might break the second law. But then I've had some other ideas since then, which makes it much more complicated. Let me recap my thoughts on the walk: I thought about how a classical universe in which some wizards (this is a toy universe) also magically have the ability to shrink *(and also enlarge) objects at will actually seem to break the second law, without breaking energy and momentum conservation (if all velocities are preserved after shrinking). The problem is that in classical physics, one has to actually discretize before entropy can be truly defined. But then when you shrink something, assuming that the discretization is in position space, you will lose information, since $2^3$ points will map into the same point, i.e.\ if you shrink to half the size. Now, GR is exactly a theory where space is not flat, so maybe there is potential here, that the 2nd law might not apply. Then I thought more about it, and thought about how photons.\,. Oh, wait a minute, now I just realize that the fact that photons gather when they approach a heavy object is exactly what might \emph{prevent} my PMM idea of working (though it doesn't seem to do that), not \emph{make} it work. Ah well, but anyway, that was my thought: That photons can gather in curved space-time, and that was what made me quite excited. Although I soon had the thought then: Well, why not just make the discretization space-dependant, and in particular make it finer the closer you get to the heavy object?\,. So that idea ruined that argument already, it seems (and now it is further ruined since my PMM does not require this gathering). Then on my way home, last leg, I thought about simulations. Now, the second law should also apply for a simulated universe, as long as information is not erased, and as long as the system is discretized, at least if it is classical. Of course my PMM relies on Planck curves, which can only be explained by QM, so how to construct such a universe/simulation? Well, after I have gotten home, I thought about what would happen, if you put two quantum systems together and then simply coupled them via the photons, making it so that photons changes wavelength when they cross the boundary between the two systems.

And that's where I'm at now, and this idea is actually quite intriguing. You should be able to simulate a whole.\,. space station, let's say, in a quantum system without GR. And they way to make photons shift there wavelength is just.\,. Oh wait a minute.\,. .\,.\,Oh, wait maybe it is nt so easy, for if the photons changes energy  when they cross the boundary, they can't be form standing waves which are energy eigenstates.\,. So what would one do to realize this universe.\,.\,? .\,.\,Hm okay, maybe that's not so easy.\,. (16:41) \ldots (16:54) Oh wait, how about you just slow down time for the other system across the boundary?\,\texttt{:D} That should work, and that also even fits the GR situation of my PMM idea better.\,:) .\,.\,So you simply just reduce all energies and interactions with a factor in the system that models what we would have far away from the heavy object. This i a fully quantum mechanical system, so the second law cannot be broken. Yet it really seems like one would be able to make PMMs of the second kind (via the red/blueshift of thermal radiation across the boundary that connects the two partitions of the system). So what on earth is going on here??\,\texttt{:D} (16:59)

(17:22) Oh wait! Maybe I have just simply forgotten to factor the time dilation into the intensity calculation.\,.\,! .\,.\,Right, that must be it, but this then begs the question, what about my earlier idea (maybe only detailed in the source comments) of having an inner Dyson sphere just outside of the photon sphere of a black hole? And given that this idea \emph{does} rely on the photons gathering in space.\,. could this not potentially result in a GR--thermal radiation PMM?\,.\,. (17:27)

Anyway, great that I caught that realization. The intensity should also increase.\,. Oh, and come to think of it, I did probably also realize that back then in 2017---I was wondering about why those notes was from so early, 'cause I'm pretty sure last time I thought about the subject was later than that perhaps even as late as the beginning of '19. .\,.\,Yeah, that makes sense, so the increased intensity is what makes it add up (making it not a PMM).

But I'm still a bit intrigued by that photon sphere version of the idea.\,.

.\,.\,(17:38) Hold on a minute.\,. The intensity should only increase by a factor of the time dilation, but when space is expanding, the intensity should increase (or decrease rather) by that factor squared, right.\,.\,? .\,.\,(17:45) And you do need the intensity to increase as the square of the time dilation to get a right Planck curve.\,. .\,.\,(Well, it needs to be increased by the cube, rather, but the increase in frequency already takes care of increasing it by one factor.) .\,.\,Oh wait, the Planck curve is intensity \emph{per} frequency, of course (as it is a distribution), so what does that mean, that expanded light shouldn't redshift into Planck curves?? .\,.\,(17:54) Hold on further; doesn't this fact just cancel the.\,. .\,.\,the increase in frequency do to the energy increase in the blueshifted light.\,.\,? .\,.\,(18:02) Ah, the intensity then needs to be increased---or decreased in the case of redshift by expending space---by the frequency shift factor cubed, compared to if one simply turned all photons more red/blue magically. But come to think of it, this is also exactly what will happen when space expands, as each spacial dimension that is stretched will result in a decreased intensity, also the longitudinal one.

But then I'm back to the puzzle of why my PMM, either in the GR form or in the simulated form, doesn't break the second law.\,?

(19:15) Oh, I actually think I eye the solution for both cases\ldots

(21:35) Ah, nu ved jeg det måske. Jeg skal sikkert også bare tage højde for, at afstande er komprimerede i $dr$-retningen. Og så tror jeg hermed, at jeg har en løsning på begge problemer. Skriver videre om det i morgen.

(21.12.23, 9:47) In terms of the simulation that I thought of, one would also need to shrink the dimensions of the faster-moving system (with faster time) compared to the other in order for the photons to get a new wavelength when they cross the boundary. And then the situation is just similar to what happens when light crosses an interface to a volume of glass or water (etc.). So no PMM after all there.

And in terms of the GR PMM, I have most likely simply overlooked that the space is compressed.\,. or rather stretched, actually, in the radial direction when seen from the observer close to the heavy object compared to an outside observer. That means that the solid angle of a beam that is close to perpendicular to the surface (i.e.\ close to the radial direction) will seem smaller, and the incoming light will thus seem to come from a smaller solid angle when looking out. This increases the intensity, and by exactly the right factor (squared), it also seems. So no PMM in this case after all either.

I can thus put this problem and these thoughts to bed. %(which actually feels nice..)



\section{Quantized, discretized GR}

I actually kinda want to look into if one really can't make a pretty simple discretized theory that models GR, namely by taking a lattice of Cartesian coordinates as the parameter space, where the distances to the adjacent points (from which the spacial metric can be derived) as part of the parameters for each point, and then also the time factor.\,. Oh, this is where I remember, that GR is second order in time.\,. But it does have some gauge invariance, but I'm not sure that this invariance is strong enough that one can eliminate the second time derivative.\,. .\,.\,Is it even worth for me to look into, then, at this point?\,.\,. (9:28, 22.12.23) .\,.\,Hm, let me read up on the gauge symmetry of GR first.\,.

(28.12.23) I've thought a little bit about this topic, but I'm not going to continue with it. One could of course try to make the first time derivative a separate parameter, and then try to make a Hamiltonian where the actual time derivative always follows this artificial parameter. But then I've thought a little bit about today: Couldn't you try to follow the exact same approach as how I derived the theory of QED in my QED paper? If there is a good Lagrangian formulation of the full dynamics, i.e.\ including matter, then it should be possible, except\ldots (\emph{pause for effect}) \ldots that second time derivative.\,. But one could maybe (not that I'm going to do it now) look into if one could make some magic happen and find a kind of Hamiltonian which can yield second time derivatives in the Lagrangian formulation, after you make the/some Gaussian integral.\,. So yeah, one could look into some of all this at some point.\,. (12:39)



\section{What on Earth?}

(03.02.24, 15:24) What on Earth?? I've just read a bit about QFT today, and I had the realization, that the argument of Weinberg, Section 8.5, only works to show that you can replace the `real' photon propagator with the Feynman photon propagator for all diagrams with a \emph{single} photon exchange! But for all other diagrams, e.g.\ with two photons that are exchanged between two fermions, you still have to use the original, `real' propagator! .\,.\,?? (15:28)

Does this not mean that I actually have a correction to conventional QED after all (in terms of how calculations are done in practice, at least)?? It kinda seems so.\,.\,!\,! (15:29)

\ldots (15:45) Well, I guess it's more correct to say that only \emph{one} photon propagator should be the Feynman propagator for any particular diagram, and the rest should be the ``real'' propagators.

(04.02.24, 16.36) Ah no, I think not. I don't think that there is an error there after all. The key is that one get that delta function *(at the top of p.\ 355) in time, which means that the full Coulomb interaction comes from having instantaneous photon exchanges between any two given fermions.


\section[More about my H\_{QED} versus the conventional one]{More about my $H_{QED}$ versus the conventional one}

(20.02.24) I just had an interesting thought. What happens with the Coulomb interaction pair production in the conventional theory?(!\,.\,.) (9:53) .\,.\,Hm, surely it is also there somehow.\,. (I'm talking about the diagrams, if you use the `real' photon propagator rather than the (Lorentz-covariant) Feynman one, where one particle comes in and three particles goes out.) .\,.

\ldots (10:24) Hm, I'm sure it is the same.\,. .\,.\,i.e.\ that the two different version of the path integrals (with the Feynman or the ``real'' (this name sounds presumptuous, but I don't know what else to call it) propagator) gives the same outcome in the end (even if the individual diagrams are completely different; but when you sum them together, it probably gives the same).\,. .\,.\,But it could be interesting to see that calculation/proof.\,. .\,.(Oh, of course there could be something about divergences, where my version for instance might converge and the other not. But I'd even bet that if one can be handled, the other can as well.\,.)
 

(29.02.24) I can actually see now how one must get the same path integrals after the Dirac sea reinterpretation (only with arrows turned around). (16:27)







\section{Decoupled vacuum}

*[(16.02.24)]
I'm actually intending of writing a second (supplementary) paper on how the vacuum decouples from the physical states. And I just went for good walk where I got a possibly very good new idea. This idea is to make the discetization such that the parameter space of $\mathbf{H}_\mathrm{vac}$ is descretized with one $\delta k$, and then the full $\mathbf{H}$ is discretized by having one more momentum parameter that is discretized with another $\delta k$, call it $\delta k_2$, and call the first one $\delta k_1$. The grand idea is then to send $\delta k_2$ to zero first, before sending $\delta k_1$ to zero! I actually think that we then might not even need $\hat H_{\mathrm{vac}}'''$ to approach a self-adjoint limit after all..!!.. (16.02.24, 14:08) .\,.\,This is really fantastic if it holds.\,.\,!\,.\,.

(15:35) Ah, but maybe there is a problem in the fact that $\mathbf{H}_\mathrm{vac}$ is only meaningful after the Dirac sea reinterpretation (DSR), and that this special discretization probably cannot be done before the DSR.\,. .\,.\,(15:42) Hm, but as long as the (anti-)commutation relations are preserved\ldots

\ldots (15:50) Hm, maybe this idea just doesn't really work. Let me therefore consider the other idea, which I had earlier on that same walk\ldots

\ldots (16:14) Trotter.\,.(?)\ldots\ \ldots (16:29) Or Theorem 16.15 in Hall and the definition above it.\,. \ldots Or a mix, rather.\,. .\,.\,No, maybe just Trotter, actually.\,. (16:54) .\,.\,(16:57) No, I am probably on the right track wanting to utilize the identity above Theorem 16.15 for bounded operators (which is not hard to derive, I'm pretty sure).\,. .\,.\,(17:07) Or maybe this Trotter idea is actually no good after all.\,.

\ldots (17:28) Åh, nu har jeg det måske, lad mig se.\,. .\,.\,Yes.\,. The idea is to simply discretize the formula of $\hat H$ first, and then discretize the ladder operators afterwards, but irrespective of the formula discretization: The discretization of the ladder (creation and annihilation) operators will have a $\delta k$ that can be send to 0 while keeping the formula discretization (which can also be done with another $\delta k$ (call it $\delta k_1$)) constant. I think this solves my trouble.\,. (17:37) .\,.\,(Oh, and it would btw be easy to update my first vacuum paper with this solution.\,.) .\,.\,Hm, but I'm pretty sure I've been over this idea before, namely when I worked on the first paper, so.\,. hopefully I just overlooked its potential and went away from it for some unjustified reason back then.\,. (17:45)


%\ 

(21.02.24, 9:03) I believe I have it now. So the first step is to discretize the formulas (rounding everywhere to the nearest point in the $\mathbb{K}$ lattice), which then allows us to rewrite in terms of the discretized ladder operators for free. The UV cutoff is by the way a part of the starting condition. I will then do the Dirac sea reinterpretation (DSR) and refer to an appendix where I show that the Lorentz covariance is preserved. This is done by Dyson expansion (treating all of $\hat H'$ as interaction terms), which is possible since $\hat H'$ is bounded. (And I'll derive this expansion in the appendix, using a similar procedure as the standard oe for deriving the path integral formulation in physics literature). I will then argue that $\hat S(\Lambda, a)$ can be resolved in terms of ladder operators (creation--annihilation operators, rather). Then I can argue that if the circuit (i.e.\ $\hat S(...) \hat U(t') \hat S^{-1}(...) \hat T(a) \hat U(t)^{-1}$, where $\hat T(a)$ is a translation) works before the DSR, when $\delta k_1, \delta k_2 \to 0$, then it must also work for the DSR case, only where you then define $\hat S$ by simply flipping the daggers on the $\hat b_2$'s as well. So far so good. Then.\,. Oh, and let me mention that yesterday, I got worried about the $\sqrt{n}$ or $\sqrt{n-1}$-factors for the ladder operators (because I very recently heard a professor worry about them), but of course, these doesn't matter unless we're in a cavity: If we are in an unbounded volume of space, then two photons will never transition to the same mode (of course). So the UV cutoff is enough to ensure that the operator is bounded.

Now, the last piece of the puzzle is about that $\phi$ state: How nice it would be if that was an actual eigenstate. And this morning, before getting out of bed, I finally finished solving this part. The solution is to simply use the Pauli exclusion principle.(!) This means that for the vacuum, when there are no `physical particles,' there are a \emph{finite} number of states that span the space of all states that can be reached (from the bare vacuum, i.e.). (Recall that we also have the initial UV cutoff.) So therefore, the system is governed by an operator that works an the same way as some Hermitian matrix, which is exactly what we hoped for! So this system will have a finite, yet spanning set of \emph{actual} eigenvectors. And then the rest is just to use this to go through a similar process as i my first vacuum paper, only where I don't argue via $\Delta - d/2$, or whatever, this time, but instead try to do this calculation more directly and explicitly. (9:33, 21.02.24)

(17:50) Hm, I have made a mistake; I don't think my double discretization technique works after all. And I probably don't get around needing $\delta k^{3/2}\hat H_\mathrm{I, vac}''$ to have a self-adjoint continuum limit.\,. .\,.\,Luckily it's bounded and symmetric.\,.
.\,.\,Hm, or maybe my double discretization is actually a good idea after all, let me think.\,. (17:57) .\,.\,(17:59) Hm, I just thought of a funky Fock-like Hilbert space, where there's ``almost only the $\mathbf{p}$ parameters''.\,.
.\,.\,Hm, this is actually really nice; I think I'm on to something good.\,. Now, what about that $\mathbb{K}_1$ discretization?\,.\,. .\,.\,Yeah well, that should then exactly make $\hat H_\mathrm{I, vac}'''$ discrete, right?\,.\,. .\,.\,Yeah, I think so.\,.\,!\,.\,.

(21:31) Hm no, it's more complicated.\,. .\,.\,Yeah, my double discretization (or `$\mathbb{K}_1$ discretization) probably doesn't work after all.\,. (21:32)

(21:50) Ah. One must be able to show a uniform upper bound (times $\delta k^{2/3}$) for the `interaction terms' for all possible $\phi$ states at once. And then I actually don't need (I think) anything other than to have either that $\hat H_\mathrm{I, vac}''$ has true eigenvectors or that $\hat H_\mathrm{I, vac}'''$ is self-adjoint.\,.


(22.02.24, 10:07) Oh no, there's a thing I've overlooked until now.\,! Maybe the $\varepsilon$-almost eigenstates of the $\hat H_\mathrm{vac}'''$ operator (for this toy theory and/or for QED) approach non-normalizable states when $\varepsilon\to 0$ in a way where the norms of the $\psi_n$'s takes a longer and longer time to decrease. So maybe the expectation value of the particle number operator grows to infinity when $\varepsilon\to 0$.\,.\,:(\,.\,. 
\ldots Well, that does make sense.\,. I'm glad I've realized this; I feel like I understand the situation better now.\,. (10:21)

(10:27) Okay, so now the only way seems to show some upper bound (times $\delta k^{2/3}$) for the `interaction terms,' as I mentioned that I thought (well, believed, but now I'm not completely sure) was possible yesterday.
.\,.\,Wait, and this isn't just trivial when the operator is bounded?\,.\,.
%
%By the way, let me mention here in the comments, that I thought about the fact that my infrared cutoff for the last sections of my SA paper i actually redundant. So I thought about whether I should correct it, but then I realized that I probably also knew that back when I introduced it, and that it probably is still actually simpler to keep it in, at least as long as I don't know whatever theorem in literature tells us that \hat A^- is bounded when given a(n) UV cutoff only.
%
\ldots Hm, 'cause $\hat H_\mathrm{phys}''$ is still bounded, right?\,.\,. (10:41) .\,.\,(i.e.\ for the DSR case.\,.)

\ldots (11:31) Oh wait, maybe my double discretization is still the key!.\,. Let me see.\,. .\,.\,Yes, I believe so!\,.\,. (11:32)


(23.02.24, 12:23) Okay, I think I have it.\,! I think I'm saved by the Baker--Campbell--Hausdorff formula! .\,.\,It's a bit complicated, but I'm pretty sure that one can show, with my double discretization procedure, that if you let $\phi$ be the eigenvector of $\lim_{\delta k_2 \to 0} \delta k_2^{3/2} \hat H_\mathrm{vac}''$, then $\exp(-i\hat H''t) \phi$ will have a fermion density that only grows as $\delta k_2^{-3/2}$, not $\delta k_2^{-3}$.\,! So that means that $\ket{\chi} \neq 0$. .\,.\,Which is \emph{so} much work just to show \emph{that}.\,.\,!\,\ldots\ (12:31)

.\,.\,Oh, and I \emph{do} believe that one can show an upper bound for the `interaction terms,' uniformly for all eigenstates $\phi$ for any given $\delta k_2$, and where this bound goes as $\delta k_2^{3/2}$, or something nice like that.\,. (12:35)

\ldots (12:46) Hm, but maybe I can do it without the double discretization (turning $\delta k_2$ back to $\delta k$).\,. \ldots Or can I.\,.\,? .\,.\,Maybe not.\,. (13:00)
\ldots (13:12) Oh wait, let's see, you could still turn $\exp(H_{vac}'' + \hat H_{phys}'')$ into $\prod (\exp(-i\hat H_{vac}'' \Delta t) \exp(-i\hat H_{phys}''\Delta t)) + O(\Delta t / \delta k_2^{3/2})$.\,.
.\,.\,Hm, or $\prod ((1 - i \hat H_{vac}''\Delta t) (1 - i\hat H_{phys}''\Delta t)) + O(\Delta t / \delta k^{3/2})$.\,.
.\,.\,(13:22) Ah, so if you simply start out with a well-behaved fermion density, then your vector will end up in as a sum of two vectors, $\chi_1  + \chi_2$, where $\chi_2$ goes to 0 and becomes arbitrarily small (norm-wise) compared to $\chi_1$ when $\delta k \to 0$, and, importantly, where $\chi_1$ has a well-behaved fermion density as well!\,. (13:25)
.\,.\,Hm, and what exactly can I do with this?\,.\,. (13:28) .\,.\,Hm, I guess the point is to be able to extract $\exp(-i \hat H_{phys} t) \psi$ afterwards.\,. (13:30)
.\,.\,Hm, maybe I should focus on the expression of the RHS of Eq.\ (29) in my 2023 vacuum paper, v1.\,. (13:36) .\,.\,Hov, der er forresten lige et $\Psi$, der skal rettes til et $\Phi$.\,.
.\,.\,Hm, if only the interaction terms decreased more rapidly than $O(\delta k^{3/2})$.\,.
.\,.\,Or wait, is that necessary?\,.\,. (13:52) .\,.\,Oh, maybe it isn't!\,.\,. (13:53)
.\,.\,Yeah, I think I'm good! The contributions from the interaction terms in the Trotter--Lie expansion will go as $\Delta t \delta k^{3/2}$, and since $\Delta t$ is allowed to go as $\delta k^{3/2}$ (due to what can be shown from the Baker--Campbell--Hausdorff formula), they can thus effectively go as $\Delta t^2$. And since there is only $1/\Delta t$ of them, their combined contribution should vanish when $\delta k \to 0$. But let me just think about exactly how the whole argument should be constructed, then.\,. (14:01)
%...(14:11) Nå, det er for godt vejr til at sidde og tænke over dette indenfor, når jeg kan gå ud a tænke over det i stedet.. Så lad mig lige overveje det lidt mere her foran skærmen, og så går jeg ud.. ..(Det handler om, hvordan jeg lige når fra Baker--Campbell--Hausdorff-formularen til at få Trotter--Lie-ekspansionen over på $\prod ((1 - i \hat H_{vac}''\Delta t) (1 - i\hat H_{phys}''\Delta t)) + O(\Delta t / \delta k^{3/2})$-formen..) ..Hm, nå, lad mig gå ud og tænke..
%..(14:19) Hov, det \emph{er} jo faktisk, så simpelt, som det kan være: Når jeg omskriver e^{-iA\Delta t}e^{-iB\Delta t} til (1 - iA\Delta t)(1 - iB\Delta t), så får jeg kun en fejl på.. hm, på O(\Delta t^2( \|A\| + \|B\|)).. ..Ah, men kunne jeg ikke undgå at omdanne e^{-iB\Delta t} til (1 - iB\Delta t), hvis vi siger at B er \hat H_{vac}''?.. (14:23) ..Jo! (14:24) For e^{-iB\Delta t} kan jo skrives som en.. ja, en Taylor-udvidelse (af en exponentielfunktion), or så kan man her bruge, at \hat A_{\psi}^\dagger ikke har nogen komposanter i sig, hvor impulserne summer til 0!.. (14:26) Okay, jeg tror, det kommer til at virke. Lad mig gå en tur (solen er godt nok gået lidt v.. næ, når man taler om den...)... (14:27)

\ldots\ (17:15) As I wrote out in the source code comments, we should actually just keep $\exp(-i\hat H_{vac}'' \Delta t)$ as is in the expansion, instead of rewriting it as $(1 - i\hat H_{vac}'' \Delta t)$. So I'll only rewrite $\exp(-i\hat H_{phys}'' \Delta t)$ that way.

\ldots (18:04) Hm, it will be easiest if I remove all final states from $\hat H_{phys}''$ that has three or more momentum vectors sum to zero, before the Trotter--Lie expansion (since this modified $\hat H_{phys}''$ will approximate the original $\hat H_{phys}''$ arbitrarily well when $\delta k \to 0$).

(18:07) Okay, so I show some upper bounds on $\hat H_{vac}''$, $\hat H_{phys}''$, and $\hat H_{vac}'' + \hat H_{phys}''$ *(and of course $[\hat H_{vac}'', \hat H_{phys}'']$). Then I use the BCH formula to show that

\begin{equation}
	\exp(-i (H_{vac}'' + \hat H_{phys}'') t) \approx
		\prod (\exp(-i\hat H_{vac}'' \Delta t) \exp(-i\hat H_{phys}''\Delta t)),
\end{equation}
where $\Delta t$ is allowed to decrease only as some constant times $\delta k^{3/2}$ for the approximation to become better and better when $\delta k \to 0$. Then I also rewrite $\exp(-i\hat H_{phys}''\Delta t)$ to $(1 - i\hat H_{phys}''\Delta t)$. .\,.\,Oh, and before that I also modify $\hat H_{phys}''$ like I just talked about. .\,.\,Then (or beforehand) I show (or have shown) the upper bound for the `interaction terms'.\,. Hm, and then I'm almost through, but let me think for a minute.\,. (18:15)

\ldots (18:43) Oh, maybe one just modify $\hat H_{phys}''$ like that. I'll look into that.\,.

\ldots (19:07) Oh, I've maybe made a mistake when I used that $\hat H_{vac}''$ commuted with $\hat A_{\psi}^\dagger$. It probably doesn't. And that's probably the root of the thing that I thought seemed fishy about my result.\,.\,:)\,.\,.
.\,.\,(19:14) Yeah, that might indeed by the key to why I need the $\phi(t)$ state to have a well-behaved fermion density (or at least the majority of it) \emph{throughout} the whole line of $\exp(-i\hat H_{vac}'' \Delta t) \exp(-i\hat H_{phys}''\Delta t)$-operations, which is what my intuition also tells me that I need.\,:) The only downside to these news is of course, that it might not be too easy to argue all this, but oh well\ldots\ (19:17)

(21:12) Oh, I think I actually want to let $\Delta t$ go as something like $\delta k^{4/2}$ (or $\delta k^{5/2}$, or $\delta k^{4.5/2}$), and then write $\exp(-i\hat H_{vac}'' \Delta t)$ as $(1 - i\hat H_{vac}'' \Delta t)$ as well.\,:).\,.

(24.02.24, 10:46) Okay, the bound on both the interaction terms, i.e.\ of $\hat H_{vac}''$ interacting (via annihilation) with the physical state and of the physical and vacuum states interacting via $\hat H_{phys}''$, will depend on the particle number of the vacuum state, as I see it. But from my expansion, you can assign a bound on the amplitude for each particle number for the vacuum state, and that should get us through.\,:)

\ldots (11:08) Oh wait, the bounds on $\hat H_{phys}''$ and $\hat H_{vac}''$ might also depend on the particle number, might they not.\,. Let me see.\,. .\,.\,Yeah, of course.\,. .\,.\,Oh, but maybe I don't need the BCH formula after all; maybe my argument works for arbitrarily small $\Delta t$ for the expansion.\,. (11:12) .\,.\,Well, I still need to rewrite $\exp(-i\hat H_{vac}'' \Delta t) \to (1 - i\hat H_{vac}'' \Delta t)$, and similarly with $\hat H_{phys}''$, but maybe\ldots\ (11:14)

(11:59) I think I forgot to square before summing!\,.\,.\,!\,! When I calculated how big the discrepancy of $\hat H_{vac}'' \phi$ and $E_\phi \phi$ was, I think I forgot to square the amplitudes before summing them! So now it seems to me (I have to go over it a bit more, though) that $\phi$ (i.e.\ an eigenvector of $\lim_{\delta k_2 \to 0} \delta k_2^{3/2} \hat H_\mathrm{vac}''$, where I'm back to using the double discretization) will be an $\varepsilon$-almost eigenvector of $\hat H_\mathrm{vac}''$ where $\varepsilon$ will go as $O(\delta k^3)$ (or maybe $O(\delta k^{3/2})$.\,.)! But let me go over it some more to be sure\ldots\ (12:06) 

.\,.\,(12:08) And if this is true, then I can just go back to the simple argument of my 2024 vacuum, v1, paper, just where I now also have the double discretization, and where I also take more care with the $\hat H_\mathrm{vac}''$ (annihilation) interaction terms.
%..(12:12) I feel like taking a walk and do the calculations of whether I'm right about this there. So let me do that...

\ldots\ (15:30) Damn.\,.\,! So close.\,! I even caught my mistake about the operator not being bounded due to the (bosonic nature of the) photons and found a way to deal with it.\,!\,.\,. But unfortunately, I've overlooked (for this latest solution idea) the fact that I also need $\hat H_\mathrm{phys}''$ to work on $\phi$. So now I guess I need to try to go back to my perhaps-single discretization idea with perhaps my BCH formula argument, or something like it.\,. So let me think about that.\,.

(17:38) I just got an idea.\,. Even if a photon-number-cutoff version of $\hat H''$ might not approximate $\hat H''$ uniformly for all input states at once, it might still approximate any finite set arbitrarily well, i.e.\ when the maximum photon number is sent towards infinity. .\,. .\,.\,And the way to show/use this could be to use a Trotter expansion.\,.

(18:49) Okay, I think I can solve it with my double discretization solution. But I'll have to \emph{assume} self-adjointness, because the starting $\hat H$ is not bounded (and I used to know that, although I've forgotten it this time around). But with that assumption, one can then argue the $\mathbb{K}_1$ discretization still. And one can make the Lorentz covariance circuit argument still. And when we then obtain $\hat H''$, we can then at that point make the.\,. Well, maybe I'll also have to assume $\hat H''$'s self-adjointness, by the way, and when I do, then I can argue that for any finite set of initial vectors, $\exp(-i\hat H''t)$ can be approximated arbitrarily well by a photon-number-cutoff version. And that photon-number cutoff version is, as far as I can see, equivalent to a Hermitian matrix, which is.\,. Hm, two seconds.\,. (18:55) .\,.\,Oh no, but if we take an vacuum eigenstate $\phi$ (or $\phi_{\delta k}$ if you will), then.\,. well, then that state will exist, first of all, right?\,.\,. .\,.\,Yeah, but more specifically, $\phi$ is an eigenvector of an idealized version of $\hat H''$, where we assume.\,. that no three momentum vectors sum to 0.\,. Something like that.\,. .\,.\,Rather its the eigenstate of a modified $\hat H''$ expressed in terms of some.\,. hm, some special ladder operators, except\ldots\ Well, I have to look more into this.\,. But if I'll get through all this, then the rest should be pretty simple from there.\,. (19:04)
.\,.\,Yeah, I think I know what to do, and I think it'll work.\,. (19:09)

(21:58) By the way, the `idealized version' is actually when we let $\delta k_2 = \delta k_1$, except in regards to the factors in front of the operators, I guess. But I'll look more into all this tomorrow.

(25.02.24, 11:11) Oh, couldn't you let $\phi(t) = \exp(-i \hat H_{phys}'' t) \phi(0)$?! .\,. .\,.\,No.\,.

(11:55) Okay, I've actually had a very productive morning/noon in terms of thinking. So here's the plan I have in mind now. I need to assume self-adjointness of $\hat H$, and also assume that it can be approximated to arbitrary precision by a photon-number-cutoff $\hat H$ for any finite set of initial states. With this, one must be able to argue that $\exp(-i\hat H t)$ for any $t$ will converge when $n_\mathrm{max} \to \infty$ such that when writing it in terms of a linear combination of ladder operator terms, the factors for each individual term will converge. So when you make the DSR, you get a bounded operator (whose bound depends on $\delta k_2$, though) where if you make a Trotter--Lie expansion of that operator, and write it in terms of a linear combination of ladder operator terms, the factors will also converge when you let $n_\mathrm{max} \to \infty$ for the DSR theory. (12:04) *[(12:55) Oh, and the you of course can't commute the $\hat a$ and $\hat a^\dagger$ operators in the same way, but.\,. Yeah, but.\,. Hm.\,.] So with that in mind, the plan is then to consider a fixed $n_\mathrm{max}$ such that the operator is bounded with a bound only depending on $\delta k_2$. Well, and it is only the vacuum part of the operator, i.e.\ $\hat H_\mathrm{vac}''$, whose bound goes as $\delta k_2^{-3/2}$; $\hat H_\mathrm{phys}''$'s bound should be constant for a fixed.\,. Oh wait, not quite.\,. Hm.\,. (12:08) .\,.\,Well, before putting my brain to work again, let me just mention, that the plan was to then figure out a way to turn the Trotter expansion into a more Dyson-like expansion, namely where one gathers the ``interactions'' (where everything, including the free energy, is then treated as an ``interaction''). And if one can show an upper bound for the contribution for each order of $\Delta t$, then one might be able to.\,. refine/bolster the bound of the operators through the expanded line of $(1 - i\hat H \Delta t)$-operators, so to speak.\,. (12:12)

(Oh, and I'm by the way not very confident anymore about my plan from last evening/ night.\,. (12:13)) \ldots (12:28) Well, maybe I do need this plan (with $\mathbb{K}_1$ etc.) after all.\,. .\,.\,Or rather, maybe it won't be too hard to make it work, which would then likely be worth it.\,. \ldots (12:40) Hm no, let me go back to thinking about the Dyson-like expansion.\,.

(12:57) I'll continue here on the inserted comment above about the $n_\mathrm{max} \to \infty$ argument.\,. .\,.\,Hm, can you say that when you commute the operators, the result has to converge (and more specifically each factor in the (reduced) linear combination has to converge)?\,.\,. .\,.\,Well, yes, of course.\,.\,:) (13:03) .\,.\,Oh wait, not `of course:' I forgot that the $\hat a$'s eat the $\hat a^\dagger$'s. Let me think for a moment.\,. (13:05) .\,.\,Oh wait, it doesn't matter, I think. If the circuit works approximately for one photon-number-bound operator, then it also works approximately for its DSR'ed version.\,. (13:13) .\,.\,Yeah.\,.

(13:16) Hm, going back to the Dyson-like expansion, when $\Delta t$ is small enough, the contribution from each order of $\Delta t$ will approximate the exponential Taylor series.\,. .\,.\,No, each order $\hat H''$, rather.\,. .\,.\,Oh wait, why don't I just try to Taylor expand it, rather then to Trotter--Lie/Dyson-expand it?\,.\,! (13:20) .\,.\,Hm, how do I compute/find the convergence speed, or more precisely the error for using a finite sum in place of the infinite one?\,.\,. .\,.\,(13:30) Hm, well I guess that the contributions start to become small when the order, $m$, becomes larger than the upper bound on the operator.\,. .\,.\,No, already when $m!$ becomes larger that said bound.\,. (13:33) .\,.\,which is very good for me.\,. .\,.\,Yeah, very good indeed.\,. (13:35)
.\,.\,Yeah, this seems to solve the problem---no double discretization needed.\,.

Okay, so how exactly to complete the argument with this nice thing in mind.\,.\,? .\,.\,Well, I should be able to just use the Taylor series directly, and then show that the `interaction terms' (both kinds) will become insignificant when $\delta k \to 0$, namely if we start out with $\phi = \ket{\,}$. .\,.\,Yeah.\,. (13:42)

\ldots\ (16:04) I was (more) right about my initial thought that the error starts to become small when the upper bound is comparable to $m$, not $m!$. But I've figured out what to do instead. For fixed photon number bound, there is an upper bound that depends on the fermion number (maxing out when all possible fermion states are occupied in the $\mathbb{K}_2$ lattice). And this upper bound should be proportional to the fermion number (times some function that depends on the \emph{fixed} photon number as well). And if that is the case, then we'll get a $O(b^m m!)$ bound for $\hat H^m$, canceling the $m!$ in the denominator to yield a series that goes as $b^m$. The point is then to simply divide $\exp(-i \hat H'' T)$ up into $\exp(-i \hat H'' t)\cdots \exp(-i \hat H'' t)$, where each $t$ is small enough that.\,. well, that the $t^m b^m$ series becomes convergent. .\,.\,And so far, so good, then how to complete the argument exactly from there?\,.\,. (16:14)
.\,.\,Oh wait, it's just $O(b^1 m!)$.\,. .\,.\,Hm, which somehow makes it a bit worse.\,. .\,.\,Yeah.\,.
\ldots But maybe that idea of looking at a small $t$ (or $\Delta t$) is worth diving more into, still.\,. .\,.\, Hm, well, it doesn't change the problem, but maybe it can still give me a good angle on it.\,. (16:47)

(17:00) Oh no, it is $O(t^m b^m m!)$ indeed.\,. Hm.\,. .\,.\,Right, so we can make the $t^m b^m$ series convergent.\,.

(18:49) Åh!\,.\,. Fik lige en muligvis rigtig god (i hvert fald interessant) idé!\,.\,. *(I just got a very interesting idea.) The idea is to do a different discretization for the different $(\hat a + \hat a^\dagger) \hat b^\dagger_{s'} \hat b_s$ terms, i.e.\ for different values of $s, s'$.\,! .\,.\,Wow, I'm really excited to look more into this idea.\,.\,!\,.\,. (18:53) .\,.\,(And I felt really stuck before that.)

.\,.\,(My first, and current, thought is that I can use it to make $\hat H''$ equivalent to a matrix after all on the subset of states reachable from the bare vacuum, i.e.\ for a constant $\delta k_1$ (so with my double discretization as well).)

(20:14) Wow.\,!\,.\,. I have considered having $\hat H_{phys}$ consisting of momentum-conservation-breaking (at scales lower than $\delta k_1$) terms, and now I've just done some $\Delta/2 + d$ thinking, and\ldots (pause for effect) it seems that $\hat H_{phys}''$ working on a $\hat H_{vac}''$ eigenstate, $\phi$, will result in a vector whose norm goes as $O(\delta k_2^{3/2})$.\,!\,! (20:18) I hope this is true.\,.\,!

(26.02.24, 9:09) It seems to work! I went over all the $\Delta/2 + d$-calculations in the bed this morning (woke a bit early), and it all works out as I've hoped.(!) And I even did it all for the Coulomb interaction as well. The result is that all the interaction terms between $\phi$ and $\psi$ will go as $O(\delta k_2^{3/2})$. And this includes the interaction terms coming from $\hat H_{vac}''$ (which I overlooked in the first version of the paper), which, by the way, one has to remember uses the momentum-conserving variant of the discretization. And $\phi$'s interaction with itself, i.e.\ coming from $\hat H_{phys}''$, also goes as $O(\delta k_2^{3/2})$ (which confirms what I concluded here last evening). And just to repeat, I did this not just for the Dirac interaction, but for all the various Coulomb interaction vertices as well.

A great thing is furthermore that I then don't really need the cutoffs on the initial operator. Well, I do, but only for part about the preservation of the the Lorentz covariance, which I intent to put down in an appendix (and just assume the validity of the DSR prescription for the main body of the paper). (And I can then also keep the appendix that discusses such cutoffs.) So all I need is to assume that the initial---oh, and final---Hamiltonian is (are) self-adjoint. (9:25)


(28.02.24, 9:49) Oh, I think I've been wrong about the thing that was my original motivation for the double discretization, which is that $\hat G_{\mathbf{k}, \mathbf{p}}^\dagger \hat G_{\mathbf{k}, \mathbf{p}}^\dagger = 0$, where $\hat G_{\mathbf{k}, \mathbf{p}}^\dagger$ is what I currently call the operator that creates two fermions and a photon with wave vectors around $\mathbf{p}$, $-\mathbf{p} - \mathbf{k}$, and $\mathbf{k}$, respectively. .\,.\,So what to do now.\,.\,? .\,.\,(9:59) Well, I guess I just need to consider the clean version of $\hat H_{vac}'''$ where the operators are bosonic ladder operators, and then use an assumption of self-adjointness instead.\,.

\ldots (10:25) Hm, I actually know that that operator will be self-adjoint (the clean version), since it is just like the position operator (expressed in terms of ladder operators). But can I work with that?\,.\,. .\,.\,Not really; I think the photon number (expectation value) will increase very rapidly, when $\varepsilon \to 0$ for the almost eigenstate.\,.

.\,.\,Hm, this all of a sudden looks bad; I'm no longer so convinced that my whole vacuum solution idea here will work.\,. But I still don't have a completely bad feeling about it, so I'm still gonna give it my best.\,. (10:35)

.\,.\,(10:39) Oh, there is still the potential to try to utilize a photon number cutoff, where $\delta k$ (aka.\ $\delta k_2$, previously) is allowed to be sent to 0 while keeping said cutoff constant.\,.

.\,.\,(10:46) Hm, is there anyway that I could use $\delta k$ for, say $\hat a$ and $\hat d$ in $\hat G$, but use $M\delta k$ (previously known as $\delta k_1$) for $\hat b$?(.\,!\,.\,.)
.\,.\,Hm, or use what I now call $\hat B$, equivalently.\,.

\ldots Hm, maybe the photon number cutoff makes more sense.\,.

.\,.\,Hm, on the other hand, maybe that $\hat b \to \hat B$ idea is really worth something.\,. (11:05) .\,.\,For it still makes the vacuum particles be confined to a $2\times 3$-dimensional space.\,. .\,.\,Well, or rather each vacuum particle triple is confined to such a space.\,. .\,.\,Hm, let me think.\,. .\,.\,(I also want to think more about the photon number cutoff, by the way, since I actually think that that idea might work. And the reasoning about whether it works or not is also worth going through more thoroughly, regardless.\,.) (11:14)

.\,.\,(11:20) Oh wow, I think that $\hat b \to \hat B$ might work.\,!\,.\,. %\ldots (11:54) Hm, or maybe it does not work.\,.\,? .\,.\,Oh, never mind; it might very well.\,.

\ldots (12:07) Yeah, and I think that the $\hat G_{\mathbf{k}, \mathbf{p}} \hat G_{\ldots}^\dagger \hat G_{\ldots}^\dagger \cdots \hat G_{\ldots}^\dagger \hat G_{\mathbf{k}, \mathbf{p}}^\dagger \hat G_{\ldots}^\dagger \cdots \hat G_{\ldots}^\dagger \ket{\,} \approx$

\noindent
$ \hat G_{\ldots}^\dagger \hat G_{\ldots}^\dagger \cdots \hat G_{\ldots}^\dagger \hat G_{\ldots}^\dagger \cdots \hat G_{\ldots}^\dagger \ket{\,}$
argument still works, as well as the rest of them. So yeah, this new $\hat b \to \hat B$ idea might indeed work.\,.\,!(!) (12:13)

.\,.\,My brain is really incredible at getting ideas.\,. And especially in cases when I seem to have just hit a dead end, then it so often happens, that I get the next awesome idea to get out of that dead end very shortly after.\,!\,:) %..It's a bit frightening in some way, since in some ways, I don't know if I will lose that skill (and/or the potential luck). But it has kept with me for so, so long now, and I guess if I ever lose it, I'll just be greteful for the time that I've had it.. Anyway, back to considering the photon number cutoff and, more precisely, the Lo. cov.-preservation of the DSR process..

.\,.\,About the  photon number cutoff and the Lorentz covariance-preservation of the DSR process, it seems that my argument works best (and perhaps only) if one can show that $\exp(-i\hat H t)$ can be grouped by perturbation theory, such that you can sum up the Taylor series up to some order, $m$ of $t$ (i.e.\ up to the term proportional to $t^m$), and then have that when we lift the cutoffs for that fixed sum up to $m$, then the dynamics converge such that when you \emph{subsequently} let $m \to \infty$, they converge to Lorentz-covariant dynamics. (12:33) .\,. .\,.\,If that applies for the initial Hamiltonian, and if the dynamics also converge for that same process, then you can (I'm pretty sure) argue this way that the DSR'ed Hamiltonian is also Lorentz-covariant.\,. .\,.\,Let me take a walk and think some more on this\ldots 


%(29.02.24, 15:26) Nå, jeg har været lidt igennem en mølle, men nu ser det rigtig godt ud. Det startede med at jeg på min gåtur her i går, lidt i to var den, fik en ret stor åbenbaring. Den handler om, at Dirac-havet faktisk godt kan approksimeres med et endeligt antal partikler (for $\delta k \to 0$).! Det har åbnet mine øjne, så jeg nu har en endnu bedre forståelse af fysikken, og af problemet. Men så ledte det mig så til at indse, at spørgsmålet om hvorvidt vakuum-løsningerne så Lo.-transformerer til andre sådanne løsninger, jo så egentligt er rigtig væsentligt.. Anyway, og i morges i sengen (ikke fordi jeg lå længe; ikke som her den anden dag, hvor jeg vågnede tidligt og lå og regnede; bare hvor jeg lige ligger nogle få minutter) kom jeg så frem til to gode ting, hvor det andet var, at jeg skal huske, at samle tingene i ordner af koblingskonstanten i stedet for f.eks. i ordner af t, og at man ovenikøbet så kan skrue på denne koblingskonstant uden at det ændrer ved Lo.-kovariansen. Nå, men det er nu nok faktisk ikke så vigtigt alligevel. Ude på en gåtur her i dag kom jeg så, lidt før kl. tolv, frem til, at jeg faktisk bare bør kunne tage udgangspunkt i de normale QFT-aksiomer. For nu er jeg nemlig ikke længere i tvivl om, at man ud fra en diskretiseret og DSR'ed Hamilton-operator må nå de konventionelle stiintegraler for QED. Og her antager man bare, at det findes en vakuum-tilstand, som transformerer næsten til sig selv. Og jeg kan altså faktisk gå baglæns fra de stiintegraler (aka. Lagrange-formalismen) og så udlede $\hat H''$ fra dem. Og bum, så kan jeg altså konstruere et skarpt argument derfra. Det ærgrede mig så dog lidt, at jeg ikke selv kunne finde frem til vejen til et bedre Lo.-kov.-argument end det, så jeg gik videre og tænkte videre. Og lidt i et kom jeg så frem til.. eller rettere, der faldt den sidste (for nu; jeg skal så til at gennegå argumentet på tasterne nu her) brik på plads, som var, at man må kunne argumentere for, at $\varepsilon$-almost-vakuum-egenvektorer også må *(Lorentz-)transformere til $\varepsilon'$-almost-vakuum-egenvektorer, hvis bare det også er $\varepsilon$-almost-egenvektorer for impulsen også (i alle tre retninger). Og her må $\varepsilon'$ så i øvrigt gå mod 0, når $\varepsilon$ gør det. Så de næsten-egenvektorer, som jeg udleder, de må altså Lo.-transformere til andre næsten-vakuum-egentilstande. Og de er jo lige netop det, jeg gerne vil vise: at der findes vakuumtilstande, som de fysiske partikler er dekoblede fra, der også transformerer til vakuum-tilstande, som de fysiske partikler også er dokoblede fra. (15:51)
%..Hm ja, og er det ikke bare det..(?) Lad mig se.. ..(15:58) Jo, for man må også kunne komme igennem hele min dekoblings-argumentation, hvis man bare starter med at have en sekvens $\varepsilon'$-almost-vakuum-egenvektorer, hvor man både har at $\epsilon'$ går mod 0, samtidigt med at vektor-sekvensen endda konvergerer til en vektor. (16:01) ..Ja, det tror jeg..!

(29.02.24, 16:02) I had a pretty great revelation yesterday, on a midday walk. I've realized that the one does not need $2\times (2\Lambda)^3/\delta k^3$ antiparticles to fill the Dirac sea.\,! Well, to \emph{fill} it, yes, but if we say only used half of that, then the Dirac sea would still work just as well in the limit when $\delta k \to 0$! I'm proud of this realization, 'cause it's so natural to think of the Pauli exclusion principle as simply meaning that two particles can't occupy the same state, e.g.\ a particular discretized momentum eigenstate. But the antisymmetry of fermion are much stronger than that (which is why it holds in all bases at the same time). And if you took a gigantic number of momentum-eigenstate antiparticles and spread them over all.\,. Hm, or let me put it this way, more simply: If one took a filled-up Dirac sea for some $\delta k$ and $\Lambda$, where the momentum eigenstates those that are created when $\hat a^\dagger(\mathbf{k})$ is integrated over a small ($\delta k^3$, to be precise) volume in momentum space, then this Dirac sea would also lead to approximately the same physics, then you look at the state in the continuum limit theory, where $\delta k \to 0, \Lambda \to \infty$. In other words, are universe could actually be approximated arbitrarily well (given that it is not \emph{actually} constructed this way in reality) by the initial, non-DSR'ed Hamiltonian, where the physical particles then sit on top of a Dirac sea with a finite (but unfathomably big) number of particles in it! This would make the third law not apply in theory, but in practice, it would still apply, since the decay rate to the unfathomably-low-energy states would be close to 0. And even though one would be able to have a seemingly (non-anti-)particle go down and disappear into the sea, the transition rate of this happening would also be (unfathomably) close to 0, due to the high density, and the anti-symmetry, of the approximate Dirac sea. (16:20)

This has made me understand the physics a bit better, and it has lead to some good ideas, and a better understanding of how one would show Lorentz covariance of my $\hat H_\mathrm{phys}''' = \hat H_\mathrm{QED}$. I have briefly described these ideas (in Danish) in the source code comments above the previous paragraph. (16:23)


(17:10) Hm, there is actually something a bit fishy about the last part of my new argument: I was arguing that my $\varepsilon$-almost eigenstates would transform into $\varepsilon'$-almost eigenstates, but that is not necessarily true, is it?\,. And it depends on the discretization, which is also not nice\ldots\ \ldots (17:33) Oh, or maybe does work.\,.

(01.03.24, 12:19) My argument here from yesterday, which the previous paragraph concerns, does not work, no. But I've had some very good ideas today. Overall the ideas are about going back to the photon and particle number cutoff, and then argue about perturbation in the order of the coupling constant. And in the shower this morning, I realized how I could use the axioms of QFT to complete my argument (my strategy for doing this from yesterday was not complete, I think). The point is to start with an axiom that one can calculate to order $m$ in the perturbation series while being free to send $\delta k \to 0$ for a fixed $m$. This is a standard (albeit perhaps somewhat implicit.\,.) axiom of QFT. You can then use this to make a particle number cutoff for the Hamiltonians, namely since this cutoff theory will still produce this exact perturbation procedure, i.e.\ when $n_\mathrm{max}$ is gradually lifted. And there you go, this derives $\hat H''$. Now, this version of $\hat H''$ does not have the double discretization, but on the other hand, it has the particle number cutoff! And I'm almost certain that I can make the argument that the vacuum decouples work with this starting point. This argument will then be very similar to the argument in the first version of my vacuum paper (but remembering the annihilation interaction terms coming from $\hat H_{vac}''$ this time around). The point is that you don't need the eigenvector sequence (i.e.\ of some eigenvectors that are all ``reachable'' from the bare vacuum) to converge when $\delta k \to 0$. For as long as the particle cutoff is constant (which it can be, as I've explained), one can make a uniform bound on the interaction terms, which goes to 0 when $\delta k \to 0$. (12:36)

Now this is all quite fantastic---except that I then don't get to show of any of my double discretization ideas (which are quite cool, and I believe that I just managed to make them work in the latest iteration, described here above). I'll get to write a very convincing second version of my paper, which does not have to change structure all that much (unless I want to change it up.\,. (for maybe I still might want to look at a simplified interaction first.\,.)), but where I just make sure to make this argument about the particle number cutoff derived from the axioms of QFT first. But it still bugged me a lot that I hadn't yet found my own way to argue (well enough) for the Lorentz covariance. However, now I think I'm also on to something quite good there.

The idea in regards to that also concerns a particle number cutoff, as well as gathering in terms of orders of the coupling constant. .\,.\,Consider an experiment under the original, non-DSR'ed Hmiltonian, but where one has filled up all $\delta k$-momentum states (i.e.\ states that are created by integrating $\hat a^\dagger(\mathbf{k})$ over some small volume of momentum space of size $\delta k^3$) of the negative-energy variety up until some $\Lambda$. We are thus dealing with an approximate Dirac sea of finite negative-energy particles. The idea that I've just figured out here a little while ago is to then observe that this approximate Dirac sea falls off in amplitude when you move away from the origin of the coordinate system in \emph{position} space. Then think about a Lorentz transformation in position space. Now if the coupling constant is 0, this transformation will just turn the approximate Dirac sea into another approximate Dirac sea (which is of course great). And then, if we turn up the coupling constant, and think of the path integral in (particle--)position space, where particles propagate to interaction vertices, interact, and then (potentially) propagate on from there, and potentially over to additional interaction vertices. The point is then that, heuristically, it makes sense that we can group this transformation in terms of orders of the coupling constant. For the Dirac sea falls off far away from the origin ($\mathbf x = \mathbf 0$) in position space, so it makes sense that we can approximate the transformation with only a finite number of interaction vertices. Then when we transform these position interaction vertices back into momentum space, we should thus still get a perturbation series of terms up to some $m$ order of the coupling constant. (That was poorly expressed, but you get what I mean, hopefully.) Now, what does that mean for the Lorentz-transformed approximate Dirac sea?\,.\,! It means that we get another approximate Dirac sea, plus a correction coming from only $m$ interactions! And what is a Dirac sea that is worked on my only a finite number of interactions?\,.\,! That is just another approximate Dirac sea, only now with up to $m$ more positive-energy particles and up to $m$ more holes in it! So this is my heuristical explanation of why an approximate Dirac sea with some particles on top must transform into another approximate Dirac sea with some particles on top. Now, you could surely also make a similar argument to argue (heuristically) that an `approximate Dirac sea with some particles on top' must approximate a real Dirac sea with some particles on top when the $\delta k$ and $\Lambda$ parameters for the approximate sea is respectively sent towards 0 and $\infty$. And with all this, and of course with the fact that $\hat H$ becomes Lorentz-covariant in the continuum limit (for any fixed number of (maximum) particles), then we obtain (heuristically) that.\,. well, that if a meaningful limit to $\hat H''$ exists, then it must be Lorentz-covariant. .\,.\,Yeah.\,. (13:12) .\,.\,!\,:) 

%(19:41) Åh, måske har jeg det... ..Hm, well.. Næsten, måske.. ..(19:46) Åh ja, næsten måske..

(02.03.24, 10:11) My heuristical(?) argument above does not hold. The problem is exactly the three-particle creation interaction, and the fact that the particle density.\,. Hm.\,. .\,.\,Yeah, the particle density (in position space) increases as you let $\delta k$ (as a Dirac sea parameter, not a parameter of $\hat H$) go towards 0.

(10:30) Oh, maybe there is a way still for my $\hat G$--$(\hat b \to \hat B)$--etc.\ idea!\,.\,. .\,.\,Oh, if we let $\hat d \to \hat D$ instead, it might actually be natural, even.\,. (I expect to see why this idea doesn't work in a minute, but I'm still somewhat excited.\,.) (10:34) .\,.\,Hm, let me see.\,. .\,.\,No, I bet this idea doesn't hold.\,. .\,.\,Oh, I must mention that the original $\hat G$--$(\hat b \to \hat B)$--etc.\ idea does work, I think, in a sense. Only it does not, perhaps, adequately explain why the Lorentz-transformed bare vacuum doesn't cause us trouble.\,.

(11:47) I took a break before continuing thinking about this new idea, which I didn't think would lead anywhere. But now it actually kinda seems like it might.\,.\,!\,!\,.\,. .\,.\,The idea is to do my $\hat A, \hat B, \hat D, \hat G$-discretization, actually without the turning $\hat b \to \hat B$ (nor $\hat d \to \hat D$, actually) for $\hat G$, and then consider this $\hat H'$ on an approximate Dirac sea.\,. .\,.\,(11:53) Oh wait, maybe the idea does not work, after all.\,. .\,.\,Yeah, nah.\,. .\,.\,Hm, unless maybe one broke momentum conservation to.\,. Hm, to make it work around.\,. No, 'cause you need to be able to go the the continuum limit while keeping the approximate DS constant.\,. (12:00) .\,.\,Ah, but you could still do it, just where $M \delta k$ for $\hat D$ is also allowed to tend towards 0 for the constant approximate DS.\,. (The idea is to actually indeed change $\hat d \to \hat D$ for $\hat G$ after all.\,.) .\,.\,Hm, this \emph{is} a bit interesting.\,. (12:05) .\,.\,Hm, nah, it probably won't work, but let me think about it still.\,.

(12:41) I've not been too motivated to continue thinking about this newest idea, but now I'm actually a bit motivated once again; isn't there actually a chance that it might work.\,.\,?

(13:04) Ah. The $\varepsilon$-almost eigenvectors of this doubly-discretized $\hat H'$ won't converge to $\varepsilon$-almost eigenvectors of $\hat H$. I have been wrong in thinking that.\,.
*(Just to underline: This ruins my whole $\hat A, \hat B, \hat D, \hat G$, etc.\ (double) discretization idea. *(Well, maybe it can work in a lot of instances, but not in this latest instance where there the final states of some transitions converge/diverge into delta functions.\,. (13:53)))

Okay, so I really seem to have come to a stop in my search for.\,. repairing my Lorentz covariance argument.\,. Luckily, it is still a very great result, if I can show that with the conventional axioms of QFT, you can remove the vacuum-perturbing terms/vertices from the calculation. Not only because this might help the renormalization calculation a great deal, it might seem, if one otherwise have to divide with a diverging $\braket{\, | \hat U(t) | \,}$ term, but not least also because this makes us able to write up a well-defined Hamiltonian.

So yeah, I should definitely continue with those ideas.\,. (13:12)

%"Planen er, at man prøver at vise selv-adjungerethed for H efter et basisskift til en tællelig basis (e.g. SHO-tilstande eller sådant). Så får man i første omgang en realistisk teori! Og hvad mere er, når man så går tilbage til mit koblingsargument, set fra impuls-basen, så er delta k allerede infinitesimal til at begynde med!! Så man kan altså muligvis fuldføre mit argument sådan! (15:53, 03.02.24)" skrev jeg i en lille note her på min (ret lange) gåtur, som jeg lige er kommet hjem fra nu her. Og for lidt tid siden her på vej hjem fra den gåtur kom jeg så i tanke om, at almost-egenvektorerne af denne $\hat H'''$ (nu dog uden nogen diskretiserede skridt, \hat H' og \hat H'', imellem \hat H og \hat H''') jo ikke i reglen vil være impuls-egenvektorer (med impuls lagt sammen til 0 præcist), men vil også kun være næsten-egenvektorer af impulsen. Men! Fordi \hat H kommuterer med impulsen, så vil hvert rum af vektorer med sammenlagte imuplser inden for et lille inteval/volumen være invariant under \hat H. Så hvis man har en næsten egenvektor af \hat H, så kan man faktisk sagtens bare ud fra denne konstruere en ny egenvektor, i et vilkårligt lille interval af sammenlagt-impuls-rummet (I ved, hvad jeg mener), nemlig sådan set bare ved at skære resten af "sammenlagt-impuls-rummet" fra. Og dette vil ikke forøge forventingsværdien af partikel-antallet! Og dermed bør man kunne komme igennem argumentet! (17:40)


(03.03.24, 12:16) I've written some great and perhaps very important notes out in the source comments above this paragraph, which I will repeat here in English at some point. But let me see about something first.\,. We have $\braket{|\hat A_{\phi}  \hat H  \hat A_{\psi}^\dagger|}$.\,. .\,.\,Hm, is this the right thing to do.\,.\,? .\,.\,Hm, yeah, and then we should move all $\hat d$'s and $\hat d^\dagger$'s over, right?\,.\,. .\,.\,Sure.\,. %..(Hm, my brain is a quite slow today..)
.\,.\,Right, the whole point is to try to verify that this rewriting is legal for the `physical' part of $\hat H$, and hopefully I will then also see that it is illegal for the `vacuum-perturbing' part of $\hat H$ (and gain insight into why).\,. (12:32) %..If my brain worked a bit better right now, I could have done it in my head, but maybe I should try to do it here on "paper".. ..Or perhaps on some actual paper for once.. (I for some reason very rarely do physics on paper nowadays---haven't done that much at all since the bachelor.. ..Probably only a handful of sheets of papers combined, I think.. *[Ah, probably a little more, perhaps... ..Hm, or maybe not..] ..Not counting study notes from the master's, of course.. ..And not counting designing Hiq..) ..Anyway, I'm stalling.. (12:39) ..No, let me take another break, see if I get my brain back, and then hopefully do it in my head (perhaps on a walk).. (12:44)

\ldots\ (14:21) Okay, so it's actually very simple. One can show that when you DSR the physical Hamiltonian, whether you do it before or after changing to a countable basis (such as e.g.\ the SHO solutions, or the spherical whatevers), it gives you the same result regardless. This also works for the discretized vacuum-perturbing part, but here the DSR'ed $\hat H$ just doesn't have a well-defined formula---at least not one that can be easily derived---in the pure momentum basis, as it diverges when you let $\delta k \to 0$. And that's the simple difference between the two.

Now, let me finally explain what this new idea is. The key part of it, as alluded to, is to change the basis of $\hat H$ to a countable basis before DSR'ing it. %(Funnily enough, this was how my thought on the DSR came to a stop when I worked on my QED paper in 2022. But now, instead of stopping me, the idea of going to a countable basis seems to be what will carry me through to the finish line, 7, 9, 13.)
If then the thusly DSR'ed Hamiltonian can be shown to be self-adjoint, perhaps using a similar technique as the one from my 2023 self-adjointness paper, %..Two seconds... ...(14:54):
then we're pretty golden already, since this provides us with a Lorentz-covariant Hamiltonian. But we can get further still: The `physical part' of the Hamiltonian can still be readily transformed back to the momentum basis, while leaving the `vacuum-perturbing' part in the countable basis. Then we can investigate a state created by $\hat A_{\psi}^\dagger \hat A_{\phi}^\dagger$, just like in my vacuum paper, v1. And because the (DSR'ed) Hamiltonian still is momentum-conserving, we can from any $\varepsilon$-almost vector $\phi$ construct a new $\phi$ with support restricted only to states whose momentum sum to some constant, plus/minus some $\delta \mathbf{p}$. And the big point is that one can make this $\delta \mathbf{p} \to \mathbf 0$ while not changing anything about the expectation value of the particle number (and more: even all orders of the particle number).\,! And then all that is left is simply to calculate the so-called `interaction terms' (this time remembering those coming from the annihilation part of the vacuum-perturbing part of the Hamiltonian, which I neglected in v1), and see that they vanish (which I believe they will).\,! And since $\varepsilon$ can now be made arbitrarily small without changing the fact that $\text{`interaction terms'}=0$, we can thus complete the argument in the same way as in v1 from there. (.\,.\,!) (15:08)

(17:04) Hm, there is also the free energy of the antiparticles, which has the divergence as well (in terms of a constant energy at each step, which however goes to infinity in the continuum limit). So how does one deal with that.\,.\,? .\,.\,Hm, that means that we do still need to discretize as intermediary steps for the DSR.\,. The argument then follows the.\,. wait, no, this does mean that the `interaction terms' are once again not completely vanishing, but are only vanishing when $\delta k$ is sent towards 0.\,. (17:10)
\ldots (17:36) Okay, so we'll just need to also assume that $\hat H''$ (discretized and DSR'ed) in the countable basis approximates $\hat H'''$ (continuous and DSR'ed) arbitrarily well. Then $\phi$ can be an $\varepsilon$-almost vector of $\hat H'''$ and $\hat H''$ at the same time. Therefore we then go to $\hat H''$ and investigate its action on $\hat A_{\psi}^\dagger \hat A_{\phi}^\dagger \ket{}$. .\,.\,And if we let $\psi$ be an almost eigenvector of $\hat H_\mathrm{phys}'''$ and/or $\hat H_\mathrm{phys}''$, we see that, because the same $\delta \mathbf{p} \to \mathbf 0$ argument still applies for $\phi$, the interaction terms can be shown to tend towards 0 when $\delta k$.\,. Hm, or $\delta \mathbf{p}$.\,. Let me think.\,. (17:44)
.\,.\,Oh, maybe there's a problem here, namely if $\phi$ indeed needs to be an almost eigenvector of $\hat H_\mathrm{phys}'''$ and $\hat H_\mathrm{phys}''$ at the same time: Then we can't just let $\delta \mathbf{p} \to \mathbf 0$ without also letting $\delta k \to 0$ at the same time, right?\,.\,. (17:47) .\,.\,Or can we.\,.(?) .\,.\,Hm, but wait, does it really matter that $\delta k$ also needs to go to 0 (before $\delta \mathbf p$).\,.\,? (17:52) .\,.\,All that matters is the particle number converges nicely (this is imprecisely put but I know what I mean.\,.) when $\delta k, \delta \mathbf p \to 0$, right?\,.\,. (17:55) .\,.\,Yeah, I think so.\,.\,:) .\,.\,Yes!\,:) (18:00)

(04.03.24, 14:55) For the $\delta \mathbf p \to 0$ argument, I'll actually use the axiom of choice to construct a $\phi \in \mathbf{H}_{DSR}$, where $\varepsilon$ is non-increasing (perhaps even sharply decreasing) for smaller and smaller $\delta \mathbf p$ volumes around $\mathbf 0$. (And one also uses the completeness of $\mathbf{H}$ in this argument.)

(05.03.24, 10:26) The thing about using the axiom of choice does not work. However, as I realized laying awake (for a few hours) this night, I don't need $\phi$'s combined momentum to be 0 for the argument to work. We can just pick out a $\mathbf p$ ($\mathbf P$) from anywhere (close to 0 or even away from it). We just need for $\phi$ to be finite almost everywhere on the hyperplane that we pick (of constant combined momentum), and also have that the contribution to $\varepsilon$ on that hyperplane is $\geq \varepsilon$.
*[(10:43, 10.03.24) Oh, and about this, we should first of all pick a $\mathbf P$ where $\phi$ is \emph{continuous} at all levels in this hyperplane. And I've found out that the argument for us being able to do this, is simply to use the Lebesgue measure definition, specifically the definition of sets of measure zero, which shows that the discontinuous point of each $\phi_m$ has zero measure. And even if we union all the discontinuous points for each $n$, the resulting set will still have measure zero. *(11:47) Oh, and from at least some of the remaining continuum of $\mathbf P$'s, the contribution to the overall $\varepsilon$ will be $\leq \varepsilon$, such that the cut-out vector around $\mathbf P$, when normalized, has a $\varepsilon' \leq \varepsilon$.]


(10.03.24, 9:50) Okay, it's not quite so easy. I've gotten some ideas since I wrote last, but there's still some trouble. Right now I'm troubled by the fact that the vacuum terms might diverge from the beginning. But I think I might have some ideas how to solve it, which concerns assuming the $\delta k$ can be sent to 0 first for the cutoffs.

But on the bright side, let me mention some ideas about ``Task 2'' (from my QED paper). I was concerned by the fact that even with a $\Lambda$, the upper bound for the interaction for each particle number, $n$, increases as $\sim n$. But first of all, I've had the good idea (in bed last night) to use the free energy to help cancel the ``productions'' (as I have called it) to the last level at the $n_{max}$ cutoff. And this cancellation is also strengthened by the fact that the interaction increases when there are more particles to positively interfere. 'Cause if the are photons already out at the $E$ area, which by the way has constant radii, as explained in an Appendix in my vacuum paper, v1, then these photons will just increase the free energy. (And the photons need to by out there already for positive interference to happen in the first place.\,:)) (10:02) And for the cancellations that needs to happen at the lower levels at the edges of the productions, cut off by $\Lambda$, one can also see that the more particles are there, the more ways to construct ``cancellation tails'' one level up.\,. Hm yeah, and this shuld also matter to the $n_{max}$ level, I guess.\,. .\,.\,A higher particle number increases the upper bound on the interaction, which is a bit troublesome, but on the other hand, it also means that more states can be used to cancel the productions. So it should balance out, I think.\,:) And note that this applies even for the photons, namely since the productions can also be canceled by states one level up where a photon has split off into two fermions (due to the ``Dirac interaction,'' as I'm calling it.\,.).\,:) (10:08)

\ldots (10:26) Ah, now I might just have gotten an idea (about the problem with the interaction terms).\,. .\,.\,With the solution that I have in mind for ``Task 2,'' if it works, wouldn't we then also be able to construct an almost eigenvector from any given original almost eigenvector, such that the modified almost eigenvector---with only a slightly larger $\varepsilon$, i.e.---decreases very rapidly after some $n$ (or rather it's norm squared does)? .\,.\,This could be it.\,.\,!\,.\,. (10:31) .\,.\,Yeah, that's it.\,.\,! Oh what a relief---7, 9, 13.\,. .\,.\,So I therefore think it's reasonable for me to assume, for the vacuum paper v2, that we can find $\varepsilon$-almost eigenvectors, $\phi$'s, of $\hat H'''$ with a rapidly decreasing norm squared for each $\phi_n$ after some large enough $n$. (10:36)

.\,.\,Hm, the downside is that this assumption is not just a standard physicist assumption, so it won't be so immediately convincing. But I think I just have to make do with that.\,.
.\,.\,Yeah, that's just life.\,. .\,.\,I'm just glad.\,.\,:) (10:42)

(11:38) Hm, maybe that `$\delta k \to 0$ first' idea is also very much worth pursuing further\ldots .\,.\,Oh yeah, for sure.\,. Then I'll assume that there is a cutoff (both ultraviolet, infrared, and particle-number-wise) but continuous (in momentum space) version of $\hat H'''$, which approximates (the dynamics of) $\hat H'''$ arbitrarily well. And then I should be able to just show that the interaction terms.\,. hm.\,. (11:44) .\,.\,(11:51) Hm, intuitively this line of thinking works: Why should the approximate Lorentz covariance be worse for a low-$\delta P$ state when $\delta k = 0$ than for one with a higher $\delta P$?\,. But mathematically, I'm not so sure about how well it works.\,. (11:53)

(13:09) Oh. If we make this assumption about the w.r.t.-$n$-rapidly decreasing almost eigenvectors of $\hat H'''$, and then go and consider $\hat H'''$ and a $\phi_{\delta P}$ in the position space, then it becomes kinda trivial that the `interaction terms' (combined) goes to 0, doesn't it?\,.\,.\,\texttt{:D} .\,.\,Well, unless they diverge to begin with, let me see.\,. .\,.\,Hm, which the might because of the Coulomb interaction.\,. Hm.\,. (13:15) .\,.\,Oh wait!\,.\,. Maybe I should consider if there could be a method for changing $\hat A_{\psi}^\dagger \hat A_{\phi}^\dagger \ket{\,}$ slightly, such that it is a member of $\mathrm{Dom}(\hat H''')$.\,.\,!\,.\,. (13:18) .\,.\,Well, there obviously is, but I need one that doesn't depend on $\delta P$, then, or something like that.\,.

(16:31) Hm, I might have just had an idea.\,. How about using a $\psi$ with localized/bounded support in position space, and then simply cutting that part of position space away from $\phi$?\,.\,. .\,.\,And the point is that $\phi$ then won't necessarily be an almost eigenvector, but it will be an ``almost almost eigenvector,'' which is `almost as good as an almost eigenvector' in some sense, isn't it.\,. (16:35) .\,.\,Hm, this actually kinda seems like it would work(!)---although it does seem a little bit magical, perhaps, so it requires some more thinking.\,. But I'm pretty positive *(as in hopeful), actually.\,. (16:37)

(16:54) No, it actually makes great sense! And even though the argument that the Dirac interaction vanishes between states that are away from each other in position space it a bit shaky, I don't need that anyway, since it is only the Coulomb interaction that seems to be a potential source of divergence *(and the Coulomb interaction is obviously bounded between particles whose distance between them in position space has a lower bound), i.e.\ once we have already assumed that $\phi$ is rapidly decreasing w.r.t.\ $n$ after some point, as I have talked about. So yeah, this must be my new plan.\,! :) (16:57)

(11.03.24, 9:15) I've thought some more. I think I will actually try to keep v2 (of the vacuum paper) on a heuristic level, where I might even just argue from simply looking at the fact that the particles are infinitely spread out in position space. And then I will just make another paper where I take a cutoff Yukawa(?) theory with $(1+1)$-spin particle and antiparticle fermions, and from there argue mathematically that the vacuum-perturbing vertices can be removed from the path integrals. But I'm getting ahead of myself. First I need to figure out how to solve the problem at hand, still.

The idea about cutting out a localized part of $\phi$ has made me think---and given me some useful thoughts. It \emph{is} a bit magical that an almost-finite number of particles can cure the vacuum-perturbation everywhere in position space at once. But with such an unbounded operator as we have here, this is still not out of the realms of possibility. These infinitely spread out particles.\,. wait a minute, let me think.\,. (9:24)

\ldots Hm, now I'm actually starting to doubt this countable-basis solution.\,. (9:54) .\,.\,It is too magical/weird, when you look at it in a countable basis of localized states in position space.\,.

.\,.\,(10:05) Hm, and don't I run into the same problem with my previous solution (described in v1 of my vacuum paper).\,.\,? .\,.\,Hm, not necessarily, 'cause there the wave functions are never \emph{infinitely} spread; only as a limit.\,. .\,.\,But I might.\,. (10:14)
\ldots (10:36) The possibility lies in the fact that the vacuum solution might change characteristics in position space and be more and more potent in each local volume when $\delta k \to 0$ ($L \to \infty$) for canceling the local vacuum fluctuations, making up for the decreasing local amplitude when $\delta k \to 0$. .\,. So yeah, I guess I need to abandon this countable-basis idea, where the discretized steps can essentially be skipped, and go back to my original ideas described in v1, where the discretized steps are essential.\,.

(12.03.24, 8:37) I think I'm nearing a solution, perhaps. I'm back to my original idea of assuming---and showing in the future (although I don't want to undertake that alone; I have done so much already)---self-adjointness of $\lim_{\delta k \to 0} \delta k^{3/2} \hat H''$, as I've talked about. Here there is not the paradox (which otherwise tells us that it won't work) of the vacuum solutions consisting of infinitely spread particles. But there is something a bit weird about how the particles can become less and less dense over any local volume, because you are extending the position-volume cutoff, far away from that given local area. But on the other hand, the action of an unbounded operator can depend on the domain. And no one says that this dependence is local: What happens out at the edges of position space can in principle change the dynamics over local volumes within, since it might change the overall physical \emph{interpretation} of the operator. And if the \emph{interpretation} of the operator changes, it might change the dynamics globally. So even though an operator is formally identical to another one over a given local volume, they might still cause different dynamics over that volume, if the have different $L$-cutoffs at the boundaries of the position volume (i.e.\ they have different $\delta k$'s).

Now, I got the great idea also yesterday to consider the Lorentz covariance in terms of having a local volume in space-time where the interaction is ``dialed up'' from 0 to $q$. Then the Lorentz transformations can be done before and after that volume where they are trivial. And if we then further go back to my idea of an approximate Dirac sea, let us consider the following thought experiment. Say that we have a vast approximate Dirac sea within an infinite, or just much vaster still, position space. Draw a line around this volume in space-time where this approximate Dirac sea is uniform and looks close to a perfect Dirac sea for the particles inside the volume, within some degree of precision. Now go deep inside this vast space-time volume and draw a smaller volume, which s where we will turn the coupling constant, call it $q$, up from 0 to its actual value. Suppose that this much-smaller inner volume is still incredibly vast. The idea from there is to then argue that the approximate Dirac sea should follow $\lim_{\delta k \to 0} \delta k^{3/2} \hat H''$ with some precision in terms of the vacuum fluctuations, albeit perhaps only with an $L$ ($\propto \delta k^{-3}$) as large as either the inner or the outer volume.\,. Anyway, whichever it is, the vacuum fluctuations will only cause a finite number of particles, effectively, to be created from the bare vacuum. And if you do the Lorentz covariance circuit (i.e.\ Lorentz transformations outside of the inner volume and two time evolutions across the entire inner volume in the two respective inertial frames), the same will therefore apply in the other inertial frame as well. .\,.\,It needs some polishing, and I'm not too confident that it would be a good route to take mathematically, but it's a very good heuristic argument, it seems.\,:) (9:10) \ldots (And the point is, that the `finite' number of vacuum particles is not proportional to either of the volumes (and thus not to $\delta k$ of the appr\ldots the ADS).\,. (9:32)) .\,.\,And more importantly, the vacuum particles do not interfere with the physical particles, such that the vacuum-perturbing terms can be removed for both time evolutions.\,.

(10:08) Oh, and I don't think I have mentioned yet---and I don't think I have written much about that line of thinking before, by the way, even though I have thought it before: If we consider the path integrals alone, one should also be able to make a pretty good heuristic argument that the vacuum-perturbing terms ought to be removed there. For if we consider paths in momentum--time space (not momentum--(angular )frequency) and look at a path where sme part of the path includes particles created from the bare vacuum, then consider the same path but where all the (.\,.\,or just the initial) vacuum-perturbing vertices/vertex happens just slightly earlier. This would give the same contribution but with a different phase, do to the altered free energy. So when you sum over all such path, it's intuitive that, unless you get a divergence (which you would then naturally try to renormalize away), you should get 0 when you sum all such paths together! And to make the argument even more clear, one might consider a case much like the one I've just described, where the coupling constant is dialed up for a local volume. And let us here say that the coupling constant is dialed up slowly (``adiabatically''). Then it should be quite clear that these paths should sum to zero, shouldn't it?\,:) (10:19)

(10:29) I'm very happy with this new approach to the Lorentz covariance argument, i.e.\ the one described in the paragraph before the previous one. And yeah, the fact alone that the vacuum-particles don't interact/interfere, and that they are very, very spread out, including especially after the volume in time, where one wants to make a measurement, perhaps, and where they thus don't interfere with the measurements of the physical particles of this end-state, these two facts alone should be enough.\,.\,:) I actually think that this argument for Lorentz covariance is enough for me, personally. I will think just a tad bit more about it, and then I will probably get on to write v2 of the vacuum paper, which should now just be a lot like v1, as it turns out. (I might make some changes, though, and I will definitely include the position-space heuristics, arguing from the fact that the vacuum particles becomes infinitely spread out, and I will also change my Lo.\ cov. section.\,.) And then I might look into writing a more mathematical paper, where I look at a cut-off $(1+1)$-spin Yukawa theory where one spin represents particles and one spin represents antiparticles, as I've mentioned above. I can then try to derive mathematically, that the vacuum-perturbing terms can be removed, given the assumptions of this theory. (10:40)

.\,.\,Yeah no, I really believe in the soundness of this new Lorentz covariance argument.\,\texttt{:D} (10:42)

(12:01) In terms of the fact that the vacuum solution changes locally based on $L$, you can say that if not for the fact that the domain of the operator has to be limited on order to retains its symmetry, the operator would have to potential always to use arbitrarily small states to cancel the vacuum productions, or rather, it certainly very well might have that potential. But it's limited by the restrictions on its domain. But when $L$ changes, these domain restrictions changes as well, and this might thus make the operator able to use smaller states (when $L$ grows (meaning that $\delta k$ decreases)) to cancel the local vacuum fluctuations. So yeah, there is potential for my idea (the one described in v1) to work.

.\,.\,(12:08) I also have to mention that I'm actually no longer quite as troubled about the potential for the expectation value(s) of $\hat n$ to grow when $\delta k \to 0$ for this argument, since I can imagine, that you would be able to show, when working on proving the self-adjointness, that the $\phi_{\delta k}$-solutions have a well-behaved convergence when $\delta k \to 0$. But I should think some more on that now.\,.

.\,.\,Hm, and more importantly, perhaps, the convergence might be well-behaved when $\varepsilon \to 0$.\,. (12:14) .\,.\,(12:22) Well, I don't know that.\,. .\,.\,(12:30) Hm wait, couldn't you also do some $\delta P$ argument (like I did for the countable-basis idea) here.\,.\,? .\,.\,Starting with an $\varepsilon$-almost eigenvector of the continuous $\lim_{\delta k \to 0} \delta k^{3/2} \hat H''$.\,. .\,.\,Hm no, here it's already a problem that $\braket{\hat n}$ might depend on $\varepsilon$.\,. .\,.\,So I'm not out of the woods yet.\,. (12:37)

\ldots\ (14:57) There actually might still be a potential for the countable-basis idea. Even though the vacuum solution is infinitely spread out, it is not necessarily uniformly spread out. So it might still work. I actually thought about this, but for some reason, I didn't like the idea before. (If a had a good reason, I have forgotten it now (but I don't think I had a good reason.\,.).) So it might still be possible to show Lorentz covariance of, well, $\lim_{\delta k \to 0} \hat H''$.\,. Hm.\,. .\,.\,Okay, it seems that it is one or the other: Either you can show that $\lim_{\delta k \to 0} \delta k^{3/2} \hat H''$ is Lorentz-covariant, and non-trivial, or you can show that $\lim_{\delta k \to 0} \hat H''$ is.\,. (15:03) But luckily, I see a potential route to make the $\lim_{\delta k \to 0} \delta k^{3/2} \hat H''$ idea work as well, at least on a heuristic level.\,. In terms of momentum space, I actually just had the idea, before sitting down, that one might be able to argue and use the fact that the $\varepsilon$-almost eigenvector stays very small in amplitude when $L$ is large, at least for some local volumes of space-time, when it is time-evolved. That opens up a potential route for completing the argument in momentum space.\,. Oh wait, that uses position space arguments as well, of course.\,. (15:08) .\,.\,Well, nevertheless, that's one potential route. And another one might be to Trotter/Dyson expand, and then argue that for large enough $L$, you should be able to continuously remove all vacuum particles in a local volume (incredibly small compared to $L^3$) between each operator (after each $\Delta t$) in the expansion. So there's another potential route.\,. (15:11) And I was about to say, that I'm actually satisfied with having reached this point now, but now I guess I should think a tad bit more about whether both the $\lim_{\delta k \to 0} \delta k^{3/2} \hat H''$ idea and the $\lim_{\delta k \to 0} \hat H''$ work at the same time (probably not), and about which I think will work (probably the original one.\,.).\,. .\,.\,But yeah, I think I'm soon good to just say: `this is as far I'll get this time around,' (and a very decent distance at that), and then hopefully when I return to the problem, it will not be alone.\,. (15:15)

(15:52) To be fair, it is way more plausible that my self-adjointess domain technique (from my self-adjointness paper) can be used for $\lim_{\delta k \to 0} \delta k^{3/2} \hat H''$ rather than $\lim_{\delta k \to 0} \hat H''$. So I would probably put my money on my original idea at this moment.\,.\,:)

Right now, I'm also wondering, at that `the $\varepsilon$-almost eigenvector stays spread out when time-evolved' idea can be transferred to the momentum space argument somehow.\,. (14:56) .\,.\,For in momentum space, it could then perhaps just become: `the $\varepsilon$-almost eigenvector stays in the $\mathbf{P}_0$ hyperplane when time-evolved'.\,. .\,.\,Hm sure, that sounds quite reasonable, at least on a heuristic level, certainly.\,. (15:59) .\,.\,And then you just need to have assumed the cutoffs so that you can make the Trotter--Dyson expansion.\,. You should then be able to gather the terms again into the time-evolved $\hat A^\dagger$-operator, I'd bet. So yeah, I think this might just work.\,.\,:)

I'd need to think more about it though. But let me think if I even want to do that, or if I should aim at making v2 by maybe just assuming something nice about $\phi$ for the momentum-space argument.\,. (16:04)
.\,.\,(16:14) Oh, but then I also need to show that $\phi$ doesn't grow it's particle number (expectation values), and that sends me back to the same square. So I guess I should just move on now, and then fix v1 of my paper (but probably keeping it much the same).\,.

(16:44) Hm, maybe I should also take that path integral (in momentum--time) idea more seriously, and perhaps combine it with Hamiltonian arguments. First of all, if we look at any $\varepsilon$-almost eigenvector, $\phi$, that is ``based around $\ket{\,}$,'' so to speak, and who has a non-vanishing inner product with $\ket{\,}$, then couldn't one show that when we dial the coupling constant, $q$, down, we can also decrease the amplitudes of the excited states of this $\phi$ state and then obtain a new $\varepsilon$-almost eigenvector of this dialed-down $\hat H$? Then the conventional wisdom will tell us, that if we dial up $q$ slowly for a space-time volume, like the one from the thought experiment described (above) this morning, then the bare vacuum state, $\ket{\,}$, will slowly turn into.\,. well, turn into.\,. Okay, some problems with this argument, first of all because there might very well be degeneracies.\,. Hm.\,. (16:51) .\,.\,Hm, but I think this line of thinking can still be made to work. So it would surprise everyone, if the bare vacuum does not slowly turn into another vacuum solution. And when we then go and consider the path integral, we can see that.\,. Wait, never mind the line between a Lagrangian/path integral and a Hamiltonian argument, 'cause why draw a hard line? Couldn't I, more to the point, not use the Dyson-expansion into a path integral in order to argue the the vacuum perturbing terms cancel out from the time-evolution (for my Lorentz covariance circuit)?\,.\,. (16:57) Hm.\,. .\,.\,Hm yeah, maybe.\,. .\,.\,So I have a lot of potentially good routes, even though they all seem kinda hard, where they are now, to make a rigid/solid mathematical argument out of.\,.

.\,.\,But then again, the Lorentz covariance argument is (I believe) heuristic as well, so it doesn't matter too much, even if we only (in my lifetime or so) make it as far as showing heuristically that the theory without the vacuum-perturbing terms is Lorentz-covariant, but still manage to show mathematically that it is well-defined. *(Oh, I meant to also say: especially if it can also be confirmed, numerically and/or experimentally, that we are right to cut the vacuum-perturbing vertices out of the paths in the path integral. (17:11)) .\,.\,So I'm pretty happy with where I'm at, actually. .\,.\,And yes, it is definitely worth trying to take the route of arguing directly from the (momentum--time) path integrals that the vacuum-perturbing vertices does not affect the dynamics of the physical particles. (17:07)

(13.03.24, 12:27) Instead of momentum--time space, one could also do this argument in position--time space up until the last vertex, far within (generally) the volume where $q$ is dialed up. And after that vertex, you could go to momentum space. Then you consider similar interaction trees for the argument, where you then argue that the different time gaps between the last vertex and the final state means that the overall contribution from this group of paths will sum to (almost) zero.

So there we are, but this is not all. For the vacuum paper, I could also derive the momentum--time path integral via a Dyson expansion (this time a true one, i.e.\ where the free propagation is kept as $\exp(-i \hat H_0 \Delta t)$). And then I could argue directly from that that all the paths that contains vacuum-perturbing vertices---and which are not closed loops(!)---will yield a zero contribution (when we take the adiabatic limit of dialing up $q$ in the past before the given experiment). (12:35) So I might actually make this argument in v2 of my vacuum paper.\,.\,! (12:35)

(14.03.24, 10:11) I was getting really happy with my path integral argument yesterday, but it seems that I might have thought to simply about it; it now seems a lot more complicated. I'm not sure my argument holds.\,. \ldots Wait, let me think some more.\,. .\,.\,Hm, with the assumption that you can calculate it as a perturbation series (in the coupling constant), doesn't the argument actually work?\,.\,. \ldots Hm, and the fact that $q$ can be anything gives a pretty good heuristic reasoning, at least, that you should be able to calculate by gathering orders of $q_0$, where $q_0$ then represents the maximum coupling in the middle of the volume, which $q$ is dialed up to.\,.

.\,.\,Okay, let us consider momentum--time space where we only consider one inertial frame, where $q$ is then a function of time (measured in that single inertial frame).\,. .\,.\,Some (quite conventional) assumptions then let's us do a Dyson expansion and gather the paths in orders of $q_0$, i.e.\ number of interactions. (And note that I'm talking about an \emph{actual} Dyson expansion here, where the action of the free energy is kept as $\exp(-i \hat H_0 \Delta t)$.) .\,.\,I think I should consider paths where the initial vacuum-perturbing vertex is the one that's allowed to vary.\,. .\,.\,By the way, let us consider a case where $q$ is slowly dialed up and then dialed slowly back down again afterwards.\,. The idea is then that when the first vertex that embodies an interaction with a `vacuum particle' and a `physical' one is close to the beginning of the time interval, the coupling is very low, and this path should not contribute with very much. And if it is away from the beginning of the time interval, then when you sum/integrate over all times where the variable initial vacuum-perturbing vertex sits, you should also get a very low contribution due to the oscillations that the free energy (between the vacuum-perturbing vertex and the first physical--vacuum interaction vertex) will cause. Hm, this does not seem so bad after all.\,.\,!\,.\,:) (11:43) *[I'm no longer very convinced at all that this argument holds. For when $q$ is dialed up more slowly, there is also more time values to sum over for the vertex/vertices.\,.]

.\,.\,(11:51) Hm, of course, the assumption about the perturbation series is not necessarily correct, but then again, standard QFT assumes that it is (I'm pretty sure, but one could look into it more), so\ldots\ (11:53)

(15:40) Okay, I think I know how to complete my original $\lim_{\delta k \to 0} \delta k^{3/2} \hat H''$ idea now.\,. I think one should be able to show that $\lim_{\delta k \to 0} \delta k^{3/2} \hat H''$ can be approximated by a bounded version, where the bound does not depend on $\delta k$. For if the continuum limit can be shown to be self-adjoint, and to be approximated arbitrary well be a version with a $\Lambda$ and an $n_{max}$, then if you introduce a small discretization to that in momentum space, one can probably show that this operator still approximates the original one arbitrarily well when $\delta k \to 0$. So $\delta k$ can be sent to 0 first, before $\Lambda$ and $n_{max}$, in other words. And then we are good, since we can then just take an $\varepsilon$-almost eigenvector, $\phi$, and.\,. wait, no.\,. Hm.\,. .\,.\,Hm, or could we take a sequence of actual eigenvectors, perhaps.\,.\,?\,.\,. \ldots (15:58) Hm, but maybe the point is to use this to show that $\hat U(t) \phi$ stays (approximately) bounded in its particle number.\,. .\,.\,Yeah, that must be the point.\,.(!\,.\,.) .\,.\,Oh, maybe this is it: the idea I've been searching for.\,. (16:02) .\,.\,(16:11) Oh, there is a problem, I think: How well an operator `approximates' another also depends on the time interval, $t$, or similarly, it depends on the energy, which is a problem, since this goes as $\delta k^{-3/2}$.\,.

.\,.\,Hm, but back to the idea of looking at a sequence of actual eigenvectors, couldn't you also ague that the continuum limit approximates the discretized (and bounded) operator in some sense.\,.\,? (16:15) .\,.\,Hm, maybe not.\,. (16:17) \ldots Hm, how about thinking about the more and more rapid oscillations of $\phi$?\,.\,. (16:38) .\,.\,(In terms of its energy.) \ldots\ (17:55) No, I don't want to try to go that route. Hm, I wrote above that I thought one could argue, that $\phi$ stays ``spread out'' when time-evolved, but I don't remember what I was thinking.\,.

(18:16) Oh, if we go back to the path integral argument/idea, wouldn't it make more sense to integrate the vacuum-perturbing vertex in position space over all possible positions, and then look at combined transitions to momentum states.\,. hm.\,. .\,.\,Oh, no. It wouldn't.\,.

.\,.\,(18:26) Oh, I guess you could argue that a $\phi$ that `stays spread out' can be found simply do to symmetry reasons.\,.\,! *(This is not enough: My big problem is really that I seem to not be able to argue that $n$ doesn't grow at some point (too soon, and to rapidly) for any $\varepsilon$-almost eigenvector, $\phi$.\,.)

Hm, I kinda want to take a break from this problem.\,. (18:40)

%(21:07) Jeg fik lige en idé om at approksimere \phi med egenvektorer af en diskretiserede H med en n_{max} også, og hvis man så kan argumentere for, at n_{max} ikke behøver at stige, når \delta k formindskes.. Hm, måske, måske ikke, men bestemt værd at tænke videre over...

(15.03.24, 10:02) Unrelated side note: In my existence paper, I write about a $\hat U(T)$ becoming an approximate identity operator, but I guess that this does not necessarily happen. I'm not completely sure. The overall argument still holds, though.\,.

(10:04) I think I'm about to leave this topic for now. The thing is, which route to take seems to depend on which of $\lim_{\delta k \to 0} \delta k^{3/2} \hat H''$ or $\lim_{\delta k \to 0} \hat H''$ can be shown to be self-adjoint. And the process of proving this might yield valuable insight which could be used for completing the last part of the argument, i.e.\ what I've been working on here. It might even be the case that such insights are \emph{needed} to complete the argument. It might for instance be the case that we need to show something about how the $\varepsilon$-almost eigenvectors depend on $n$ (and on $\varepsilon$).\,. .\,.\,So yeah, I'm thinking about putting this problem aside for now, and just make some adequate corrections to the vacuum paper (v1) on arXiv.\,. .\,.\,Yeah, it seems like the right thing to do.\,. .\,.\,putting it aside, i.e.

.\,.\,Copied from above: ``My big problem is really that I seem to not be able to argue that $n$ doesn't grow at some point (too soon, and to rapidly) for any $\varepsilon$-almost eigenvector, $\phi$.\,.'' (10:15)


%(18.03.24, 11:32) Jeg fik en muligvis kæmpestor idé her i sengen for små to timer siden---gik tidligt i seng og stod sent op---men min hjerne har af en eller anden grund bare været rigtig sumpet siden da, på trods af en god, lang nats søvn, så.. Men nu prøver jeg lige at sparke mig selv i gang ved at skrive lidt løst her om den nye idé: ..Lad mig lige fortælle først, at jeg egentligt har lagt.. ..Vent lad mig lige tænke over noget først.. ...(12:16) Årh, jeg kan virekligt ikke tænke særlig godt.. Men jeg tror nu, at jeg har fat i noget. Lad mig bare fortsætte, så, og sige, at jeg egentligt havde lagt det på hylden. Og i går tror jeg nærmest ikke, jeg tænkte over det overhovedet. Jeg havde virkeligt givet op, egentligt. Men så i sengen her i morges kom jeg alligevel til at tænke over, hvor meget interactions termern vokser som funktion af partikel antallet. Dette er jo meget natrligt at gøre, men jeg troede ikke rigtigt på før, at det ville lede mig hen til noget interessant. Men så kom jeg alligevel frem til, at amplituden både i første såvel som i anden må gå som $n$, nemlig fordi de individuelle bidrag vist ikke overlapper. Og så har vi jo pludselig noget interessant alligevel, for interaktionstermerne går jo som $\delta k^{3/2}$, kvadreret til $\delta k^{3}$, som er omvendt proportionelt med $n$ for et diskretiseret (approksimativt) Dirac-hav.! Og så kommer den store idé, for dette i sig selv er jo ikke så brugbart, når de to ting bare går ud med hinanden. Men hvis vi ser på en egenvektor af $\hat H''$, så kan vi jo ikke have, at de alle sammen kun har amplituder oppe ved $n=n_{max}$. Og man sågar argumentere for, at.. ..Ja, og nu bliver det så meget løst men: at der må være lige så meget amplitude.. Nej, eller rettere at for en konstant $\varepsilon$-almost egenvektor, \phi, som bliver under en vis $n$, så må denne opløses i vektorer, der alle har.. Hm.. Okay, det skal jeg tænke mere over, og især hvis det skal gøres matematisk, men min tanke i morges i sengen var: Hvis to egenvektorer udligner hinanden oppe omkring n_{max}, så skal de være antiparallelle der, hvilket, da de er ortogonale, må betyde at de er parallelle nede ved de lave n. Og dette kræver at amplituderne altså også er tilsvarende store dernede. Så når \phi opløses, så kan må egenvektorerne altså være nogenlunde ligeligt fordelt, nærmest, over og under en vis $n$-grænse.. Ja, det er ikke et særligt godt argument, men det var altså, hvad jeg tænkte. Og jeg er stadig ret sikker på, at man kan komme videre med den tanke. Nå, og det slog mig jo så, at det kunne vise, at de mest normale interaction terms ville blive mindre og mindre for $\hat H''$ / for et approksimativt Dirac-hav, når \delta k \to 0. Men så var der lige problemet med de interaction terms, hvor to vakuum-pertikler annihileres. For der går amplituden som n^2, fordi man så får alle kombinationer. Men efter at være stået op (vistnok i badet, eller kort efter, men det lige røget.. ..Hm ja, jeg tror i badet eller mens jeg tørrede mig..) så kom jeg i tanke om, at $\Delta/2 + d$-billede jo netop også ændrer sig, når man ikke tager to partikler, der er med i samme "triplet" (af partikler med impuls 0 sammenlagt), men når man tager to partikler fra hver deres vakuum-triplet.! Og sidenhed er jeg så lidt kommet løst frem til igen (og det minder i øvrigt om mine tidligere tanker omkring udelukkende-annihilations-interaktions-termerne (dem jeg glemte i v1)), at ja, når man gør det sidstnævnte, så vil man få et endnu mindre bidrag, som så lige netop kan konkurrere med det der n^2.! (12:46) ..Og hvis det holder, og fordi amplituden for egenvektorerne af \hat H'' jo nok må falde omkring n_{max}, når denne stiger, så vil den faldende amplitude altså nok vinde over det stigende antal kombinitioner for interaction term-transitionerne, hvilket må bringe os i mål.!! ..7, 9, 13. (12:49)

%Nå, jeg vil stryge min hjerne lidt med hårene nu, og nok gå en tur inden længe, og så håbe på, at den kondenserer mere og mere, så jeg lige kan få regnet det mere igennem---og muligvis finde (lidt) ud ad, hvordan jeg skal udføre argumentet for, at "amplituden for egenvektorerne af \hat H'' [...] må falde omkring n_{max}."...

%... (16:40) Mit argument holdt ikke, for selv hvis normen er jævnt fordelt over all n, så kan det godt være, at normen er så meget desto mindre i hvert niveau, men så bidrager hvert niveau jo til gengæld bare hver især.
%Nå, men jeg fandt min hjerne i skoven, hvor jeg så fik indset dette. Så kom jeg dog til at tænke på min Baker--Campbell--Hausdorff-idé. Og omkring tyve minutter efter, kl. tyve i to, kom jeg så frem til noget rigtig stort, tilsyneladende.! Hvis vi har en Trotter-udviklet $\hat H''$, og så lader tiden stige med en faktor \delta k^{-3/2}, så vil dette jo nok med al sund fornuft kun kræve i omegnen af \delta k^{-3/2} flere faktorer i ekspansionen. Og hvis vi f.eks. lod der være \delta k^{-(3+\varepsilon)/2} gange flere faktorer (i.e.\ operatorer på formen \exp(-i\hat H'' \Delta t)) i ekspansionen, hvor \varepsilon er meget lille, så vil vi sikkert få højere og højere præcission, endda. Og lad os så erstatte \exp(-i\hat H'' \Delta t) med (1 - i\hat H'' \Delta t)! Så får vi en ekspansion af kun \sim \delta k^{-3/2} af disse operatorer, som hver især kun kan forøge partikelantallet med 1--4 partikler. Så vi kommer altså aldrig op i nærheden af \delta k^{-3} antal partikler via denne tidsevolution, hvis vi altså starter med en \phi, der har er meget hurtigt aftagende efter et vist, konstant n.! Og så kan jeg (ved at bruge det jeg ellers skrev om her i morges) komme igennem på denne måde! :D (16:54)

%..Sikke en kæmpe lettelse, hvis jeg alligevel kan komme igennem dette argument for min originale idé også (jeg er jo rimeligt overbivist om, at vi sagtens kan komme igennem, hvis operatoren med den tællelige basis kan vises at være selvadjungeret). Det er dog som om, at det ikke helt er sunket ind endnu.. eller også er jeg bare skeptisk, hvilket er klogt nok.. Men det vil virkeligt være kæmpe stort for mig, ingen tvivl om det.. (Jeg skrev egentligt her forleden, at jeg var godt nok tilfreds, men jeg har alligevel bare kunne mærke siden, at det føltes som om det tog luften lidt ud af hele projektet, nemlig at jeg kun muligvis kunne pege i retning af målet, hvor jeg, hvis dette holder, nu kan pege meget mere kraftigt og sikkert på målet..(!..)) (16:59)

%(18:30) Ah, når jeg nu antager UV og IR cutoffs, så kan jeg lade \phi være en næsten-egenvektor af \hat H_{vac}''' i stedet, og vigtigere, se på egenvektorerne af \hat H_{vac}''. Og så kan jeg nemlig lige netop få, at vakuum-partiklerne kun er entangled med op til 4 partikler hver især, hvilket jeg nemlig skal bruge for at kunne bruge det/de argument(er), som jeg beskrev i morges, hvor n^2-afhængigheden altså alligevel (i hvert de eksempler jeg lige har fået regnet (lidt) på i hovedet) udkonkurreres. (18:34)
%...Hm, jeg glemmer vist annihilations-delen af \hat H_{vac}'', som også kan foresage yderligere entanglement.. ..Hm, hvís man nu starter fra det bare vakuum... (men tør jeg det ift. Trotter-ekspansionen?..)..  ..Hm, men måske kan man også godt bare vise, at denne annihileringsdel vil have forsvindende virkning, når den ikke annihilere tripletter (eller kvartetter), men annihilerer partikler fra en større entangled gruppe.. (18:57) ..Det må man kunne, ja.:).. ..(19:05) Hm, og så kunne man starte med en pre-\hat H_{vac}'', hvor tripletterne er entydigt identificerbare, og hvor annihileringsdelen så kun virker på tripletterne (eller kvartetterne, når det kommer til Coulomb-delen, selvfølgelig) hver for sig. Og ved så at se på "perturbationen," hvor man lader annihileringsdelen virke på kryds og tværs af alle partiklerne, således at vi kommer tilbage til den rigtige \hat H_{vac}'' med denne "perturbation," så kan må man så kunne argumentere for---med UV og IR cutoffs---at forskellen på (næsten-)egenvektorerne kun er forsvindende, når $\delta k \to 0$. (19:10)

%*(22:54) Jeg kan bare bruge, at der er $\sim \delta k^{-3/2} - x$ partikel-tripletter/kvartetter, der ikke er blevet yderligere entangled, og $\sim x$ partkler, der er.. ..(muligvis..)

%*(19.03.24, 10:36) In a way, I was wrong about the \delta k^{-(3+\varepsilon)/2} thing: We can only derive a bound on \Delta t that goes as \delta k^{3}, it seems. But I've actually realized that it might work still. For the thing is: even though their might be some parts of the (1 - i\hat H'' \Delta t)^N expansion that reaches up around n_{max}, these parts will generally go as \Delta t in transition amplitude for large enough.. $N$.. Hm no, let me see.. (10:43) ..Hm, it should go as \Delta t, right?.. ..Oh no, not generally, but when \Delta t goes as \delta k^{3}, and you look at a transition to some level with particle number equal to $x \delta k^{3}$, where $x$ is some constant fraction, then yes, it should go as $\Delta t \sim \delta k^3$.. ..And there we go.:)(!!) This therefore means that one \emph{can} argue, that the vacuum particles becomes less and less dense when \delta k \to 0, within the whole time interval that itself goes as \delta k^{-3/2}.:) ..!! (10:58)

%*(11:32) Hm, I should probably use a \Delta t that decreases even more rapidly, actually, or simply sent it towards 0 for every \delta k.. ..Right..:) ..Or equivalently use the Taylor expansion formula.
%*(12:05) Åh, måske skal jeg faktisk tilbage til den idé fra i går aftes, som btw nok nærmere burde have været formuleret som: "der er $\sim \delta k^{-3/2} - x$ kreationstermer og $\sim x$ annihileringstermer." Og så skal jeg måske bare bruge denne idé allerede her for at vise, det jeg overvejer nu, nemlig at: "the vacuum particles becomes less and less dense when \delta k \to 0, within the whole time interval." ..Ah, jeg kan mærke, at dette kan bringe mig i mål (7, 9, 13).. (12:10) ..Hm, og så skal jeg jo nok ikke Taylor-udvikle, men skal i stedet $(1 - i\hat H'' \Delta t)^N$-udvikle.. ..Hm, tja... ..Lad mig forresten springe ned under de to kommende renderede paragrafer..


(18.03.24, 19:20) Some seemingly very(!) important notes (in Danish) above this paragraph in the source code comments.\,!\,.\,. .\,.(I will summarize them here at some point.\,.)

(19.03.24, 10:31) Maybe the two operators $\lim_{\delta k \to 0} \delta k^{3/2} \hat H''$ and $\lim_{\delta k \to 0} \hat H''$ \emph{can} both be self-adjoint (and non-trivial). Maybe this is not a contradiction. For they might just have different domains that each makes one well-defined and non-trivial, and not the other.\,. Very interesting, but luckily, I think I can complete the argument for $\lim_{\delta k \to 0} \delta k^{3/2} \hat H''$, as I'm currently writing about in the source comments. \ldots\ (17:18) Oh, I've also completely forgotten that the Hilbert spaces are also different to begin with (not just their domain).\,x) .\,.\,So yeah, maybe they can indeed both be self-adjoint.\,:)


%(12:15, 19.03.24) Jeg havde en rigtig god fornemmelse med den her idé fra forrige ikke-renderede paragraf, og det har jeg stadigvæk, men jeg kan også mærke, at jeg lige skal tænke noget mere over det først... ...(12:46) Okay, jeg kan (vist) bruge Taylor-udviklingen, og så netop bruge omtalte argument til at vise, at man kan lave et vist cutoff på m, hvor m er ordnen i Taylor-udviklingen. ..Og "omtalte argument" handler altså om at bruge, at selvom, ja, der er flere partikler i spil for \hat H_{vac}''^{m} \phi, så er de fleste af dem kun entangled i tripletter og kvertetter, hvorved de deremd ikke kan bidrage med transitioner med særligt høje amplituder alligevel. ..Nå, så nu skal der altså tænkes over, hvordan jeg helt præcist finder dette øvre bound på \hat H_{vac}''^{m} \phi... (12:51)

%(20.03.24, 11:37) Okay, Coulomb-interaktionen er faktisk virkeligt problematisk.! Uden den tror jeg sagtens godt, man kan komme igennem med mit nye argument her (for min originale $\lim_{\delta k \to 0} \delta k^{3/2} \hat H''$ idé). Med med den virker det bare ikke rigtigt til..!:S ...(12:11) Ah, vent.. ...Nej..

(20.03.24, 13:03) I feel like I've come so far on the way to completing the argument for the original $\lim_{\delta k \to 0} \delta k^{3/2} \hat H''$ idea (see the source comments), but the Coulomb interaction actually seems to be in the way of completing it fully. I kinda want to keep at it, but I also have to get on with my life (and more importantly, other projects) at some point.\,. .\,.\,The good thing is that now I see a higher chance the the $\lim_{\delta k \to 0} \hat H''$ idea could also work, i.e.\ the `countable basis' idea. And if that works, it already yields us a well-defined theory, even if one cannot get rid of the perturbed vacuum. Yet I feel pretty confident that one could complete the argument I had in mind (carving out a $\delta \mathbf P$ and all that) for that route. So I guess I should just try to leave it be and put this problem on the shelf for now.\,. (13:11) .\,.\,Hm, I guess I ought to do that, yes.\,. I probably cannot help thinking about it here on my afternoon walk soon, and maybe I'll also think about it some more throughout the rest of the day, but I should probably also try to tell my mind to move on to new things.\,. (13:14)

%(13:29) Hm, men hvad nu hvis man kunne bruge Trotter-udviklingen for \hat H''' og så på en måde argumentere for (og så bruge), at man godt kan lade \delta k \to 0 men samtidigt beholde et ikke-forsvindede \Delta t..?

(13:36) Okay, I just had an idea: `What if one used the Trotter expansion and were able to argue that when $\delta k \to 0$, you could still keep a constant (small enough) $\Delta t$ in order to adhere to some precision?' And now that I've thought a tiny bit about it, it does seem somewhat reasonable, and what's more, it would solve my problems, wouldn't it.\,. Well, perhaps.\,. Hm, it's actually somewhat back to an earlier point of my recent idea; maybe it could even work if I only showed that $\Delta t$ had to decrease only as fast as $\delta k^{3/2}$.\,. Okay, I should think\ldots

(14:20) I've realized that even if I manage all this, it still might not be quite enough, as I'm worried that when we get back to the `interaction terms,' it might also be trouble even if the particle number only grows as $\delta k^{-3/2}$ over the whole time interval. So maybe we actually need for my countable basis idea to work, anyway. So I guess I'll actually put my money on that: I hope (and find it reasonable) that the Dirac-sea reinterpreted Hamiltonian in a countable basis will be self-adjoint on some domain. And in that case, I'm pretty sure that my `carving out a $\delta \mathbf P$ and all that' route will work, at least in the end. This idea is described above (and perhaps my poorly encrypted working notes about it can also help a little bit in understanding it). So I think I'm actually satisfied with where I am enough that I can actually put the problem away for now. I ought to also make a correction to v1 of my vacuum paper, but maybe I should focus on finding a job first, and get on with web-development.\,. (14:28) .\,.\,Yeah, I'll probably at least take a decent break before I go back to doing that (no one is reading it anyway, so I guess there's no hurry at all).\,.

.\,.\,Let me just reiterate: I think my countable basis idea, described above, will work (and I feel like I basically solved the post-showing-self-adjointness part above, 7, 9, 13).\,. (14:31)


\section[Nelson renormalization, Friedrich's extension]{Just discovered\footnote{Or rather, prof.\ Oliver Matte, who has just e-mailed me back and said he wants to read my paper(!), recommended the reading (in the same e-mail).} Friedrich's extension and Nelson renormalization}

(10.05.24, 10:34) I know I've seen the Friedrich's extension theorem before in Reed and Simon, but for some reason I didn't really think much of it.\,. Well, maybe it was just because I couldn't use it due to the required positiveness of the operator.\,. And I've also just read about the Nelson renormalization (read the original source for a paper that prof.\ Oliver Matte just recommended me (via e-mail, see the footnote)). So this mor.\,. this noon, %..(The last couple of hours has really slipped by me..) *(I got to bed around eleven so how is it almost eleven now?..)
I will think about this changes anything in relation to my vacuum paper problem. (And maybe I will just think about and read about math for the rest of the day.\,.) .\,.\,(Or maybe I will read some more first.\,.)

(11:02) Hm, isn't it actually a simple matter to show that the Hamiltonian is bounded from below after the Dirac sea reinterpretation!\,?\,.\,. .\,.\,Or when you add the free energy to my SA paper operator for that matter.\,. .\,.\,So if we add the free energy (like I talk about in one of the appendices), and if we are fine with not deriving the domain, but only showing its existence, one could just make a paper where one simply defines $W$ (and $W_\Sigma$), shows that $\hat A$ is defined and symmetric on $W_\Sigma$, and then, without ever defining $V$, just use the Friedrich's extension theorem and be done from there.\,. (11:14) .\,.\,Yeah, I would think so, actually, but then if it is still needed/handy to show the self-adjointness of the $\hat H_{QED}$ before the Dirac sea reinterpretation, then my $V$ ($\cap\, U$) domain could still come in handy there.\,. .\,.\,(11:24) (And my $W$ domain is of course still important, if no one's really used it before, even if the Nelson renormalization is also possible, for the $W$ solution makes for much simpler proofs than using Nelson renormalization.\,.) .\,.\,Hm, only thing is that Nelson renormalization might be useful (if it works for the model/theory) if my `$E$-sets with constant radii' idea doesn't work as well.\,. (11:34)

(11:41) Wait, \emph{is} it really so easy to show that the operator is bounded from below? Or is it in fact not so simple.\,.\,?

(12:11) Ah, bounded from below!\,. Yes, maybe it is possible.\,. maybe $\hat A + \hat A_0$ \emph{is} actually bounded from below on $W_\Sigma$.\,.\,! It's not necessarily positive, at all, but yeah. The thing is, if we simplify matters, or if we use my simplifying assumption that I used for the SA supplement paper, then $\hat A \chi$ is only significant at levels $m-1$ and $m+1$, and $m$ if you absorb $\hat A_0$ into $\hat A$ as well. And yes, this means that if you construct a state of two $\chi$-solutions added together, then they can yield a negative $\braket{\psi| \hat A \psi}$. But the negative value you can maximally (or minimally, rather) achieve hereby might still very well be bounded from below. So there we go.\,.(!\,.\,.) (12:18)
*[I forgot that $D_{m+1}$ can expanded arbitrarily.\,.]

.\,.\,Ah, but this might not be the case when the fermion number can vary (in a Fock space), i.e.\ for the full and final $\hat H_{QED}$. But still a possibly important realization.\,. (12:23)

(14:05) Hm, maybe the solution to the problem of my vacuum paper is to show the existence of a ground state somehow, something that I have no expertise/skill in, i.e.\ of $\hat H_{vac}$ (when one multiplies with $\delta k^{3/2}$ at every step going to a limit). That could do it.\,.
*[Well, duh. Of course it would be handy to find a solution to the empty vacuum. Never mind.]

.\,.\,Hm, that could also be a potential way to rewrite my paper, by the way: Assume the existence of a ground state in order to show how this gets us the same interesting conclusion.\,.
*[Never mind.]

.\,.\,(14:16) Hm, could it be possible to simply use, if it can be shown, that a stable point in the energy graph of the operator (over the set of generalized eigenvectors) exists.\,.\,?\,.\,. .\,.\,Oh!\,.\,. (14:20)
.\,.\,(14:26) Hm, when $\delta k \to 0$, $\varepsilon$ of the almost eigenstate of $\hat H_{vac}$ is also allowed to decrease at some rate (says all intuition).\,. .\,.\,The state then evolves over al longer period of time from its perspective, i.e.\ when time is seen as going as $\delta k^{-3/2}$ in the vacuum part of the Hilbert space, but on the other hand, the state can become more stable when $\varepsilon$ decreases (when the almost eigenstate is in the neighborhood of a stable point).\,. .\,.\,(14:36) Ah, this doesn't help me at all anyway, since the particle number might also increase to begin with when $\varepsilon$ decreases, i.e.\ at $t=0$. 

(11.05.24, 11:18) I forgot that $D_{m+1}$ can expanded arbitrarily. So I'm not so sure that $\hat A + \hat A_0$ would be bounded from below, even with only one fermion.\,. Well, let me think about it.\,. %...Det går lidt langsomt i dag..
\ldots Hm, if we take $\chi^{(m)} + \chi^{(m+1)}$ and let $\chi^{(m+1)}$ go as $\sim k^{-2.5}$, then both the free energy and the matrix element between the two $\chi$'s diverges exactly. So it's not a simple matter to say that it's bounded from below or not.\,. (which is a good thing, in a way, i.e.\ that it's not necessarily \emph{not} semi-bounded.\,.) .\,.\,Hm, I bet that it's bounded from below, then.\,. (11:55)

\ldots\ (13:43) I realized that if you indeed let $\chi^{(m+1)}_{m+1}$ go as $\sim k^{-2.5}$ and just have a cutoff that you let tend towards $\infty$, and you then have a factor, call it $\alpha$ in front of $\chi^{(m+1)}$ that you decrease while increasing said cutoff, then you can get the matrix element to win over the free energy. And then it seems that the operator is not semibounded. Interesting. But after a further while, I also realized that the tail on $\chi^{(m+1)}$ contributes to the free energy as well. And there we might have it: The operator might then very well be semibounded (and it might not be hard to show). At least for my solution with the $E$-sets with growing radii. With my potential solution (that I've only thought of but not shown) where the radii are constant, the matter might be a bit more complicated. But maybe the radii only needs to be constant with respect to $n$, the particle number, and not w.r.t.\ $k$ (in order for us to be able to show that an UV cutoff is valid). So yeah, it might be possible to show that such operators are semibounded.

Quite interesting stuff, and now I'm even more motivated to try to understand what others have chosen for their domain when treating the so-called semi-relativistic Pauli--Fiers Hamiltonians\ldots\ (13:52) \ldots (14:20) Oh, I'm actually not sure that these have been treated without the UV cutoff.\,.

(17:02) Ah, the Nelson renormalization also requires adding positive constants, since the lower bound will otherwise tend to $-\infty$. So it is well-known that it does, even when the free energy is quadratic (but also when it is relativistic, I think).\,.

(21:27) Wait, when having the free energy, can't we construct eigenvectors simply by continuously adding new $\chi$'s with their head proportional to what the $\hat H \psi$ would otherwise be at that level, and then continue to do this all the way up? If one can make eigenvectors this way, we can maybe construct solutions to the empty vacuum ($\hat H_{vac}$ from my vacuum paper) this way, which could do the trick.\,.\,!\,.\,. (I'm sure I'm overlooking something, but I can't immediately see what it would be.\,.) (21:32)
.\,.\,Well, $\hat H_{vac}$ doesn't really have a free energy in the limit, but then each new $\chi$ would just have to be equal to $\hat H_{vac} \psi$ at that level, before this new $\chi$ is added. So that shouldn't really change anything. And by the way, the thing is, if each $D_{m+1}$ is always minimal, each $\chi$ shouldn't add much.\,. oh wait, maybe I'm forgetting the absorption transition for each new $\chi$.\,. (21:42) .\,.\,Hm, and if $D_{m+1}$ is indeed minimal, that should then cancel the state at one level down.\,. .\,.\,Hm, can one make a zero-energy state this way.\,.\,? (21:45) .\,.\,Hm, or would it be an energy 1 state.\,.\,? .\,.\,Hm, it would be an energy 1 state, and the absorption transition is actually what (maybe) makes it work.\,. (21:53)

.\,.\,(22:00) Hm, it also just occurred to me that $F_1 = \emptyset$, so.\,. Hm, that makes $\chi^{(0)}$ close to an eigenstate already, but maybe it's actually better to just take a non-vanishing $\chi^{(1)}$ anyway.\,.

.\,.\,I will think more about this tomorrow. Could be pretty wild if this turned out to be possible, and thus turned out to be a way to solve my problem from the vacuum paper.\,. (22:05)

.\,.\,Another potentially pretty cool potential alternative might be if we could just change the domain while $\delta k \to 0$ such that $\chi^{(0)}$ tends more and more to an energy 0 eigenstate (with $\varepsilon$ going to 0 together with $\delta k$), namely by using larger and larger radii for the $E$-sets.\,. (22:12)

(12.05.24, 11:50) The last idea from yesterday seems pretty awesome. *(to say the least.(!)) And I also have to keep the other idea (of constructing energy-1 states) in mind as well, if it turns out not to be so easy to let the radii of the $E$-sets grow.

There is the problem of the ultraviolet cutoff, which doesn't work well with my solution from my SA paper. One could imagine never introducing the ultraviolet cutoff, but we need it anyway at some point, so no. Then there's my `solution with $E$-sets of constant radii' idea. If that works, and I definitely think it could, then that could be the solution to the problem: Lift the ultraviolet cutoff more and more while $\delta k$ tends towards zero, such that the ``constant radii'' (constant with respect to the photon number, i.e.) can increase while the UV cutoff increases. (11:56) .\,.\,I guess I then ought to work on my `$E$-sets of constant radii' solution, but I'm not sure if I want to devote a lot of time to that now.\,.

%(13:05) Hm, "result in state that is no longer normalizable." fra mit vacuum paper skal muligvis rettes til, at tilstanden *(altså \hat H_\Lambda \psi) bare ikke konvergerer til \hat H \psi, som den skal.. ..Hm, men ja, jeg tror, jeg har ret i, at cutoff'et rigtigt nok er validt, hvis man bruger min "constant radii for the $E$-sets"-løsning, og hvis denne altså rigtignok fungerer.. ..(13:23) Hm, pointen med den (mulige) løsning er vel, at jeg så også skal udsende fotoner på symmetriseret vis, når jeg definerer hvert $\chi_{n+2}$.. ..Ja, det må da kunne fungere.. Og så skal jeg bare lige gennemgå, om beviset stadig vil holde.. ..(13:31) Hm, men F er jo allerede symmetriseret, så det er vil bare at udnytte dette med en bedre W-mængde, hvorved vi så kan ændre p_n til noget, der ikke er n-afhængigt.. måske..
%...(13:55) For symmetri-argumentet skal vi bare have, at sætningnen der følger lign. (88) i mit SA paper (v4.1) stadig passer, og at exp(p_N)-udtrykket således stadig vinder over \sum\sqrt(N i_l)-udtrykket.. ..Hm vent, min idé kræver vel netop, at p_n bare er en konstant, både uafhængig af n \emph{og} af k..?.. ..Tja, nej, det kan ikke lade sig gøre.. ..Vil lige gå en tur og tænke videre over det hele... (14:05)

(15:37) If I was right about the possibility to approximate $\hat H \psi$ with $\hat H_\Lambda \psi$ via adding that ``tail'' that I talked about in Appendix B of the vacuum paper (v2), then I think I only need to be able to change my solution be removing the lower $\exp(p_n(\ldots)/2)$ bound on $E_n$. Now, I know I use in several places that $k_n$ is the largest one of the $k$'s, but\ldots\ 

(18:50) I use it to show that $W \subset V$, but in principle I could just use the same $E_n$ for the $\chi$-formulas, and thus define $W$ the same way, even if I redefine $V$ by removing the lower bound on $E_n$ (i.e.\ the lower bound on $k_n$ in $E_n$'s definition).\,. .\,.\,Hm, but it ruins Eq.\ (24), which I use several times.\,.
\ldots Hm, removing the lower bound on $E_n$ does not work.\,. .\,.\,Oh, I just had another idea. What about, instead of using areas om the parameter space, we try to use vectors for our $E$-sets that are orthogonal to the emission vectors?\,. And we could define this for each $X_{n, \mathbf i}$ area individually.\,.\,!\,.\,. (19:15) .\,.\,Oops, no, 'cause then they also don't work for canceling the ``productions''/emissions one level below.\,. .\,.\,(19:29) But still, then you could maybe recognize vectors that are emitted states of emitted states as the members $E_{n+2}$, and so on.\,. Hm, interesting.\,. .\,.\,That actually sounds very interesting.\,. (19:31) .\,.\,That almost ought to work.\,.\,!\,.\,. (19:34)

\ldots (19:54) Wait, how does it help exactly, 'cause I still need to define $E_n$ kinda the same way, or what?\,.\,. (Let me see.\,.) .\,.\,Hm, maybe not, but then the solution actually kinda seems to be too good.\,. maybe.\,. I don't know.\,. .\,.\,Oh right, yeah, I still have to define an $E_n$ area (/volume) as well.\,. .\,.\,But then hopefully it can be a ball in this case, exactly like I want.\,. .\,.\,Hm, maybe it can. I'm not sure, but I think I might be onto something (really great) here. I'll think more about it.\,. (20:13)

(13.05.24, 10:00) Ah, so close, but I actually have I hard time believing I was right about my `constant radii solution' idea.\,. .\,.\,I have a hard time believing that the fact that a larger cutoff can mean a smaller correction ``tail'' can make up for the fact that you also get all the more emission/creation transitions (/``productions'') that you need to cancel. (10:04) .\,.\,Hm, unless.\,.

%(10:30) Okay, lad os se, hvis vi nu bruger en k^{-2} forskrift til halerne, så bliver det \sim\int k^{-1/2}dk \sim k^{1/2} for transitionen nedad.. ..Så \Lambda^{1/2}. Vi kan så gange med \Lambda^{-1/2}. Og det giver et norm-kvadrat på \sim\int k^{-2}dk \sim k^{-1} ganget med \Lambda^{-1/2}, hvilket giver.. Hm.. ..Noget der er begrænset af en værdi proportionel med \Lambda^{-1/2}..? ..Hov, ganget med \Lambda^{-1}, rettere, så \Lambda^{-1}..(?) (10:38) ..Hm, og hvis jeg har ret i det, så er det nok mere optimalt at bruge en forskrift lige under k^{-3/2}, altså k^{-3/2 - \epsilon}, således at vi får \Lambda^{-2 + \epsilon^2}.. (10:42) ..Hm, men det skal vel så i så fald slå \sim\int k^{3/2} dk \sim \Lambda^{5/2}, og det kan det ikke.. (10:46) ..Lad mig tænke lidt over, om der kan være andre muligheder for at løse UV-cutoff-problemet, og så kan jeg regne mere på det her senere..
%...(11:12) Jeg overvejer nu en forskrift over -3/2, f.eks. 0. Lad os se (min hjerne skal hjælpes ligenu ved at gøre det her på tasterne..), så får vi \sim\int k^{3/2}dk \sim \Lambda^{5/2}. Og norm-kvadratet bliver \sim\int k^{2}dk \sim \Lambda^{3} ganget med \Lambda^{-5}, hvis jeg ikke tænker galt her, hvilket giver \Lambda^{-2}.. ..Som så heller ikke slår \Lambda^{5/2}.. ..(11:19) Hm, men med en forskift på k^{1}, så ville vi vel med disse udregninger få \Lambda^{7/2} og \Lambda^{4}, hvorved vi så får \Lambda^{4-7} = \Lambda^{-3}. Og dette kan slå \Lambda^{5/2}. Men jeg er nu ret sikker på, at jeg må have gjort noget galt eller overset noget her.. (11:21) ...(11:41) Ah vent, et bliver da også \Lambda^{5} og ikke \Lambda^{4}. And there we are, så jeg kan nok aldrig komme over \Lambda^{-2}, og dermed aldrig slå \Lambda^{-5/2}..

(11:46) Yeah, no, I don't think it works.\,.

\ldots (11:57) It seems that we would need to show that a given $\varepsilon$-almost eigenstate has some bound in terms of how far out it has its cancellation tails, i.e.\ such that when the cutoff grows, then the inner radius of what emission transitions we even need to deal with also grows at some (sufficiently high) rate. And I don't like the prospects *(chances, i.e./rather) for being able to do this.\,.

.\,.\,So close yet so far.\,. But I guess that, even if I don't solve this problem, I can then at least still correct and edit my vacuum paper to just not assume the UV cutoff, and then possibly get through that way, without it, can't I.\,.\,? (12:06) .\,.\,Hm, but is that really worth doing, when the UV cutoff is kinda needed anyway, or shouldn't we rather focus on the UV cutoff first.\,. Hm.\,. .\,.\,Hm, maybe it could at least be possible to show that the particular energy-0 $\varepsilon$-almost eigenstate stat I have in mind would still work with a UV cutoff (that is let tend towards $\infty$).\,. Hm, which maybe is the most important thing in a way.\,. kind of.\,. Hm.\,. (12:13) .\,.\,Well, that is what I essentially need for my vacuum paper, yeah, so in that sense it is the most important thing for sure.\,. (12:14) .\,.\,And then even if my SA solution (even my new modified one from yesterday evening) doesn't work somehow (possibly because it might fail with the UV cutoff (hopefully not, but who knows for sure)), and the actual solution is some kind of Nelson renormalization (that is somehow made to work for varying fermion numbers), then my grand vacuum discovery would still work nonetheless.\,!\,.\,. .\,.\,(12:21) Hm, but let me actually not get my hopes up that my particular energy-0 $\varepsilon$-almost eigenstate will work with a UV cutoff, or at least that this will be a modest task to show.\,. (12:22)

%(12:52) Vent, er jeg sikker på, at det er \Lambda^{5/2}, det skal slå? Giver det mening at tænke i 'norm-kvadrat pr. amplitude'?.. ..For når amplituden af emissionen fordobles, så vil norm-kvadratet jo ikke fordobles, min firedobles. Og derfor, når det halveres, så vil det kvart-eres.. Hm, så havde jeg ret i, at der kunne være en mulig løsning her alligvel..? ..Tror jeg vil gå en tur og så tænke over det... (12:57) ..(13:08) Okay, inden jeg går ud ad døren, et hurtigt "overslag" siger mig jo, at det så bliver \Lambda^2, vi skal "slå." Og hvis man så lige netop bruger en overskrift, hvor vi får \int k^1 dk på den gode side, så vi får en faktor ln k i stedet for en faktor k^0 på den gode side.. ..Det ville jo så være k^{-5/2}.. ..Som bliver til \int k^{-5+2=-3} dk \sim \Lambda^{-2} (hvilket giver mening, btw, for vi kan lade den indre radius stige med \Lambda.. tror jeg.. *(nej, så ikke alligvel..)) til den anden side, så det ville jo netop muligvis lige præcis gå op sådan her.. Okay, lad mig gå og tænke videre over det... (13:14)

%... (14:23) Jeg fik ikke taget solcreme på, så gik bare en kort tur (og så på haletudser *(tudse- og frø-)---og blishøns og lappedykkere, der henholdsvis sloges og dansede), og fik faktisk ingen gang rigtigt tænkt videre på fysikken/matematikken her. Så det vil jeg bare prøve at gøre nu..:)

(14:50) I actually think it might work after all.\,:) I'm thinking a bit slowly right now, but let's see if `thinking on the keys' help a bit. The argument is that if we use ``tails'' that goes like $k^{-5/2}$ to cancel the emissions that no longer get canceled due to the cutoff, then for any local part of the emission with amplitude $\alpha$, the amplitude of this add-on tail, $\beta$ will need to solve $\alpha = \beta \int_c^\Lambda 4\pi k^2 k^{-1/2} k^{-5/2} dk \propto \ln \Lambda - \ln c$ for some (more or less) constant $c$. So $\beta \sim \alpha / \ln \Lambda$. This means that the norm squared of the tail is $\sim \alpha^2 \ln^{-2}(\Lambda) \int_c^\Lambda 4\pi k^2 k^{-5} dk \sim \alpha^2 \ln^{-2}(\Lambda) (c^2 - \Lambda^{-2})$.\,. Hm, maybe this might not work after all, at least with a constant $c$. Let's see.\,. (15:08) .\,.\,Can I use $\Lambda / 2$ instead of $c$, which was what I kinda thought?\,.\,. .\,.\,Well, yes and no, 'cause then I lose the $\beta \sim \alpha / \ln \Lambda$ result.\,. .\,.\,How about some $\Lambda^a$, $a<1$, instead.\,.\,? .\,.\,I guess not.\,. (15:15) .\,.\,Wait what about something like $a = -1$?\,.\,. (15:16) .\,.\,No, that's worse.\,. (15:17) .\,.\,(15:22) Hm, but couldn't we exploit the logarithm the other way around if we use $\Lambda / 2$ instead of $c$? Let's see.\,. .\,.\,This would mean a add-on tail that goes like $k^{-3/2}$ instead.\,. .\,.\,We would then get $\alpha = \beta \int_{\Lambda/2}^\Lambda 4\pi k^2 k^{-1/2} k^{-3/2} dk \propto \Lambda$. So $\beta \sim \alpha / \Lambda$. And the norm square of the tail would be
$\sim \alpha^2 \Lambda^{-2} \int_{\Lambda/2}^\Lambda 4\pi k^2 k^{-3} dk \sim \alpha^2 \Lambda^{-2} (\ln \Lambda - \ln \Lambda + \ln 2)$.\,. .\,.\,Which doesn't work either.\,. .\,.\,And how about with $\Lambda^a$ instead?\,.\,. .\,.\,Nah.\,. And $\Lambda - c$?\,.\,. .\,.\,would also not work.\,. .\,.\,And what about just $c$ once again?\,.\,. .\,.\,That makes it worse.\,. (15:36)

.\,.\,(15:42) Ah, but maybe we don't need to \emph{beat} $\Lambda^{2}$ (coming from square-integrating $1/\sqrt{\mathbf k}$ at the end). Maybe we just need to \emph{meet} it.\,.\,! For even though we don't know how fast the volume grows for which the $\varepsilon$-almost state handles canceling its own emissions, without any add-on tail, we know that the volume will grow at \emph{some} rate when $\Lambda \to \infty$. And that might be exactly the fact that gets us over the finish line, meaning that we only need to meet the $\Lambda^{2}$, and then said fact will do the rest to take us over. (15:47)

.\,.\,Well, unless this logic doesn't work, and this fact doesn't actually help the fact the we need to \emph{beat} $\Lambda^2$.\,. I will think about this.\,. \ldots\ (18:16) No, it doesn't work. It's really irritating to be \emph{so} close, yet so far away.\,.

(19:19) Let me mention: When I walked earlier, I had the idea that maybe one could make a $p$-dependent cutoff on $k$ for the fermions. But I'm not sure it's a good idea. But now I've had another idea. I'm thinking about whether a cutoff on $k$ for the interactions, and only on $k$, could mean that the Coulomb interaction could still be used to cancel the emission transitions / ``productions,'' i.e.\  since it also includes an integral over a $\mathbf p$ parameter that is not cut-off.\,. .\,.\,And for the vacuum transitions, you obviously also have $\mathbf p$-parameters.\,. .\,.\,(19:35) Okay, this might actually be something. For it is reasonable to only put the cutoff on the $\mathbf k$-parameters. And yeah, this difference that some of the parameters aren't cut-off could make the.\,. difference.\,.

I'm not sure I have the energy to start to do all those calculations this evening, though. But I'll see.\,. (19:37)

\ldots (19:58) Oh, the Coulomb doesn't include any photon $\mathbf k$ parameter: The $k$ that I use has nothing to do with the photons. So that's actually pretty promising.\,. .\,.\,(20:03) Well, I guess that's not completely true, is it, as the interaction comes from the $A^\mu$ field, still?\,.\,. .\,.\,Yeah, no, the cutoff should then also be on the $\mathbf k$ in the Coulomb interaction.\,. (20:10) .\,.\,Unless we were to show that it can sent to infinity before the Dirac interaction cutoff, of course. But who knows, maybe the whole thing could work with having a cutoff on all $\mathbf k$'s for the interactions, and not the $\mathbf p$'s.\,.

(20:59) Okay, it does seem that one would be able to cancel the Dirac emissions with a Coulomb-interaction-based add-on tail, only requiring that the $\mathbf p$ cutoff, if any, is much larger than the $\mathbf k$ cutoff. So that's pretty exciting.\,!\,:) However, this of course immediately begs the question of how well / if this would work for the Coulomb emissions.\,. .\,.\,And it also opens up the problem of showing self-adjointness when the Coulomb interaction is included as well.\,. For all, or at least some of the various $\hat H$'s in play in my vacuum paper (i.e.\ the initial one, the final one, and the vacuum Hamiltonian).\,. So yeah, that's obviously a lot. But still, very exciting that specific problem that I've worked on today seem to be solvable if you utilize the Coulomb interaction for the ``add-on tail''.\,. (21:06)

(22:11) But this applies only to the Hamiltonian(s) after the DSR, so what about before?\,.\,. .\,.\,There the Coulomb interaction can't really be used the same way.\,. .\,.\,But again, the really important thing is what happens for the Hamiltonian after the DSR, and the fact that the vacuum particles decouples from the physical ones.\,.
%*[I wonder if the negative free energy could help somehow in the pre-DSR case, but I'm not gonna look further into this at the moment.. (11:28, 14.05.24)]

(14.05.24, 9:56) I forgot about the free energy, which makes the problem harder. Maybe there could still be a way if you were able to exploit the integration over $k$ as well somehow, and then make $\Lambda_{k}$ grow at the same rate as $\Lambda_{p}$. But I don't think I will look into this for the moment. For the thing is: The vacuum Hamiltonian does not have a free energy, or rather, it can be made insignificant be letting $\delta k \to 0$ (for any fixed $\Lambda$). So the idea might still work as is for $\hat H_{vac}$. And I feel like: if I can solve that problem, then my job is done. Then whether my SA solution is the one for the final, or for the initial (before the DSR), Hamiltonian doesn't really matter that much: Physicists believes that it can be done, regardless. But by showing that the energy-0 almost eigenstate that I have in mind is in the domain of $\hat H_{vac}$ and, not least, is also an energy-0 $\varepsilon$-almost eigenstate for the cutoff versions (when small(er and smaller) add-on tails are added), such that $\varepsilon \to 0$ when $\Lambda \to \infty$, then I can complete my vacuum paper and show that the vacuum particles doesn't interfere with the physical (localized, not spread out infinitely) particles.

I think I will calculate on the problem just enough to see if this solution is plausible. And then I will make the rather simple correction to my vacuum paper, if so, where I just assume that the $\varepsilon$-almost eigenstate does not have increasing (and diverging) particle number when $\varepsilon \to 0$, and where I simply talk about the possibility of showing this in an appendix, perhaps replacing the `constant radii idea' one. (10:09)

(11:21) Well, the fact that the energy can be neglected makes the problem a lot easier/more likely to be solvable, doesn't it.\,. (For my SA paper, I had to take great care to make the $\chi$'s have finite free energy, even though I didn't add the free energy yet, but this is not a requirement here.\,.)

(11:32) Hm, when there's an UV cutoff, $\hat H_\mathrm{vac}$ is bounded, which means that the energy-0 almost eigenstate will automatically be in the domain if it exists. And we don't actually ever need to consider the non-cut-off version of $\hat H_\mathrm{vac}$ for this problem. We just need to show that there is a family of $\varepsilon$-almost eigenvectors (with eigenvalue 0) for the family of cut-off $\hat H_\mathrm{vac, \Lambda}$'s such that $\varepsilon \to 0$ when $\Lambda \to \infty$ while the particle number of the $\varepsilon$-almost eigenvectors does not diverge when $\Lambda \to \infty$, but remains pretty much constant (expectation-wise or as a distribution). So this perhaps makes the problem even easier.\,. (7, 9, 13).\,. (11:38) .\,.\,(11:43) In particular, I don't need to use my new idea for changing the domain (where the states of $F_n$ are also divided up into $X_{n, \mathbf i}$-areas, and only states *(localized over these $X_{n, \mathbf i}$-areas) that are parallel to states that are formed from having a even amount of emissions (not odd) are included in $F_n$, and the rest are removed), which I would most likely need otherwise *(as I believe that this can allow us to remove the lower bound in the formula for $E_n$).\,.
.\,.\,But when there's a cutoff, all states are valid members of the domain, and then we don't even need to consider all this.\,. (11:51) .\,.\,Oh, and even when there are a very small free energy (going as $\delta k^{3/2}$---which can be sent towards 0 independently of $\Lambda$), then the domain is just.\,. Hm, well, maybe this might matter a little bit.\,. well, it shouldn't, but it might make the proof a little more complicated. But can't I just send $\delta k$ all the way to 0 for each $\Lambda$?\,.\,. .\,.\,Hm, maybe not, actually.\,. \ldots (12:08) Okay, $\delta k$ can be sent to 0 independently of $\Lambda$. And since I already need the particle number to be bounded/limited, this doesn't therefore change the problem, i.e.\ that the free energy going as $\delta k^{3/2}$ is there as well.\,.

.\,.\,Hold on, the constant radii idea is still important for this all to work: I need the add-on tail to also be able to cancel itself, and that requires.\,. maybe.\,. something like that constant radii idea.\,. (12:13) \ldots (12:31) Yeah, the solution to $\hat H_{\mathrm{vac}, \Lambda}$ must be.\,. Hm, can it be something like $\sum_{n=0,2,4,\ldots} \alpha_n (\hat H_\mathrm{vac}^+)^n \ket{\,}$.\,.\,? .\,.\,Where $\alpha_n$ can then hopefully decrease quite rapidly when $\Lambda$ increases, such that.\,. .\,.\,Ah, such that the particle number expectation/distribution converges.\,. (12:40)

.\,.\,(12:44) Ah, and I think I've already argued that the annihilations where the annihilation operators do not all hit creation operators of the same triple or quartet are negligible in my earlier notes, so if those arguments hold, it might not be too hard of a problem.\,. .\,.\,(Exciting.\,.\,:)) (12:47)

(13:45) Oh!\,.\,. Maybe, just maybe, the Coulomb interaction is what makes it doable, when one also adds an infrared cutoff (which I guess is just given by $\delta k$, but let's see.\,.).\,.\,! .\,.\,The idea is that if you do the same $\beta$--$\alpha$ calculation like I did before to see how the amplitudes of the possible solution goes when $\Lambda$ approaches infinity, then for the $p$ cutoff, you get.\,. Well, if we look at the Coulomb interaction, you get that $\beta$ can go as $\Lambda_{p}^{3 \times 2} \alpha$. And when you then calculate its norm squared, you also get.\,. Hm, let me see again.\,. .\,.\,Oh, right you get $\Lambda_{p}^{3 \times 4}$, which matches the $\Lambda_{p}^{3 \times 2}$ once you have also squared the latter. But then when you look at the $\mathbf k$ parameter, you get that $\beta \sim k_{min}^{-1} \alpha$, if we take $k_{min}$ to be the infrared cutoff. But when you then compute the norm square, you then you are not integrating $k^{-4}$ like before, but rather you are integrating two separate $k^{-2}$'s over two separate $\mathbf k$-parameters. And that gives $(\ln \Lambda_{k} - \ln k_{min})^2$.\,.\,!\,.\,. .\,.\,Oh wait, damn, I guess this does beat $k_{min}^{-2}$, and not the other way around.\,.\,:\textbackslash\ (13:59) .\,.\,Oh, wait, it gives $\sim \Lambda_{k}^2$, not $(\ln \Lambda_{k} - \ln k_{min})^2$.\,.\,! (14:03) .\,.\,Does that mean that if I only let the infrared cutoff decrease more rapidly than the UV cutoff (for $k$ in particular), then it can work?\,.\,.(!\,.\,.) (14:04) .\,.\,Very interesting.\,.\,! (14:06) .\,.\,It seems legit.\,. And I should be able to then also use the Coulomb interaction, exploiting the infrared part, to cancel the Dirac triple productions.\,. So it kinda seems like I might have the solution here.\,.\,:)\,.\,. (14:15)

\ldots (16:11) Oh. Isn't the picture actually completely different for $\hat H_{\mathrm{vac}}$, compared to my SA paper, since the tail doesn't produce new emissions: There is only one emission from the bare vacuum that needs to be canceled.\,. (16:13) .\,.\,Hm, but then again, 
$\sum_{n=0,2,4,\ldots} \alpha_n (\hat H_\mathrm{vac}^+)^n \ket{\,}$ does kinda seem to make sense (perhaps just with $\hat H_\mathrm{C, vac}^+$ instead, but I'll see.\,.).\,. .\,.\,Yeah, I think so, so maybe it's not really that different.\,. (16:17)

.\,.\,So yeah, it seems that the solution could just be $\sum_{n=0,2,4,\ldots} \alpha_n (\hat H_{\mathrm{C, vac}, k_\mathrm{min}, \Lambda_{k}'}^+)^n \ket{\,}$, where $\Lambda_{k}' \leq \Lambda_{k}$ can actually just taken to be a constant, independent of $\Lambda_{k}$, and it might even (perhaps better) be decreasing as $\Lambda_{k}$ increases.\,.(!\,.\,.) (16:23) .\,.\,Or rather as $k_\mathrm{min}$ decreases, anyway.\,. .\,.\,We should also by the way be able to choose a $k_\mathrm{min} > \delta k$ (and even $\gg$), that we are then still able to send to 0 as $\delta k$ goes to 0.\,. (16:27) .\,.\,Oh wait, I guess it should actually look like $\sum_{n=0}^\infty \alpha_n (\hat H_{\mathrm{vac}, k_\mathrm{min}, \Lambda_{k}}^+ \hat H_{\mathrm{C, vac}, k_\mathrm{min}, \Lambda_{k}'}^+)^n \ket{\,}$ instead.\,. (16:30) .\,.\,Or maybe like $\sum_{n=0}^\infty \alpha_n (\hat H_{\mathrm{vac}, k_\mathrm{min}', \Lambda_{k}}^+ \hat H_{\mathrm{C, vac}, k_\mathrm{min}, \Lambda_{k}'}^+)^n \ket{\,}$, where $k_\mathrm{min}'$ can be taken to be larger than $k_\mathrm{min}$ by some factor/formula.\,. (16:32)

.\,.\,Hm, how about trying to get a second version of my vacuum paper done, where I just correct the mistakes, and then mention this possible solution in an appendix, perhaps replacing the `constant radii' one. And then when I get back from Norway (I'm taking a vacation), I can then see if I want to pull said appendix in as a section, or what.\,. Hm.\,. .\,.\,I think this might be a good idea. It's good to finally get those mistakes fixed, sooner rather than later, and especially now that there are some professors (Oliver Matte and Fumio Hiroshima) that might actually look at it (as opposed to before where no one really saw it). Yeah, I think I've convinced myself.\,. (16:47) .\,.\,I'll make it a small section at the end, rather than an appendix.\,.

\ldots\ (19:02) For the final $\hat H_{QED}$ we also have the one-to-three particle Coulomb interaction term, which, as I see it, will also have that infrared behavior that can be exploited. So maybe by focusing the construction of $E_n$ and $F_n$ around this infrared divergence instead of around the UV divergence as I did in my SA paper, one might be able to achieve find a domain where one can make UV (and IR) cutoffs and still show that the dynamics converges to the non-cut operator when the cutoffs are gradually lifted. That definitely seems like a possibility.\,:) I'm pretty excited about that (naturally). And the thing is, if we can indeed show that the dynamics converges for the DSR'ed Hamiltonian, then we might not need to show the same for the initial, pre-DSR Hamiltonian, namely since the dynamics of one should be derivable from the other.\,.\,:) So this could potentially complete the task.\,.\,!\,.\,. (19:09)

.\,.\,I'm really excited about this. And again: Even if my SA solution (in any of its versions, including the potential future one focused around this IR divergence) doesn't do it, it's still something that we ought to expect being possible. So regardless, my vacuum result (that the ``vacuum particles'' decouple) will still hold true. And that is truly a fantastic result in and of itself. (19:13)

%(15.05.24, 10:41) Wait, when we let $\delta k \to 0$ while keeping the cutoffs constant, do we even need this new (quite fantastic, it seems) infrared divergence exploit solution.\,.\,?? Why can't we just.\,. Wow, am I crazy.\,. Why on earth can't we just let $\delta k \to 0$ first, and then use a very simple argument that says.\,. Oh.\,. Because we might expect the vacuum to fill up until the Pauli exclusion limit, of course.\,. Okay.\,. (10:44) %Let me just out-comment this.. ..Yeah, no, that logic (that idea) that motivated the whole thing for me just didn't really hold..

%(12:52) Hm, I wonder if I should \emph{just} let the IR cutoff follow \delta k, and let \Lambda and n_{max} be independent.. I guess that's the preagmatic thing to do.. ..Hm, how sure am I that there's not some IR divergence problem in my argument Section \ref{sect_how_vac_decouples} argument?.. ..Hm, but does it matter..

%... (14:12) I should of course also utilize the fact that \ket{\phi} appoaches, not just any state, but \ket{\,} in \mathbf{H}_{vac}. That will make Section \ref{sect_how_vac_decouples} even easier, and actually kinda trivial.. But now I can't help but worry a little bit about this "infrared exploit".. ..Is there not a chance that it might ruin the final \hat H_{QED}'s possibility of having a valid UV cutoff, rather than \emph{make} it possible?... (14:16) ..Because once the cutoffs are there, the domain is all of the Hilbert space, which should also allow us to do the same shenanigans, just with the one-to-three (fermion) particle Coulomb interaction term instead.. (14:20) ..Damn, this does seem like some conundrum.. (14:23)

``[\ldots] But now I can't help but worry a little bit about this "infrared exploit".. ..Is there not a chance that it might ruin the final $\hat H_{QED}$'s possibility of having a valid UV cutoff, rather than \emph{make} it possible?... (14:16) ..Because once the cutoffs are there, the domain is all of the Hilbert space, which should also allow us to do the same shenanigans, just with the one-to-three (fermion) particle Coulomb interaction term instead.. (14:20) ..Damn, this does seem like some conundrum.. (14:23 [15.05.24])''

.\,.\,(14:32, 15.05.24) Maybe the answer is that the UV cutoff just has to go to infinity before the.\,. Hm.\,. .\,.\,Ah vent, er der ikke bare noget, jeg har glemt!\,?\,.\,. .\,.\,Did I just forget to square the amplitude somewhere?\,.\,. .\,.\,Ah yes, I think that was simply it. When I wrote ``but rather you are integrating two separate $k^{-2}$'s over two separate $\mathbf k$-parameters'' above, it should actually have been `but rather you are integrating two separate $k^{-4}$'s over two separate $\mathbf k$-parameters'.\,. .\,.\,Okay, so all that excitement was perhaps over nothing (as is often the case, admittedly.\,.).\,. (14:39)

.\,.\,So I seem to back to where I was yesterday morning. I did read in my notes above from March (I believe) that I thought back then I had a solution where I used the free energy somehow to help cancel the no-longer-canceled productions, but when I think about it now, it does seem a bit unlikely.\,. (14:43)

.\,.\,Wait, what if I go back to me very first idea, which I wrote about in the paragraph above of 14:50, 13/05, and then simply use $1/\Lambda$ rather than $c$.\,.\,! Oh, but I guess I still have that overlooking-the-free-energy problem, then.\,. But still crazy if I overlooked that solution, even though it was starring me in the face all that time.\,. (14:50) (Ha.) .\,.\,Ha, yeah, so it seems like that idea would work after all.\,. Oh! And I don't have the free energy for $\hat H_\mathrm{vac}$!\,.\,. (14:52) .\,.\,Oh, and maybe I can also find a way to still exploit the same infrared divergence, but not in that crazy manner like it seemed before, but simply exploit it in the fact that the free energy is then not a problem in that part of the Hamiltonian!\,.\,. (14:54) .\,.\,Ha, so typical that almost the second (sometimes a bit more, but still) that I've seemed to reach a dead end is where I all of a sudden get a load of new ideas.\,.\,\textasciicircum\textasciicircum\,.\,.

.\,.\,Hm, but if that solution had the tail go as $k^{-5/2}$, that would already be a solution that uses the infrared region, wouldn't it.\,. .\,.\,Hm, yeah, so the free energy contribution should actually be less problematic than simply getting the norm square to decrease in the continuum limit (when the cutoffs are also lifted).\,. (15:04) .\,.\,So maybe I have the solution now after all, and one that I can even perhaps use for all three cases: $\hat H_{init}$, $\hat H_{vac}$, and $\hat H_{QED}$.\,. (15:07)

.\,.\,(15:16) Hm, it's kinda crazy that I have overlooked the potential to use the infrared region for the cancellation tails all this time.\,.\,!\,.\,. .\,.\,Hm, that \emph{is} actually crazy.\,. .\,.\,(15:21) Hm, and it's not simply that I'm wrong now.\,. .\,.\,(15:27) Oh right, no, it doesn't work.\,.

.\,.\,(15:30) Hm, but for the Coulomb interaction.\,. .\,.\,Then that would be $k^{-1}$ instead.\,. .\,.\,So we would have $\beta \sim \alpha / 2\ln(\Lambda)$.\,. .\,.\,Which would give a norm squared of the ``tail'' (the first part of it, at least.\,.) of $\sim \alpha^{2} \ln^{-2}(\Lambda) \int \mathbf{k}^{-2} d\mathbf{k} \int \mathbf{k}^{-1} d\mathbf{k}$.\,. .\,.\,$\alpha^{2} \sim \ln^{-2}(\Lambda) \Lambda^{1} \Lambda^{2}$.\,. .\,.\,Disregarding the $\mathbf{p}$-integrals here.\,. .\,.\,Okay, what am I even calculating exactly here, let me think a bit more it all.\,. (15:42) .\,.\,(15:51) Ah, no, I would need $k^{1}$, not $k^{-1}$.\,. Hm.\,. .\,.\,Hm, that makes it worse.\,. .\,.\,I'm confused.\,. .\,.\,No, yeah, I'm completely confused. Let me take a break.\,. (15:58) .\,.\,(16:00) Oh, I forgot to square, so it would be $\int \mathbf{k}^{-4} d\mathbf{k} \int \mathbf{k}^{-2} d\mathbf{k}$ instead, giving $\Lambda^{1} \Lambda^{1}$ with the infrared cutoff, which is not what we want.\,.

(16:33) Ah, there's actually the Cauchy--Schwarz inequality that means that it is optimal to chose the same formula as the interaction (so e.g.\ $k^{-1/2}$ is optimal for the Dirac interaction), like you would expect. And the inequality means that what I'm trying to achieve here is not possible.\,.

.\,.\,(16:39) However, if there is a way to make a domain like in my SA paper, but where an infrared divergence is utilized instead for the cancellation tails, rather than an ultraviolet one, then the whole problem might still be solvable (both for showing that the `UV cutoffs are valid' and for completing the argument of my vacuum paper).\,. (16:41)

.\,.\,(And for the solution to the vacuum problem, I'm thinking about constructing that energy-0 almost eigenstate, which becomes more and more an eigenstate when the cutoffs are lifted, which I had the idea for this Saturday.\,.)

\ldots\ (19:18) These \emph{is} an exploitable infrared divergence. If you thus have an initial state that you want to cancel and you work on it with a Coulomb creation interaction term, either the zero-to-four term for $\hat H_{vac}$ or the one-to-three term for $\hat H_{QED}$, only where instead of the transition amplitude of $1/\mathbf k^2$ you use $1/\mathbf k^a$ with $a \in [1, 3/2)$, then you have a square-integrable state, but this state nonetheless diverges when you work on it with the corresponding annihilation Coulomb interaction term (now with the true $1/\mathbf k^2$ dependence). So there we are! We might therefore be able to make infrared ``cancellation tails,'' using the DSR'ed Coulomb interaction.

Now, this seem to have the advantage that when the infrared cutoff shrinks, it doesn't increase the ``productions''/emissions (when the UV cutoff is still fixed). It only increases the potential for the annihilation operator to cancel things.

It seems that my $\sum_{n=0}^\infty \alpha_n (\hat H_{\kappa', \Lambda}^+ \hat H_{\mathrm{C}, \kappa, \Lambda'}^+)^n \ket{\,}$ idea would still work when $\kappa$ is allowed to go to zero independently of $\Lambda$.\,. But this also worries me a bit for some reason.\,. (19:29) .\,.\,I want to celebrate the confirmation of this infrared potential *(as in `possibility'), but I fear that it's a bit to early, 7, 9, 13.\,.

\ldots (19:54) Oh, maybe the regular Coulomb interaction term can do something about this (about this thing that actually seems \emph{too} powerful.\,.).\,.

.\,.\,It seems a bit hard for me to calculate, so it might take me some time, but I hope that this is indeed exactly the thing that cures this seeming \emph{too} powerful trick for $\hat H_{QED}$, while not curing it for $\hat H_{vac}$ which would mean that we are still free to utilize it there. Of course, I don't know for sure that this trick/thing is ``too powerful,'' but it does kinda seem so. So it would be a great relief if the regular (two-to-two particle) Coulomb interaction prevents it.\,. (20:07) .\,.\,(But ideally it would cure it only just enough that we are nonetheless still able to use it to show that the `cutoffs are valid' for $\hat H_{QED}$.\,.)

(21:20) The good thing is I can always still complete my vacuum paper, regardless of this. For the assumption that one can impose such cutoffs is standard for QFT. So that's obviously great. But it would be nice to get an idea of how/whether it might be possible to show that $\hat H_{QED}$ converges dynamics-wise when cutoffs are imposed and then subsequently lifted.\,.

(21:45) Ooh, when I think about it, there could indeed be a divergence when you let the regular Coulomb term act on that $1/\mathbf k^a$, $a \in [1, 3/2)$, solution.\,.\,! .\,.\,I think there will be.\,. .\,.\,And that seems to the ruin the potential of that kind of ``cancellation tail'' (which I actually feel happy about).\,.\,! (21:49) .\,.\,For yeah, who knows whether my SA solution is the right one for $\hat H_{QED}$: maybe it needs a Nelson renormalization instead. And if a Nelson renormalization works for it, then you are still able to make the same cutoffs for $\hat H_{vac}$, which I believe that I can now indeed show, with this new infrared divergence idea, has that energy-0 eigenstate sequence, that even approaches $\ket{\,}$, and with which I can correct and complete my vacuum paper argument.\,.\,!\,! (21:53)

(16.05.24, 14:22) New realizations and ideas.\,! First of all, I had the realization earlier that the creation part of the Coulomb interaction terms that we have been talking about also (of course) have an infrared divergence. And that's obviously really important. It means that the infrared ``cancellation tails'' have the same problems to overcome as the UV ones, like what I dealt with (for the no-cutoff case) in my SA paper.

Later on, on the same walk, I then also had the thought that maybe having multiple divergent interactions together at once in a theory could then help them cancel each other. And I realized something else important, and that is that.\,. well, maybe the fact that more created particles means that more of them can be used for the cancellation tails can help us (as I thought when I had the `constant radii' idea), but on the other hand it also increases the amount on emissions. But then I just had another possibly great realization/idea: I looked at only one fermion in my SA paper. But if we instead start out with some great number, $N$, then these particles can essentially help each other cancel each other's emissions!\,.\,. (14:30)

.\,.\,Oh, and for $\hat H_{vac}$, the created particles don't mean more emissions, so there we \emph{can} utilize the fact that more particles created means that they get more and more power to cancel the emissions (in this case ``emissions'' from the bare vacuum), not to make an almost eigenstate that is close to $\ket{\,}$ necessarily, but still one that can be approximated to arbitrary precision with a particle number cutoff ($n_\mathrm{max}$).\,:) *[(23.05.24, 10:37) Hm, I'm not completely sure about this.\,. I'm not sure that it helps to have more particles when the annihilation terms in question do not have any.\,. creation operators.\,. Hm.\,.]

.\,.\,Now, I cannot say already how/if this could help us show that `cutoffs are valid' for $\hat H_{QED}$. I'll think about this some more. But at least it could still suggest that it could be possible, namely if one guesses that the almost eigenstates of the non-cut-off Hamiltonian will only essentially use cancellation tails that don't generally ``go closer to the given divergence'' (UV or IR) than the emissions that they are there to cancel.\,. (14:39)

.\,.\,And if you were able to construct a domain that ``forbids'' (weakly (like how my SA paper domain ``forbids'' a state from canceling it's own emissions to $F_{m+1}$)) that the cancellation tails ``go closer to the given divergence than the emissions that they are there to cancel,'' then that would make this guess even more plausible.\,. (14:47)

.\,.\,Yeah, it would definitely make it seem likely. For the almost egienvector obviously has to be part of the operator's domain. And if that domain ``punishes'' vectors that does the thing that's ``forbidden,'' yeah, it would definitely suggest that they most stop doing that at some point (or rather do it less and less, of course). And maybe this is even something that can be shown. So maybe the plan is just to construct such a domain for $\hat H_{QED}$. And for $\hat H_{vac}$, just to underline, the solution is to just find one energy-0 almost eigenstate, where $\varepsilon \to 0$ when the cutoffs are lifted, but not one that approaches $\ket{\,}$ (or $\ket{\,}_{vac}$ if you will) necessarily (probably not, in fact). As long as it can be shown to be arbitrarily approximate-able by a state that has an $n_\mathrm{max}$ cutoff, then we're still golden to complete the argument of my vacuum paper (it seems, 7, 9, 13). (15:03)

.\,.\,(15:07) Oh my, this is honestly a great idea.\,.\,(!!) .\,.\,All you need is enough particles.\,. How easy is that.\,.(!\,.\,.)

(16:50) Det ser altså ud til at holde!\,\texttt{:D} And what's more, of course the $\varepsilon$-almost eigenstate is part of the domain, and I do think that this can be used to show that for large---and small---enough cutoffs, the almost eigenstates of the non-cut-off Hamiltonian will also be almost eigenstates of the cut-off one. So it seems that this idea, i.e.\ of utilizing more and more particles as part of the ``cancellation tail'' (instead of e.g.\ just the one that I have in my SA paper (where the photons don't count since the photon-to-two-fermions Dirac interaction is not part of the Hamiltonian there)), will potentially solve both.\,. Task 2 and Task 5 (at least the `completing the DSR' part of Task 5)!\,\texttt{:D} (16:57) .\,.\,How awesome.\,.\,!\,!

%..I'm going on a vacation now to Norway, and when I get back, in almost a week, actually, I will correct and update my vacuum paper!:D

(10:42, 23.05.24) I'm not completely sure that this `use multiple particles for cancellation' idea works for the vacuum Hamiltonian.\,. ``I'm not sure that it helps to have more particles when the annihilation terms in question do not have any.\,. creation operators.\,. Hm.\,.'' .\,.

(13.50) Ah, now I see it. .\,.\,Ah, or rather, I recall what the point was: We don't get more emissions, and we can utilize the fact that there are more and more particles, in particular, in this case, since $\alpha_n$ in $\sum_{n=2,4,\ldots} \alpha_n (\hat H_{\kappa, \Lambda}^+)^n \ket{\,}$ can go as something like $\alpha_{n-1}/n$.\,. .\,.\,which of course makes $\sum_n \alpha_n^2$ convergent.\,. .\,.\,Yeah, this should be right. So it's easy enough for the vacuum as well.

And I still think that is works for the initial and final $\hat H_{QED}$ as well, i.e.\ the thing about `using multiple particles for cancellation.' One might even be able to use an exact similar solution (i.e.\ like $\sum_{n=2,4,\ldots} \alpha_n (\hat H_{\kappa, \Lambda}^+)^n \ket{\,}$), but in any case, I still think it works.\,:) (14:02)

%(20:09) Hm, I wonder if this was exactly my 'constant radii' idea back then.. ..Oh well.. ... (21:58) It probably was, actually.. ..Yes, I definitely think so, actaully..

(24.05.24, 10:06) I've realized that this \emph{was} exactly my `constant radii' idea (from the appendix of my vacuum paper). I it just took me some time to re-realize that $\alpha_n$ would go like $\alpha_{n-2}/n$, and not something like $\alpha_{n-2}/(n - (n - 2)) \propto \alpha_{n-2}$.

I have also just realized that for the $\sum_{n=2,4,\ldots} \alpha_n (\hat H_{\kappa, \Lambda}^+)^n \ket{\,}$ solution, the $\varepsilon$-almost eigenvector will in fact also approach $\ket{\,}$ when the limits of $\kappa$ and $\Lambda$ are approached. Not that it matters, really, but it's still kinda neat. \ldots\ Oh, I'm not sure that it will, but let's see.\,. (14:06)

(18:34) The $n$ in $\alpha_n \sim \alpha_{n-2}/n$ will actually not be there, but will be canceled due to the fact that the vacuum triplets/quartets behave as bosons. But luckily we don't need it. $\alpha_n$ will still decrease exponentially when $\Lambda$ and $\kappa$ are close enough to their respective limits.

(25.05.24, 16:41) It just occurred to me, out on an afternoon walk just now, that the fact that $\ket{\phi_{\delta k}}$ approaches $\ket{\,}$ for $\Lambda \to \infty$ means that the convention of looking at scattering matrices where one starts out and ends with the bare vacuum will actually be \emph{validated} by my discovery.\,.\,! .\,.\,And since the energy of $\ket{\phi_{\delta k}}$ also approaches 0 like $\delta k^3$, we might not even need to remove the vacuum-perturbing terms, even just for the sake of renormalization. This is of course somewhat unfortunate for me: It would be much cooler if my result could help the theory in practice, i.e.\ calculation-wise.\,. But oh well, it's still also definitely a good thing, in a lot of ways, that the conventional way is validated. And it still shows that we at least need to put an end to the bad habit of explaining results from QFT via the vacuum fluctuations, even when these do not actually appear directly in the calculations of the given results. (16:49)

(19:35) Okay, I was wrong about $\ket{\phi_{\delta k}} \to \ket{\,}$, which would be great on its own if not for the fact that I might also very well be wrong about $\|\phi_{\delta k}\| < \infty$. .\,.\,(which is bad, of course.)

(19:51) This is potentially quite bad: It might even set me all the way back to a point where I might think that my countable-basis idea might be as good a bet as this original remove-vacuum-perturbing-terms idea. But now I just had another interesting idea, though.\,. What about coherent wave states?\,.\,. .\,.\,More specifically, is there a chance that we might be able to trade the stableness of $\ket{\phi}$ for a cyclic stableness instead, and make it work still?\,.\,. (19:55)

\ldots I should also consider a $\ket{\phi} = \sum_{n}\alpha_n (\hat A^+)^n \ket{\,}$ solution (where the eigenvalue then does not, perhaps, need to be 0), rather than the $\ket{\phi} = \sum_{n=0, 2, 4, \ldots} \alpha_n (\hat A^+)^n \ket{\,}$ solution (that isn't normalizable, it seems).\,. (20:09) .\,.\,(20:14) Oh, wouldn't that actually work if we add a factor of $1/2$ to the recursive formula of $\alpha_n$ (or $\xi_n$) exactly?\,.\,. .\,.\,No, not quite like that.\,. (20:18) .\,.\,But maybe there could be a solution to this.\,. Hm.\,. Although it does seem unlikely, since we essentially have system of.\,. .\,.\,Oh.\,. .\,.\,Hm, some interesting thoughts.\,. %(Typical; the moment I seem to hit a wall.. (7, 9, 13)..) (20:21)
.\,.\,(20:25) Oh, but since they behave like bosons, we would expect a kind of $\hat H = \hat X$ *($\propto \hat A^- + \hat A^+$) behavior, which \emph{would} mean virtual eigenvectors with infinite particle numbers, I think.\,.

.\,.\,Yeah, I am essentially solving the $(\hat a + \hat a^\dagger) \ket{\phi} = E \ket{\phi}$ problem, which we know has no true eigenvector solution. So I should instead look into how the particle number distributions goes when $\ket{\phi}$ is taken to be a small step function (or Gaussian) with width going towards 0.\,. (20:34)

(26.05.24, 12:02) Oh, maybe I got it now.\,.(!!) My idea is to look at, not an almost eigenvector of the Hamiltonian, but at the ``ground state'' if we treat $\hat A^-$ and $\hat A^+$ as.\,. as the ladder operators (timed a factor $\braket{\,|\hat A^-\hat A^+|\,}^{1/2}$) of a SHO.\,. Hm, and this ``ground state'' would just be the bare vacuum. So we start out in the bare vacuum, expecting to not end in that state as the final state. And then we analyze how many particles are excited over the time $\delta k^{-3/2}t$ by analyzing $\exp(-i (\hat A^- + \hat A^+) \delta k^{-3/2}t)$.\,. Oh, but I actually don't know that this will work, I just realized. Okay, let me think some more. And maybe it's not a good idea to not have the same final state, but I'll see\ldots\ .\,.\,Yeah, no, this might not work.\,. Well, I need to think\ldots\ .\,.\,Hm, no, I feel like I've done this calculation before.\,. (I have, and I \emph{think} I remember that the outcome wasn't in my favor.\,.)

.\,.\,Hm, but wait, Stirling's approximation gives me that

$\ln[\delta k^{-3m/2}]/\ln[m!] \approx\ln[\delta k^{-3/2}] m / (m \ln m - m) = \ln[\delta k^{-3/2}] / (\ln m - 1)$, which suggests that $m$, the particle number can go like $\delta k^{-3/2}$.\,. .\,.\,And looking at $\ln[\delta k^{-3m/2}/\ln(m!)]$, we get the same suggestion.\,. (12:27) .\,.\,That sounds like a very good sign, doesn't it.\,.\,!\,.\,.\,?
.\,.\,Yeah, looking up at my earlier notes above (2--3 pages down from the beginning of the `Decoupled vacuum' section), this was exactly what I was trying to show back then! So maybe if I can just complete this analysis, I might get exactly what I need!\,.\,. And the fact that this solution will show that the bare vacuum state actually evolves quite dramatically in the $\delta k^{-3/3}t$ time span (but not dramatically enough that it prevents the decoupling (hopefully)) just makes it all the more exciting!\,.\,. (12:35)

.\,.\,This is incredible.\,. (12:37)

(13:52) It's really great 'cause if this holds, it still means that the conventional method of QFT is likely valid, at least approximately, but probably with quite high precision, namely since the conventional method also includes a division by the bare-vacuum-to-bare-vacuum transition amplitude as a renormalization. But if this holds (and I think it does!), it means that the only renormalization needed is the removal of the vacuum-perturbing terms and nothing more! This is at least unless the final $\hat H_{QED}$ also requires a Nelson renormalization. However, I do also believe that my newly modified `constant radii' idea---now with the change of $V$ to allow for $E_n$ not to have a lower radius bound, as well as the insight of `utilizing several particles'--- work. And if it does, it contradicts the Nelson renormalization option (I think), and means that no further renormalization is needed. And wouldn't that just be so great and amazing?\,. \texttt{:D} (14:00)

(27.05.24, 9:50) For the Lorentz covariance argument, I simply need to show the same thing about $\hat U_\mathrm{vac}(t)$, not just for $\ket{\,}$ and all other `vacuum states,' but for all $\hat A^\dagger_{\psi}\hat A^\dagger_{\phi}$ states as well. .\,.\,Oh, but do I know that `vacuum states,' i.e.\ states spanned by $((\hat A^\dagger)^n \ket{\,})_{n\in \mathbb{N}_0}$, transform into other such states exactly?\,.\,. And is it necessary to show?\,.\,. Otherwise I would have continued: Then we know that the vacuum states and the physical states transform independently of each other, namely since the time-evolve independently. And by showing that $\hat U_\mathrm{vac}(t)$ will never create more than $\sim \delta k^{-3/2}t$ particles in the time span of $t$, regardless of what the initial vacuum state is, we get that we can ignore the vacuum particles completely when we Lorentz-transform, and therefore we can ignore the vacuum-perturbing terms. This is what I would have written. But the question is: do I need to show that $((\hat A^\dagger)^n \ket{\,})_{n\in \mathbb{N}_0}$-spanned states transform into such, or what?\,.\,. (10:00) .\,.\,Ah, or do the property that no more than $\sim \delta k^{-3/2}t$ particles are created.\,. follow from the Lorentz transformation.\,. maybe not, hm.\,. .\,.\,(10:05) Ah, or maybe it does.\,. .\,.\,Hm, but then I just need to show the `$\hat U_\mathrm{vac}(t)$ creates $\sim \delta k^{-3/2}t$ particles' property for all of $\mathbf{H}$.\,. .\,.\,Ah, no! I just need it to show it on a space of states with a maximum particle number of $\sim \delta k^{-3/2}t$.\,!\,.\,. (10:10) .\,.\,(10:19) The property doesn't follow from the Lorentz transformation, no. Unless we also use the fact that the vacuum particles are evenly spread out in position space (in any local volume not close to the edge). And while this \emph{might} not be easy to show, it's still something that we know to be true intuitively. So I think it's okay to use it. .\,.\,And where does that get me?\,.\,. (10:22) .\,.\,Then is does follow that $\hat U_\mathrm{vac}(t)$ will only create $\sim \delta k^{-3/2}t$ particles, at least if $\ket{\,}$ is somewhere within reach with a not too far Lorentz transformation (and time-evolution).\,. .\,.\,Hm.\,. .\,.\,Hm, maybe we can then just allow ourselves to assume that the vacuum particles are recognizable from the physical particles.\,. (10:30)
\ldots Well, we can.\,. .\,.\,(10:49) Oh, it won't actually be hard at all to show that the vacuum particles are spread out in position space, by the way.\,. .\,.\,(10:53) Okay yeah, so it follows, then, from the Lorentz transform that the spread-out vacuum particles are $\sim \delta k^{-3/2}$ in number, at all hyperplanes that are part of the given Lorentz-transformation--time-evolution circuit. And that's enough to know that these vacuum particles won't affect the localized, `physical' particles in any time-evolution in the circuit, which means that the vacuum-perturbing terms can be removed in both inertial frames, without changing the dynamics of the physical particles. Hm, and is that it, or do I also need to.\,. Nah, I shouldn't need to show that they can be removed in the.\,. no, never mind. This is it, indeed. This shows the Lorentz covariance.\,. (11:00) \ldots Yes, great.\,:) (11:12)

(13:57) Oh damn, I get $O(\tau^2 \delta k^3)$ instead of $O(\delta k^3)$, and instead of $O(\tau \delta k^3)$, which is what I actually had in mind.\,. .\,.\,Oh, but shouldn't it be $O(\delta k^6)$ instead for the commutator.\,. .\,.\,No.\,:\textbackslash\,.\,. .\,.\,Øv, så giver det mere mening, at jeg overså denne mulighed i februar--marts.\,. (14:13) .\,.\,(14:15) Ah, but wait a minute.\,! If we consider $\exp(\hat A + \hat A^\dagger) = \sum_{n=0}^{\infty} (\hat A + \hat A^\dagger)^n/n!$, then this will be equal to what we get if we reduce $\sum_{n=0}^{\infty} (\hat a + \hat a^\dagger)^n/n!$ by normal ordering and then turn $\hat a \to \hat A$ and add a $O(\delta k^{3})$, would it not?\,.\,. Hm, maybe not, but let me see.\,. .\,.\,Yeah, when we normal-order $\sum_{n=0}^{\infty} (\hat A + \hat A^\dagger)^n/n!$, we should indeed get a similar expression as $\sum_{n=0}^{\infty} (\hat a + \hat a^\dagger)^n/n!$ normal-ordered plus a $O(\delta k^3)$ term.\,!\,.\,. (14:23) .\,.\,Hm, but then what? 'cause we can't use the BCH formula, then, can we?\,.\,. .\,.\,Ah, but only the terms without $\hat a$ will survive when we let this work on $\ket{\,}$!\,.\,. (14:27) .\,.\,Then the big question is: what does this give us?\,.\,. .\,.\,Hm, I guess this didn't really get me closer, as this was really already the question to begin with, I guess: What do we get for that?\,.\,. (14:31) .\,.\,Oh wait, we \emph{can} use the BCH formula to calculate this, can we not!\,?\,.\,. .\,.\,For what we get is then the same contributions as we get from $\sum_{n=0}^{\infty} (\hat a + \hat a^\dagger)^n/n! \ket{\,}$, plus a $O(\delta k^3)$ term, right?\,.\,. .\,.\,Right.\,.\,!\,! (14:36)
\ldots (15:12) No, then I'm still forgetting that $\tau^n \propto \delta k^{-3n/2}$ is part of it.\,. .\,.\,Yeah, and $[\hat A \tau, \hat A^\dagger \tau] = \tau^2 \braket{\,|\hat A \hat A^\dagger|\,} + O(\tau^2\delta k^3)$, unfortunately.\,.\,:\textbackslash\,.\,. (15:16)

(15:25) Wait, but do I even need for the vacuum not to make cross annihilations? Can't I just jump straight to the Stirling's approximation argument directly?\,.\,. .\,.\,Yeah, why on earth not?\,.\,!\,!\,.\,. (15:28) .\,.\,This would jump over so much, since it would even mean that I don't need to look at the $\Delta/2 + d$-heuristics for the perturbed vacuum.\,. .\,.\,Woah.\,. (15:32)

%(15:45) Jeg forstår det ikke helt, jeg må næsten have overset noget.. Hvordan kan jeg have overset det her i februar(-marts)?.. Og kan det virkelig være rigtigt.. ..Hm, jeg husker godt nok, at jeg ikke mente, at jeg kunne putte et n_{max} cutoff på, \emph{før} jeg havde vist dette $n \sim \delta k^{-3/2 (+ \varepsilon)}$ forhold, så måske var det det, der forhindrede mig i at se det / bruge det.. ..Det var det nok.. ..Men jeg behøver rigtigt nok ikke at vise dette forhold først, for man kan bare nøjes med at vise, at det gælder for $\hat H_{QED}$ (final), og så kan man bare overføre det samme cutoff på \hat H'', og derpå argumentere ved at bruge, at når dette n_{max} cutoff bliver stort nok, så vil vakuum-partiklernes interaktion of interferæns forsvinde---når også \delta k \to 0---og derfor kan man altså fjerne de vakuum-perturberende termer. .. (15:55) ..Og ja, det holder jo, det her: Bare erstat \hat A^\dagger med \hat A + \hat A^\dagger i den nuværende lign. (37) (v2_47.tex), og så kan man jo argumentere på helt samme måde, som jeg havde tænkt mig derfra. (Og jeg ved lige hvordan, jeg skal argumentere; det fandt jeg frem til på min gåtur her lidt tidligere: Jeg viser bare et øvre bånd, når n/e er stort nok til at brøken bliver lig 1/2.) (15:59)

%(19:43) Jeg skal jo lave $\hat A_\psi^\dagger \hat A_\chi^\dagger$-sektionen om, da \phi ikke længere vil være en almost-egenvektor. Og her kan jeg så i stedet bare bruge BCH-formlen, hvor jeg så bare skal vise, at kommutatoren forsvinder når \delta k \to 0.

(28.05.24, 10:19) Øv! When $\delta k$ decreases, I also have to increase $n_{max}$ by at least $\delta k^{-3/2}$. But this makes the operator norm go like $\delta k^{-3/2}$ as well! So I was not lost back in February--March: I still have this problem of showing that the particle number only grows as $\delta k^{-3/2}$.\,.\,! (and that we can thus indeed allow $n_{max}$ to grow as $\delta k^{-3/2}$.) (10:23)

%.\,.\,(10:25) Hm, but maybe it's possible after all.\,. .\,.\,Hm, no, that might not work.\,.

(10:34) Yeah, and this was exactly why I recalled that this prospect of having $n!$ in the denominator out-compete the numerator didn't work out back then. .\,.\,Back to the drawing board.\,.

.\,.\,Wait, isn't $\|(\hat A^\dagger)^n\| = n \|\hat A^\dagger\|^n$, or something like that? (10:41) .\,.\,If so, then it could work.\,.\,!\,.\,. .\,.\,Yeah, it must be, right.\,.\,?\,! (10:43) .\,.\,I'm pretty sure that I found it to go like $\|(\hat A^\dagger)^n\| \sim n! \|\hat A^\dagger\|^n$ back then, but maybe this was a mistake (that then completely threw me off an otherwise easy track).\,. (10:45) .\,.\,(10:51) Oh, it's probably right that $\|(\hat A^\dagger)^n\| \sim n! \|\hat A^\dagger\|^n$, isn't it?\,.\,. .\,.\,I mean $\|(\hat A^\dagger)^n \ket{\,}\| \sim n! \|\hat A^\dagger \ket{\,}\|^n$, of course, and yeah, it is.\,. (10:53) .\,.\,Wait no, it's $\sqrt{n!}$.\,. (10:54) .\,.\,Yes, it is, and that could do it! This is the small thing (it seems to be.\,.) that I have missed back then, that makes it all work.\,.\,!\,!\,.\,. (10:58)

%(11:20) Then I actually do have to keep (and finish) the 'bosonic statistics' section, 'cause it's very important that we get \sqrt{n!} and not, say, \sqrt{(2n)!}.

(12:06) Oh, but I'm now worried about $\braket{\,| (\hat A + \hat A^\dagger)^n |\,}$ being more than $\sim n! \braket{\,| \hat A + \hat A^\dagger |\,}$.\,. .\,.\,Hm, I can probably solve that, but it just might not be completely easy.\,. (12:09) .\,.\,Okay, and I'm not completely sure that it's solvable, even.\,. (It's actually $\braket{\,| (\hat A + \hat A^\dagger)^{2n} |\,}$ that I need to consider.\,.(!)) .\,.\,Yeah, that seems problematic.\,. (12:15) .\,.\,Hm, can't I use that normal-ordering logic, that I was going to use before?\,.\,. (12:19) .\,.\,Oh, I think so, yes.\,. (12:20)

(12:39) Oh, maybe I can also still use the BCH formula directly, even though we have $O(\tau^2 \delta k^3) \approx O(1)$ instead of $O(\delta k^3)$.\,.\,!\,.\,. .\,.\,Hm, how to show that $\exp[\hat A \tau + \hat A^\dagger \tau + O(\tau^\varepsilon)] \approx \exp(\hat A \tau + \hat A^\dagger \tau)$.\,.\,? (12:45) .\,.\,One \emph{could} use the ``Taylor expansion''.\,. .\,.\,Well, or rather $\exp[\hat A \tau + \hat A^\dagger \tau + O(\tau^\varepsilon)] \approx \exp(\hat A \tau + \hat A^\dagger \tau) \exp(O(\tau^\varepsilon))$, I guess.\,. Oh, and that definitely calls for the BCH formula, then.\,. (12:50) .\,.\,Hm, I'm afraid that doesn't work, so I might be wrong about that conjecture. So back to thinking about normal-ordering, I guess.\,. .\,.\,Nah, there must also be a way here, but perhaps not with BCH; perhaps using eigenvector (and almost eigenvector) logic instead.\,. Not sure if it's better than the normal-ordering argument strategy, though.\,. (12:56) .\,.\,Nah, I think the normal-ordering argument strategy is better.\,.

%(13:03) Hm, maybe one could define an altered \hat A, call it maybe \hat A', for which $[\hat A', \hat A^\dagger] = \braket{...}$ exactly (without the O(\delta k^3)). That might be neat..

(13:21) Hm, maybe the eigenvector argument is actually the best strategy.\,.

%(13:52) Okay, it's actually not such an easy problem to figure out, 'cause it's a bit different from the usual kind of stuff: I'm not trying to show that the "perturbation" (\hat A \to \hat A') is vanishing, nor simply that it is bounded, but that it can be given an $n_{max} \sim \delta k^{-3/2}$ cutoff.. ..Hm, unless I could show that exp(\hat A + \hat A^\dagger) = exp(\hat A' + \hat A^\dagger) exp(\hat A - \hat A') + something vanishing.. (13:56) ..Well, that does follow from the BCH formula, but what about when \tau is included?.. (13:59) ..Ah, but.. Ah! ..It's okay to get \sim \delta k^{-3/2} factors of exp(O(1)) in an expansion.. I should think.. Hm.. (14:03)

(14:09) Oh, wait, maybe I \emph{can} just consider $\| (\hat A + \hat A^\dagger)^n \tau^n/n! \ket{\,} \|^2 = \tau^{2n} \braket{\,|(\hat A + \hat A^\dagger)^{2n}|\,}$
$/ (n!)^2$ after all.\,. .\,.\,Yes!\,.\,. (14:17) 'Cause I'll get $n! \times 1$ plus $(n-1)! \times \binom{n}{1}$ (or something like it) plus etc.\,!\,.\,. .\,.\,So that should still give $\sim n!$.\,:) .\,.\,(14:29) Hm, it's not quite that easy, I think.\,. .\,.\,Oh, there are only $2^{2n}$ terms in total! I don't know why I thought there would be more.\,x) (14:34) .\,.\,Okay, so it's actually easy enough to show that $\| (\hat A + \hat A^\dagger)^n \tau^n/n! \ket{\,} \|^2 \leq (2\tau)^{2n} \braket{\,|\hat A + \hat A^\dagger|\,}^{2n} / n! + O(\tau^{2n} \delta k^3)$.\,. .\,.\,No, we need to show it to be $O(\tau^{2n} \delta k^3)/n!$ instead, or something to that effect.\,. (14:41)

(15:07) We'll just use that $\| (\hat A + \hat A^\dagger)^n \tau^n/n! \ket{\,} \|^2 \leq (2\tau)^{2n} \braket{\,|\hat A + \hat A^\dagger|\,}^{2n} (1 + O(\delta k^3))/ n!$.

%(15:44) Hm, jeg kan ikke så godt lide, at O(\delta k)-termet kan afhænge af n.. ..Yeah, I need to include the n-dependency also in some way.. ..Ah, men det bør faktisk være nemt nok at argumentere for, at amplituden bare vokser med n allerhøjest, ikke..? ..Det skal jeg så bare gøre med det samme i boson-statistik-sektionen.. ..Hm, eller er det lidt lige meget, lad mig lige se.. (16:00) ..Nej, det er det nok ikke.. ..Nej, fornuftigt at vise n- og \delta k-afhængigheden på én gang for alle $\hat A \hat (A^\dagger)^n|\ket{\,}$-udtryk.. ..Hm, amplituden kan vel højest gå som n^2 her, ikke?.. (16:07) ..Ja, og vi kan sagtens håndtere potenser af $n$; det er ikke noget i sammenligning med \tau^{2n} eller $n!$. Så det er bra..:) (16:09) ..Great.:) (16:10) ..Højest n^4, rettere..
%(17:57) Hm, der er måske dog et problem med $(\hat A)^n \hat (A^\dagger)^n|\ket{\,}$, fordi \hat A'erne så også kan virke på fejl-termet flere gange.. Hm.. ..Ah, men det vil jo bare gange det med n allerhøjest, så det gør vel ikke noget.. ..Ah, og det er jo lige præcis det, jeg har tænkt med $O(n^5)O(\delta k^3)$..x) (18:02)

(19:34) Oh shit, am I completely wrong about the conclusion that $\sqrt{n!}$ will work for me!\,?\,.\,. Doesn't this mean that we will then need $n$ to be $\sim \tau^2$.\,. Yeah, it must.\,. Damn!\,.\,. (19:36) .\,.\,Hm, then again, if we simply multiply $n$ with 2, we should get two times the exponent in $n^n$.\,. Hm.\,. (19:42) .\,.\,Ah, but that also applies to $\tau^{2n}$, so that doesn't help. \emph{Ohh} daamn.\,.\,! (19:44) .\,.\,How.\,. irritating.\,. .\,.\,unfortunate.\,.

(19:49) Oh, but hope is not lost, actually. It just means that the ``Taylor expansion'' strategy probably does not work. But maybe an eigenvector (perturbation) argument strategy could.\,. \ldots (20:09) Oh, but I just realized that it's a really bad sign that this analysis seems to show that $\exp(-i(\hat a + \hat a^\dagger)t) \ket{0}$ has.\,. hm, wait a minute, perhaps not.\,. (20:11) .\,.\,Hm, that's odd.\,. .\,.\,With my BCH logic for $\hat a$ and $\hat a^\dagger$, I seem to get that $\exp(-i(\hat a + \hat a^\dagger)t) \ket{0}$ does not necessarily have norm 1. Hm, what have I done wrong.\,.\,? (20:16) .\,.\,Oh, I'm just forgetting the normalizing factor in front that I get from the commutator. So yeah, it unfortunately seems.\,. wait!\,.\,. That factor also has a $\tau$ dependence!\,.\,. Okay, I'm maybe done yet.\,. (20:18) .\,.\,Ah, I got confused by $-\tau^2$ there for a moment, but $\tau$ includes an $i$ so it does make sense.\,. .\,.\,(20:31) Yeah, I think that one can actually show that the particle number for $\exp(-i(\hat a + \hat a^\dagger)t) \ket{0}$ grows like $t \ln t$.\,.\,!\,.\,. (20:32) .\,.\,That would be awesome, for sure.\,.

.\,.\,And then how would I get this proof to work for $\exp(-i(\hat A + \hat A^\dagger)t) \ket{0}$ as well.\,.\,? (20:34) .\,.\,Then I would first have to argue that $\exp(-i(\hat A + \hat A^\dagger)t) \ket{0} \approx \exp(-i(\hat A' + \hat A^\dagger)t) \ket{0}$, I guess.\,. (20:37) .\,.\,(Or rather $\exp(-i(\hat A + \hat A^\dagger) \delta k^{-3/2} t) \ket{0} \approx \exp(-i(\hat A' + \hat A^\dagger) \delta k^{-3/2} t) \ket{0}$.\,.) .\,.\,And don't I get this with my $O\big(n^5(n!)^{-2}\big)O(\delta k^3)$.\,.\,? %(in Eqs.\ (31--32) in Damgaard_vacuum_paper_v2_52.tex)
.\,.\,I should think so.\,.\,! (20:44) Okay, this is incredibly exciting, but I'll need to wait until tomorrow before I can start to go all this through more adequately.\,.

(22:20) No, we can't look at the operator norm of two operators and then conclude anything about how close they are to each other. But maybe I'm on the right track, still.\,. .\,.\,Yeah, just don't take the norm.\,. (22:22) .\,.\,Yeah, I think there is a way.\,. (22:24)

(29.05.24, 9:45) The maximum of the particle number curve goes as $\tau^2$. (And it's not at all a steep decent after that.) So the particle number definitely goes as $\tau^2$.\,.\,:\textbackslash\ .\,.\,Meaning that it goes as $\delta k^{-3} \propto \mathcal{V}$.

(9:49) Hm, and what about those coherent states.\,.\,?

(16:24) The coherent states doesn't give me anything. And now I've investigated what happens if we take a Gaussian with $\sigma \sim \tau^{-1}$ and look at the particle number (for the initial state, $t=0$). Having looked up the asymptotic expansion of the SHO eigenstates, 
%(https://en.wikipedia.org/wiki/Hermite_polynomials#Asymptotic_expansion)
it then seems that the projection onto $\psi_n$ will only decrease with on the order of $n \sim \sigma^{-2}$, meaning that $n$ will go as $\tau^2 \propto \delta k^{-3} \propto \mathcal{V}$.\,:\textbackslash\ (16:31)

%\ldots Hm, maybe it's even worse than that, 'cause I didn't even account for the fact that the amplitude grows.\,. .\,.\,Hm, or does it?\,.\,. .\,.\,(16:56) Ah, no, the amplitude converges.\,. .\,.\,But yeah, doesn't matter.\,.

(17:01) Hm, there is the idea of then letting $\sigma$ go as $\tau^{-1/2}$ instead.\,. .\,.\,Hm, sounds kinda crazy, but maybe it's not.\,. (17:04) %..Hm, \omega \propto \sigma^{-2}, funnily enough.. ..So \hat X \propto \sigma (\hat a + \hat a^\dagger).. ..\propto \tau^{-1/2}(\hat a + \hat a^\dagger).. which should then make the particle number grow as \sim \tau^.. wait.. (17:15) ..Ah right, I should use \sigma^{-1}, 'cause we are looking at the original \hat X in the eyes of the narrower \psi_0, not the other way around. So the operator should look like \tau^{1/2}(\hat a + \hat a^\dagger) with the narrower \psi_0's eyes, which means that.. ..That the particle number \emph{with the eyes of the narrower \psi_0} (i.e. in the changed coordinate system), should go as \tau^1 \propto \delta k^{-3/2}, by what I have found out when I considered \hat U(t) \ket{\,}. And what will that be in the original coordinate system? (17:21) ..Hm, it probably is crazy.. (17:23)

(17:36) Wait, say that the perturbed vacuum is in a coherent state. Then when a physical particle interacts with this coherent state, could it then not be the case that we would simply get a new physical state $\otimes$ the same coherent vacuum state?\,.\,!\,.\,. (17:38) .\,.\,And then we could probably (perhaps) simply change this interaction into a physical one by adding an interaction term that does this, and removing the coherent vacuum state (if it never changes, anyway).\,. (17:41) .\,.\,This actually seems kinda plausible.\,.\,!\,!\,.\,.\,. (17:43) .\,.\,Oh wait, if it's in a coherent state, then its interaction transition amplitude will not grow as $\delta k^{-3/2}$. So what can we expect?\,.\,. (17:49)

.\,.\,Oh, the vacuum `interaction terms' also don't vanish when the particle number goes as.\,. hm, or do they.\,.\,? (17:56)

.\,.\,(18:06) I just had the idea to simply think of all the possible vacuum states as different kinds of bosons. This seems helpful.

%(20:23) Nu håber jeg lidt på at finde en næsten-egenvektor til \hat a + \hat a^\dagger, som har mindre og mindre og \varepsilon, når \alpha vokser (eller rettere når n vokser som \delta k^{-3}), men som stadig også er en næsten-egenvektor til \hat a. Men jeg fortsætter i morgen..

(30.05.24, 16:41) I've tried investigating coherent states plus some correction, and I think I can find an $\varepsilon$-almost eigenvector of $\hat a + \hat a^\dagger$ where $\varepsilon^2$ goes as $\alpha^{-1}$, but that's not enough, perhaps not even for the idea where we try so let the vacuum be a coherent state with $n \sim \mathcal{V}$. But I don't think that the latter idea has good chances either, anyway. Now I just had a completely different thought, though: What if we were able to put a particle number cutoff, $n_\mathrm{max}$, on the Hamiltonians that are independent on $\delta k$?\,. Then we would not need to care at all about how many particles the perturbed vacuum excites. An interesting thought, 'cause I did actually kinda argue at some point that this would be possible.\,. So let me definitely think some more about that.\,. (16:47)

%(16:55) I wrote about it the moring of the 27th (so this Monday), here above.. ..(Or rather "noon," except I think I've been using that term incorrectly. It apparently refers to midday, but I'm talking about the prenoon here, as it's apparently called..)

\ldots (17:05) No, it's not exactly what I have argued before (see notes from 27/05 above), but maybe one could make the argument, I don't know.\,. Hm, it doesn't sound too plausible, though, but maybe.\,.

.\,.\,Keep in mind, we can allow $\kappa$ to be fixed while sending $\delta k \to 0$; I don't see a problem with that.\,.

.\,.\,The idea is then to argue the Lorentz covariance from the ``Dyson--Trotter-expanded'' (as I've been calling it) operators for the Lorentz transformation circuit.\,. (17:11) .\,.\,Hm, still sounds like a slim chance, but.\,. (17:13) .\,.\,Oh, and the fact that the interference isn't vanishing is also a nightmare, isn't it.\,. I did not think about that.\,. (That also applies to the `perturbed vacuum is a coherent-like state' idea, it seems.\,.) Hm, not great.\,. Perhaps I need to abandon all this and put my hopes in the `countable basis idea' basket.\,. But let me think some more.\,. (17:17)

(19:33) Hov, nej, hvis $n_{max}$ netop godt kunne være fastholdt uafhængigt af $\delta k$, så ville vakuum-partiklerne ikke fylde rummet op.\,. If $n_\mathrm{max}$ is allowed to be fixed when $\delta k \to 0$, the vacuum particles will not fill up the space (i.e.\ the momentum or position lattice).\,. So yeah, let me definitely think on that some more, after all.\,.

.\,.\,Hm, and could you gather the terms in orders of the coupling constant for the ``Dyson--Trotter expansion''.\,.\,?\,.\,. (19:38)

(20:10) Woah!\,.\,. I think the `interaction terms' of my v1 vacuum paper have fermion statistics!\,.\,. Let me see.\,. .\,.\,Yeah, it seems so.\,. Omg, that would be incredible.\,. (20:14)

.\,.\,That would make the interaction terms.\,. it would ``add'' a dependency that decreases with $\alpha$, it seems, where $\alpha$ is the parameter for the coherent states, given that this is the solution that we try.\,. (20:18) .\,.\,Well, or another way to look at it is just that it will cancel the $n$-dependency.\,!\,.\,. (20:20) .\,.\,Which is obviously incredible.\,.\,!

.\,.\,Oh wait, maybe I'm wrong.\,. (20:25) .\,.\,Yeah, I must be wrong. Damn.\,. (20:25)

%(31.05.24, 10:45) Jeg var ikke super motiveret for at stå op i morges, så lå bare i sengen og tænkte over problemet. Og så slog det mig, her lidt i ti, at fordi vukuum-Coulomb-annihilerings-interaktionen går som n^4.. eller n^2..? Hm.. ..Hm, der er nok n^2, men det betyder j stadig, at vi kun skal have $n \sim \delta k^{-(3 + \xi)/2}$, før den interaktions styrken vokser som mere en \delta k^3.. Så min tanke er altså: Gad vide om denne kanal ligesom kan tappe fra hoved-boson-tilstanden, sådan at sidstnævnte holdes under et niveau på k^{-(3 + \xi)/2}? Jeg tænker det altså lidt som et laser-system, eller tilsvarende, hvor der er den indledende \hat A^\dagger-tilstand, der bliver pumpet af \exp(-i(\hat A + \hat A^\dagger)t), men så er der også nogle andre tilstande, som bosoner i den indledende tilstand kan henfalde til, hvilket dermed kunne holde N for den indledende tilstand nede. Lad mig derfor prøve at regne på, hvad der ville ske for netop sådan et laser-system.. (hvis jeg kan huske/finde/udregne formlerne..) (10:57)

(31.05.24, 10:58) I've had an interesting new idea that I've written about in the source comments above this paragraph. I was really about to lose all hope *(except for the `countable basis idea'.\,.), but maybe there's still some.\,.

%(11:00) Hm, eller er det egentligt n^4?.. Det må jeg lige finde ud af.. ..(11:03) Åh, jeg tror det er n^{3+1/2}.. Lad mig se.. ..Nej. Det er n^2. Ok. (11:05)

(11:47) Okay, if the interaction strength is $a$, then $n(t) = a^2 t^2$, meaning that $n'(t)= 2a^2 t = 2a\sqrt{n(t)}$, which is exactly as we expect.\,. .\,.\,($n(t) \propto a^2 t^2$ for the boson occupation number of a single state when the time evolution is like $\exp(-ia(\hat a + \hat a^\dagger)t)$.) .\,.\,So when $n = \tau$, we have $n'(t) = 2a \tau^{1/2}$.\,. .\,.\,While the transition down to the second state, if $a$ for that transition is proportional to $n^2$ of the first state---so let's take $a_2 = b n^2$ for that transition---is given by $n_2'(t) = 2 b n^2 \propto \tau^2$, even when $n_2$ is taken to be 0 (as it's actually $\sqrt{n+1}$ rather than $\sqrt{n}$, of course).\,. So a rate of $\tau^2$ defeats a rate of $\tau^{1/2}$.\,. Hm, can that really be.\,.\,? (12:00) .\,.\,It seems so.\,.\,!\,!\,.\,. (12:02)

.\,.\,(12:06) Oh, I forgot the fact that $a_2$ is also proportional to $\delta k^{3}$ for a moment. But then if $n_1 \propto \delta k^{-(3 + \xi)/2}$, $\xi < 1$, then the ``pumping rate'' for $n_1$ is $n_1'(t) \propto \delta k^{-(3 + \xi)/4}$, wheres as $n_2'(t) \propto \delta k^{3} n_1^2 \propto \delta k^{3} \delta k^{-(3 + \xi)} = \delta k^{-\xi}$. (12:11) .\,.\,Ah, but does that work for us then?\,.\,. .\,.\,No, not without trying to take into account the fact that $n_2$ is also rising to some level.\,. (12:15) .\,.\,Oh wait, it should be $-\xi$, let's see.\,. .\,.\,Hm, with $n_1 \propto \delta k^{-(3 + \xi)/2}$, $\xi < 3$, we get the same, but then.\,. then $\delta k^{-\xi}$ \emph{can} become greater than $\delta k^{-(3 + \xi)/4}$!\,.\,. (12:22)

.\,.\,(12:27) Oh wait, the interaction that I'm thinking of actually has a strength of $\delta k^6$, not $\delta k^3$, so that wouldn't work, I assume.\,. .\,.\,No, and what else do we have?\,.\,. \ldots (12:40) Hm, it doesn't seem to work with any of the other vacuum interactions, when $n_2 = 0$, that is.\,.

\ldots Okay, I'm not sure that this idea will work.\,. But I'll continue thinking.\,. (13:06)
\ldots Oh, except that the number of different `vacuum boson' states is infinite (in the $\delta k \to 0$ limit).\,. (13:18) .\,.\,Ah, but it seems that the transition rate for each one will only pick up once $n_{m-1}$ is of the order of $\delta k^{-3} \propto \mathcal{V}$. So this might not help us.\,. .\,.\,Yeah, 'cause I've noticed a pattern for the vacuum interaction terms, and that is that $\Delta/2 + d = c - 3 m/2$, where $c=3$ for the Dirac interaction and $c = 9/2$ for the Coulomb interaction, and $m$ here is the number of vacuum triplets and/or quartets involved.\,. (13:26) .\,.\,Therefore, $n_2'(t) \propto \delta k^{3 m/2} n_1^{m/2}$, meaning that we need $n_1 \sim \delta k^{-3}$ before the rate picks up.\,.

.\,.\,(13:35) Wait, but what about the case where we start out with all the `vacuum boson states' having $n_m \propto \delta k^{-3 + \xi}$?\,.\,. .\,.\,Then $n_m'(t) \propto \delta k^{3 \mu/2} n_{m-1}^{\mu/2} \sqrt{n_{m} + 1}$.\,. Wait, and I'm not forgetting some important point about $n_2'(t)$ also increasing with $n_1$ even if the interaction strength, $a$, was constant?\,.\,. (13:41) .\,.\,No, that's contained in $n_{m-1}^{\mu/2}$, obviously, where $\mu$ would normally be 1. Okay, so
$n_m'(t) \sim \delta k^{3 \mu/2} n_{m-1}^{\mu/2} n_{m}^{1/2} \propto \delta k^{\xi \mu/2} \delta k^{-(3 - \xi)/2} = \delta k^{-3/2 + \xi(\mu + 1)/2}$. .\,.\,We have $\mu \geq 2$ (unfortunately), so the rate.\,. .\,.\,(13:55) Wait.\,. .\,.\,Nah. If we solve $-3+\xi = -3/2 + \xi (\mu + 1)/2$, we get $(1 - \mu/2 - 1/2)\xi = 3/2 \Rightarrow \xi = 3/(1- \mu) = -3/(\mu - 1)$, and that is no good.\,. (14:02)

\ldots\ (15:46) I went for a walk, and pretty early on that walk I had an idea, which I have thought some more about, and I'm quite excited about it. Let me just think a little more.\,. .\,.\,Yeah, the thing is, if you consider $\hat A^\dagger (\hat A^\dagger)^n \ket{\,}$, then the transition amplitude for $\hat A^\dagger$ working on the state $(\hat A^\dagger)^n \ket{\,}$ will not decrease remarkable due to the Pauli exclusion principle until $n$ gets close to the maximum (of $\sim \delta k^{-3}$). However, if we consider the amplitude of the $(\hat A^\dagger)^n \ket{\,}$ state itself, then the story seems to become a bit different.

If we consider the chance of picking out a component at random from $(\hat A^\dagger)^n \ket{\,}$, assuming for a brief moment that $\hat A^\dagger$ is purely bosonic, then we see that for small enough $n$, the chance should be more than $n/2 \times a/2$, where $a$ is the fraction of $n/n_{max}(\delta k)$. The (overhead) reasoning behind this comes from considering $(\hat A^\dagger)^{n/2} (\hat A^\dagger)^{n/2} \ket{\,}$. So when $n^2$ is of the order of $4 n_{max}(\delta k)$, we should start to get a chance about to $1/2$ at that point. And because chance is the number of combinations where the property (that there are two particles occupying the same state, in our case) is true divided by the total number of combinations, we see that we should hit half-and-half at a point when $n^2 \sim n_{max}(\delta k)$.\,. (16:03) .\,.\,So if, say, $n = a n_{max}(\delta k)$ for any fixed $a < 1$, then the amplitude of $(\hat A^\dagger)^n \ket{\,}$ should really be much, much smaller than what it would have been in the purely bosonic case, when $n_{max}(\delta k)$ gets large enough. So that why I'm excited.\,.(!) (16:06)

.\,.\,Okay, the number of components of $(\hat A^\dagger)^n \ket{\,}$ where none of the particles occupy the same state is given by.\,. .\,.\,by $\binom{N}{n}$, where $N \equiv n_{max}(\delta k)$. (16:10) .\,.\,Okay, so we are interested in $\binom{N}{n}/N^n$.\,. .\,.\,Hm, Stirling's gives us that this is $\approx (N/n)^n / 2N^n = n^{-n}/ 2$.\,. (16:15) .\,.\,And $\approx \sqrt{N/n} n^{-n}/2$ if we want to be a bit more precise.\,. (16:16) %..Omg, I'm typing \emph{so} slow..
.\,.\,Hm, that's a very small fraction; it almost seems hard to believe, let's see.\,. (16:18) .\,.\,Oh never mind, with the $\sqrt{N/n}$ in front, it makes sense.\,. .\,.\,Wait, it's the other way around.\,. .\,.\,No.\,. .\,.\,Ah, but it should have been $\approx \sqrt{N/n} e^{-N+n} n^{-n}/2$. .\,.\,Which does make sense, finally.\,. .\,.\,No.\,x) It's $\binom{N}{n}/N^n \approx \sqrt{N/n} e^{-N+n} n^{-n} N^{N-n}/2$. (16:27) .\,.\,And again I'm wrong. It should be $\binom{N}{n}/N^n \approx $\ldots\ well, actually, never mind, that doesn't help.\,. (16:31) .\,.\,(It should have been $n!$ instead of the $2$ in the denominator.) .\,.\,(And $(N-n)!$ instead of $n!$, of course.) But yeah, what should I do.\,.\,? .\,.\,No, let me actually look at the Stirling approximation to it:
$\binom{N}{n}/N^n \approx \sqrt{N/(2\pi (N-n)n)} e^{-N+(N-n)+n} N^{N-n} n^{-n} (N-n)^{-N+n} = \sqrt{N/(2\pi (N-n)n)} n^{-n} (N/(N-n))^{N-n}$.\,. .\,.\,Yeah, I don't see this helping me much.\,. (16:43) .\,.\,Except that you can see that if $n = a N$, for some fixed $a < 1$, then it really decreases fast!\,.\,.\,:)\,.\,. (16:47) *(Well okay, that's not so easy to see, so never mind; it requires more thought.. (16:54))

.\,.\,Okay, let me just reiterate: This \emph{is} really exciting.\,. (16:49)

(16:57) Wait. $\binom{N}{n} \leq N^n/n!$.\,! So $\binom{N}{n}/N^n \leq 1/n!$.\,. .\,.\,Hm, that can't be right.\,. (17:00) .\,.\,Oh, I guess we shouldn't divide by $n!$; I think we should just look at $N!/(N-n)!/N^n$.\,. (17:01) .\,.\,Yes.\,. .\,.\,Hm, Stirling's then gives us $\approx \sqrt{N/(N-n)} N^{N-n} (N-n)^{-(N-n)}$, which equals
$\sqrt{N/(N-n)} (N/(N-n))^{N-n}$.\,. .\,.\,Hm, $= (N/(N-n))^{N-n+1/2}$, but that can't be right.\,. (17:08) .\,.\,Or rather $e^{-N+(N-n)} (N/(N-n))^{N-n+1/2} = e^{-n} (N/(N-n))^{N-n+1/2}$, but still.\,. .\,.\,Hm, maybe.\,.

.\,.\,Hm, if $n = a N$ and $1-a = e^{-1}$, then we get $e^{-aN} e^{e^{-1}N + 1/2} = e^{-(1-e^{-1})N} e^{e^{-1}N + 1/2} = \sqrt{e} e^{-(1 - 2e^{-1})N}$.\,. .\,.\,which decreases exponentially (since $e^{-1} \approx 0.368$).\,.

.\,.\,Hm, and $1-a = e^{-f(N)}$ gives us $e^{-(1-e^{-f(N)})N} e^{f(N)e^{-f(N)}N + f(N)/2}$.\,. (17:27) .\,.\,$= e^{f(N)/2} e^{-(1 - (1 + f(N))e^{-f(N)})N}$.\,.

.\,.\,Trying.\,. .\,.$f(N) = \ln ((N+c)/N)$, we get $e^{f(N)/2} e^{-(1 - (1 + f(N))e^{-f(N)})N}$ 

\noindent$= \sqrt{(N+c)/N} e^{-(1 - (1 + \ln ((N+c)/N)) ((N/(N+c))))N}$.\,. .\,.\,I need to begin an equation environment for that, but let me take a break first.\,. (17:40)

\ldots (17:56) Ah, before I go out on a walk: We ought to find that whenever $n \propto N^{1/2 + \xi}$, $0 < \xi < 1$, then we should get a decreasing function multiplied on the $(\hat A^\dagger)^n \ket{\,}$ state. And this is a very good sign, which tells us that the boson number might grow as $\tau^{1}$ after all.\,.\,:) (17:59) .\,.\,7, 9, 13.\,.

\ldots\ (19:43) I know another way to get an upper bound. .\,.\,But the best thing is, all I need is just for $(\hat A^\dagger)^n \ket{\,}$ to have a norm that increases slightly slower than $\sqrt{n!}$, that's it.\,. Okay, I guess, let me not get ahead of myself, 'cause I have bad experiences with that (it's kinda inevitable for me, on the other hand.\,.), but all I need is for $n_{max}$, i.e.\ the particle number cutoff, to be allowed to go at a slightly slower rate than $\delta k^{-3}$. Then because the interaction terms having amplitudes of $\delta k^{3 \mu/2} n^{\mu/2}$, these will never become significant. .\,.\,Or more precisely, their effect will vanish. And the interference will vanish as well. My Lorentz covariance argument will hold. And I still think that my new version of the `constant radii idea,' also including `utilizing that there are several particles' will work for showing self-adjointness, in a way where the cutoffs can be shown to be valid (i.e.\ they give the correct limit when lifted). And that completes the whole thing, pretty much. All those combined years of work. And it all seems to hinge on us being able to use this new realization that I had today to show that $n_{max}$ can be allowed to grow at a slower rate than $\delta k^{-3}$. And since this seems very likely (and I'll continue trying to confirm this in a minute), it once again seems that I've solved it all.\,. .\,.\,!\,! (20:00) %..And after these couple of days where hope really seemed to be lost for me.. This makes this feel all the more crazy and surreal..!.. *(This means so much to me. So I've also really been frustrated and feeling down these last few days, naturally.. *[I've luckily been busy most of that time, but still..] ..It really means so much to me..) ..I actually don't dare to feel confident yet that I've got it (which is reasonable, of course, given my history..). (20:02)

(21:17) $\ln[e^{-n} (N/(N-n))^{N-n+1/2}] = -n + \ln(1-n/N)(-N+n-1/2)$

\noindent$\approx -n + (-n/N)(-N+n-1/2) = -n + n - n^2/N - 1/N = - (n^2 + 1)/N \approx n^2/N$.\,. .\,.\,So $e^{-n} (N/(N-n))^{N-n+1/2} \sim e^{-n^2/N}$, which looks like what the kind of thing we would expect.\,:) (21:25)

.\,.\,So $\| (\hat A^\dagger)^n \ket{\,} \|^2 \tau^{2n} / (n!)^2$ goes as $e^{-n^2/N} \| \hat A^\dagger \ket{\,} \|^{2n} \tau^{2n} /n!$, rather than $\| \hat A^\dagger \ket{\,} \|^{2n} \tau^{2n} / n!$, as it would otherwise go like, and there we have it.\,. .\,.\,$\sim e^{-n^2/N} \| \hat A^\dagger \ket{\,} \|^{2n} N^{n}/n!$.\,. .\,.\,$\sim e^{-n^2/N} e^n \| \hat A^\dagger \ket{\,} \|^{2n} N^{n} n^{-n}$.\,. .\,.\,$= C^n (N/(ne^{n/N}))^n$.\,. .\,.\,Which has its maximum at $ne^{n/N} = CN$.\,. .\,.\,$n\ln n = N \ln N + \ln C$.\,. Oh, damn.\,. Hm.\,. .\,.\,Oh, so maybe it doesn't work after all!\,.\,. Oh, no.\,. (21:52) .\,.\,Wait, am I not missing a 2.\,. Does it matter?\,.\,. .\,.\,Maximum at $ne^{2n} = CNe^N$ instead.\,. .\,.\,Nah, that doesn't help. Damn!\,.\,. (21:57)

\ldots No, $ne^{2n/N} = CN$, but still.\,.

(23:55) Oh, the free energy might have a say in the matter.\,. .\,.\,It goes as $\delta k^{3/2} n^1$.\,. .\,.\,What's the minimum of $x + \delta k^{3/2} x^2$? .\,.\,It's at $x=\delta k^{-3/2}/2$.\,. .\,.\,The eigenstate might also be wider, which I don't know is a bad thing, or actually a good thing.\,. (00:05) \ldots (00:20) No, it's not wider. It's a coherent state with $\braket{n} = \alpha^2 = \delta k^{-3}/4$.\,.

(00:45) Hm, now I'm thinking about the other ``physical'' interactions on the vacuum states, and in particular about the Coulomb interaction: We have an extra $\delta k^{3/2}$ in front, giving us $\delta k^{9/2}$, I think.\,. $\Delta$ is actually $+3$.\,. and $d$ is 0.\,. .\,.\,Hm, I think it's the same thing: It matters when and only when $n \sim \delta k^{-3}$.\,. (00:52) %..And now to bed..

(02.06.24, 21:21) I have thought a bit more about the potential for the Pauli exclusion principle to help us. And don't think there's a very high chance at all, but I actually need to finish the calculations that I've been thinking about, if I am to continue on this problem, so I'll get back to these thought as some point. (They're about adding the free energy and then calculating the commutation relations, removing the `interaction terms' but keeping the Pauli exclusion correction, to all orders, in order to use the BCH formula.) But this evening (out on a walk) I got I kinda crazy idea, that actually now seems to me like it has a lot of potential. (It's about using different states than $\ket{0}$ as the initial states to get differentials. For instance, $\ket{1} - \ket{0}$---or rather $\exp(-i\hat a) \ket{\psi} = \ket{1} - \ket{0}$---gives us a differential operator in practice, $-i\hat P$, and $\ket{2} + \ket{0} - 2\ket{1}$ gives us $-\hat P^2$. The idea is then that since the distribution is a Poisson distribution, which becomes a Gaussian with $\sigma^2 \sim C \tau$ *(no, $\tau^2$) asymptotically, then the Fourier transform becomes a Gaussian with $\sigma^2 \sim C^{-1} \tau^{-1}$ *(also $\tau^{-2}$). And when we therefore basically are able to get $\braket{\phi_{Pois}| \hat P^{N^\xi} | \phi_{Pois}}$, $\xi < 1$, then we might actually get lower and lower values, which then therefore means lower and lower $\braket{\psi|\hat n|\psi}$! *(I mean: lower and lower compared to $N$ (at least)) So that's the idea!) I look forward to continue working on this line of thinking, and to write about it some more, tomorrow.\,:) (21:37)

(22:31) I'm not completely sure about the premise after all, and even if it holds, I forgot that we need to compute $\braket{\mathcal F \psi|\hat P| \mathcal F \psi}$ instead, which means that the lower $\sigma$ is to our disadvantage.\,. .\,.\,Hm, but doesn't $\braket{\phi_{Gauss}| \hat P^{n} | \phi_{Gauss}} = 0$ for $n=1$, for instance.\,. well, what ever; I've made a mistake, then. I'll think more about it tomorrow.\,. (22:38) .\,.\,Oh, I also get $\exp(i x p)$-like factor. But again, let me continue tomorrow.\,. (22:40)

%(23:45) Maybe there's a bigger chance that the Pauli exclusion principle will make it work out after all, 7, 9, 13.. ..But I'll see..

(03.06.24, 10:29) OH MY GOD.\,. I think I have made a mistake in saying that the expression $C^n (N/(ne^{n/N}))^n$ has its maximum at $n e^{n/N} = CN$!\,.\,. .\,.\,I've realized that what we get is a Gaussian with $\sigma \sim \sqrt{N}$ times a Poisson distribution, which is approximately a Gaussian with $\sim (n - CN)^2/2CN$ in the exponent. And when you analyze the maximum of that, it's at---let's calculate it more exactly---at $(n - CN)/CN + 2n/N = 0 \Rightarrow n/CN + 2n/N = 1 \Rightarrow n = 1/(1/C + 2)N$.\,. That can't be right.\,. (10:39)  *(No, $n = N/(1/C + 2)$) .\,.\,Oh no, I'm wrong! I thought the.\,. .\,.\,I somehow thought the variance went as $N^2$, but no, the Poisson distribution also has a variance of $\sim CN$, so there we are.\,. .\,.\,At least we get to throw $C$ out for large enough $N$ *(or in general, it seems, actually), which could be useful, but.\,. (10:50)

(11:01) Okay, there is a trøstepremie *(consolation prize), it seems. 'Cause this means that when $N$ grows, the vacuum state will never be excited up to the particle number cutoff.\,. Hm.\,. .\,.\,(11:10) Yes, so it seems that the solutions of our discretized Hamiltonian will likely converge to some states are either coherent states exactly, or close to them.\,. And if they are indeed coherent states exactly, then my idea about being able to remove the vacuum-perturbing terms are wrong (and the original grounds that backed this idea up is also removed, by the way, since the perturbation theory argument that I mentioned in the SA paper doesn't work, since I didn't account for the fact that the vacuum state does not need to change energy in the transition---in fact it doesn't even need to change state in the transition (if it's a coherent state)!\,.), \emph{but} we might still be able to solve the overall problem of finding a well-defined DSR'ed Hamiltonian.\,! For then we solution might very well just be to find almost-bosonic almost eigenstates of the perturbed vacuum, which converges in the continuum limit, and which then means that the resulting $\hat H_\mathrm{QED}$ will just some added vertices (infinitely many, in fact), where each vertex is $\alpha$ (of the coherent state solution) times the outcome of interacting with a `vacuum boson.' So for instance, if we look at the most simple of the `vacuum bosons,' the Dirac triple boson, then the vertex would be with one particle incoming (or outgoing) and.\,. well, that depends on the interaction, actually: For the Dirac interaction, we get one particle incoming (or outgoing) and three particles outgoing (or incoming). (11:23) .\,.\,And yeah, the vertex where three particles turn into one is also a part of $\hat H_\mathrm{QED}$ indeed, namely since the three of the four particles, after one is emitted, will be ``absorbed'' into the given `vacuum boson.' (11:31)

(11:34) There will also be an ubiquitous interference, but there's a chance that this will just result in a factor on all transitional interactions, effectively (which is the same as just dividing the free energy with that factor (which would be $1/2$)).

.\,.\,Oh, and I expect that the $\alpha$ from the last paragraph before the previous one will converge, by the way (although I don't recall the exact argument in this very moment). (11:42) .\,.\,Well, or $\alpha$ of the coherent state won't, but the factor we get on the vertex I strongly suspect will converge---and also when the UV and IR cutoffs are let tend towards their limits, by the way. (11:44)

(11:44) Wait. My argument for why the factor, or the coupling, we could call it, will converge when e.g.\ $\Lambda \to \infty$ rested on the fact that $\alpha$ of the coherent state would grow as a consequence, did it not?\,.\,. So would that really mean, given the $\exp(-n^2/N)$ factor, that the coupling for these `vacuum boson interactions' will actually vanish when $\Lambda \to \infty$??\,.\,. (11:48) .\,.\,Oh my.\,. This is interesting indeed.\,. (11:49)

.\,.\,That is very interesting, and even if not, it still seems that $\hat H_\mathrm{QED}$ is obtainable (perhaps just with a lot more effort, if this $\Lambda \to \infty$ thought does not hold true), which is the important part. So in any case, I can actually be hopeful and happy once again, about the fact that all my work can result in a concluding $\hat H_\mathrm{QED}$ at some point. (11:52)

(16:57) I've been thinking about things. First of all, it would be weird if the coupling to the vacuum bosons depended on $t$, so we would need to take the approach where we try to find (almost) eigenvectors of the perturbed vacuum instead of looking at $\hat U(t) \ket{\,}$, which also makes a lot more sense in general. Second of all, if we divide momentum space into local parameter volumes (sorta like my $X_{\mathbf i}$ in the SA paper), then we get several independent vacuum bosons thereby, except that they can interfere with each other. And then the weird, and interesting, thing is that when $\Lambda \to \infty$, we get more and more interference between these split-up bosons, which means that their particle density cannot each go as $\sim N$. Interesting indeed, but then this got me thinking of the third thing, just a few moments ago, which is that the amplitude of the `interaction terms' should indeed go as the square root of the particle number, but only.\,. wait, what am I saying, I guess not.\,. (17:05) .\,.\,(i.e.\ not the thing I was about to say with ``but only.\,.'') .\,.\,Yeah, nah, never mind that.\,. (17:07) \ldots (17:26) Wait, unless.\,. .\,.\,Hm, $d$ (in $\Delta/2 + d$) is 0 for the most common vacuum--physical interaction terms.\,. .\,.\,So there's no positive interference, and $\hat b_{s, \mathbf k}$ should only grow in strength if there are several particles in exactly the same $\delta k^{3}$ volume, which obviously has a smaller and smaller chance when $\delta k^{3}$ shrinks.\,. (17:30) .\,.\,Would you look at that.\,. .\,.\,Wait, $\hat b_{s, \mathbf k}$ cannot grow (in transition amplitude) in the first place, but $\hat a_{\mathbf k}$ can.\,. .\,.\,So if the particle number ``only'' grows as $\delta k^{-3}$, then $\hat a_{\mathbf k}$ should be $\sim 1$ in strength (for fixed cutoffs). And then these interaction terms should vanish when $\delta k \to 0$, like they would if $\hat a$ did not grow in strength with $n$.\,. .\,.\,Holy\ldots\ (17:40)

(20:17) Okay, this last part doesn't work after all. It would have been quite unbelievable if it did (then I would have spent months dancing around a pretty obvious fact). But no, it's part of my $\Delta/2 + d$-heuristics calculations that the amplitude goes as $\delta k^{3\mu-3}$ for the vacuum particles, but then if there are $n$ times that amount of vacuum triplets/quartets/etc., the interaction becomes $n$ times stronger.

But if we go back to the second thing about the interference, I actually now think that this can be the solution to it all, and one that still shows us that the vacuum-perturbing terms can be removed. The thing is, if we consider one particular $\mathbf{p}$ fermion as part of a vacuum (almost) boson, then it's antiparticle in this vacuum boson can be in only one of the $\sim \delta k^{-3}$ amount of fermion states, which means that the amplitude, for the fixed $\mathbf{p}$, must go as $\delta k^{-3/2}$. That gives us converging `interaction terms,' but then what happens when $\Lambda$ grows?\,. Then there will be more $\mathbf p'$-states that share the same $\mathbf{p}$. And by my thought calculations today, we expect each individual split-up (think $X_{\mathbf i, \mathbf j}$) vacuum boson to have an occupation number that grows up to $\sim N \propto \delta k^{-3}$, and we expect the factor to go like the coupling strength divided by the free energy difference. So this ought to ``diverge'' in the sense that the $\mathbf p$ state ought to be shared by more and more $\mathbf{p}'$-antiparticle states as $\Lambda$ grows, such that the amplitude over each $(\mathbf{p}, \mathbf{p}')$ decreases with $\Lambda$, and ought to vanish when $\Lambda\to \infty$. (20:33)

.\,.\,And that's how the `interaction terms' ought to vanish. Now, I'm pretty sure that I need to include the free energy, multiplied by $\delta k^{3/2}$, in $\hat H_\mathrm{vac}''$. But I might be able to leave out the ``physical'' interaction terms, and only deal with them later, when we consider $\ket{\chi} = \hat A^\dagger_{\psi} \hat A^\dagger_{\phi} \ket{\,}$. But either way, I think we can make it work. So yeah, this is the plan now, for sure. I hope it works.\,.\,:) (20:38)

(04.06.24, 14:32) I've been thinking today about how to make this argument in a decent way. I went on a midday walk, which I just got home from. At the middle, or rather the apex, of the walk, I had a pretty good idea about using some special cutoff, not with a sharp $\Lambda$, but where the cutoff depends on the momenta, as a way of making sure that each $\hat b^\dagger_{s, \mathbf p}$ state is shared by a growing number of $\hat d^\dagger_{s', \mathbf p'}$-states, where most of these $\mathbf p'$-states are actually decoupled from each other.
%Lige lidt memorabilia fra turen: Jeg så en gigantisk larve, måske omkring 12-13 cm, og så undersøgte jeg, hvilke nogle insekter laver det skum, man nogen gange ser på planter. Så så jeg også en kæmpe stor flåt :S, siddende under et blad på en stor græs-plante, måske omkring en halv centimeter. Så fik jeg min først idé. Lidt efter, på vej under en tunnel, så jeg en lille mus pible hen over tunnelen. Det var lige øjeblikket efter, at jeg fik følgende idé:

But this idea didn't seem to work perfectly, so I thought some more, and then I got the following idea, which is to let $\delta k$ for the antiparticles (i.e.\ the $\hat d^\dagger$-particles) decrease much more rapidly than for the $\hat b^\dagger$-particles, namely by breaking momentum conservation slightly for the discretization of $\hat H$. Then I think that you get a discretized vacuum, where all the oscillators with different $\mathbf p$ (and $s$) in their $\hat b^\dagger_{s, \mathbf p}$ will be effectively decoupled from each other! And that system is solvable, it seems! (14:47) .\,.\,And yes, you do indeed get that each $(\mathbf p, \mathbf p')$ pair will get an amplitude that decreases when $\Lambda$ grows, vanishing completely when $\Lambda \to \infty$. So it seems that this will solve all our trouble! 7, 9, 13! :) (14:49)

(05.06.24, 10:08) The double discretization idea of letting $\delta k$ for the antiparticles decrease more rapidly than for the particles does work. The easiest way to see this is to note that a discretized $\hat d_{\mathbf p'}$ is already equal to a sum of $\hat d$-operators with smaller $\delta k$. So this double discretization will not prevent the negative interference at all, really. The next immediate idea would be to approximate $\hat H$ by choosing a single narrow $\hat d_{\mathbf p'}^\ddagger$ for each $\hat b_{\mathbf p}^\ddagger$, but one can see that this does not approximate $\hat H$. For if we look at two states that will interfere negatively with each other after a transition, these will then no longer interfere this way, giving us a different final state. So back to considering how to solve this problem once again, perhaps by using that funky cutoff idea from yesterday.\,. (10:16)

(16:06) Okay, I'm quite excited about a new idea to potentially find an $\varepsilon$-almost eigenvector of the perturbed vacuum where $\varepsilon \to 0$ when $\delta k \to 0$, and where the amplitude over each $(\mathbf{p}, \mathbf{p}')$ point indeed vanishes when $\Lambda$ is subsequently sent towards infinity. I was felt almost a bit stuck all day, otherwise, until I took a good walk, which I just got home from, and on that walk I start thinking about a potential almost solution to try.

The idea is this: If you look at each individual, fixed $\mathbf{p}$ and try to solve that system for all the $(\mathbf{p}, \mathbf{p}')$ vacuum states---we're talking only about the Dirac interaction here, and we just omit the $\mathbf k = \mathbf{p}' - \mathbf{p}$ for simplicity---then perturbation theory gives a certain solution when $\delta k$ approaches 0 (which means that the number of $\mathbf p'$'s goes as $\delta k^{-3}$ for the fixed $\mathbf{p}$). The idea is then to try a solution where you take each individual solution when there's only one, fixed $\mathbf{p}$, find the combined creation operator of that state, $\hat A_{\psi_\mathbf{p}}^\dagger$, and then construct the trial solution as $\sum_\mathbf{p} \hat A_{\psi_\mathbf{p}}^\dagger \ket{\,}$. By my calculation out here on the walk, I then got that you would get the same state back, when working with $\hat H$, times some energy which I believe goes as $\delta k^{3/2}$ (but I could of course be mistaken) *(I meant $\delta k^{-3/2}$, growing), plus an error vector that is the sum of $N \propto \delta k^{-3}$ vectors with norms that goes as $N^{-1}$. The crucial point is then that it seems that all these vectors will be approximately orthogonal to each other (because they involve\ldots).\,. Hm.\,. I was about to say: `because they involve an annihilation which takes two of these oscillators down to their unperturbed ground state, and because they are almost orthogonal to their unperturbed ground state to begin with,' but this seems fishy, 'cause the whole point is that this results in.\,. Oh no, it does not result in the probability of finding an excited fermion at any individual $\mathbf{p}$ being less than one: That probability should indeed approach 1 when $\delta k \to 0$. But the amplitude over each \emph{$(\mathbf{p}, \mathbf{p}')$ pair} should converge, nonetheless, and vanish when $\Lambda \to \infty$. So yeah, maybe I \emph{am} right that these $N$ contributions to the error vector are orthogonal to each other, and that their norm when combined therefore only goes as $\sim N/N^2 = N^{-1}$. And that would make it an $N^{-1}$-almost eigenvector of the perturbed vacuum! (16:30)

Needless to say, I'm very excited about this potential. The thing is also: It would make my paper 1000 times more eye-catching as well, which is a really important thing for me as well. If I solve this vacuum problem, it would mean the world to me, as I've mentioned, since it would truly crown all my (physics and mathematics) work (especially if my new, modified SA domain idea indeed works). But I also just really need something that's eye-catching and would make people interested in actually reading about and studying my results. .\,.\,I \emph{really} need something to get peoples attention, which could then finally be the start for getting my ideas out to the world. So if this idea could make my vacuum paper do both, i.e.\ crown my work and finally make up something that's attention-catching, then it would needless to say be.\,. incredibly awesome.\,. But I'm getting ahead of myself. I could still very well be wrong about my calculations so far, so let me get them on to paper (digitally and/or in a notebook).\,. (16:37)

(16:46) Oh, and the fact that $\varepsilon$ might go as $N^{-1} \propto \delta k^{3}$ (which decreases faster than $\delta k^{-3/2}$ increases) is also really good, if it holds.\,.

(17:42) I mean $\prod_\mathbf{p} (\hat A_{\psi_\mathbf{p}}^\dagger) \ket{\,}$, of course.

(20:10) No, for each $\hat H_{\mathbf p}$ we will get, let's see.\,. $N$ contributions with amplitude $\sim N^{-1} \times N^{-1/2} = N^{-3/2}$. And this will be $N^2$ contributions in total.\,. .\,.\,So even if they are orthogonal, we will only get $N^2/N^3$.\,. oh, that does get us to where we want be.\,. .\,.\,And are they? .\,.\,Yes, I think so.\,! Or rather, each contribution has only one other corresponding contribution that are not (almost) orthogonal to it, unless I'm simply counting the same contribution twice. *(Or maybe I'm wrong that there are two, to begin with.\,.) Either way, it seems to hold.\,. (20:19)

.\,.\,The Coulomb interaction is more complicated. It probably requires a fresher brain.\,. And I might want to do it after I have a good handle on the Dirac problem first.\,. (20:24)

(06.06.24, 10:55) Oh, there is a danger that the norm of $\prod_\mathbf{p} (\hat A_{\psi_\mathbf{p}}^\dagger) \ket{\,}$ decreases with $\delta k$, in which case $\varepsilon$ might not be $\sim N^{-1}$.\,. .\,.\,Ah, but then there's also a chance that this same decrement, that I didn't consider until now, also applies to the error vector. Let's see.\,. (11:01)

.\,.\,We ought to get a factor of $\delta k^{9/2}$ to all the terms, basically, if my quick calculations from yesterday evening is correct.\,. *(because I get/got that the unperturbed ground state amplitude goes as $\delta k^{9/2}$ for each $\psi_\mathbf{p}$.) .\,.\,Yeah, so I think we are good.\,.\,:) I think we get exactly the same decrement as well for the $\varepsilon$ (error) vector.\,. (11:07)

(11:35) (Actually, the unperturbed ground state amplitude goes as $\delta k^{3/2}$, rather, since it also includes a sum of $|\alpha(\mathbf{p}, \mathbf{p}')|^2$ over all $\mathbf{p}'$.)

(13:01) Oh, I think the fact that is goes as $\delta k^{3/2}$ rather than $\delta k^{9/2}$ (the amplitude of the unperturbed ground state) is exactly what makes it work!\,.\,. 'Cause then we don't get less of an amplitude when $\hat d$ hits.\,. Hm, let me see first.\,.

(13:21) I'm writing down some calculations in my vacuum paper v2 draft, and I've reached a point where it gets a little bit complicated (but I \emph{hope} my logic from yesterday holds up in the end). However, I just had the thought, which I thought I'd mention here, that maybe the excited states does not have amplitudes that goes as $\delta k^{3/2}$ for each fixed $\mathbf{p}$ after all. Maybe they go like $\delta k^{3}$ due to.\,. Oh, wait, maybe never mind.\,. .\,.\,(13:30) Hm, I might be right, but it all then still hinges on the fact that this almost-eigenvector solution works, or course.\,.

(13:35) I guess we can argue combinatorially that $\|\phi\|^2$ (I now call the almost solution $\ket{\phi}$, and have also renamed $\hat A_{\psi_\mathbf{p}}^\dagger$ as $\hat A_{\phi_\mathbf{p}}^\dagger$) goes like $N!/N^N \approx \sqrt{2\pi N} e^{-N}$.\,.

(14:01) Hm, I'm sadly now getting that the error vector is a sum of $N^2$ vectors whose norm is.\,. Oh, wait!.\,. Whose norm goes as $\|\phi\|^2$, which of course also decreases when $N$ grows!\,.\,. Okay, so hope is definitely not at all lost for this almost solution, let's see.\,. (14:03) .\,.\,``Oh, never mind, I need the error vector to decrease \emph{compared} to $\|\phi\|$!\,.\,. This is unfortunate.\,. (14:06)''

``.\,.\,Wait, but we are talking about the norm \emph{squared} of
$
\hat b_{\mathbf{p}'''}^\dagger
	\prod_{\mathbf{p}'' \notin \{\mathbf{p}, \mathbf{p}'''\}}
		(\hat A_{\phi_{\mathbf{p}''}}^\dagger) \ket{\,}
$.
So its \emph{norm} should go as $N \|\phi\|$.\,.\,! So we have $N^2$ contributions of norm $N^{-1} \|\phi\|$.\,. Ah, but that will still not work, unless there's negative interference.\,. (14:10)''

(15:24) I forgot a $\hat d_{\mathbf{p}'}^\dagger$ in
$
\hat b_{\mathbf{p}'''}^\dagger
	\prod_{\mathbf{p}'' \notin \{\mathbf{p}, \mathbf{p}'''\}}
		(\hat A_{\phi_{\mathbf{p}''}}^\dagger) \ket{\,}
$. I realized this an a walk, which I then cut quite short. And then I wrote the following in the vacuum paper draft working notes when I got home: ``(15:15) Or maybe that $\hat d_{\mathbf{p}'}^\dagger$ in the equation just above, which I forgot before, is exactly what matters!\,.\,. The whole point of the $N!/N^N \approx \sqrt{2\pi N} e^{-N}$ formula is that there are $N$ antiparticles that can interfere negatively with each other. But we are only removing one of those antiparticles. The norm squared should go as $N!/(1!)/N^{N-1} = N\times N!/N^N \approx N \|\phi\|^2$, which means that the norm of each of the $N^2$ contributions should go as $\sim \delta k^{12/2} \sqrt{N} \|\phi\| \propto N^{-3/2} \|\phi\|$. So if these contributions are approximately orthogonal, we would indeed get that the norm squared of the error vector would go as $N^2/N^3 = N^{-1}$, giving us a $\varepsilon \sim N^{-1/2} \propto \delta k^{3/2}$. (Hm, which is by the way not perfect, since we would rather have a greater rate at which $\varepsilon$ decreases, but it would still be good, nonetheless, I think.\,.) (15:23)''

.\,.\,So I might be on to an almost eigenvector solution after all.\,.\,! (15:27)

(15:31) Hm, about $\varepsilon \sim \delta k^{3/2}$, this would not matter if my thought from earlier is right, that the excited states goes as $\delta k^{3}$ rather than $\delta k^{3/2}$ (in terms of amplitude). Then we don't need the eigenstate other to show that.\,. Oh wait, this doesn't make sense, never mind, I guess.\,. (15:34) .\,.\,Yeah, never mind that. But it would help the Lorentz covariance argument, since then wouldn't \emph{technically} need to show that the result that the amplitudes vanish when $\Lambda\to  \infty$ applies to all eigenvectors, or at least to those that our given one Lorentz-transforms into. But note that this is somewhat of a technicality anyway since most people will likely accept that it is Lorentz covariant. But it would still be a great and delightful result if it's true, nonetheless. (15:38)

``.\,.\,Wait, we have $N^3$ contributions, don't we?\,.\,. Okay, I'm losing hope a little bit now for this would-be solution.\,. (16:40)''

\ldots\ (18:35) I went on another (longer) walk just now, and on it, I first realized that of course I still need to include the $\hat a_\mathbf{k}$-operators as well. I'm cheating myself by omitting them, since we get fewer error vector contributions by correctly including them.
%Få sekunder efter jeg indså dette, faldt jeg over en mus, der bare overhovedet ikke var bange af sig. Den krøb lige forbi mig og var omkring mig. Og jeg kunne komme næsten helt tæt på, ca. en halv meter fra den eller endnu mindre. Sød.:) Nå, det var rimeligt tidligt på turen for resten, of lidt efter fik jeg så følgende idé:
.\,.\,A little after that, I then realized that because I actually think I'm right about this thing that makes the excited terms get an extra factor of $\delta k^{3/2}$, i.e.\ because it cause more terms to cancel each other in the overall sum whenever an antiparticle is excited, this should mean that the probability of finding a (non-anti-)particle in a $\mathbf{p}$ state should go at a rate below $N$ after all. And then I can use my $\Delta/2 + d$-heuristics to get that the error vector contributions vanish when $\delta k \to 0$, since these terms are just the Dirac vacuum self-interaction terms, i.e.\ where three triplets get one of their particles annihilated together each. So this should mean that we do get a $\varepsilon$ that decreases quite rapidly, I believe! (18:17)

We should that the probability of finding a particle at $\mathbf{p}$ goes as $N^{-2}N=N^{-1}$, rather than $N^{-1}N=1$, if I'm correct in this, which means an amplitude of $N^{-1/2}$. So the particle number goes as $NN^{-1/2} = N^{1/2}$, like I have hoped for it for so long. And then the vacuum self-interaction term (combined) in question (i.e.\ the purely Dirac one involving three vacuum triplets) should get a norm that goes as $\delta k^{3-(-3/2)} n^{3/2} \sim \delta k^{9/2} N^{3/4} \propto N^{-3/2} N^{3/4} = N^{-3/4}$.\,. (18:57) .\,.\,$\propto \delta k^{1/4}$.\,. .\,.\,Yeah, my rule above was indeed $\delta k^{3 \mu/2} n^{\mu/2}$.\,. .\,.\,So yeah, this vanishes, but maybe not at the rate we would hope if everything went our way.\,. (19:03)

.\,.\,(19:07) Hm, I wonder if I should get an extra factor for the fact that every excited photon comes with an excited antiparticle.\,. Hm, but probably not.\,.

(19:22) Oh, it's not exactly like the $\Delta/2 + d$-heuristics, perhaps, since we group everything in terms of the same non-anti-particle.\,. well, maybe that has nothing to say.\,. \ldots (19:37) Well, the ``triplet'' coming from $\hat A_{\phi_\mathbf{p}}^\dagger$ isn't necessarily created from the same $\hat A^\dagger$. We are not evolving the bare vacuum state in this case. So yeah, they are not the same ``interaction terms'' in that sense. So let me see, maybe there's a small chance that the error vector will be smaller.\,.

%(19:50) Jeg skal se på det i morgen, men t hurtigt overslag er, at jeg får N^4 termer, nemlig ét for hvert p, p', p''' og p^{(4)}, hvor p^{(4)} er den oscilator hvor jeg tager det ene k fra (som altså i modsætning til p' er fastlagt ud fra de andre valg). Og disse termer vil vel få en amplitude prefaktor af $\delta k^{12/2 + 3/2}$. Og så mister jeg stadig kun én antipartikel, hvilket giver en faktor af $N$. Så N^4 termer med norm hver især af $N^{-5/2}N= N^{-3/2}$.. Hm, og så går det jo slet ikke godt, hvis det passer.. (19:57)

%(20:19) Hm, two things: Shouldn't we get a decrement when the photon and the non-ant-particle is annihilated, since we then have two extra anti-particles for sure, and not just in a superposition with their respective ground state. And second, Isn't it weird that we, by my logic until now, get more terms when an antiparticle is annihilated, due to the less negative interference? I mean, the terms where the other antiarticles are in the same state are already not there; they are zero. And they shouldn't appear out of then air when we operate with \hat d, should they? (In a way, I can see this make sense, but in another way I also see it not making sense..) (20:25) ..Yeah, we shouldn't get N more terms out of thin air when we annihilate an antiparticle, right?.. (20:26) ..(20:34) I hope I'm right about both things (that would be truly awesome..)..

%(21:57) Oh, I actually \emph{do} think there is a chance that both these things can be true, and that this could indeed gives us the rapidly vanishing \varepsilon that we are looking for. 7, 9, 13. I think I'm right about the fact that annihilation of an antiarticle should cause more terms (I kinda must be..), and in terms of the first thing, that should also be true if I'm right about the norm of the excited states going as \delta k^{3} rather than \delta k^{3/2}. 'Cause everytime I've added a factor of \delta k^{3/2} from annihilating a particle (since that is the amplitude when you consider \phi_{\mathbf{k}} in isolation), I should really add a factor of \delta k^{3} instead. How great if al this turns out to be true..!.. (22:03)

%(22:39) Yeah, I think both of these things are indeed true, which might very well give us a rapidly vanishing $\varepsilon$! 7, 9, 13!

(22:40) I think I've been making some mistakes, and that we actually will get a rapidly vanishing $\varepsilon$! See the comments above this paragraph in the source file. .\,.\,I'm not sure yet, but I'll look more into it tomorrow.\,! (22:43)

(07.06.24, 9:27) Okay, we choose three particles out of the $N$ possible. And for the one we annihilate, we \emph{can} choose the antiparticle as well, but we can also leave the sum there. Then we choose an antiparticle from another of the three triplets to annihilate, and for the last triplet, we can then choose only one $\mathbf{k}$ for the annihilated photon. Let's just pick both antiparticles such that all nine particles are now fixed for each contribution, giving us $N^5$ contribution in total. The total number of terms for each of the contributions, when we pick states from all the other oscillators (one for each non-anti-particle) will then be $\sim (N-3)!/$.\,. Wait no, this is assuming that they are all excited, but I also need to include the ground states.\,. (9:39) .\,.\,So we should sum over all cases for how many excited states there are, I guess.\,. (9:40)
.\,.\,
%\begin{equation}
%\begin{aligned}
%	\sum_{m=0}^{N-3}..
%\end{aligned}
%\end{equation}

.\,.\,Oh, I just realized that we also need to deal with the fact that the photons can positively interfere.\,. (9:44) .\,.\,This is going to be so complicated, unless we can approximate our way out of it.\,. .\,.\,Well, or maybe not: Maybe we can just count the number of photons ad add a factor for how much positive interference we get.\,. (9:47) .\,.\,Okay, we'll see about that, but the total number of terms are.\,. .\,.\,Let's calculate it for $\|\phi\|^2$ first.\,.\,:
\begin{equation}
\begin{aligned}
	\sum_{m=0}^{N} \binom{N}{m} (N-m)! =
		\frac{N!(N-m)!}{m!(N-m)!}
	=
		\frac{N!}{m!}.
\end{aligned}
\end{equation}
Okay, that doesn't look too bad. And for our contribution to the error vector just described.\,.\,:
\begin{equation}
\begin{aligned}
	\sum_{m=0}^{N-3} \binom{N-3}{m} (N-3-m)! =
		\frac{(N-3)!}{m!}.
\end{aligned}
\end{equation}
.\,.\,Oh, I was doing it wrong. It's
\begin{equation}
\begin{aligned}
	\sum_{m=0}^{N} \binom{N}{m} \frac{N!}{(N-m)!} =
		\binom{N}{m}^2 m!
\end{aligned}
\end{equation}
for $\|\phi\|^2$, and
\begin{equation}
\begin{aligned}
	\sum_{m=0}^{N-3} \binom{N-3}{m} \frac{(N-3)!}{(N-3-m)!} =
		\binom{N-3}{m}^2 m!
\end{aligned}
\end{equation}
for our contribution.\,. (10:05) .\,.\,Hm, it looks complicated, but I think we can approximate most of the $m$-contributions to these expressions away.\,. (10:11) .\,.\,Nah, we can't.\,. (10:13) .\,.\,Unless.\,. (10:13) .\,.\,(10:18) Hm, it's maximum is at.\,. .\,.\,Hm, $m = (N-m)^2$ for the first expression, which gives us $m^2 - (2N+1)m + N^2 = 0$.\,. (10:21) .\,.\,This gives us a maximum at $m = (2N+1)/2 > N$.\,. Hm, would you look at that.\,. (10:24) .\,.\,That can't be right.\,. .\,.\,Oh, I need to \emph{solve} the equation.\,x) (10:26) .\,.\,So $(2N+1)/2 \pm \sqrt{(2N+1)^2 - 4N^2}/2$.\,. .\,.\,$=(2N+1)/2 \pm \sqrt{4N + 1}/2 \approx N - N = 0$.\,. Yeah, that.\,. Well, that looks right for $m = (N-m)^2$, but it doesn't make sense that we get most contributions when there are almost exclusively ground states in play, so I must have made a mistake.\,. (10:34) .\,.\,Oh wait, $m$ was the number of ground states. Then it makes sense.\,. (10:42) \ldots (10:53) Hm, and now I get a little bit stuck.\,.

.\,.\,Hm, but I can maybe compare the two expressions.\,. .\,.\,Ah yeah, 'cause for each $m \leq N-3$, we have $\binom{N}{m}^2 m!/\binom{N-3}{m}^2 m! = N!(N-3-m)!/(N-3)!(N-m)!$.\,. (11:02)
.\,.\,$\approx N^3/(N-m)^3$.\,. .\,.\,Hm, this is unfortunately roughly equal to 1 when $m$ is small compared to $N$, which it is on the $m$-axis around where the contributions to the sum are greatest, for large enough $N$, that is. So we should actually, if I'm correct here, expect that the two expressions are asymptotically equal, I guess, which would not be great news.\,. (11:10) .\,.\,Oh, $N^6/(N-m)^6$, rather, but that gives the same issue.\,. .\,.\,Wait! $(2N+1 - \sqrt{4N + 1})/2$ is not at all $\approx N - N$. It's approximately $N - \sqrt{N} \sim N$. Hm.\,. (11:15) .\,.\,Ah, and that obviously makes more sense for $m = (N-m)^2$, duh'.\,. .\,.\,So $N^6/(N-m)^6$ becomes $\approx N^6/N^3 = N^3$ on that area.\,!\,:) Great.\,.\,!\,.\,. (11:19)

.\,.\,(11:21) Ooh, and about the photons, we only have one less in the mix after the annihilation, and \emph{from} the annihilation we also get a factor that grows as the square root of the occupation number!\,.\,. (11:22) .\,.\,Yeah, annihilating a photon should only make the norm increase, oh, but I guess that's also what we don't want to happen.\,. .\,.\,Oh wait, we shouldn't need to consider the photon occupation number when we count them all as distinct photons to begin with (I think)!\,.\,. (11:27) .\,.\,Yes, never mind about that!\,\texttt{:D}\ (11:28)

.\,.\,So if we go back, we should thus have $N^5$ contributions whose norm squared goes.\,. wait.\,. (11:33) .\,.\,Ah yes, whose norm square goes as $(\delta k^{3})^2 N^{-3} \propto N^{-5}$.\,. Hm, I don't like that, but let me think about that extra antiparticle in the mix again.\,. .\,.\,Hm, shouldn't I also get the three $\delta k^{3/2}$-factors from the oscillator solution, or is that double counting?\,.\,. (11:39) .\,.\,Hm, in my second expression above, I'm counting how many states there are \emph{given} that the five (plus one, plus three) momenta are fixed, and that the three particles have just been annihilated.\,. .\,.\,So I ought to count those factors as well for sure, right?\,.\,. (11:46) .\,.\,Hm, I'm not sure. Let me think for a bit.\,. (11:48)

(11:58) I guess I do need to consider photon interference for $\|\phi\|^2$, but I'm not sure.\,. .\,.\,Yeah, probably.\,.

.\,.\,Okay, but I think I \emph{should} count the factors of $\delta k^{3/2}$ on top of the earlier result, which would then give us $N^5$ contributions of norm squared $\sim N^{-1}N^{-3}N^{-3} = N^{-7}$.\,! (12:01)

.\,.\,And then there's just the photon interference to consider, and not least the question of how orthogonal the contributions are, and then also the smaller matter of proving the $N^{-3}$ factor, i.e.\ completing and bolstering my calculations here above.\,. Let's start with the orthogonality.\,. (12:04) .\,.\,Wait no, I need to conclude about these $\delta k^{3/2}$-factors first.\,. .\,.\,Oh, now I think that I shouldn't include them, as that would be counting them double. Hm.\,. (12:08) .\,.\,Yeah, no, unfortunately.\,. I think it's just $\sim N^{-1}N^{-3} = N^{-4}$.\,. (12:11) .\,.\,No, $\sim N^{-2}N^{-3} = N^{-5}$, sorry, but still. (12:12) .\,.\,Yeah.\,. .\,.\,And if we don't fix the extra antiparticle, it just becomes $N^4$ contributions of norm squared $N^4$ instead, by the way.\,. (12:16)

.\,.\,So I guess I should shift my focus and try to argue the amplitudes always vanish when $\Lambda \to \infty$ instead.\,. (12:19)

\ldots (15:15) I have couple of things to write, but let me start by noting that I forgot to divide the ground state amplitude with the eigenvalue, $E$. It seems that this will go as $N^{-1/2}$, but I don't rely on my calculations from fore-yesterday evening.\,. If it's true, however, then the ground state should go as $\sim 1$ instead of $\delta k^{3/2}$, amplitude-wise, which would obviously change things.\,.

.\,.\,But the other thing are: I had an idea, on the walk that I was just on, to try to excite the vacuum sixtet.\,. .\,.\,sextet boson first from the ground state, and then apply $\prod_\mathbf{p} (\hat A_{\psi_\mathbf{p}}^\dagger)$ afterwards. Divide this with the energy of the almost eigenstate, $\prod_\mathbf{p} (\hat A_{\psi_\mathbf{p}}^\dagger) \ket{\,}$, and add it to this same state. The hope is then that when we apply $\hat H$, the contribution from this added state will then cancel most of the previous error vector, and hopefully result in a smaller $\varepsilon$. Possibly an interesting idea.\,.

Prior to that, actually, I had the idea to look at $\varepsilon$ as a function of $\Lambda$. If it decreases as a function of $\Lambda$, we should be done, essentially, especially if the thing about the number of excited states only going as $\sqrt{N}$ remains true.\,. I guess. Interesting, for sure.

And now there's this potential about having previously forgotten to divide the ground state amplitude, which I've been calling $a$ in my other notes, with $E$.

But there is also something not so great. At the end of my walk, I realized something that I believe I haven't thought of before.\,. Or maybe I have at some point, but not regarded it as important back then, I don't know. The thing is that if we can show that the final $\hat H_{QED}$ is Lorentz-covariant, i.e.\ without the vacuum-perturbing terms, from the fact that the initial Hamiltonian is, then we should also just as well---in fact much more easily, actually---be able to turn it around once again and show that the initial Hamiltonian is still  Lorentz-covariant even when you remove the pre-vacuum-perturbing terms from it.\,!\,.\,. (15:33) And I don't like this too much, although to be fair, we cannot know for sure that this isn't also the case.\,. (15:33)

.\,.\,But this realization still makes me believe a tad more in the possibility that there are `vacuum bosons' whose amplitudes converges when the cutoffs are lifted, and when $\delta k \to 0$. (15:37) .\,.\,(Or just converging vacuum solutions in general; they don't necessarily have to be divisible into those almost-bosonic states.\,.)

(15:48) Oh, if the amplitudes of the ground states increases by $\delta k^{-3/2}$, then this could definitely mean that we get to add three factors of $\delta k^{3/2}$ for the three annihilated particles after all.\,!\,.\,. .\,.\,And that would then definitely get us to where we want to be (if I can also show approximate (or complete) orthogonality).\,. (15:51)

(15:54) Wait a minute, wouldn't this actually also increase the number of excited states, perhaps (somewhat counterintuitively)?\,.\,. .\,.\,No, it would decrease the number of particles.\,. (16:01)

(17:04) Okay, I actually do get that $E \sim \delta k^{3/2}$, and that the ground state amplitude for $\phi_\mathbf{p}$ therefore goes as $\sim 1$, i.e.\ it converges. (And the amplitude of the excited states of course still goes as $\delta k^{3/2}$, otherwise we would need further normalization.) (17:07)

(17:14) I guess we then need to compare
\begin{equation}
\begin{aligned}
	\sum_{m=0}^{N} \binom{N}{m} \frac{N!}{(N-m)!} (\delta k^{-3/2})^m \propto
		\binom{N}{m}^2 m! N^{m/2}
\end{aligned}
\end{equation}
and
\begin{equation}
\begin{aligned}
	\sum_{m=0}^{N-3} \binom{N-3}{m} \frac{(N-3)!}{(N-3-m)!} (\delta k^{-3/2})^m \propto
		\binom{N-3}{m}^2 m! N^{m/2}.
\end{aligned}
\end{equation}
.\,.\,Which should give us the same $\approx N^6/(N-m)^6$ fraction, but where $m$ might be different around where the expression have their maxima, and thus where they have their significant contributions from.\,.
.\,.\,Hm, its maximum should be at $m = (N-m)^2 \sqrt{N}$, I guess.\,. .\,.\,And this time, let us just guess that $m \approx N - N^{1/4}$, then. Nice.\,. (17:29) .\,.\,And this then gives us a fraction of $\approx N^6/(N-m)^6 \approx N^6/N^{6/4} = N^{9/2}$, rather than $N^3$.\,.\,!\,.\,. .\,.\,Hm, but it's unfortunately not as great as we would have wanted it to be.\,. (17:33) .\,.\,Oh wait, we are calculating the norm squared, so it should be $(\delta k^{-3})^m$ instead in the formulas above!\,.\,. .\,.\,So $m = (N-m)^2 N$, which gives us $m \approx N - 1$, as you would expect.\,. Wait no, I would actually expect closer to a half of the oscillators to be excited on average, so what am I doing wrong?\,.\,. (17:40) .\,.\,Hm, something.\,.

.\,.\,Ah, but the contribution can still be slightly greater from the one where.\,. No.\,.

.\,.\,Well, put it this way, the negative interference can only be greater when we have just de-excited three oscillators, since this means that the spot of their antiparticles, even the newly annihilated one, should be left out in the full state. And if we just assume that the negative interference is unaffected, we should still get $N^{-3}$ times the terms in $\|\phi\|^2$, and we should get a factor of $\delta k^{9} \propto N^{-3}$ as well. This should give us a norm squared of $N^{-2} N^{-3}N^{-3} = N^{-8}$ for each of the $N^5$ contributions. (17:59)

.\,.\,(17:59) Oh, and since I now, however, expect that the number of excited particles will still go as $N$, then there's a chance that I will actually reach the conclusion hereby that the vacuum.\,. Oh, except that it might still depend on $\Lambda$. But we'll see. The important thing is just to solve the vacuum, which I might have done here, essentially. And if the amplitude of the vacuum states then turns out not to vanish, but to converge to some non-vanishing states, well, then this is at least as exciting. (18:03)

(18:13) Ooh, the amplitudes actually does seem to converge when $\Lambda \to \infty$. So I've likely been wrong about my idea that the vacuum-perturbing terms can be removed. (And my initial motivations for thinking so are now gone, and my recent motivation (from around the beginning of this week) that each $\mathbf{p}$ is shared by more and more $\mathbf{p}'$'s is by the way also gone, since today I realized that we could easily have that.\,. hm, that each $\mathbf{p}$ is shared only by a more local group of $\mathbf{p}'$'s.\,. hm.\,.).\,. Hm, before I continue, let me just think it over some more.\,. (18:18) .\,.\,(18:21) Oh no, the amplitude over each $(\mathbf{p}, \mathbf{p}')$ should vanish, in fact. So maybe we \emph{can} remove the v.p.\ terms.\,. (18:22) .\,.\,(Because each $\mathbf{p}$ will indeed be shared by more and more $\mathbf{p}'$'s.\,.)

.\,.\,So yeah, I think I might have solved the perturbed vacuum, essentially, and shown that we can remove.\,. Wait, what about the Lorentz covariance argument?\,.\,. .\,.\,Hm, that will not work out completely as is, then.\,. (18:28)

(19:33) Oh, maybe the particle number actually goes as $\sqrt{N}$, or at least at a rate less than $N$, after all. And then we can remove the vacuum-perturbing terms, and the Lorentz covariance argument holds.\,.

.\,.\,The point is that once we start to get $\sim \sqrt{N}$, we start to have that half the terms cancels out, and it only gets worse from there. Hm, let me actually see.\,. .\,.\,Hm, I think it ought to go like $\binom{N}{n} e^{-n^2/N} \sim e^{-n^2/N} e^{n} e^{N-n} N^N/(e^{N} n^n (N-n)^{N-n})$.\,.  $= e^{-n^2/N} N^N/(n^n (N-n)^{N-n})$.\,. .\,.\,Nah, this should still be greatest around $n \approx N/2$, I'm sure.\,. .\,.\,Ah, let's divide with the constant $N^N$ to get $(e^{-n/N}/n)^n (N-n)^{-N+n}$.\,. %.\,.\,$= (e^{-n/N}/n)^n (N-n)^{-N} (N-n)^{n}$.\,.
.\,.\,This grows with about $e^{-n/N} (N-n)/n$ every time $n$ increases with 1.\,. .\,.\,So we are solving $e^{-n/N} (N-n) = n$ rather than $(N-n) = n$. This gives us $(e^{n/N} + 1)n = N$. And this still gives us $n/N = \text{some constant}$, just not $n/N = 2$ exactly.\,.

.\,.\,So we should get that $n$ goes like $N$.\,. (20:01)

.\,.\,Well, $e^{-n^2/N}$ is not a good approximation when $n$ is close to $N$, so let me consider my logic above with the squared binomial coefficients and all that.\,. (20:10) 
.\,.\,Well, instead of $e^{-n^2/N}$, I should just use $N!/((N-n)! N^N)$.\,. .\,.\,So we get $\binom{N}{m}^2 m!/N^N$, which should still, regardless of the constant $N^{-N}$, be most significant at $m = (N - m)^2$.\,. Ah, but here $m$ is then actually $n$, the number of excited states. So we'll get $n \approx N - \sqrt{N} \sim N$.\,. .\,.\,But we shouldn't get more excited states due to the negative interference, so I'm making mistakes.\,. .\,.\,Oh, it should have been $\binom{N}{n}^2 n!/N^n$.\,. .\,.\,which should have its maximum at around when $Nn = (N-n)^2$, I think.\,. .\,.\,And that would give us $n \approx N -$.\,. well, we can see that $n$ should be some constant fraction of $N$.\,. .\,.\,$(1-x)^2 = x$, yeah. So there we are, $n \sim N$. (20:29)

(20:40) It's a bit irritating, 'cause I've might have achieved what I sought, if these calculations from this afternoon--evening holds true, and if the amplitude thus vanishes with $\Lambda \to \infty$, but now I no longer feel like that's quite enough. Now I feel like there is a real potential that one vacuum solution might make us able to remove the vacuum-perturbing terms, but that this particular solution might not Lorentz-transform into a similar one. And it then might be the case that there is some solution to the perturbed vacuum, a ground state perhaps, that transforms into itself, and which we then need to find in order to solve the problem fully. And that sounds.\,. well.\,. not too great.\,. (20:47)

.\,.\,Well, it's okay. I still get to finish my vacuum paper, then, where I can just point out the potential hole for the Lorentz covariance. And then I think that the problem of searching for ground states and/or for states that Lorentz-transform into themselves is a problem for somebody else.\,. (20:53)

(08.06.24, 9:19) Hm, I thought of something. Even if we find a solution with `vacuum bosons' and all that, then we run into the same problem, right? 'Cause that will still yield a Hamiltonian where the vacuum perturbing terms are replaced, not with nothing, but with multi-particle interactions. And we should then, by the same logic, be able to go back by making a reverse DSR, giving us a different Hamiltonian from the initial one as well. I think so. And this would mean that the only way that we can't do this, is if we can never replace the vacuum-perturbing terms in the first place, which would be the case if the vacuum can be excited into several different states (infinitely many, likely), which means that it is no longer a `vacuum,' i.e.\ no longer a backdrop for the physical system, but is part of the physical system. The ``vacuum states'' would then constitute physical states---maybe like a kind of physical elementary particles, in a sense.\,. So there we are, maybe the thought that the pre-vacuum-perturbing terms \emph{can} be removed from the initial Hamiltonian without ruining the Lorentz covariance is not the weirdest possibility out of all these options.\,. (9:27)

(10.06.24, 19:38) Woah! I just had a potentially great idea.\,.\,!\,.\,. But before I start writing about it, let me finally note that the $N^{-2} N^{-3}N^{-3}$ result from above was wrong. I am also double-counting the $N^{-3}$'s in this case, regardless of the fact that the ground state amplitude is now $N^{1/2}$ times greater than the excited states'.

Let me also mention that.\,. Nah, let me wait with that. Let me just write quickly about this new idea: It's about constructing a domain, much like my previous ones, for the countable-basis-DSR'ed Hamiltonian, but where one uses the divergence in the basis vector count, rather than any UV or IR divergence, to cancel the (pair) productions *(enough to make them finite, and so on). (19:46)

%(19:54) (`Great' is an understatement, by the way, if it works out..)

(11.06.24, 10:57) Hm, for the countable-basis idea, there's no $\delta k^{3/2}$ on the `physical' part of the Hamiltonian, so maybe one could also use the `physical' emissions/absorptions for the $E$-set.\,.

(13:16) I still need a cutoff for the number of (countable) basis vectors to include, so my idea here from yesterday evening doesn't solve my problems, at least not immediately.\,.

.\,.\,Could a basis-vector-dependent UV cutoff make sense at all.\,.\,?\,.\,. (13:22) .\,.\,Hm, I don't like that, thinking of the approximate Dirac sea (which for the countable-basis idea could look like a sea of negative-energy particles filling up the $M$ first states of the countable basis (and the bases for the photons and the particles are actually free to be chosen however we want, then)).\,. (13:25)

(13:51) Åh! The UV cutoff on the physical Hamiltonian does not need to match that of the vacuum Hamiltonian---or more specifically, there can be a different cutoff for the antiparticle $\mathbf{p}$'s.\,! And in that sense, it might actually make sense with a basis-vector-dependent UV cutoff after all!\,.\,. (13:53) .\,.\,Well, not exactly, no, but the idea is that the physical UV cutoff can be made to follow (or ``lead''.\,.) the basis-vector number cutoff towards infinity.\,. (13:57) .\,.\,Which allows us to then use an (antiparticle-)basis-vector-dependent $E$ set. (13:59)

\ldots\ (16:44) The things I wanted to mention yesterday (``Let me also mention that.\,. Nah, [\ldots]'') was that one could potentially continue my $\prod_\mathbf{p} (\hat A_{\psi_\mathbf{p}}^\dagger) \ket{\,}$ idea by trying to patch it, first by adding $\delta k^{3/2} \prod_\mathbf{p} (\hat A_{\psi_\mathbf{p}}^\dagger) \hat A_{\chi}^\dagger \ket{\,}$, where $\chi$ is the six-particle state that comes from having three Dirac vacuum triplets and then then let one particle from each triplet annihilate each other together. And then from there you could perhaps continue adding small corrections, for instance to further cancel (part of) the final state where there are two $\chi$'s created (i.e.\ via yet another three-triplet--three-particle annihilation), and also to cancel.\,. the free energy contribution from the $\delta k^{3/2} \prod_\mathbf{p} (\hat A_{\psi_\mathbf{p}}^\dagger) \hat A_{\chi}^\dagger \ket{\,}$-addition, or something like that (I don't remember what other term I thought of, but I thought of something other than the mentioned two-$\chi$ state). And by continuing adding such corrections/patches, one could perhaps get down to an $\varepsilon$-almost eigenvector, where $\varepsilon$ vanishes at a faster rate than $\delta k^{3/2}$.

I also thought about the fact that one might be able to do something similar but where one tries states constructed from coherent states instead (where the ``patches'' then include coherent states with the other ``vacuum bosons,'' as I have called them). But I'm not sure (rather: I don't think) that this is more promising than to start with the $\prod_\mathbf{p} (\hat A_{\psi_\mathbf{p}}^\dagger) \ket{\,}$ instead, as described in the previous paragraph.

Okay, that was good to get those ideas out of the way. I'm nt planning on pursuing them, even though it could be awesome to find/see such a solution. But the problem is, as I've talked about, that we would need to show that the amplitudes vanishes when $\Lambda \to \infty$ for \emph{all} eigenstates at once (which was my goal with trying to solve that problem), and not just this particular (potential) one.

Let me now take a small break, and then I will write about some more thoughts about this new idea from early afternoon today. (17:01)

(17:12) It seems that this realization from earlier today could do the trick. The point is that in my modified version of my SA (self-adjointness) domain, I have only the upper radius for $E$ (the lower one is removed by a trick, which I have described (briefly) above), which is only as large as the maximum $|\mathbf{k}|$ (or $|\mathbf{p}|$) in the state that it helps cancel. But we could then basically let the $\mathbf{p}$'s of the antiparticles `count for more,' essentially, when they are of a basis vector indexed by a higher number (in the countable set). So we let the radius of $E$ depend, not just on $\mathbf{k}$, but on $\mathbf{k}$ and $m$, where $m$ is the highest basis vector index that appears in the state that is to be (partly) canceled.

Let me also finally mention that for the countable basis, I'm thinking of first dividing into a $\delta k$ grid in momentum space, and then for each grid use a basis of sinus functions. The neat thing is that if the electrical coupling is turned down to zero, an approximate Dirac sea of such states should transform into a similar approximate Dirac sea, of similar states, I'm pretty sure.

This is nice since if we are then able to show that there are almost eigenstates where the particle number remains bounded (practically) over $t$, then we would possibly be able to argue that such a state Lorentz-transforms into another approximate Dirac sea of a similar kind, where the number of holes in the sea (also known as antiparticles) are practically bounded as well. (17:26)

%(17:31) Let me take a longer break and get think some more about it all, before I continue. I'm not doing anything else today anyway..

%(18:53) Maybe I'll actually just finish up tomorrow instead..

%(12.06.24, 12:54) Jeg havde nærmest sådan en mental blokade, når jeg har prøvet at tænke på emnet i går aftes og i morges. Men det kan egentligt være meget sundt, for så kan man lige reboote lidt, og komme tilbage og se på det med mere friske øjne. Nu har jeg så indset en ting omkring min tidligere strategi, og det er at det approksimative Dirac-hav har et problem: Når man kun fylder op til nr. M tilstand i en (tællelig) basis, men lader \Lambda gå mod undeligt, så kommer ens fysiske tilstande jo til at koble til tilstande med energy omkring 0, nemlig når en partikel ryger ned i en af de uendeligt mange høj-impuls-huller, og udsender en foton i den anden ratning, hvilken for store k nogenlunde vil udligne den negative energi fra det udfyldte hul / pre-DSR-antipartiklen. Og da koblinging går som 1/\sqrt{k}, i tre dimensioner, så vil de fysiske tilstande kun koble kraftigere og kraftigere til disse tilstande. Så derfor tror jeg altså ikke længere på, at et approksimativt Dirac-hav kan bruges til at approksimere den sluttelige \hat H_{QED}. (13:03)

%Nå, jeg skal dog stadig lige skrive færdigt om, hvad jeg tænkte kunne være/give en løsning på hele problemet, inden jeg fik denne indsigt og approksimtiv-Dirac-havet. Men nu er der så også lige kommet nogle andre idéer:
%Hvis ikke vi kan bruge approksimtiv-Dirac-havet, så må beviset for Lo. cov. indebære en Lorentz-transformation--tidsudvikling-kreds. Og nu har jeg så overvejet, om ikke man kan gruppere det samlede udtryk for operatoren for hele denne kreds i ordner af koblingskonstanten, som jeg også har tænkt på før. Og jo, det bør man altså kunne.. Og hvis man kan, så må det jo betyde, at tidsudviklingerne i sig selv også kan approksimeres, og blive vilkårligt tæt på Lorentz-kovariante, hvis man ekspanderer/udvider/udvikler dem mht. koblingskonstanten. (13:10)
%Men aha, det må vel betyde, at man kan nøjes med at inkludere endeligt mange knuder i stiintegralerne. Og dt må betyde at vi kun når at få endeligt mange "vakuum-partikler" i disse stiintegraler.
%Så det er altså hertil, jeg er kommet i mine tanker indtil videre, og nu vil jeg så tænke videre.. (13:13)

%..(13:15) Ah, og så kan jeg måske bare gå tilbage til Trotter-ekspansionen, hvor man så kun har endeligt mange \exp(-i\hat H_{vac} \Delta t)-knuder/faktorer i (bidragene til) et samlede udtryk. Og med det så burde jeg kunne bruge $\Delta/2 + d$-heuristikken direkte.. (13:17)

(12.06.24, 13:19) Woah, I just got some potentially awesome new ideas.\,. See the comments (In Danish) above this paragraph.\,.
%(I virkeligheden er det faktisk bare tilbage til nogle gamle idéer, som jeg af en eller anden grund gik væk fra igen, muligvis fordi jeg ikke syntes, at jeg kunne argumentere skarpt nok for, at man må kunne samle i ordner af koblingskonstanten---eller måske faktisk bare at jeg ikke helt kom i mål med det, og at jeg så blev distraheret af nogle andre idéer, der så ledte mig væk fra det. Det må næsten være det.. Men nu når jeg lige tænker over problemet igen, så tror jeg altså godt sagtens, man kan vise, at man rigtignok \emph{kan} samle i ordner af koblingskonstanten.. (13:24))

%..(13:32) Det vil jo være fantastisk, det her, for så \emph{vil} jeg kunne færdiggøre og lave en rigtig god version af min vakuum-artikel..!..

%... (16:07) Jeg har tænkt mere over det, og det kræver jo nok en antagelse om, at Lorentz-transformations-operatoren kan udvides i ordner af koblingen.. ..Og at den kan opskrives med hæve-sænke-operatorer.. Men så kan vi jo så bare ikke være sikre på, at der ikke er vildt mange hæve-sænke-operatorer selv i de små ordner of koblingskonstanten.. Hm..

(13.06.24, 12:24) Okay, I've thought about various things. The ideas i mentioned in the previous paragraph do not hold as is, due to the divergence when $M$, the maximum basis vector number, increases. Well, but I do need to mention that I no longer believe that my `approximate Dirac sea' works for simulating the physics of $\hat H_{QED}$, because there will always be a coupling to states with energies close to 0, i.e.\ when a negative-energy particle is emitted (with a large $\mathbf{p}$) together with a photon (with roughly the opposite $\mathbf{k}$). So I think we have to consider the Lorentz-transformation--time-evolution (and translation) circuit instead. I've then thought about gathering in orders of the coupling constant, and yes, you can do that, but I hoped that I could then use this to show that for each order, only finitely many vacuum particles are created, and therefore they don't influence the physical particles' dynamics. But if we choose the simple $\delta k$-grid basis for the antiparticles, we just see that the number of contributions will increase.\,. well, long story short, there is no intermediary normalization when you expand in terms of the coupling constant, so if the amplitude increases, so does the overall contribution from the given order, and if the number of contributions increase, so does the overall contribution, also.

Today I've been thinking about the $\prod_\mathbf{p} (\hat A_{\psi_\mathbf{p}}^\dagger) \ket{\,}$ solution, and what happens when $\Lambda \to \infty$, and it turns out that you can do a lot with this, potentially. .\,.\,So that's pretty great, but I need to figure out how to use it exactly for the full Lorentz covariance (using the mentioned circuit) argument.

Now, I still need to write more about my self-adjointness (SA) domian idea for the perturbed vacuum---in fact for the full $\hat H_\mathrm{QED}$. So let me also do that.\,. (12:41)
\ldots (12:53) It would be truly awesome if it holds. Let my just reiterate about my modified SA idea in general that it uses an $E$ set that is a set of vectors, rather than just a volume in parameter space. This allows us to recognize vectors who are parallel to $\hat A^+ \psi$-vectors, where $\psi$ is in $F_n^\complement$. And then we can define $F_{n+1}$ as a subset of that $\{\hat A^+ \psi\}$ set, intersected with the set that's only supported on the $E_{n+1}$ volume. (I hope this explanation is right---otherwise, I hope that one can still see the overall point.) This trick then ought to make us able to remove the inner radius of $E_n$. And as far as I can see, using my `utilizing several particles' idea, we can also then even lower the outer radius to just $\max(|\mathbf{k}_{n-1}|, \ldots, |\mathbf{k}_{1}|)$. .\,.\,Well, make that $\max(|\mathbf{k}_{n-1}|, \ldots, |\mathbf{k}_{1}|; |\mathbf{p}_1|, \ldots)$.\,.

Then for $\hat H_\text{DSR, countable basis}$, the idea is to instead of $\max(|\mathbf{k}_{n-1}|, \ldots, |\mathbf{k}_{1}|; |\mathbf{p}_1|, \ldots)$ use that value multiplied by some function of $m$, where $m$ is the maximum index of the basis states that the antiparticles occupy for the given state. Note that we can use my `$\delta k$ grid with sine functions for further resolution' basis, which is an example of a basis where $\mathbf{p}$ is bounded for each basis vector (as it only has support in a single $\delta k$ grid (where $\delta k$ is constant in this scenario: we never let it tend towards 0)). (13:11) .\,.\,The idea is then that the increased radius as a function of $m$ then allows us to cancel more and more of the vacuum emissions with increasing $m$.\,. Wait, let me think, 'cause don't we then have to leave all the $E_{m+1}$'s be, which would result in an infinite vector when we work with $\hat H$?\,.\,. (13:15)

\ldots (13:31) Hm, but can't we just let all of $\hat H_\mathrm{vac}^+ \psi$, $\psi \in F^\complement$ (or $T^\complement$, if you will), be in $F^\complement$ (or $T^\complement$).\,.\,? .\,.\,For we are never using $\hat H_\mathrm{vac}^-$ anyway to cancel ``productions''/emissions.\,. .\,.\,That sounds right.\,.\,:)\,.\,. (13:35) .\,.\,Oh, and $\hat H_\mathrm{vac}^+ \psi$, $\psi \in F$ (or $T$)?\,.\,. .\,.\,The same. 'Cause the radius for $E$ is just 0 always for the $\hat H_\mathrm{vac}^+$ states.\,. well, that's a bit easier said than done, since $\hat H_\mathrm{vac}^+$ might have been part of a previous emission, before the current one.\,. .\,.\,But I think it can work out.\,. (13:41)

Okay, so that's the idea. With a larger and larger $E$ radius ($p_n$, if you will) for increasing $m$, it means that the vacuum pair productions can be canceled more and more fully. So I think this idea could work.

Now, if indeed it is a necessity to let the $E$ radius increase with $m$, that means that $\Lambda$ needs to grow with $M$, where $M$ is the maximum number of.\,. Oh wait, it shouldn't be $\max(|\mathbf{k}_{n-1}|, \ldots, |\mathbf{k}_{1}|; |\mathbf{p}_1|, \ldots)$ \emph{multiplied} with a function of $m$, it should be a function of $m$ that is \emph{added} to the $E$ radius.\,. .\,.\,Alright.\,. .\,.\,This then allows us to use.\,. Wait, I need to think about all this some more before I can start to say whether this idea has a chance of working or not.\,. (13:50) But I should also think some more about if and how one could use that $\prod_\mathbf{p} (\hat A_{\phi_\mathbf{p}}^\dagger) \ket{\,}$ solution.\,.

(17:51) The point was to multiply the $|\mathbf{p}_m|$'s in $\max(|\mathbf{k}_{n-1}|, \ldots, |\mathbf{k}_{1}|; |\mathbf{p}_1|, \ldots)$ by a function of $m$ for each antiparticle state, such that is is actually $\max(|\mathbf{k}_{n-1}|, \ldots, |\mathbf{k}_{1}|;$ $|\mathbf{p}_1|, \ldots, |\mathbf{p}_n|; f(1)|\mathbf{p}_1'|, \ldots, f(m)|\mathbf{p}_m'|)$, instead, where the $\mathbf{p}'$'s are the antiparticle momenta, and $f$ is the mentioned function.

And it could indeed be awesome if it works. But I've actually just found out how we can maybe complete the argument, and in a way where it is not needed to show self-adjointness of the $\hat H_{DSR}$ ($\hat H_ {QED}$ before the v.p.-term removal) specifically. It's a little bit complicated, but I'll write about it at some point after a break\ldots\ (18:00)

(18:17) Okay, the point is to consider $\hat S \exp(-i \hat H t) = \hat T \exp(-i \hat H t') \hat S$, where $\hat S$ is a unitary Lorentz transformation operator, and $\hat L$ is a translation operator. We then Taylor expand all these operators in orders of the coupling constant, and rewrite them in terms of creation and annihilation operators (if they are not already written as such). For each order of the coupling constant, we get a separate equation from $\hat S \exp(-i \hat H t) = \hat T \exp(-i \hat H t') \hat S$. And here's the point: If we rewrite all creation and annihilation operators, at least for the (pre-)antiparicles/negative-energy states, in terms of a countable basis, the equation should be approximately true such that for any finite set of vectors in the Hilbert space, the equation should become more and more true, i.e.\ the difference/error should become a smaller and smaller vector, when we increase $M$, which is the cutoff for how many basis vectors we include from the countable basis. The reasoning for this is that the operators for any vector can be approximated by such $M$-cutoff (and with other cutoffs as well that are also gradually lifted) operators, to arbitrary precision (which follows from the fact that they are unitary, if not otherwise.\,.) And when we also look at a set of initial vectors without any component that includes an antiparticle-$m$ greater than or equal to $M$, we see that we can therefore remove all the creation and annihilation operators from the expression with an $m \geq M$, and then, for high enough $M$, have that the two sides of the equation give the same outcome for the given set of initial vectors. (I might be over-complicating this explanation.\,.) .\,.\,Therefore, if we look at the two expressions on either side of the equation for a given order of the coupling constant, the their difference should converge to $0$ when we let $M \to \infty$.

Then, when we DSR the system, and flip the daggers on some of the ladder operators, this property should still apply: The difference of the two sides of the equation should converge to 0 for each order. .\,.\,Well, or to an operator that only includes some high-$m$ creation and annihilation operators, I guess.\,. (18:41) .\,.\,(18:47) Yes, after the normal-ordering, the difference will only consist, approximately, of operators that has annihilation operators with high $m$ (meaning that the contribute nothing when there are no such state as part of the initial state).\,. .\,.\,So any part of the expression that consist only of ladder operators within some set with bounded $m$ will converge to zero, i.e.\ when we compute the difference of the two sides. So when we DSR, the same will apply. (18:52)

.\,.\,Before I continue, let me talk about what we are trying to get. We are trying to get that a DSR'ed $\hat H$ with a countable basis for the antiparticles will become Lorentz-covariant in the limit when $M \to \infty$, at the same time as all the otther cutoffs are lifted. And I actually want this to apply specifically when we use the good old $\delta k$-discretization basis (from my vacuum paper v1, etc.). So I actually want this rather than a basis where the vectors are constant. And I believe that the above argument applies for this case as well. So we have, or rather we would like to get, that our discretized and DSR'ed $\hat H$ will become Lorentz-covariant in the continuum limit (where $\delta k \to 0$ and all cutoffs are lifted), as long as we can show that is has dynamics that converges, despite the seemingly infinite pair production from the perturbed vacuum. Note that this is what I have always assumed, so all this arguing was really just to get back to the same place where we were before, so to speak.\,. Okay, but then we are left with the same old problem of how to show that the vacuum decouples from the physical states. And the point here is: It \emph{can} decouple, at least if you have enough freedom to choose any initial vacuum state that you like for any given $\delta k$ and $\Lambda$ (and $\kappa$ and $n_\mathrm{max}$).

This is where my thoughts and ideas from this morning/prenoon comes into the picture. I've realized that there are $\sim \delta k^{-3}$ possible eigenstates for each oscillator, and the one with the highest energy has an energy, $E$, that's approximately equal to the free energy of the most energetic excited state(s). And most of the amplitude of this eigenstate will sit at those most energetic excited states. So an idea could be to always choose the eigenstate with the highest energy for each $\phi_{\mathbf{p}}$ in the $\prod_\mathbf{p} (\hat A_{\phi_\mathbf{p}}^\dagger) \ket{\,}$ solution. Now, I think that it might work as well if we simply choose the lowest-energy $\phi_{\mathbf{p}}$ for each oscillator instead (which would be a more natural choice), but I'm not sure. In any case, it's a good idea. And I'm pretty sure that if we choose the highest-energy $\phi_{\mathbf{p}}$'s (where $\mathbf{p}$ by the way could by the momentum of the antiparticle, instead of the non-anti-particle as it was before), these oscillators then decouple from the physical states in the continuum limit. (19:13) *(Oh, I should also specify that the point is to let $\Lambda \to \infty$ first, and specifically for the photons and the non-anti-particles, whereas the $\Lambda$ for the antiparticles can remain small in comparison.\,.)

But all this is rather complicated, so it requires some more thought.

.\,.\,I'm pretty sure that I have some other things left to write about, that I haven't gotten to yet, but that'll come to me in time.\,. (19:15)

(20:32) One of the things I have left to mention is that it might actually make plenty of sense that we can (possibly) remove the pre-vacuum-perturbing terms from the Hamiltonian without destroying the Lorentz symmetry. The point is that if we look at the path integrals, then it is natural to think that in position space, we get a some interaction vertices in each point of space (after the Dyson expansion), and that each $\mathcal H_\mathrm{I}(x)$ will transform into a similar $\mathcal H_\text{I}(x)$ after a Lorentz transformation. Ad if we look at the pre-vacuum-perturbing terms specifically, it makes sense to guess that these $\mathcal H_\text{pre-v.p.t.}(x)$ will then transform into similar $\mathcal H_\text{pre-v.p.t.}(x')$ for the transformed system. In other words, it makes sense to expect each $\mathcal H_\text{pre-v.p.t.}(x)$ contribution in the path integral to yield a similar $\mathcal H_\text{pre-v.p.t.}(x')$ contribution for the transformed path integral, also consisting only either exclusively of creation or of annihilation operators *(well no, not before the DSR; \emph{after} the DSR, these term will consist of such.\,.). So therefore it makes sense to guess that we are actually free to remove any such vertex, without destroying the Lorentz covariance, and that nature (seemingly) just happen to have only removed the terms that would otherwise make the theory mathematically ill-defined, and kept all the other particle--antiparticle interactions.

Now, it might even be possible that one can show this. But I don't have the competence to do so (and I'm not going to study QFT to that great of an extent to be able to do so in a near future). But this means that the thing that I'm trying so hard to show, there's actually a small (perhaps very small, but still.\,.) chance that somebody already knows how to do this using the conventional Lorentz-covariance logic from conventional QFT. .\,.\,That wouldn't be a bad thing, certainly, and I would also personally be happy, actually, if that was the case. But yeah, since I don't know that this is possible, and since I don't think I am able to undertake that route myself, I still need to continue this work (for now).\,. (20:47)

(14.06.24, 12:39) I think this idea of letting $\Lambda$ for the photons and non-anti-particles grow much more rapidly than for the antiparticles (letting it grow to infinity as $\delta k \to 0$) is really important. And I think I'm so close to having figured it out now. But I just had fear that it wouldn't hold because this solution also implies, I'm pretty sure, that each antiparticle state (in the $\delta k$-grid) is occupied with a probability tending to 1. But then I thought of my argument from my vacuum paper, where we construct $\ket{\chi} = \hat A^\dagger_{\psi} \hat A^\dagger_{\phi} \ket{\,}$, which doesn't seem to care about the interference. And indeed I think this argument still holds: As long as the `interaction terms' vanish, $\chi$ will still be an $(E_\psi + E_\phi)$-eigenvector, despite the almost complete negative interference when $\psi$ includes antiparticles.\,. Oh, but what about the norm of $\chi$, then?\,.\,. (12:48) .\,.\,That will then vanish as well, and that might be a problem, indeed, if it vanishes at a rate comparable to that of the `interaction terms'.\,.

(12:59) Hm, but if we just included one coupling to a non-anti-particle--photon pair that's far above $\Lambda$ for each antiparticle state.\,.
\ldots (13:22) Okay, there doesn't seem to anything that prevents us from doing that. For the pre-DSR'ed case, this just amount to keep one transition from any given negative-energy state up into a singular high-energy photon--non-anti-particle state (with $|\mathbf{k}|$ much greater than $\Lambda$). And that shouldn't change the dynamics of the system, and it shouldn't go against my circuit argument.\,.

(13:36) Hm, if we use this trick, it still means that the ground state amplitude of each oscillator will tends towards 0 when we increase $|\mathbf{k}|$ for the transition, meaning that the probability that any given antiparticle state is occupied tends towards 1.\,. .\,.\,(So I'm not sure that this trick helps us.)

(13:42) I just realized that the negative interference might also decrease the amplitudes of the `interaction terms' to a similar degree.\,. .\,.\,Hm, not the interaction terms where a vacuum antiparticle annihilates together with a physical particle, it seems.\,. (13:44)

(13:57) I have gotten another idea. Instead of using just a single transition above $\Lambda$ for each antiparticle state, we can use as many as we want (it seems), and we can even make it so that they are all completely different for each antiparticle. More specifically, we can make the sets of photon and particle momenta for each antiparticle, i.e.\ for all the momenta above $\Lambda$, be disjoint (between any two antiparticle states, i.e.). And instead of picking the highest possible eigenvalue, $E$, for each oscillator, we can just pick the lowest instead. With this we'll still get a ground state amplitude that is tending towards 0 (in the limit of this whole procedure). But so will each individual amplitude for all the excited states of the vacuum oscillator, as long as we just include enough transitions for each oscillator (for our set of transitions with $|\mathbf{k}| > \Lambda$). .\,.\,And therefore all `interaction terms' should be affected by the negative interference (including the ones where a vacuum antiparticle is annihilated with a physical particle). I furthermore think that we can even keep $\Lambda$ for the physical interactions down such that the physical particles don't even interact at all with the vacuum photons and non-anti-antiparticles, if this helps us (but I haven't thought this through yet). So all in all, there really seem to be plenty of opportunity to make the vacuum decouple from the physical states if we can indeed just pick these sets of `transitions above $\Lambda$' this freely. (14:11)

I have also thought a bit more about the Lorentz-circuit argument. The point is indeed that when you rewrite in terms of ladder (i.e.\ creation and annihilation) operators and also gather in terms of different orders of the coupling constant, then you must get that any set of transitions with a bounded $m$, and with any other cutoffs that you'd like as well, will tend towards $\hat I$ (pointwise) when you then let.\,. Ah, when you lift the other cutoffs, and let $\delta k \to 0$, while keeping the set constant.\,. Okay, well, the set changes when $\delta k \to 0$ for our $\delta k$-grid, so let me see.\,. (14:16) \ldots (14:25) Hm, but we guess we can just change to the $\delta k$-grid basis after we carry this argument out for some other (countable) basis for the antiparticles that's constant.\,.

%(14:28) Hm, lad mig lige tænke lidt på tasterne.. Pointen er så, at hvis man kan vise at en DRS'ed \hat H også kan approximeres arbitrært godt, hvis man laver cutoffs på transitionerne (og løfter dem gradvist), så må den samme Lorentz-kreds også gælde for denne Hamilton-operator, i grænsen hvor man løfter cutoffs'ne.. (14:30) ..Og \emph{hvis} vi kan vise at vakuummet dekobles fra de fysiske tilstande, så vil dynamikken jo approximeres abitrært godt med en operator, hvor vi kan lave cutoffs på transitionerne.. Okay lad mig prøve at tage den igen.. (14:32) ...(14:44) Okay, jeg trænger til en gåtur, og vejret trænger til at blive gået i, så lad mig gå-tænke noget mere over det, og så komme tilbage og skrive videre.. (14:45) ..Okay, nu lavede jeg mig godt nok lige en ny kaffe, så lad mig lige blive lidt endnu, måske.. ..(14:49) Okay, pointen er, at for den DSR'ede Hamiltonian der gælder Lorentz-kredsen stadigvæk, således at alle transitioner med begrænset $m$ vil gå mod \hat I i græsnen.. Hm.. ..Ah ja, og min tanke er så, at hvis man så også kan vise, at transitionerne hvor vi har høje $m$ i.. begyndelsestilstanden.. Hm..(?) (14:54) ..Hm, jeg kan måske godt starte med at have vacuum-oscillatorerne i grund-/vakuum-tilstanden, hvis det kan hjælpe mig.. ..Tja, nej, de skal måske nok være i en næsten-egentilstand.. (14:57) ...(15:12) Nå, det er på høje tid: Jeg må gå-tænke over det...

\ldots\ (17:59) I actually don't think that my $\prod_\mathbf{p} (\hat A_{\phi_\mathbf{p}}^\dagger) \ket{\,}$ solution will lead me anywhere, even with my ideas from today. But on the other hand, I do think that my self-adjointness domain for $\hat H_{DSR}$ could indeed work. And the idea is really just to use $\max(|\mathbf{k}_{n-1}|, \ldots, |\mathbf{k}_{1}|;$ $|\mathbf{p}_1|, \ldots, |\mathbf{p}_n|; f(1)|\mathbf{p}_1'|, \ldots, f(m)|\mathbf{p}_m'|)$ for the radius of $E$ like I talked about, and not much else (apart from using my other new ideas about self-adjointness domains where the cutoffs can be shown to be valid (I have very briefly described these ideas above)). And I'm not sure that I have mentioned this yet, but the good thing is that I should be able to still show that the vacuum-perturbing terms can be removed from there. For if you can show that $\hat H_{DSR}$ is self-adjoint, where specifically the basis of the (pre-)vacuum-perturbing terms is chosen to be a constant (countable) basis (unlike the $\delta k$-grid basis, in which basis vectors change when $\delta k$ changes), then you use this to argue that one must be able to make $\varepsilon$-almost eigenstates with a particle number cutoff, $n_{max}$, where $\varepsilon$ can tend towards 0 when $n_{max}$ is let tend towards infinity. And that means that we can approximate the dynamics arbitrarily well with only a bounded ($n_{max}$) number of vacuum particles, even though we have a divergent volume, $\mathcal{V}$. And we can argue this by using the fact that $\hat H$ commutes with the momentum operator (for all three spatial directions), which means that the vacuum must have $\varepsilon$-almost eigenstates that are spread out in position space. So if we take that as our $\phi$ in $\ket{\chi} = \hat A^\dagger_{\psi} \hat A^\dagger_{\phi} \ket{\,}$, we must get that the interaction terms between $\psi$ and $\phi$ must vanish, and that $\chi$ thus also must be a almost eigenstate, with $\varepsilon$ equal to the sum of $\psi$'s and $\phi$'s.

This then also means that I can still finish my vacuum paper.\,! For I can then just, after getting to the `look at have the vacuum-perturbing terms cause trouble' part, suggest the countable basis approach, then assume the self-adjointeness of this operator (which has a Hermitian symmetric formula), and then I can finally argue that this means that we should be able to insert a $n_{max}$-cutoff, despite the diverging volume in position space, and that thi must mean that the vacuum decouples (or it least it \emph{can} do so) from the physical states. And then I can also back this argument up with the calculations for $\ket{\chi} = \hat A^\dagger_{\psi} \hat A^\dagger_{\phi} \ket{\,}$ in momentum space, and thus end the paper in a similar way as v1. :) (18:19)

.\,.\,It's so good to finally (well.\,. once again.\,.) be able to see an end to all this.\,.\,!

(15.06.24, 10:45) Let me just note how important that little detail of only using the constant (countable) basis for the (pre-)vacuum-perturbing terms (for the DSR process), and using simply the (changing) $\delta k$-grid basis for the rest of the terms. This means that we will still renormalize that infinite free vacuum energy away, and that the result of this countable-(constant-)basis approach, given that it indeed works, will be the same $\hat H_\mathrm{QED}$ as we would get if we were able to carry the ``original approach'' of using purely the $\delta k$-grid basis through. The only differences are that in the latter approach, we need to show that the vacuum-perturbing terms can be removed before we can show the Lorentz covariance, and for the former, we can simply wait until after the Lorentz covariance is shown to then also show that we are able to remove these v.p.\ terms. And with the former approach, which is the one that I believe will work, and which I will write about now for my vacuum paper v2, we also get the result that the Hamiltonian is also sensible and Lorentz covariant (and gives same result) when it is with the v.p.\ terms, as when it is without them.

About the Lorentz covariance argument, the Lorentz circuit argument should be pretty simple when we are able to take the non-cutoff Hamiltonians directly (having shown that there are cut-off versions of them where the dynamics converge to the same thing when the cutoffs are gradually lifted). (10:58)

.\,.\,So there we are. How \emph{great} this is.\,.\,! :) (10:59)

%(13:54) Jeg tror bare, jeg starter med at arbejde på en rimelig quick fix af min artikel. Så kan jeg altså se på, om jeg får lyst til at omredigere den mere grundigt, eller om jeg bare vil lade det være noget, der kan vente til, at jeg begynder at få noget opmærksomhed..

%(10:06, 16.06.24) I thought a bit about the fact yesterday that my idea for the SA domain for \hat H_{DSR} (which has it's pre-vacuum-perturbing terms changed to a constant (countable) basis before the DSR) are meant to use physical interactions to cancel the vacuum emissions (and other ones), in order to utilize my `utilize several particles' idea. So if we were to try to finish my $\prod_\mathbf{p} (\hat A_{\phi_\mathbf{p}}^\dagger) \ket{\,}$ solution, it might make sense to let each oscillator couple to some further excited states as well, namely ones where a high-energy photon is emitted (perhaps specifically be the antiparticle in the given triplet). And it seems to me that there might be some potential here. But I'm not going to try this myself, at least for now, anyway, as I'm already pretty hopeful that my other idea (for the SA domain for \hat H_{DSR}) will work out, and get us through it all in the end. But sure, seeing such a solution would still bolster the belief that the perturbed vacuum is decoupled from the physical world.



(28.06.24, 17:24) Oh, maybe my `utilizing several particles' idea actually doesn't work. For when you get more emitting particles.\,. well, let's say we have $n$. Then you have $n$ emissions that are canceled from absorptions from $n^2/2$ areas (the division by 2 is not important here, but it will be in a moment). But each emission only has $n$ areas (at the $m+2$ level, if you will) that contributes to canceling it. So the amplitude over those $n^2/2$ areas can only by divided by $n$, given an amplitude squared over each area of $1/n^2$ (disregarding the factors of $1/\braket{\,|\hat A^- \hat A^+|\,}$ for now (squared for the amplitude squared)). So when we sum this, we get something going as $1/n^2 \times n^2/2 = 1/2$, which means that increasing $n$ (i.e.\ `utilizing several particles') won't help.

However, this factor of $1/2$ might just be our savior. For if it happens that each level ($m+2i$, $i=1,2,\ldots$) of the tail has an amplitude squared that is $1/2$ of the previous level, then we still get something that is convergent. (Note also that this trick doesn't work for the vacuum.\,. wait a minute: It doesn't?\,.\,.) Hm.\,. Something, or rather some things, to think about.\,. (17:39) .\,.\,(17:46) I guess it ought to work, and I guess that I also expected it.\,. wait, no.\,. .\,.\,Ah, I intended to use the `physical interactions' for the cancellations, as I recall. Okay, but maybe there is no need, for maybe that $1/2$ factor might work, and might also apply for the perturbed vacuum without the `physical interactions' as well.\,. (17:49)

\ldots (18:01) So I'm thinking $\ket{\phi} = \sum_{n=0, 2, 4, \ldots} (-\sqrt{2} \braket{\,|\hat A^- \hat A^+|\,})^{-n} (\hat A^+)^n \ket{\,}$.\,. Then we have
\begin{equation}
\begin{aligned}
	&(\hat A^+ + \hat A^-) \ket{\phi} = 
	\\&\quad\quad\quad
		\sum_{n=0, 2, \ldots} \pm \big((\sqrt{2} \braket{\,|\hat A^- \hat A^+|\,})^{-n} (\hat A^+)^{n+1} - (\sqrt{2} \braket{\,|\hat A^- \hat A^+|\,})^{-n-2} \hat A^- (\hat A^+)^{n+2}\big) \ket{\,}.\,.
%	\\&\quad\quad\quad=
%		\sum \pm \big((\sqrt{2} \braket{\,|\hat A^- \hat A^+|\,})^{-n} (\hat A^+)^{n+1}  - (\sqrt{2} \braket{\,|\hat A^- \hat A^+|\,})^{-n-2} \hat A^- (\hat A^+)^{n+2}\big) \ket{\,}
\end{aligned}
\end{equation}
Oh, I mean $\ket{\phi} = \sum_{n=0, 2, 4, \ldots} (-\sqrt{2} \braket{\,|\hat A^- \hat A^+|\,})^{-n/2} (\hat A^+)^n \ket{\,}$. Then we have
\begin{equation}
\begin{aligned}
	&(\hat A^+ + \hat A^-) \ket{\phi} = 
	\\&\quad\quad\quad
		\sum_{n=0, 2, \ldots} \pm \big((\sqrt{2} \braket{\,|\hat A^- \hat A^+|\,})^{-n/2} (\hat A^+)^{n+1} - (\sqrt{2} \braket{\,|\hat A^- \hat A^+|\,})^{-n/2-1} \hat A^- (\hat A^+)^{n+2}\big) \ket{\,}.
\end{aligned}
\end{equation}
And since $\hat A^- (\hat A^+)^{n+2} \ket{\,} = \sqrt{2}$.\,. no.\,. $\hat A^- (\hat A^+)^{n+2} \ket{\,} \approx \sqrt{n+2} \braket{\,|\hat A^- \hat A^+|\,} (\hat A^+)^{n+1} \ket{\,}$, so we want something like.\,. .\,.\,$\ket{\phi} = \sum_{n=0, 2, 4, \ldots} (n!)^{-1/2} (-\braket{\,|\hat A^- \hat A^+|\,})^{-n/2} (\hat A^+)^n \ket{\,}$.\,. .\,.\,No, $\ket{\phi} = \sum_{n=0, 2, 4, \ldots} (\prod_{m=0, 2, 4, n} m)^{-1/2} (-\braket{\,|\hat A^- \hat A^+|\,})^{-n/2} (\hat A^+)^n \ket{\,}$, which then grows with a factor of $-(n+2)^{-1/2} (\braket{\,|\hat A^- \hat A^+|\,})^{-1}$ when we go from $n$ to $n+2$. This then cancels the factor from $\hat A^- (\hat A^+)^{n+2} \ket{\,} \approx \sqrt{n+2} \braket{\,|\hat A^- \hat A^+|\,} (\hat A^+)^{n+1} \ket{\,}$, and there we go. .\,.\,Now, what is the norm squared of this?\,.\,. .\,.\,Hm, I think goes like $\sim n^0$ (as a constant), so the series is divergent.\,. .\,.\,Ah no, it's worse than that. Okay, so that was without thinking of the `physical interactions.' What about if we think about those?\,.\,.

.\,.\,The difference is, I think, that the `physical' emissions don't have bosonic statistics, as the vacuum emissions do (approximately), which means that the emissions might not interfere at the $n+1$ level, but the can at the $n+2$ level.\,. I hope so.\,. (18:49)

\ldots\ (20:17) Okay, I do indeed think that that $1/n^2 \times n^2/2 = 1/2$ logic can save me. The point is that each local area of the $m+2$ state can contribute to canceling two different emissions to the $m+1$ level at once. It will probably not be easy to prove rigorously, though, so I look forward to finding somebody to work together on that; I don't intend to try to do it alone. But yeah, I'm really glad that got these thoughts and ideas this evening *(and late afternoon).\,:) (20:20)

%(29.06.24, 10:06) Nu kan jeg ikke lade være med at gå og bekymre mig over, om denne strategi nu også virker, og hvor svært det bliver at vise. Men jeg må nok bare prøve at skyde tankerne væk og så sige, at det må vi bare finde ud af til den tid.. Hm.. ..Ah, vent, der er jo også det med at de tidligere emissioner fra underliggende niveauer også kan hjælpe..

(29.06.24, 10:15) Hm, there is also the fact the the emissions from the lower levels ($m-i$) can be absorbed as well and help in the cancellation.\,. .\,.\,So we can essentially hope that $\hat A^-$ will work something like $\hat A^- (\hat A^+)^{n+2} \ket{\,} \approx \sqrt{n+2} \braket{\,|\hat A^- \hat A^+|\,} (\hat A^+)^{n+1} \ket{\,}$ for the `physical' emissions, while $\hat A^+ (\hat A^+)^{n} \ket{\,}$ does not have the full bosonic positive interference and therefore does \emph{not} go like $\sqrt{n+1}$.\,. (10:21) .\,.\,Oh, I should replace $\ket{\,}$ here with some $\ket{\psi_m}$ state, by the way.\,.

.\,.\,Hm, this idea was my original idea behind the `$E$ with constant radii' idea, by the way; it just doesn't work for the perturbed vacuum (in my old way of treating the vacuum (not the current one, where I use a constant (countable) basis for the vacuum-perturbing interactions, and only them)).\,.

.\,.\,Hm, it does kinda make sense, indeed, that $\|(\hat A^+)^{n} \ket{\psi_m}\|^2$ should just go something roughly like $(\braket{\psi_m|\hat A^- \hat A^+|\psi_m})^{n}$ instead of $n (\braket{\psi_m|\hat A^- \hat A^+|\psi_m})^{n}$. So yeah, maybe this works.\,.\,:) (10:37)

%(10:40) Okay, great..! I'm very excited about this. And now I can go back to working on my crypto paper.:)

(18:10) Oh, maybe there's a flaw in the part of the underlying idea where I have to show that the almost eigenstates of the non-cutoff operator are also almost eigenstates, when cut-off themselves, of the cut-off operator. Just finding a domain for the cut-off operator is not enough. This domain also needs to work when we lift the cutoffs completely. And I'm not completely sure that the `$E$ with constant radii (or rather radi\emph{us})' domain works for the non-cutoff operator. For we need both the vector of the operator itself to converge, as well as its image. So that `$\ket{\psi} = \sum_{n=0, 2, 4, \ldots} \alpha_n (\hat A^+)^n \ket{\psi_m}$' approach doesn't really work as is, it seems.\,. (18:17)

(18:37) Hm, so it could be that one needs some kind of Nelson renormalization instead, after all.\,.

(19:22) Oh, maybe I've got it.\,. .\,.\,Instead of trying to show that almost eigenvectors of of the full operator can be approximated by ones of the cut-off operator, I should probably instead try to show the opposite.\,. .\,.\,And maybe this can allow us not to put that strict radius limitation on $E$.\,.\,!\,.\,. .\,.\,Oh, this sounds \emph{so} interesting.\,. (19:30)

(19:55) Hm, I'm not sure it works, though.\,. .\,.\,Oh, maybe there could be a way to make it work.\,. But it requires some thinking.\,. (20:00) .\,.\,Hm, because we now recognize the level (i.e.\ recognize what $2i$ is in $m+2i$) from the shape of the (level-specific) vectors themselves, we could make the radius of $E_{m+1}$ independent on $m$, as well as on the momenta.\,. .\,.\,And then if this is so, what happens if we choose this radius to be equal to the given cutoff, i.e.\ of the given cut-off operator that we are trying to ``approximate'' (``approximating the cut-off operator with the full one, so to speak)?\,.\,. .\,.\,Then we are on to something at the $m+1$ level, since we would then seem to be able to cancel most of the emissions that falls outside of this radius, at this level at least.\,. (20:09) .\,.\,And we then of course ought to put the tail at the $m+2$ level inside the same radius as well.\,. .\,.\,And now we are inside of $F$ (or $T$, if you will), so can't we just cancel everything else (mostly), and with a very small tail from there? (20:13) .\,.\,Wow, %(wauw)
this sounds great.\,.\,!\,.\,. (20:15)

.\,.\,This sounds really amazing.\,.\,!\,.\,. (20:17)

(23:10, 30.06.24) No, it actually doesn't make sense. So never mind, I guess, except that I should still think some more about it.\,.

(23:27) Hm, but maybe the solution could be to choose an $F$ (or $T$) such that.\,. .\,.\,for all $m+1$-level emission vectors where the emitted particle(s) has momentum/momenta less than or equal to $k_{cutoff}$ (considering only the UV divergence (an disregarding the IR one) here just for simplicity), $E_{m+2i}$, $i=1,2,\ldots$, has a radius of $k_{cutoff}$, but then for $m+1$-level emission vectors that reach beyond $k_{cutoff}$, the radius of the $E$ sets can then get much greater (growing very rapidly). Okay, let me think about this.\,.

.\,.\,Oh, but I can't let the domain of the non-cut-off operator depend on the cutoff; that ruins the whole point.\,. (23:40)

(00:17) Wait, I know I have thought about this before, but couldn't a separate cutoff on the fermion momenta, that is allowed to grow much slower than the cutoff for the bosons, do the trick.\,.\,? \ldots No. (00:55)

(01.07.24, 10:29) Hm, I actually don't even think that having an $E$ with a radius equal to the maximal momentum would actually work in the first place.\,. Maybe the problem needs some Nelson renormalization instead.\,. I really like my domain solution (from my SA paper, etc.), but despite it's beauty, maybe it's not what we really need, mathematically.\,. I don't know.\,.

%(11:21) Jeg bør ikke hænge mig mere i fysikken her, i virkeligheden. Og det er da i det mindste rart, at jeg lige nåede at opdage det her, antaget at der kommer til at ske ting, når jeg udgiver min crypto-artikel. Men det kunne være rart lige at finde et pejlemærke, eller i det hele taget lige overveje, hvad planerne er med hele dette fysik-matematik-projekt, efter at jeg har fået rettet min vakuum-artikel..

\ldots\ (13:03) I took a walk thinking about physics instead of working on the crypto paper, and I'm glad I did. First of all, I was wrong (I think) when I wrote about `$E$ with a radius equal to the maximal momentum' not helping. It would, but I don't see how that would be possible to find/show, on the other hand. So maybe a Nelson renormalization would be our hope. But then I also thought, well, maybe a cut-off version of the Hamiltonian just doesn't approximate the dynamics of the non-cut-off one. Maybe the cutoff will always introduce artifacts. Now, my own way of approaching the Lorentz covariance problem sorta requires a cutoff, and a discretization. But that's \emph{my} approach. And I haven't been interested in reading other approaches yet (for when I thought that my approach lead to a different Hamiltonian, I wouldn't believe the conventional approach anyway, and since then I also just haven't quite had the motivation for it). But maybe there is a way to argue Lorentz covariance directly from the path integrals. This then got me thinking about a particle-number-dependent cutoff. I'm sure that such a cutoff would yield a Hamiltonian that approximates the non-cut-off one. And while such a Hamiltonian would not be bounded, it would still have all vectors with a bound on the particle number as part of its domain. And that should mean that we can still develop the path integrals from a Trotter expansion.

So far so good. And my thought is then that once you have this path integral, with a particle-number-dependent cutoff, then there might be a way to argue the Lorentz covariance just from that. This is kinda like my thoughts above about why it might make good sense that we can remove the pre-vacuum-perturbing terms from the pre-DSR'ed Hamiltonian. More precisely, it might be the case that if we consider the same local interaction density for the path integrals in one inertial system, then it will cause the same transitions between the free-energy eigenstates as in another (any other) inertial system. In fact, if you consider the path integrals in positions space (somehow), then it certainly does make a lot of sense, since of you look at the transitions of just a local volume of space-time, then these should be that same as in any other inertial frame, since you can just turn up the interaction in that particular volume (one would think). And yeah, maybe all this can be shown mathematically somehow. Oh, and I suspect that this particle-number-dependent cutoff will also transform into a similar cutoff in other inertial frames. So this could be a way to show Lorentz covariance, who knows. (13:21)

Okay, this must be enough for this time around. I will of course fix my vacuum paper, and then the plan is to wait and see if someone is interested (maybe someone who contacts me) in working together on continuing this project. But I must admit, if someone is interested in working with me on my Web 3.0, Semantic Network project, then that definitely takes priority. (13:24)

\ldots (13:48) I just had I thought. If you can develop the path integrals---and from a Trotter expansion---doesn't it go without saying that you can then subsequently make a particle number cutoff on this path integral? .\,.\,(13:51) Woah.\,.

(14:26) Hm, I just had an additional thought: If you choose $F$ (or `$T$') such that pair-production interactions are used to cancel emissions, then.\,. .\,.\,(14:30) Oh, maybe you could also just make the cutoff fermion-momentum-dep.\,. wait.\,. .\,.\,How about a particle-number and fermion-momentum-dependent cutoff?\,.\,. .\,.\,I mean fermion-number-dependent.\,. .\,.\,Oh wait, the fermion number is constant when we reverse-DSR.\,. .\,.\,Ah, but we could make an $F$ ($T$) where the fermion momentum keeps increasing, and where any eigenstate must at some point obey $F$ more and more, which means that a fermion-momentum-dependent cutoff should approximate the non-cut-off Hamiltonian, I think. And if that is true, then maybe my approach to the Lorentz covariance problem could still work, where one just ends up with a pre-DSR'ed *[Did I mean DSR'ed rahter than pre-DSR'ed here?\,.\,. \ldots\ I'm pretty sure I meant DSR'ed, but I'm more interested in my particle number-dependent cutoff from just above, anyway.\,. *Hm, maybe I meant pre-DSR'ed after all, given the third paragraph after this one (but whatever).\,.] Hamiltonian (if we go backwards (compared to e.g.\ my 2022 (QED) paper)) where one has fermions on top of a discretized $A^\mu$ field, where the interaction with that (classical) field then is allowed to depend on the fermion momenta (i.e.\ with a fermion-momentum-dependent cutoff). That's certainly an idea. I'm glad that I have a few roads open now for continuing (and perhaps completing) the project, it seems. (14:47)

.\,.\,Well, you can't make sure that the `fermion momentum keeps increasing,' but still.\,. (14:51) .\,.\,Still, I think there's a possibility it could work.\,.

.\,.\,Anyway, it's nice to have some roads open. And at the end of the day, if we can just show that my proposed (in my near-future version of my vacuum paper) $\hat H_\mathrm{QED}$ is self-adjoint on some domain (non-cut-off), even with those vacuum-perturbing terms in a constant (countable) basis, and thus that these vacuum-perturbing terms can be removed from this $\hat H_\mathrm{QED}$, then even if the argument that $\hat H_\mathrm{QED}$ retains its Lorentz covariance isn't fully mathematically rigorous, it should still catch the attention of (most of) the rest of the physics community, I'd say. (15:00)

(15:24) (We could, by the way, also toy with the idea of making the DSR before the field modes are quantized as photons, which could then potentially allow us to make a fermion-number-dependent cutoff if needed. So there's also that little potential road, it seems---even though I have mostly stayed clear of that path in the past for some reason.\,. *[Well, I wouldn't know how to obtain the Coulomb potential, and all that, if I started with a DSR'ed (semi-classical) Hamiltonian.])

(14:56, 15.07.24) I really like my particle number-dependent cutoff idea. I realized on a walk earlier that as long is the cutoff grows rapidly enough with $n$ in order to make an SA (self-adjointness) domain possible, then the physics/dynamics ought to be the same regardless of the exact growth. And this also means, if we turn it around, that all kinds of SA domains, regardless of exactly the choice of how ``fast $F$ (or $E$ or $T$, if you will) should grow,'' should have the same physics/dynamics. And how delightful a thought that is.\,! (15:01)

In terms of the Lorentz covariance, the argument about the local operator density only really works if you don't have that instantaneous Coulomb interaction. So the way forward would probably be to absorb that into the Feynman propagator (for the photons), and then argue the Lorentz covariance in the conventional way from there.\,:)

I really should read about this conventional way at some point, if/when I continue this project again, after this last todo of correcting my vacuum paper. Of course, I hope that someone with skill and knowledge on that topic would want to work together with me at that time. But otherwise *(and regardless, I guess), yeah, I should read some more up on that topic at some point. But I think I will promise myself not to do any more physics for at least a year, unless somebody contacts me about it.\,. (15:08)

(13:03, 16.07.24) I'm starting to believe more and more in these `particle-number-depen-dent cutoff' domains. In fact, now I actually have a hard time believing that domains with particle-number-independent cutoffs could work, and thus also that a Nelson renormalization could work. I'm even puzzled that it could work for the Hamiltonian in Nelson's own paper. So let me think about that, I guess.\,.

*(13:41) Oh never mind: When you have a free energy like $\mathbf{p}^2$, then you can just use that to cancel the emissions (and you do not \emph{need} the absorption transitions to do this). :) Okay, so the Nelson renormalization does make sense, just not when you need absorption transitions for canceling the emissions (of whatever almost eigenstate of $\hat H$ that you are considering (with and without the cutoff)). (13:44)

I should also mention (and I will get back to it more) that I'm a bit unsure how I should actually complete my vacuum paper v2.\,. And maybe I \emph{should} even read up on the conventional approach to Lorentz covariance in QFT.\,. But I'll think about that as well.\,.

\ldots (13:45) Great, I really am starting to think that you need my solution for the SA domains, not a Nelson renormalization. .\,.\,which is a bit exciting, of course.\,.\,:)

.\,.\,(13:50) Well\ldots\ Let me think.\,. .\,.\,Yeah, the point is, and I think this is right, that an almost eigenvector of the non-cut-off Hamiltonian should be more and more of an almost eigenvector (with $\varepsilon$ approaching the same value) of $\hat H_\kappa$ when $\kappa \to \infty$. (13:55) .\,.\,Yeah, and only with an energy that then diverges like $N E_\kappa$.\,. .\,.\,Yes. .\,.\,But if you chose a step function as your cutoff function, giving the kind of $p \leq \Lambda$ cutoff that I always like to work with (for good reason due to my Lorentz covariance approach), then that means that an almost eigenvector, $\psi$, most be an almost eigenvector as well for the cut-off $\hat H_\kappa$, which just seems unrealistic: How can $\psi$ cancel its own emissions out at $\Lambda$ without requiring a large component two levels above the given level? .\,.\,Well, unless there \emph{is} a way to make my `constant radii $E$' ideas work, but I'm not sure there is.\,. (14:04) .\,.\,My previous analysis kinda indicated that this isn't possible. And if not, one must first of all be able to show this pretty easily. And if it can be shown, well then this would show that a Nelson renormalization is not possible, I'm pretty sure. (14:06)

So yeah, that is kinda exciting, i.e.\ that my SA domain idea might be required for a full theory/Hamiltonian of QED after all.\,. (14:07)

%... (15:56) Okay, I do indeed think that arguing the conventional way about the Lorentz covariance, using path integrals, perhaps in (\omega, \mathbf k) space, is the right way. But I've also realized that if we just asumme that \hat H_{QED, pre-DSR} and \hat H_{QED} are self-adjoint with some cutoff, perhaps a.. wait.. ..Hm, I can't just say 'a (perhaps) photon-number-dependent cutoff,' 'cause of the Coulomb interaction.. Hm.. (16:01) ..(16:06) No, that falls apart.. ..I thought I could make the Lorentz circuit argument work with a (perhaps) n-dependent cutoff, but now I'm not completely sure.. ...Yeah, nah, it has to be momentum-dependent only, if the cutoff should transform to another (decent) cutoff.. ..Oh, and this is a problem for the approach of following the conventional way for showing/arguing Lo. cov., since we need to be able to develop the path integrals no matter what.. oh no, not if we can just start with the DSR'ed path integrals, I guess.. Hm.. (16:41) ..Yeah, so if we can do that, and thus never have to work with the "pre-DSR'ed" Hamiltonian, then it could work.

%But that of course brings me back to not knowing how to fix my vacuum paper without having to do all that research (which I'm not gonna start on now, no chance..) *(and I'm not just talking about reading up on some things here..).. ..Well, I \emph{was} atually toying with the idea of just letting it be for now, until somebody starts reading it.. And that could definitely be the decision now.. (16:47) ..Yeah, I think I'm gonna at least take a break. (I'm kinda exhausted, actually, anyway.. *(from working so so much with seemingly almost no payoff..)) So let me at least take a break, and then I can always see if I'm really going to extend that break (until somebody starts reading it), or what I'll do.. (16:49) ..Maybe I should take a small summer vacation.. (16:52)

%..Wait, maybe I \emph{can} make it k(/p)-dependent.. (16:56) ..Oh no, never mind, it \emph{is} more complicated.. (16:59)

%..Wait, without a DSR, how do I even argue about the whole thing.. Hm.. ...Hm, this stumped me... (17:17)

%... (19:16) Okay, I think the solution could just be to start with the conventional argument for why the path integral of \hat H_{QED} is Lorentz-covariant, then make the particle-number-dependent cutoff and also factor out the Coulomb interaction from the photon propagator, while arguing that the Lorenntz covariance in retained in the limit when the cutoff (both UV and IR) is lifted. Oh, and when momentum space is also discretized. (So this is actually essentially the starting point: Showing that the discretized and cut-off path integral (with \hat V_C factored out) is Lorentz-covariant.) And then from there you change the basis of the vacuum-perturbing terms. The resulting interaction will then also be particle-number dependent. And hopefully you can show that this resulting Hamiltonian is self-adjoint on a.. I was about to say a Damgaard domain *(Damgaard--<whoever proves the generalized version> domain..:)), but that's not up to me.. on a domain of the type that I have developed (and showed an example of in my SA paper).. And then from there it's just the argument that, well, if it's self-adjoint in the limit when \mathcal{V} \to \infty, then the "vacuum particles" will be more and more spread out, which should mean that you can remove the vacuum-perturbing terms and get the same dynamics for the "physical particles" (i.e.\ the ones that don't just live on the infinitesimal \mathbf H_{vac}).

%So that's the strategy I would persue. But needless to say, I'm not going to do this right now. Instead.. I will finally move on with my life.. (19:29, 16.07.24)

(19:30) I think the strategy is to start from the path integrals of the DSR'ed system with the conventional argument why this is Lorentz-covariant, then argue that it retains this symmetry when discretizing and imposing a particle-number-dependent cutoff (both UV and IR)---and also factoring out the Coulomb potential. Then one changes the basis of the vacuum-perturbing terms and hopes to show that the resulting Hamiltonian is self-adjoint on some domain (and that the almost eigenstates of the non-cut-off Hamiltonian are also approximately (with $\varepsilon' \to \varepsilon$) almost eigenstates of the cut-off operator in the limit when the cutoff is lifted). And then at last one argues that when $\mathcal{V} \to \infty$, the ``vacuum particles'' will be more and more thinly spread out, and that one can therefore remove the vacuum-perturbing terms without changing the dynamics of the ``physical particles.'' (19:36, 16.07.24)

(17.07.24, 9:30) Hm, if I'm right, then this might mean that one in principle have to have a number-dependent cutoff when doing path integrals (of scattering experiments), which is interesting.\,.


%(12:45, 28.10.24) I have been a bit repulsed by the thought of doing physics again at the moment, but yesterday evening, I managed to (and happened to) find a bit of interest again.. And now I've just gotten a bit distracted by the thought again. I think you can argue by looking at a scattring experiemnt, arbitrarily complex, but where you turn the interaction up in the center. And then if by the Scattering Approximation you get Lorentz-invariant path integrals, then you're good to go, it seems, 'casue we can still gather in orders of the coupling constant (regardless of the factor between 0 and 1 that you impose on it in space-time). (So we can gether in terms of the actual, non-cutoff, coupling contant, q, in other words.) And then it's just a matter of saying that these integrals convergence when you make the integration with the n-dependent cutoffs (but where there's also a n_max determined by the order of q that you are calculating), which they should if one can show self-adjointness of the Hamiltonian with the same n-dependent cutoff. And if they converge, they are then Lorentz-invariant, meaning the the scattings experiemnts will be Lorentz-covariant, and with appropriate arguments (which I have, although they are not mathematical (but I don't think they really need to be)) we can then say that this must mean that the actual theory, without the turned-down, space-time-dependent coupling constant (but with a constant coupling), is Lorentz-covariant as well. Then there's just the matter of arguing that the vacuum-particles can be thrown away.. (12:56) ..Yeah, and you need to include the vacuum particles in the scatting approximation path integrals, but just hopefully be able to argue, when some parameter goes to 0, that their influence on the dynamics (of the "physical" particles) described by the resulting Hamilton will vanish in that limit.. (13:00) So yeah, something to be distracted by..
%(13:08) (And we get to make \delta p smaller and smaller when we extend the volume of space-time where the coupling is turned up.. ..And we can extend this volume far beyond the space where the interactions of the scattering experiment actually happens..) ..(13:14) Wait, what is stopping us from making \delta p smaller without needing to change n_max?..!.. (That was the whole thing that troubled me, right: the fact that the number of vacuum particle might rise when one makes \delta p smaller?. That's what I remember..) ..Yes, and it's reasonable that we might get more vacuum particles with increasing volume.. So why does it seem that I can let n_max be here, when letting \delta p \to 0..? (13:18) ..I'm gathering in orders of q, but that's not a new idea.. So why didn't it work before, i.e. before this idea of using the scattering approximation and the Lorentz-invariance of the integrand of the path integrals..?.. ..The difference is the *(n-independent) p_max that i had before.. Was that the troublemaker?.. (13:21) ..Well, it made trouble in terms of being able to argue the self-adjointness.. But..? ..Hm, and without knowing the convergence of the path integral, how can you gather in orders of q?. .. (13:26) ..Hm, but for a long time, I thought I could show the self-adjointness \emph{with} the n-independent p_max, so there must have been something else that made the gathering in orders of q not quite work..(?) (13:29) ..Well, it must have been one of the limits that I took, where I couldn't argue that you would get the same contributions from each order.. Hm.. (13:31) ..Oh, wait, there was also something about arguing about the.. Ah, I know. It was probably that the Hamiltonian only converges if the vacuum terms \emph{can} be neglected.. (13:32) ..And isn't that also a problem for me now?.. ..Hm, and another thing (or perhaps another angle on the same thing) is that you will have vacuum particles that are generated in your scattering experiement, with high likelihood, and more an more will be generated when you increase the volume.. (13:35) ..So it would actually be weird if the q-coefficients that you gather are independent on the volume.. (13:37) ..But a field theorist would say that it will be volume-independent once you remove, or rather "renormalize" away, the so-called vacuum diagrams.. (13:39) ..Yes, but even so, we are not just troubled by vacuum diagrams here: We are also troubled by generated final particles.. ..But a field theorist would then way, if they don't interfere with the experiment, then they just mean an overall factor.. And my point is also reying on the fact that they shouldn't interfere.. Hm.. ..Yeah, that is what I want: The vacuum particles might elolve and multiply, as long as they just stay very spread out, and don't truly interact or interfere with the "physical" particles.. (13:44) ..But trying to eliminate the vacuum particles this way is then exactly what gives me, and has given me in the past, this problem that we can't assume a n_max that is independent on the volume.. ..And the fact that n can grow for the vacuum particles is exactly what prevents us from disregarding them, dispite the "smaller and smaller volume that they occupy in p-space".. (13:47)
%..(13:51) But can't you still gather in orders of q for the (p, \omega) path integral..? I know you probably can't, but I also don't see why not.. ..Hm, there might be a way to quantize/"measure" the spread-out-ness of a combined state.. Hm.. (13:55) ..By the way, the path integral must be directly dependent on the space-time volume (Fourier-transformed or not)..
%..(14:00) Wait, diagrams where vacuum particles are created and becomes part of the final state looks the same in both inertial systems, don't they, even in (k, \omega)-space?.. ..Hm, only when they don't interact with the physical particles.. ..And we can hope that when they do, then we can still gather in orders of q for all these diagrams, and get a converging result despite the growing volume.. Hm, but getting convergent results in QFT is not something to be expected, in fact, we won't.. (14:06) ..But it's loops that are seen as the problem there.. (14:07) ..And we can't get around loops..
%(14:16) Hm, there must also be a (x, t) path integral, only where the x almost-eigenstates might have to depend on inertial frame.. ..And because it might depend on that, does it not mean that we won't know that it retains the Lorentz symmetry if we remove the vacuum-perturbing vertices?.. ..Oh, but maybe, by choosing a large enough p_max, we can get the states to depend less and less on the intertial frame..(!..) (14:20) ..Oh, I think so..!.. (14:21) ..Hm, but it's unfortunately hard to argue about "only small errors" when it comes to path integrals, 'cause these errors add up.. ..Still, though.. (14:26)

%(14:41) Hm, just had an idea: I don't think I've thought about making the discretization n-dependent before.. ..Hm, and you could even Dirac-sea-transform this system.. Well, or maybe not, I'm not sure.. (14:44)

%(14:54) Nah, I would need the (x,t)-space path integral.. And yeah, we can probably get a Lorentz-invariant one, indeed.. *(For we can't even define n in (k,\omega)-space..)
%..Hm, could the strategy really just be to argue that removing the v.p. terms in one inertial frame is the same as removing them in the other?.. (14:59)
%..Hm, could there really be such a simple solution?.. (15:03)

%(29.10.24, 9:13) Pretty soon after I wrote the last thing here, in the beginning of my midday walk, I thought about the fact that, if you can really show that we can discard the v.p. terms, then why not show it for the pre-DSR'ed case first. And then when you think about it, there is this "paradox" that if my hypothesis is true, then it ought to follow from it that the pre-DSR'ed Hamiltonian also ought to be Lorentz-covariant without those terms. So yeah. why not try to go the other way around. Later, however, I kinda reached the conclusion that it would be hard to make the whole thing work with the n-dependent cutoff, namely since n is not the same for a path in the two different intertial frames at the same time. So I actually concluded that I wanted to leave the subject be, maybe even for good, almost, this time. But in bed while falling asleep I thought about the fact that maybe by n-dependent cutoff actually don't need to lead to a different path integral. And that's where I'm at now. I should check whether it isn't true that you can derive the same path integrals, as long as the Hamiltonian is self-adjoint. And first of all, if that is true, how amazing it would be..! Then my SA solution will never be worse than any Nelson renormalization; they will be the same. (Mine will just be contructively defined, and therefore better in that particular sense..) But I'm getting ahead of myself. But if it's true, then I want to use it to show that we can remove the troubling terms from the pre-DSR'ed Hamiltonian, without breaking the L. symmetry, and then we can take it from there. And let me also mention, the dream would be to end up with a result that I can market as 'a new renormalization technique: get rid of the v.p. terms'..
%But let me try not to think more about this subject till evening. A have much else to do..

%(10:56) I didn't want to think about this till evening.. But it does really seem that you can derive the same path integrals form my n-dependently cut-off Hamiltionian (with my type of domain)..!! And if this is true, then my domain should be the limit of any cut-off Hamiltonian as well. In particular, it should be the limit of any Nelson renomalization. (Of coure, my solution has some freedom of how you define the domain, but all these domains also lead to the same physics.) This would really be grand..!!.. (10:59) ..(But yeah, till evening..)

%(12:33) Okay, I'm taking a break to write a little more about this topic. I do think that we can derive the same path integrals from the Hamiltonian with the n-dependent cutoff (which for the pre-DSR'ed Hamiltonian just means the photon number). So I think we are good to go. My argument for being able to remove the pre-vacuum-perturbing terms is then to look at a space-time volume within a larger volume, where q is abruptly turned up (to its actual value) in the inner volume, and is 0 outside of it. Let us then look at the point right in the cornor of the rhombus, assuming that the inner volume has this shape (with edges of equal time in one of the two inertial frames).. ..Any localized state in that coordinate will then contribute directly to the final momentum states that are leaving the scattering experiment (inside the inner volume). And the point is then to use this to show that the transitions of this localized state should go to the same (or corresponding, rahter) momentum eigenstates with the same amplitudes in both frames.. ..And as I've said, by letting k_max increase, the sets of localized states will converge in the norm topology to each other for the two frames.. (12:43) ..And we should be able to argue from this that the transitions, when including the interaction, to one hyperplane just after a certain point in space-time should in the one frame then match the transitions in the other frame, of going to the other (equal-time) hyperplane that comes just after the point in the other frame.. ..Oh, or we can just argue that the locilized states must be more and more the right L.-tranforms of each other, when p_max \to \infty..!.. (12:48) For when we look at the point in that corner, then propagate it out of the inner volume from there, then L.-transform the final state, and then finally propagate it backwards in time by the appropriate amount, then we get exactly the same state again (when using the momentum state the transforms easily, of course), only with a small error that vanishes when k_max \to \infty. So we hereby know how to transform all localized states! And then we can also argue that the transitions between these localized states (as points in the space-time lattice) must match.. Hm, and how do we argue that, even after we remove the pre-v.p. terms?.. (12:52) ..By adding a single layer of lattice points to the aforementioned rhombus, I think.. (12:54) ..Yeah, that will do it.. ..At least before having removed those terms, and what about after: will the transitions still be equal?.. (12:55) ..Oh wait, we have multiple particles.. ..So a state is in several different points.. (12:57)

%(13:18) Hm, I also got an idea a little later on that walk yesterday about going all the way back to the Dirac equation, i.e. on a A^\mu field/lattice.. ..Here you only have one particle.. So if you can then use the same rhombus argument here to show that we can remove any particular type of transition, out of the 4 \times 4 possible ones.. (13:21) ..Oh, and this is similar to having several Dirac fermions on a A^\mu field as well (having not quantized it to get the photons), also before the DSR, but yeah, we might as well just look at the case with only one fermion.. ..Oh, but if we remove it already here, it might influence the Coloumb interaction.. But then again, how awsome would that be, if you just needed to correct the Coulomb interaction in order to remove the troubling terms.. (13:26) ..Hm, I do think that my rhombus argument works in this case, and that we can remove the troubling terms, the "pre-vacuum-perturbing terms," at this early stage.. (13:29) (..!..) ..But will this then not change the gauge symmetry of the coupled Dirac equation?.. ..Hm, maybe not..(?) (13:31) ..Well, maybe it will.. ..Wait, but.. Hm, let me look at my QED paper.. ...(13:55) Hm, the action is independent of the gauge, though.. ..Well, save for some factor, right?.. ..Wait. Insn't it a point that we can keep dividing the rhombut into smaller and smaller pieces, and that each little small piece should give the same.. transitions for the localized states.. Hm.. ..Ah, yeah, maybe.. ..So instead of looking at a small cornor of the rhombus, we just look at a very small rhombus.. Then the idea is to argue that the transitions correspond in this smalle rhombus, and that you can assemble any larger volume from such small rhombuses.. Hm, and the question is then, can we then still say that the transitions are the same for each small rhombus, even in the presence of others..? (14:02) ..Well, yeah, they should act independently, at least for each layer, if we add them in.. "half".. layers at the time, where only the sharp corners touch for the neighbors in a layer, and not the whole edges.. ..Yeah, I think we can do this.. (14:04) ..Yeah, we make a very fine mesh of very thin rhombuses, and then we argue that each rhombus will yield the same momentum-to-momentum transitions, only with different phases due to different locations in space (and time, if we use the Dirac picture..).. Hm.. (14:07) ..Hm, and the transitions of each layer in one frame should match those of the same layer in the other frame.. (14:09) ..Wait, the particular vacuum perturbing terms doesn't even make sense before we get the photons, d'oh.. So yeah, then we are back to thinking about path integrals of several particles.. ..Which in position space means that it is hard to define what the vacuum perturbing terms are due to.. Well, you can define them still if you compute the transitions via the momentum basis.. Hm.. (14:14) ...Let me take a walk and think... (14:25)
%..(14:28) Oh, perhaps one could simply argue that the paths that are cut out have the same contributions to the path integral in both frames..

%... (16:39) I think I might have it.. The point is to look at.. two seconds.. ..(16:42) The point is to look at the unitary time evolution operators over just these small rhombuses (all in momentum basis). Now, the whole time-evolution can then be approximated arbitrarily well, when the size is small enough, by $1 - i H_n \Delta t$, where H_n is the generator for the evolution over the nth rhombus. And the argument is then to say that removing the pre-v.p. terms in one frame.. Hm, let's see.. (16:45) ..Well, the point is that the transitions of this H_n should be invariant/covarient when we transform to the other frame.. (16:47) ..And it should commute with all $1 - i H_m \Delta t$ (at least approximately), where m is a rhombus in the a volume that is purely spatially separated from the nth rhombus (i.e. with only space-like distances between them). Two seconds.. ...(17:04) Hm, well, maybe.. ..\emph{If} we can turn the coupling up and down, perhaps yes, but there is also that Coulomb interaction that work over arbitrary distances.. ..Well, if the incoming and outgoing particles start and end far apart.. (17:10) ..Yeah.. (17:14) ..Oh, and that argument then also applies for small inner volumes, like the small rhombuses. So yeah, maybe it'll work.. (17:18) ..Wait no, the point was to use a large volume to make sure that the Coulomb action is the same in both frames.. ..(17:27) Hm, would it help to express the Hamiltonian purely in terms of localized states, perhaps a countable basis?..

%(17:51) I just read a bit in Lancaster and Blundell.. Time-ordering ought to be the same as normal-ordering, plus a so-called contraction, and contraction ought to essentially just join to lines in a diagram together. So imagine if a time-ordering is just changing the diagrams that you have to do a bit. Then since each diagram ought be give a Lorentz-invaraint contribution, then maybe we \emph{can} just time-order our way to get rid of the pre-vacuum-perturbing terms.. Imagine if this is so.. (17:55)

%... (18:42) Well, but when the Coulomb interaction is absorbed (in the photon propagator), then we would have to get that back out in order to try to time-order away the pre-v.p. terms of the Coulomb interaction..

%(19:20) Maybe one can analyse the path integral before quantizing the A^\mu field (and also before pulling out the 0th and 3rd component of field after making the basis change of my paper) and argue from there about the equality of the transitions from each rhombus.. ..Maybe where one assumes that the transitions are at most in the order of q^1, i.e. for a small enough volume.. (19:30)

%(10:28, 30.10.24) I've slept well. And then I laid a while in bed before getting up, thinking about physics. And I think you can shoot a white arrow after showing that those small-rhombus time evolutions will give you nice operators expressed in ladder operators that are then the same in both frames. No, the Coloumb interaction, whether treating it before doing anything to A^\mu or after, just makes it too complicated. So let me give up on that.
%On the other hand, you \emph{can} instead analyze the whole time evolution of a large volume. Now, if the charged fermions end up close to each other, or start close, then we're still in trouble, I think. But if we consider evolutions where the start and end fermion states are far enough apart in position space, then we should be good: Then we should be able to make.. Well, consider a very large rhombus with q turned up inside. And then consider putting two Lorentz transformations (on half-volumes) at both ends to extend the states on the edges of the rhombus with equal time in Frame 2 down/up to the hyperplanes that has equal time in Frame 1, before and after the rhombus. In other words, we time-evolve over the rh.. well, if we start at a hyperplane with t = const. (in Frame 1, denoted by having no prime), then transform half this hyperplane up to the first t' = const hyperplane that is a lower edge (if time is in the general upwards direction) of the large rhombus. Then we time-evolve over the rhombus, and finally we tranform the "half-hyperplane" to the other side up from its t' = const. (new constant) hyperplane and up to the t = const. (new constant) hyperplane. And make it so that both these (Lorentz) tranformations are done with the free Hamiltonian, turning q down to 0. That's the thing we are considering. Then \emph{if} we look at final states here, where the fermions are far enough apart, and if they start far enough apart, then it's intuitive that we ought to get the same transitions here in both (inertial) frames. And maybe this can also be shown mathematically without too much hardship.. So let's assume that.
%Now, the big idea from the bed this morning (and hour ago or so) is to then argue that to any finite order of q the two time evolution operators, when sandwiched which these far-apart initial and final fermion states (creation operators at the bottom/start and annihilation operators at the top/end), must be approximately the same for said order of q. And I think we might be able to do much with this..!.. (10:47)
%..'Cause this means that we can gather in orders of q and say for each order, when \delta p and all such parameters are sent towards their limits, the combined evolution operators should approximate each other, and more and more for further and further apart start and end fermion states. Now let's do the Dirac Sea Reinterpretation (DSR), as I tend to call it, which we by the way haven't done until now: All the talk was for the pre-DSR'ed Hamiltonian. We then do this exactly by arguing from these orders of q! Let's see.. (10:52) ..And I'm hoping that this will also give us the possibility to limit the interactions for the reinterpreted time-evolution, which means that we can probably remove the vacuum-perturbing terms, and then when we afterwards patch together all the orders of q to get the full time evolution in the DSR'ed system, then we get the evolution that is generated by my proposed Hamiltonian for QED.:).. I hope this will work..(!) (10:55)

%(10:58) Hm, but maybe I actually want to do some programming, before analyzing if this idea (finally!..) does the trick.. ..'Cuase I feel like i need to walk on it anyway, so that would be a good midday activity. Plus if I'm right, then I know that I will start working on the new version of the paper tomorrow (morning)... (11:00)

(30.10.24, 11:49) Copied from the source comments:

``[\ldots] On the other hand, you \emph{can} instead analyze the whole time evolution of a large volume. Now, if the charged fermions end up close to each other, or start close, then we're still in trouble, I think. But if we consider evolutions where the start and end fermion states are far enough apart in position space, then we should be good: Then we should be able to make.\,. Well, consider a very large rhombus with q turned up inside. And then consider putting two Lorentz transformations (on half-volumes) at both ends to extend the states on the edges of the rhombus with equal time in Frame 2 down/up to the hyperplanes that has equal time in Frame 1, before and after the rhombus. In other words, we time-evolve over the rh.\,. well, if we start at a hyperplane with t = const.\ (in Frame 1, denoted by having no prime), then transform half this hyperplane up to the first t' = const.\ hyperplane that is a lower edge (if time is in the general upwards direction) of the large rhombus. Then we time-evolve over the rhombus, and finally we transform the ``half-hyperplane'' to the other side up from its t' = const.\  (new constant) hyperplane and up to the t = const.\  (new constant) hyperplane. And make it so that both these (Lorentz) transformations are done with the free Hamiltonian, turning q down to 0. That's the thing we are considering. Then \emph{if} we look at final states here, where the fermions are far enough apart, and if they start far enough apart, then it's intuitive that we ought to get the same transitions here in both (inertial) frames. And maybe this can also be shown mathematically without too much hardship.\,. So let's assume that.
Now, the big idea from the bed this morning (and hour ago or so) is to then argue that to any finite order of q the two time evolution operators, when sandwiched which these far-apart initial and final fermion states (creation operators at the bottom/start and annihilation operators at the top/end), must be approximately the same for said order of q. And I think we might be able to do much with this.\,.\,!\,.\,. (10:47)
.\,.\,'Cause this means that we can gather in orders of q and say for each order, when $\delta p$ and all such parameters are sent towards their limits, the combined evolution operators should approximate each other, and more and more for further and further apart start and end fermion states. Now let's do the Dirac Sea Reinterpretation (DSR), as I tend to call it, which we by the way haven't done until now: All the talk was for the pre-DSR'ed Hamiltonian. We then do this exactly by arguing from these orders of q! Let's see.\,. (10:52) .\,.\,And I'm hoping that this will also give us the possibility to limit the interactions for the reinterpreted time-evolution, which means that we can probably remove the vacuum-perturbing terms, and then when we afterwards patch together all the orders of q to get the full time evolution in the DSR'ed system, then we get the evolution that is generated by my proposed Hamiltonian for QED.\,:)\,.\,. I hope this will work.\,.(!) (10:55)''

\ldots\ (14:47) First things first, the way to argue that we can make a $n$-independent $k_{max}$ cutoff for the path integral is to first take a large finite almost-basis of vectors (as a resolution of the Hilbert space) of Dom($\hat H$) with the $n$-dependent cutoff, and then you take an even larger almost-basis of vectors of a more regular type, and in particular one would often choose a almost-basis of momentum almost-eigenvectors with a certain small $\delta k$, and with the (very large) $k_{max}$ cutoff as well. Now, because we have that the matrix element of $\hat H$ for each of these.\,. well, the naive matrix elements.\,. Let me start somewhere else: You first use the first almost-basis to make an initial path integral for $\hat H$. Then you change the basis of each transition in this path integral such that they all go from one basis vector in the second almost-basis to another (simply be approximating each basis vector in the first almost-basis by resolving it in the second). When the second almost-basis is fine enough (in terms of its resolution), there will be a vanishing error from this process. And then you look and see that because the ``naive''/formal matrix elements of $\hat H$ over the second almost-basis converges, then the resulting transitions of this basis-changed path integral will just be the ones given by.. Well, or they will approximate the transitions coming from the formal matrix elements of $\hat H$ in this second almost-basis. And then you just look and see that the resulting path integral is therefore exactly what you get when you derive the path integrals of the $k_{max}$-cut-off version of $\hat H$. So the $k_{max}$-cut-off version of $\hat H$ will thus yield the same path integral as the original $\hat H$ with the $n$-dependent cutoff, or rather the two path integrals will yield equal results in the appropriate limits. (15:03)

*((15:16) Hm, it's really kinda crazy to all of a sudden make so short work of something that has troubled me so much in the past, i.e.\ showing that you can introduce an ($n$-\emph{in}dependent) $k_{max}$ to the operator and get the same dynamics.\,.(!\,.\,.))

Next you take a look at the path integral of the $k_{max}$-cut-off version of $\hat H$, and also with that $\delta k$ for its basis vectors. And then you see that this is the path integral derived from a discretized version of the same $\hat H$, namely where all ladder (/ creation and annihilation) operators are exchanged for those discretized versions. And then you take into account the Lorentz covariance, and conclude that.\,. Well, and you also consider the rhombus time-evolution-and-transformation circuit that I wrote about above this morning, although actually the volume can really be any shape for this argument. And then you see that for any given.. ..Well, long story short, for now, the idea is then to see that after integrating over all inner momentum parameters, as well as all possible times for where to place the vertices, the result should be the same in both frames. And what's more, the system is not just Lorentz-covariant, it is Lorentz-covariant for all values of $q$, meaning that the result for each particular transition (after the whole path integral over the combined volume) should be invariant for all individual orders of $q$ as well!\,.\,. And yeah, I've just re-realized that this is probably indeed the case, so this is where I have to think about how to finish the argument from there: Do I use this to introduce an antiparticle number limit on the path integral in the DSR'ed case, and then use that further to remove the v.p.\ terms, or what.\,.\,? (15:13)

(16:14) Yeah, I think that is it, exactly.\,. .\,.\,We get to the point where we have argued about the Lorentz invariance of our discretized transitions. Then we introduce a particle cutoff, and notice that if we start out with few enough particles, then the resulting path integral should still give us the same result for the all the small enough orders of $q$. And these should therefore be Lorentz-covariant. And the big point is then, I think, to argue that we can make $\delta k$ (/ $\delta p$, or whatever I called it.\,.) arbitrarily small, while this should not change the expected transition amplitudes. Of course, when we let $\delta k \to 0$, we also get a divergence in our DSR'ed Hamiltonian. But it still stays meaningful, and the transitions to all small enough orders of $q$ should still, I think, converge to the same thing, regardless of $\delta k$. (Or rather, they should converge when $\delta k \to 0$.) And then from there we just need to argue that, well, with a limit on $n$, and a smaller and smaller $\delta k$, i.e.\ one that doesn't require $n_{max}$ to grow when we let it tend towards 0, then my arguments for why we should be able to remove the v.p.\ terms should work after all. (The reason the argument fails in v1 of my paper, is that we don't know that $n$ stays bounded (expectation-wise) when we let $\delta k \to 0$.) So yeah, I think that this might just be it.\,. (.\,.\,!\,.\,.) (16:25)

(16:33) Oh, never mind about the `$n$-dependently cut-off $\hat H$' above, actually. Just replace that with the `non-cut-off $\hat H$' instead, i.e.\ the one with the ``Damgaard--\ldots domain'' or whatever we might call it (a domain of a similar kind as introduced in my SA paper).\,. .\,.\,(Or any other domain on which it is self-adjoint, for that matter.)

(16:43) And if we were to go the other way, then we'd start with my proposed Hamiltonian, then add a particle cutoff, a discretization, and add the vacuum-perturbing terms, arguing in reverse that with said particle number cutoff, these should not change the dynamics. Then we'd derive the path integrals, and see that for all small enough orders of $q$, this should give us the same transitions as the (still discretized) pre-DSR'ed Hamiltonian, only where we've flipped the daggers on the antiparticle ladder operators. And then we'd do my argument above.\,. Well, let's just stop here, and repeat my argument above, that shows that this operator is unitary (by showing that the path integral for the self-adjoint Hamiltonian approximates the more regular path integral). Oh and before that, there's my whole QED paper in order to argue its Lorentz covariance. But then you get that Lorentz covariance of my proposed Hamiltonian, at least when assuming that it is self-adjoint to begin with. And yeah, I should thus also include a similar argument at the beginning here (or at the end when going in the ``forward'' direction) similar to that of the 14:47--15:03 paragraph above, which shows that the path integral of the DSR'ed Hamiltonian converges as well (in both the two almost-bases).\,. .\,.\,It's crazy to say, but I really think I might have it here.\,. (16:54)

(31.10.24, 9:35) Oh no, we won't get the ``regular'' (momentum--time-space) path integral by doing that process (in the 14:47--15:03 paragraph above). It will instead be a ``low resolution'' of that path integral in a way, since we're essentially resolving $\hat H$ with the first almost-basis, which is a rougher resolution than the second (``regular'') almost-basis.\,. (9:38) .\,.\,(9:41) Hm, if we add more resolution to the first almost-basis subsequently, then we just get a lot of transitions with vanishing amplitude, don't we?\,.\,. .\,.\,Meaning that we might still be able to change to the same second (``regular'') almost-basis, with the same fineness i.e., without causing a greater error for the whole path integral, but now we all of a sudden have a finer resolution for the first almost-basis, meaning that we \emph{will} get the path integral (approximately) that is derived from the ``regular'' almost basis.\,.\,!\,.\,.\,? (9:46) .\,.\,Well, maybe the fineness required of the second almost-basis is still determined by the fineness of the first, also when we add the extra resolution.\,. Let's see.\,. (9:49)

(9:54) Hm, the $n$-dependent cutoff, going back to that, makes us able to get a bounded Hamiltonian, which allows us to change $\exp(-i\hat H \Delta t)$ into $1 - i\hat H \Delta t$ in the Trotter expansion.\,. .\,.\,Hm, and it means that we can resolve it in any basis that we want, meaning that we can just start with the ``regular'' almost-basis/resolution.\,. (9:58) .\,.\,But then we'll of course we that cutoff there.\,. .\,.\,(But maybe there's no real getting around that, 'cause we need to turn $\exp(-i\hat H \Delta t)$ into $1 - i\hat H \Delta t$ to get anywhere, I think.\,.) .\,.\,(10:03) Oh, could we maybe also use the $q$-gathering argument here, not to say that the dynamics is equivalent, at least not directly so, but to instead argue that the Lorentz covariance is preserved.\,.\,? .\,.\,Hm, if we maybe don't need for the regularly cut-off Hamiltonian to converge in the limit, but you just need it to be Lorentz-covariant, somehow, with any sufficiently high-resolution discretization.\,.\,? (10:07)

.\,.\,(10:08) Oh wait, going back to my two-almost-bases idea, maybe we don't need to turn $\exp(-i\hat H \Delta t)$ into $1 - i\hat H \Delta t$ before we get the second resolution, but maybe we can do it after.\,. .\,.\,(10:12) Oh! In fact, maybe having the $\exp(-i\hat H \Delta t)$'s in the middle instead is exactly what might make it work.\,. Let's see.\,. .\,.\,'Cause then you know that the extra added vectors to the first almost-basis will keep having a vanishing norm squared (when summed).\,. .\,.\,Oh, but you can't turn $\exp(-i\hat H \Delta t)$ into $1 - i\hat H \Delta t$ afterwards, then, when it is hitting the vectors of the second almost-basis. Yeah, this probably won't work. Maybe I do need that $n$-dependent cutoff.\,. .\,.\,Well, maybe I shouldn't write it out completely, but I won't succeed if I try to show that every single time step has preserved transitions when making the change. You have to argue about the whole thing at once, if it is to possibly succeed, I think.\,. (10:19)

(10:44) Yeah, no, getting the $n$-\emph{in}dependent path integral is still a hard problem for me; I didn't make short work of the problem now. Hm, but let me think about that other idea.\,.

(10:52) Hm, and if I instead leave this problem be, and perhaps assume that some Nelson Renormalization exists, or just something to that effect, what would I need to do then for my paper?\,.\,. .\,.\,Hm, then I'd just cut those ends off of the combined argument.\,. .\,.\,Let me do that.\,.

.\,.\,Hm, but if you look at a single almost-eigenvector of $\hat H$, can't you show something about the path integral (with some resolution with a ``regular'' almost-basis) of that.\,.\,? (10:59) .\,.\,If we could just regular-path-integrate that, then we could just combine enough almost-eigenvectors to get the result that we seek.\,. .\,.\,Hm well, yes, of course; that doesn't really change the problem.\,. .\,.\,Only that the state in this case ought to remain approximately the same, and \emph{if} it does, then we are successful, but I'm not sure that this helps.\,. .\,.\,Hm, looking at the non-cut-off $\hat H$, could there not be a way to make this argument.\,.\,? (11:07) .\,.\,Well, no, that's back to the thing I've looked at many times, namely the problem that there can be transitions from outside of the cutoff that changes the state significantly.\,. .\,.\,Hm, we \emph{can} cut off the state itself and still have it be an almost-eigenvector, but that might not help.\,. .\,.\,Hm, and we can make a whole resolution of cut-off states that are almost-eigenvectors.\,. .\,.\,And we can turn $\exp(-i\hat H \Delta t)$ into $1 - i\hat H \Delta t$ for the path integral of those, right (where $\hat H$ is the non-cut-off version of itself).\,.\,? .\,.\,No, of course not, that's the problem.\,. .\,.\,You'd need to resolve it into states of $\hat H$'s domain in order to do that, and then you're back.\,.

(11:24) Hm, how about looking at the $n$-dep.\ cut-off path integral, and then gather orders of $q$? .\,.\,Well, it seems to be a problem for me that the cutoff increases at every single level.\,. (11:26)

(11:34) Wait, I feel like I've looked at this before, but can't the cutoff be fermion-momentum-dependent instead?\,.\,. .\,.\,It's not so neat, 'cause a fermion can emit a high-$k$ photon to then get back to a small $p$ again.\,. .\,.\,Oh, I should also remind myself to check that the $n$-dependent cutoff can be derived, or whatever cutoff I land on (if I were to continue on this problem, which I might not, as I've said).\,. .\,.\,Hm, if only there was a pure fermion--fermion interaction, I mean before we do anything to the $A^\mu$ field.\,. (11:41) .\,.\,Oh, but for the DSR'ed Hamiltonian, maybe we can just use the.\,. Coulomb interaction, well, and maybe we can also use that for the.\,. Well, it doesn't really make sense to have a Coulomb interaction with a finer $k$-resolution than the photon resolution.\,. Or does it?\,.\,. .\,.\,Well, the Coulomb interaction isn't gonna help us anyway in the pre-DR'ed case, I don't think.\,. (11:45) .\,.\,Hm, could you add a small purely fermionic decay to the otherwise ``free'' fermion Hamiltonian, and then just turn that down when you get all the way back to the full $A^\mu$-field-with-fermions-on-top path integral.\,.\,? .\,.\,Hm, this \emph{sounds} pretty good.\,. (11:50) .\,.\,A ``regularizing fermionic decay''.\,. .\,.\,Hm, that actually sounds very good.\,.(?) (11:52) .\,.\,Well, we should then let the fermions emit some kind of imaginary boson, then.\,. perhaps even a boson with no momentum.\,. .\,.\,A regularizing boson field on top that interacts with the fermions, but which is uncoupled (directly) to the photons and the $A^\mu$ field (and whose discretization importantly doesn't need to depend on the discretization of the $A^\mu$ field).\,. (12:00)

(12:02) It sounds like such a trivial solution to the problem, now that I think about it.\,. .\,.\,(Not that it is trivial to prove, 'cause that requires a generalization of my SA domain first, but trivial to come up with.\,.) (12:04) .\,.\,Of course, all ideas are trivial, once you have gotten them.\,.

(12:07) Well, but do we know that the almost-eigenvectors of $\hat H$ will necessarily ``follow'' the chosen SA domain?\,.\,. I feel like I've concluded that, yes, they (probably(?)) will, before. And maybe I should also just leave it at that for now.\,. Well, I'm not going to add any of this in the paper, but I might as well just look into whether this perhaps-former conclusion is sound.\,. (12:10) .\,.\,Well, the almost-eigenvectors are \emph{in} the chosen domain.\,. (12:12) .\,.\,So yes, they ``follow'' the domain, right?\,.\,. (Or did I mean anything else by ``follow''.\,.(?)) .\,.\,Hm, the point of the matter is: Can we make the $k_{max}$ cutoff, if the domain uses this regularizing boson (emission and absorption) to cancel its own photon emissions and fermion pair emissions.\,.\,? .\,.\,Well, if we take an almost-eigenvector, then resolve it into vectors that ``follow'' the domain in a neat way, then the whole thing ``follow'' the domain neatly, and it's still an almost-eigenvector provided that the resolution is only fine enough.\,. (12:18) .\,.\,And then for that resolution, shouldn't we be able to make said $k_{max}$ cutoff?\,.\,. (12:19) .\,.\,Well, that gets me back to checking my logic around how to derive the $n$-dependently cut-off path integral holds up, or in this case a somewhat similar kind of path integral.\,. (12:20)

(12:45) Well, I'll have resolved you vector into vectors that says absolutely within the $k_{max}$ cutoff, while still conforming to the chosen domain.\,. .\,.\,Hm.\,. .\,.\,And since the combined vector is an almost-eigenvector, the transitions outside of this resolution should be limited.\,. (12:49) .\,.\,But not necessarily for the individual resolution vector, only for the combination.\,. .\,.\,Hm, but if you only look at the almost-eigenvector itself, then you can make a cutoff on $k$, right?\,.\,. .\,.\,Well, the (combined) resolved version of it will still be in the domain, yet have a cutoff, so yeah, the transitions when working with $\hat H$ should be limited outside of the $k_{max}$ cutoff.\,. (12:54) .\,.\,'Cause you choose $k_{max}$ such that all the vectors of the almost-basis (that follows the domain neatly) will have vanishingly small transitions to momentum states with $k$'s outside of that cutoff.\,. Yeah, and Bob should be your uncle.\,. (12:57) .\,.\,Brilliant (it seems).\,. (12:59)

.\,.\,Oh, but then I still need to check that we can get to the path integral with the ``regular'' resolution from there.\,. (13:00) .\,.\,Well, that path integral just comes from seeing that you must be able to put a $n$-dependent cutoff on the calculation for each of the $(1 - i\hat H \Delta t)$-operators, where $n$ in this case is the number of our regularizing bosons, and then the path integral should fall out of that.\,. (13:04) .\,.\,Wait, and how do we go from $\exp(- i\hat H \Delta t)$ to $(1 - i\hat H \Delta t)$ in the first place, when we have the non-cut-off domain?\,.\,. (13:06) .\,.\,Well, when the resolution is a subset of the domain, then we can maybe use that theorem called.\,. .\,.\,Well, I was thinking about the identity for the generator.\,. .\,.\,Yeah, I was thinking about the definition of the (infinitesimal) generator.\,. .\,.\,(13:16) Ah, for a fixed resolution for the subset-of-the-domain almost-basis, call it our `first resolution,' the resolved operator is bounded, which can then get us from $\exp(- i\hat H \Delta t)$ to $(1 - i\hat H \Delta t)$.\,. .\,.\,And then from there, isn't it just to consider the actual calculation for $\hat H$ on this resolution, and see that the result can be approximated with $n$-dependently cut-off integrals?\,.\,. (13:19) .\,.\,Yeah, and that is that, right?\,.\,. (13:23) Let's see, we first choose a resolution of almost-eigenvectors, then we.\,. .\,.\,Yes, then we choose the finer resolution of vectors, also in the domain (but not necessarily almost-eigenvectors), that each has a $k$ cutoff for $\hat H$ that yields a vanishingly small error. Then we choose a large enough $k_{max}$ to work for all vectors at once in this finer resolution. We also turn $\exp(- i\hat H \Delta t)$ into $(1 - i\hat H \Delta t)$, giving us an error that depend on $\Delta t^2$, and either the first or the second resolution, depending perhaps of when we do it.\,. .\,.\,Let's do it for the first resolution, I guess.\,. .\,.\,Yeah, it doesn't matter: when we have our $(1 - i\hat H \Delta t)$, then we can our integrals, and we can then argue, from the second resolution, that we can cut off these integrals with a $n$-dependent cutoff (where $n$ in this case is the number of our imaginary bosons, but the argument also works to show the $n$-dependent cutoff, where $n$ is the photon number, e.g.).\,. .\,.\,And then we are at our goal, right?\,.\,. (13:33) %..(Oh, how time flies..)
.\,.\,Yeah, we are.\,. (13:34)

Okay, so it seems that this ``regularizing boson field'' might potentially be the solution to getting path integrals with a constant discretization for the $A^\mu$ field. That would be very awesome. But I'll still not try to include all this in the paper. So instead I will just assume that it is possible to show that the $k_{max}$-cut-off path integrals are Lorentz-invariant.\,. and converges.\,. in the limit. Hm.\,. .\,.\,Well, converges at least with some regularization, that doesn't break the Lorentz covariance when the ``(coupling) strength'' of the regularization (in my case, the coupling to the regularizing boson field) goes to zero.\,. (13:40) .\,.\,Yeah, and then maybe this regularization is the adding of that energy that is part of a Nelson renormalization process, or the regularization is adding that imaginary boson field on top (with a coupling to the fermions that is subsequently sent towards 0), that will not be specified in my paper. I will just assume that one or the other, or a third method, works.\,. (13:43)

.\,.\,(13:45) Ha, I'm so happy for that idea (introducing something like this regularizing boson field).\,.\,! .\,.\,And it seems so simple (almost trivial).\,.\,\texttt{xD}\textasciicircum\textasciicircum

%..(13:50) Okay, let me take just a small break (maybe I'll actually postpone taking a walk), and then get back to do some brainstorming over the new structure of the paper (v2 of it).. ..Well, I could also take a walk and do some thinking.. (13:54) ..Yeah, I'll take a walk.. (13:55)

\ldots\ (16:03) The idea about introducing this regularizing boson field does seem a bit magical/weird, but so far, I cannot see why it wouldn't work. I mean, it requires a bit more thought than what I have previously written so far. But still, it does seem that it could have a chance of working.

My arguments are this: Let us consider a one-dimensional boson ``field'' of just boson on a line. Let's just call the parameter that this line represents $x$, as to not confuse with e.g.\ a momentum parameter. For the way I think about it, the fermion do not change momentum when they emit these imaginary bosons. Now, our plan is to turn down the coupling with these $x$-bosons at the end. But this also means that the ``$E$ interval'' for the domain (in the $x$-boson Fock space) has to grow when we let this coupling, call it $g$, is turned down. However, I've argued before that.\,. Well, let me put it this way: I believe that there exists sequences of sequences of $k$-cutoffs where $k$ grows (rapidly) for each ``level'' in each sequence, such that the rapidity of the growth grows in the sequence sequence (the outer sequence, sorry for poor formulation here, I'm a bit tired.\,.), and such that the path integral converges when you.\,. Well, to formulate it more precisely, let $k_{max, i, j}$ be the cutoff at the $i$th level in the $j$th sequence of rapidly growing cutoffs. Let the $k_{max, i, j}$'s be strictly increasing both w.r.t.\ $i$ and $j$. I believe that there exists such a set such that the path integral converge when $j \to \infty$. Or put more simply, we can essentially make the growth of the cutoffs be more and more rapidly, while having the dynamics converge.\,. .\,.\,So you can essentially increase the growth of the cutoff with respect to $n$ for free, and get converging dynamics. Okay, so let us look at a specific $n$-dependent cutoff, growing sufficiently rapidly in order for the dynamics to approximate the non-cut-off thing. Let us then look what happens when $g$ is turned down. My argument is then that if you consider the $n$-dependently cut-off path integrals, you see that the dynamics converge when $g$ goes to zero to the system where $g = 0$, and where the fermions are again free from these imaginary bosons. Now, as I said, when we turn down $g$, we also have to turn up the $E$ interval. But since doing so is just the same as choosing a higher $j$ for our sequence of sequences, we see that we could just have chosen a higher $j$ to begin with. So I think you can argue that when turning down $g$, the dynamics approach the completely decoupled system ($g=0$), even despite this issue that you also have to increase the $E$ interval in principle while turning down $g$.

Now, if this is true (sorry for the long explanation), then when we go back and look at the full path integral, which we have hopefully shown has converging dynamics, i.e.\ via a theorem that extends my SA proposition, then we can argue that when $g$ is turned down, the ``bare'' fermions (i.e.\ bare with respect to the $x$-bosons, i.e.\ not entangled with these) are almost-eigenvectors of the coupled system, when $g$ is small enough. This must mean that we can just exchange all ``bare'' fermion states with slightly ``dressed'' (i.e., entangled slightly bit with these $x$-bosons) fermion states, which are approximately equal to the bare states in the norm topology, then the formerly ``free'' fermionic part of the Hamiltonian (that is temporarily turned into the free part plus the perturbation that couples it to the $x$-bosons) will once again turn into an almost ``diagonal'' operator on this space. I feel like I could formulate this better.\,. .\,.\,In other words, when turning $g$ down, the previously free fermion states should once again behave like free particles in the $g \to 0$ limit. And what does this give us then?\,. It gives us a path integral where we still have our $k_{max}$ cutoff for the $A^\mu$ field (and thus the photons), but where the fermions are once again unperturbed by the ``regularizing boson field.'' And this way, one might thus be able to get that the $k_{max}$-cut-off path integral, which is the result at the end of all this, is the limit of the $n$-dependently cut-off integral ($n$ here being the $x$-boson number) when $g$ is sent towards 0. (16:39)

.\,.\,And since the former is Lorentz-covariant, given that its dynamics converge (which then will be the case), then it must follow that the latter is Lorentz-covariant, i.e.\ in the limit where $g \to 0$. .\,.\,Well, we don't need to get that; the result that we are after is just exactly that the $k_{max}$-cut-off path integral will converge when $k_{max} \to \infty$, which is what we might be able to show via this process that I've just described. (16:44)

.\,.\,So yeah, I think I will leave it at that. Now, I might be wrong, but then on the other hand, there might also be some Nelson renormalization process that works instead. It's just nice to have a potential backup idea if there's no known Nelson process for this Hamiltonian, or these Hamiltonian\emph{s}, rather, that is proven to work, and if proving it turns out to be hard. Then I have this potential other idea (which might actually conflict with the Nelson renormalization, if the added energy is non-vanishing, I don't know) which might work---and if my idea happens to work, then a nice benefit is also that my solution is constructive, meaning that we also get information about the domain, which further means that we'll for instance be able to find/validate almost-eigenvectors, at least in principle. (16:50)

(01.11.24, 9:36) I actually still think that there's a real, non-zero chance that this `regularizing bosons' idea might work.\,.\,:)

When it comes to the paper, let me also just mention the part about the fermions starting and ending far apart when making the assumption of the Lorentz covariance of the time evolution operator. So we'll just make the assumption, and then say that at least we assume it to approximately be true, as long as the fermions are far enough apart in the beginning and end when we initialize and measure them, respectively.

And then let's look into what I should do after this assumption.\,. (9:40)

\ldots (10:00) Hm, I'm just realizing something peculiar\ldots\ .\,.\,It seems to me, that we maybe ought to fix the momentum of every created antiparticle when comparing time evolution transitions and computing whether they are equal (due to the Lorentz covariance).\,. .\,.\,(For in the pre-DSR picture, those created antiparticles would have been incoming ones.\,.) .\,.\,But doesn't this actually kind of ruin the Lorentz covariance argument, or what.\,.\,? (10:06) .\,.\,(On the other hand, it seems to fix the divergences, doesn't it.\,. Hm, peculiar stuff.\,.)

.\,.\,In order to get away from these fixed inner momenta (inner in the DSR'ed case), we would have to argue about the Lorentz covariance of some integral of the initial and final states.\,. .\,.\,Ah, but if you have a particle that starts out in a state that is spread out over a very large part of the momentum space, then it should Lo.-transform into approximately itself.\,. (10:13) .\,.\,So can't we use that?\,.\,. .\,.\,That does mean that we integrate over all initial momenta equally for the full path integral, so yeah, maybe that's right.\,. .\,.\,Hm, it does give us factor that should then technically be included for each created antiparticle, but maybe we can gather in terms of $q$, and then still reach that Lorentz covariance result.\,.(?) (10:16) .\,.\,Hm, I guess you'd have to gather in terms of created antiparticles first.\,.(?) (10:18)

.\,.\,(10:25) Hm, maybe my ``approximate Dirac sea'' can come into play, only where we make $\delta k$ for the approximate DS much smaller than that of the Hamiltonian.\,.

.\,.\,(10:28) Hm, I guess that does solve this ``peculiarity''.\,. .\,.\,But it's important for argument.\,. Well, but there goes the idea of being able to assume that the fermions begin and end far apart.\,. (10:30) .\,.\,Well, then again, can't you argue for the Lo.\ cov.\ of a set of paths with fixed antiparticle momenta, then integrate this afterwards?\,.\,. (10:32)

.\,.\,(10:40) Hm, I think we just need to gather in terms of the number of antiparticles in the paths first.\,. .\,.\,(10:47) Yes, and then we are free to also gather in terms of $q$ afterwards if we want (due to the argument that the theory is Lo.\ cov.\ for all $q$).\,. 

.\,.\,Hm, as far as I can see, this means that we can actually introduce a factor to the theory for each vertex that includes an antiparticle line, either a creation or an annihilation. And that is a really wild result, if that is true; then we need to go to the labs in order to confirm that this factor is 1 in our nature, if it is.\,. (10:51) .\,.\,(A bit similar to how people have been (and probably still are) occupied by checking that the mass of antiparticles is the same as that of their counterparts.)

.\,.\,Well, let's see.\,. .\,.\,Yeah, one can probably gather the paths in terms of antiparticle vertex number, and then argue that each of these gives Lo.-cov.\ contributions to the time evolution.\,. .\,.\,And we can put a momentum cutoff on these vertices, which we should then send to infinity in order to get the Lo.\ cov.\,. .\,.\,Hm, and yeah, if the argument holds up from there, then I think we indeed get this possibility of having a factors that is $\neq 1$ on all antiparticle vertices, which means a factor of those interaction terms in the Hamiltonian.\,. How spacey if that is true.\,. (11:00) .\,.\,Hm, that also means that we could have no antiparticles.\,. Oh!\,.\,. Ha!\,.\,. This could then actually be one possible explanation of why we only mainly have particles: This factor could have been turned down for some reason during the creation of all our matter. Heh, pure speculation, but still a funny thought.\textasciicircum\textasciicircum\ (11:03)

.\,.\,Ha. But to get back on track: My idea was to then try to argue that you can put on a particle number cutoff.\,. .\,.\,Well, being able to gather first in terms of antiparticle vertices, don't we actually get the power to make an (anti)particle cutoff for free? (11:08) .\,.\,Oh, how sweet that could be.\,. And yeah, it does kinda seem so.\,.(!\,.\,.) (11:09)

\ldots (11:19) It does seem that this is the case.\,. And then, yeah, we are free to put the antiparticle number cutoff on the Hamiltonian, without breaking the Lorentz covariance. And if so, then my arguments for why the vacuum particles.\,. well, not only will et hold, it will be kind of trivial: Of course vacuum particles won't matter if there's a limited number of them, and the Volume is free to be sent to $\infty$, while keeping this number cutoff fixed. Hm, but what about the fact that this might then halt all other antiparticle creations as well, even those with an incoming photon, e.g., due to the fact that the space will likely always be at exactly the $n_{max}$ level, i.e.\ when $\mathcal{V}$ is large enough.\,.\,? .\,.\,Oh, this might be a problem, indeed.\,. (11:26)

(11:42) Hm, if you choose the right point to do the Lorentz transformations around, then you shouldn't pick up any phase factors when you transform the waves (of the initial and final states); you should only shift the wave functions in momentum space (when choosing bases where the spinor indices are also always preserved in the Lorentz transformation). So that shouldn't trouble us, at least.\,.

(11:48) Ah, *(continuing from the 11:19--11:26 paragraph) maybe we can show that despite having an antiparticle number cutoff, the chance of being.\,. Nah, when $\mathcal{V} \to \infty$, we should be in trouble.\,.

(11:53) Wait, can we not also gather in terms of the vertex \emph{types}?(!\,.\,.) .\,.\,Well, if that is so, then we should be able to easily remove any offending (or non-offending) kind of term that we want to.\,. .\,.\,(And that would therefore also eliminate the ``paradox''/puzzle of why my desired result would then also mean, seemingly, that you can remove the same vertex for the pre-DSR'ed system.\,.)

(12:07) The point is that for the pre-DSR'ed system (and maybe we only need to consider this alone in order to remove the v.p.\ terms, or rather the pre-v.p.\ terms), we should first of all be able to gather powers of $q$, and then gather operators further by their ``incoming'' and ``outgoing'' annihilation or creation operators (respectively).\,. Well, actually we should gather in terms of those first, right?\,.\,. .\,.\,Well, you can always gather in terms of $q$ in any case, either before or after.\,. .\,.\,Yeah, I think this is the case, and then what you get is a collection (a sum, if you will) of operators (adding up to the full time evolution), then has the same ``incoming'' and ``outgoing'' ladder operators, but which may have inner momentum integrals, at well as time integrals (for where the vertices are located in time). And then you can equate these (approximately) for both frames.\,. (12:14) .\,.\,Well, I guess this means that we can't separate further in terms of any inner vertex where the momentum isn't fixed.\,. But isn't it fixed for the pre-v.p. terms, though?\,.\,. (12:16) .\,.\,Oh, maybe that's right; maybe the momenta are fixed here.\,. at least for the antiparticles specifically.\,. Hm.\,. (12:17) .\,.\,Hm, the variable momenta in the pre-DSR'ed case arise from vertices where a photon is emitted.\,. Hm, so we can just integrate over the photon momenta.\,. well, that probably won't help directly.\,. Let's see.\,. (12:21) .\,.\,Well, if we then look at the DSR'ed case, that should still mean that one out of the three momenta, if we look at the Dirac v.p.\ terms, is fixed.\,.

(12:32) Hm, if you don't have any incoming negative-energy fermions to begin with, then we should only have to worry about those being created.\,. .\,.\,Hm, which by the way in the DSR'ed case means that an incoming antiparticle is annihilated.\,. Hm, I kinda think that that bodes well.\,. Let's see.\,. (12:35) .\,.\,For in the pre-DSR picture, we are free to limit the number of incoming negative-energy particles to 0, and in the DSR picture, we are free to limit the number of incoming antiparticles to 0.\,. (12:36) .\,.\,Hm, that does seem to bode well, indeed; maybe this will be the solution.\,. (12:38)

\ldots\ (14:08) That idea won't work. If it works, it would only show that we can remove the antiparticles altogether. And that's not what we are after. But I think I might be on to something else.\,.

(15:18) Hm, if I go back to that other place i was, arguing that we can put an antiparticle cutoff on the Hamiltonian, can't I then use something abut the bosonic nature of the vacuum triplets/quadruples.\,.\,? .\,.\,Yeah, when the number is bounded, then we don't get interactions between these bosons, so the vacuum Hamiltonian should reduce to something quite simple.\,. (15:21) .\,.\,To a system equivalent just a couple of different bosons that has a mass and an interaction on the form $\hat a + \hat a^\dagger$.\,. .\,.\,So equivalent to a shifted *(translated, and plus a constant energy, i.e.) SHO, right?\,.\,. (15:25) .\,.\,Well, except that number cutoff.\,. .\,.\,But that is still a nice discrete system, where there must be eigenvectors that are not approximately parallel to the state with the maximal number *(only at most one can be that).\,. Oh, and the systems are correlated as well.\,. Well, unless.\,. Hm.\,. (15:29)
.\,.\,Wait, maybe there was something wrong with that conclusion: How can we argue about putting a number cutoff based on having a limited number of interaction vertices, but then go and look at a time evolution where we include an arbitrary number of vertices?\,. .\,. (15:33) .\,.\,Hm, but maybe we didn't do that, maybe we just argued that the cut-off Hamiltonian will be Lo.\ cov.\ to some order of $q$, when only the cutoff is large enough, ah yeah, and then the point was to reduce $\delta k$.\,. .\,.\,Yeah.\,. (15:36)

.\,.\,Hm, and an eigenvector is just constant to all orders of $q$, or rather it remains parallel to its initial state to all orders of $q$.\,. .\,.\,Oh, can't I treat it as a single boson, i.e.\ summing over all spinor indices, and combining from both the Dirac interaction and the Coulomb interaction?\,.\,. (15:40) .\,.\,Well, not quite, perhaps.\,. .\,.\,(Not necessarily the (latter) `combining' part.\,.) .\,.\,(15:48) Hm, but this might also not be necessary for the argument; I think I might just need to part about it being an effectively discrete system.\,. .\,.\,'Cause it will have eigenvectors, then, and these can't all have maximal antiparticle numbers.\,. (15:50) .\,.\,Oh, there's a chance that this might be it.\,.(!\,.\,.) (15:51)

\ldots (16:04) Yeah, and as nice as it would have been to just have an algebraic proof that shows that we can get rid of the (pre-)vacuum-perturbing terms, even in the pre-DSR'ed case, I don't actually think that this is very easy to show. If so, the solution has alluded me so far. No, I think I just ought to complete my paper (v2) with this approach.\,. I mean, assuming that I'm right, and that this does it.\,.\,:) (16:07)

(16:15) Oh, I just got an idea, though, which might help in showing it algebraically.\,. .\,.\,If we look at a ``small'' subset of paths where a fermion is produces with some momentum around a certain value, and then look at what happens when there is an incoming fermion of the same type with a momentum that is also located around exactly that value (and with a constant phase w.r.t.\ that momentum parameter), what do we get then? Will the contributions from that path then vanish?\,.\,. (16:19) .\,.\,Oh, and it's an inner fermion that is part of a loop, by the way.\,. .\,.\,Meaning that, yeah, it should vanish, right? Or what?\,.\,. (16:21) .\,.\,Hm, no, perhaps not.\,. (16:22) .\,.\,Oh, yeah!\,.\,. I think it might.\,.\,! .\,.\,Yes!\,.\,. Okay, that changes the game, then. Then we might be able to show it more algebraically as well.\,.\,! (16:26)

.\,.\,Of course, the other solution was more fun, but still.\,.\,\texttt{xD}

.\,.\,Hm, it changes the game, indeed, since then we can basically fix all inner momenta, it seems.\,. (16:31)

.\,.\,Then the only possible freedom for the two operator expressions to vary from each other is some potential permutations, right?\,.\,. (16:39)

.\,.\,Wait, there's something fishy to all this.\,. (16:42) .\,.\,Being able to gather in terms of powers of $q$.\,. Am I sure that we can turn down $q$ as lw as we want, without having to increase the space-time volume over which we integrate, or something like that.\,.\,? (16:44)

.\,.\,Oh, but in case I'm wrong about that, i.e.\ being able to separate the orders of $q$, maybe I could still get through, 'cause maybe I only need for the vacuum boson system to be discrete, not finite.\,. Hm, but will it still be discrete when $n$ is not limited?\,.\,. .\,.\,Oh, I need $n$ to be limited (bounded expectation-wise, more precisely) regardless.\,. (16:49)

.\,.\,And, yeah, there must be something wrong about the logic of being able to gather in powers of $q$, 'cause how can the time evolution be momentum-preserving in both frames to, e.g., the first power of $q$?\,. (16:51) .\,.\,It can't.\,. .\,.\,Oh, I guess unless the first power vanishes.\,. Hm.\,. (16:54) .\,.\,Oh, and the operator \emph{does} depend on the space-time volume, so yeah, it is \emph{expected} that the contribution to time evolution of first order in $q$ will vanish with a large enough volume.\,.\,! (16:57) .\,.\,Great!\,.\,.

.\,.\,Hm, the operator expressions \emph{will} be different, though, for the two frames, so they do have some degree of freedom to vary.\,. .\,.\,Hm, yeah, maybe it's better to just focus on my other (more fun (and actually also very close to my first version of the paper)) solution.\,. (17:01)

(20:40) I have just realized that there might be a problem with my solution, which is that I still might need for states with the exact maximal antiparticle number to have non-vanishing amplitudes. But here's a question: Even if we have no bound on $n$, doesn't the discreteness of the vacuum boson system mean that the vacuum Hamiltonian will have true eigenvectors? (20:42) .\,.\,Hm, and unlike earlier times, I'm not talking about the discreteness due to $\delta k$ here, but due to the bosonic nature.\,. (20:46)

(21:35) Hm, maybe you could also just use an almost-eigenvector that has a vanishing amplitude for the last $m$ levels out of the $n$ levels up to our cutoff, where $m$ is the order of $q$ that we are looking at. We should be free to send $n$ towards infinity for any fixed $m$, right?\,.\,.

(02.11.24, 9:56) There's some other trouble. .\,.\,One thing: When you Lo.-transform, we get a differently shaped amplitude distribution (w.r.t.\ momenta), and that seems to cause trouble. Also, when you look at e.g.\ an antiparticle that is created and then annihilated again within the path integral, this corresponds to a part of the operator in the pre-DSR case that is vanishing (when $\delta k \to 0$). And you can say the same weird thing about a negative-energy fermion being created/produced and annihilated/turned back inside the path integral in the pre-DSR case; that also corresponds to an operator that should be vanishing for the desired DSR'ed Hamiltonian. So it seems there are some big thoughts to be made\ldots\ (10:03)

(10:40) Wow.\,. I just realized that we might desire there to actually be an enhancing factor on all positive-to-negative-energy transitions, and an opposite dampening factor on the negative-to-positive-energy transitions, proportional to $\delta k$, before we get the desired physics for the DSR'ed Hamiltonian.\,.

.\,.\,(10:45) Oh, and maybe we could get this exactly by considering transitions with incoming and outgoing negative-energy fermions with spread out momentum distributions.\,.

.\,.\,But I should try to get the Lorentz covariance argument to work again, and then see if that gives us any altered factors, with or without some kind of freedom to adjust them.\,. (10:49)

(11:05) Hm, it seems that we maybe will get that $\delta k$-dependent factor automatically when considering spread out wave functions for the incoming and outgoing negative-energy fermions.\,. That's quite exciting.\,.

\ldots (11:28) Hm, maybe that was wrong.\,. .\,.\,Hm, or maybe not, I'll see. But first I need to figure out how to even make the Lo.\ cov.\ argument work.\,.

(11:45) Oh, one could maybe use the momentum distribution coming from making.\,. consecutive boosts of an initial momentum state.\,.\,? Hm.\,. .\,.\,Ah, yeah, that is the point.\,. (11:47) .\,.\,Oh, but that might work in one dimension, not necessarily in three: I don't think rapidity as additive in three dimensions.\,.\,? (11:48) .\,.\,Or is it?\,.\,. Or more to the point: Can't you make a circuit of boosts in all three direction and then expect to get back to the frame that you started in? Or.\,.\,? (11:50) .\,.\,No, I don't think so.\,. (11:57)

(12:03) Ah, but despite the seeming distorted integration measure that we seem to ought to get from one frame to another, I think we might be able to instead argue via very thinly spread out wave functions, i.e.\ some that approximately transform to themselves, and then get the desired euclidean integration measure.\,. (12:06)

(12:16) But we can't come in with more than one fermion in this particular spread-out state.\,.

(12:20) Oh, forget about the thing about `needing a $\delta k$-dependent factor'.\,.

(12:21) Hm, and can't we just come in with a very ``thinly spread-out'' approximate Dirac sea?\,.\,. .\,.\,Wait, maybe there's no need: Maybe you can just argue that the operator is Lorentz-covariant when we sum over all initial annihilation operators (in the pre-DSR picture).\,. (12:25) .\,.\,But what does that mean, then?\,.\,. .\,.\,Ah, that when sandwiched with some creation operators at the beginning and some annihilation operators in the end, the result is the same. Hm, but this is in the pre-DSR case, and that means that we still have to then gather in terms of inner antiparticle momentum when we make the DSR.\,. (12:28)

(12:36) Hm, maybe it's not a problem after all: If the contributions from each inner antiparticle momentum is Lorentz-covariant, then the sum of two contributions is also Lo.-cov., and so on.\,. .\,.\,Ah, yeah, I think that's right.\,. (12:39) .\,.\,And then we just sum more and more paths together and get more and more of the full path integral in both frames (of the DSR'ed system).\,. (12:40)

.\,.\,Hm, doesn't this mean that I'm now back to where I (thought I) was last night, i.e.\ at the 21:35 paragraph?\,.\,. (12:42)

.\,.\,We do indeed then have control over how many antiparticles are included in the paths, namely since we can sum them together in order of starting with $n=0$ (of antiparticles), and then go up through increasing $n$ from there.\,.

.\,.\,Oh, and it seems that we are even free to add an arbitrary factor for each antiparticle vertex, and maybe even one that is different depending on whether the antiparticle is created or annihilated.\,.(!\,.\,.) (12:50) .\,.\,Hm, not that I think we'll need it, though.\,. (12:51) %..Okay, let me take a walk...

\ldots\ (14:49) I belive that I'm right about this approach for DSR'ing while preserving the Lo.\ cov., but I haven't quite been able to confirm that idea from last evening/night, or about making that $n$ cutoff work.\,. Let me try to see if I can carry it home.\,.

.\,.\,The point ought to be that when you $n$-cut-off the Hamiltonian, you should get an operator that is Lo.-cov.\ for all small enough orders of $q$.\,. .\,.\,And even when you come in with a vacuum state that is also bounded w.r.t.\ $n$, and even one that is bounded to $n-m$, where $m$ is the number of order that we are interested in.\,. (14:55) .\,.\,Hm, yes, that is the point.\,. .\,.\,And we should then also be able to show that said operator can be approximated by a similar operator without the v.p.\ terms.\,. .\,.\,Which means that the latter should also be approximately Lo.-cov.\ up to $m$ orders of $q$. We can then let $\delta k$ go to 0, and $m$ go to $\infty$. And since the resulting operator, well, we assume that is is self-adjoint in this limit, and that the dynamics converge, therefore the resulting operator, my proposal for the operator of QED, should be Lorentz-covariant.\,. (15:00)

.\,.\,Hm, but how exactly do we get that the.\,. Hm, I'm considering part where we come in with a vacuum state bound to $n_{max} - m$.\,. What's the argument here exactly.\,.\,? (15:02) .\,.\,Hm, I guess the thing is about having that almost-eigenvector, let me see.\,. .\,.\,Yeah, 'cause you can get a vacuum state in this $n$-cut-off Hamiltonian that stays approximately the same, and where.\,. Ah, but forget the `$n_{max} - m$' part; that doesn't work since the full time evolution is still much larger than what it would be for only $m$ orders of $q$ to be enough to approximate it.\,. (15:07) .\,.\,But if $n$ is large enough, then the vacuum state and the ``physical'' state should still be approximately decoupled, namely since the also don't interfere with each other much due to the cutoff, then.\,. (15:09) .\,.\,Oh, but that might actually still be provable via an argument where you come in with that $(n_{max} - m)$-bounded state.\,. (15:11) .\,.\,Hm, or maybe not.\,. (15:11)

.\,.\,No, I think you should instead argue by looking at the fact that each of the up to $m$ antiparticle creations in the ``physical'' part of the path integral just removes the $n$th level of the vacuum state.\,. (similar to a measurement).\,. (15:16) .\,.\,Hm, so for each path, you just get up to $m$ ``measurements'' that the vacuum state is not in the $n$th occupation state.\,. .\,.\,And that will give you an error of at most $m/n$, or something like that.\,. (15:19) .\,.\,Now, is that error for each individual path, or can we aggregate it such that it becomes an error only on the whole path integral.\,.\,? (15:20) .\,.\,Hm, well, I guess we can at least gather in terms of the $m$ different time slices where the $m$ vertices are placed.\,. (15:21) .\,.\,Well, but hopefully I can also just use that it is an almost-eigenvector, and therefore the measurement should hopefully just aggregate to a combined measurement that the vacuum is not in any of the last $l$ states, $l \leq m$.\,. (15:23) .\,.\,Well, no, it will be a bit more complicated than that, I think.\,. (15:24)

(15:38) Oh wait, don't we just have actual, physical vacuum bosons in our universe?\,.\,. If I'm not mistaken, their occupation number, when assuming no vacuum--vacuum interaction, will only be proportional to $\delta k^{-3/2}$, meaning that we get nice, smooth, coherent-wave-like solutions in the $\delta k \to 0$ limit.\,. .\,.\,Then we also have all the higher-order bosons, coming from the (``vacuum--vacuum'') interaction between these bosons, but wouldn't we be able to show something similar with those, i.e.\ that the solution ought to ``converge'' in a sense, when $\delta k \to 0$?\,.\,. Hm, and would their interaction with the ``physical'' particles, or rather the non-vacuum particles, converge as well?\,.\,. If so, then surely that is just the answer instead.\,. Hm, I feel like I've concluded before that this was not so neat, but now I'm kinda excited to try again.\,. (15:45) .\,.\,(Hm, or there could also have been some other reason why I decided not to pursue this idea further than I did, I don't really remember.\,.) (15:46) .\,.\,Hm, and it wasn't just the fact that the wave function of the vacuum bosons depend on the momentum cutoff?\,.\,. (15:48) .\,.\,Hm, I know that I've most likely considered this before, but I just had the thought, perhaps again: Couldn't this be the reason why their interaction with the ``physical particles'' vanish?\,.\,. (15:51)

(16:04) The (average) occupation number of the vacuum boson state (disregarding the vacuum--vacuum interaction for now) would increase w.r.t.\ $k_{max}$. But its amplitude over each momentum point as a normalized state would also decrease at the same time.\,. .\,.\,Hm, if $a(k_{max})$ is the norm of the vacuum boson's un-normalized wave function, then the occupation number would grow as $a(k_{max})^2$, right?\,.\,. Or is it $a(k_{max})$?\,.\,. .\,.\,No, it's $a(k_{max})$, and that's exactly what the amplitude decreases by. So maybe we could have vacuum bosons as part of our physical universe (its EOM, i.e.).\,. (16:11)

(16:12) Hm, and if so, and if the vacuum state doesn't really change at all when you excite it or lower it one step, then surely there would exist a Hamiltonian on the Hilbert space without these vacuum bosons that gives the same dynamics.\,.
.\,.\,(16:21) Oh, and that would actually be very exciting as well, by the way.\,.

(16:24) Ah, and I'm sure that that occupation number going as $\delta k^{-3/2}$ is then exactly what will make the interactions with it converge.\,. .\,.\,(between the ``physical'' (non-vacuum) particles and the vacuum bosons).\,.

(16:27) Now, let me see, when we increase $k_{max}$, what happens then.\,.\,? .\,.\,$a(k_{max})^2$ is $1/k$ integrated in 6 dimensions, giving us something that goes as $k^5$.\,.\,Well, it's $k_{max}^2 p_{max}^3$.\,. .\,.\,Yes, it is.\,. And for the Coulomb interaction, we get $k_{max}^{-1} p_{max}^6$.\,. Hm, and that was $a(k_{max})^2$, not $a(k_{max})$.\,. (16:35) .\,.\,Well, if we let $p_{max} \propto k_{max}$, then we get $a(k_{max}) \propto k_{max}^{5/2}$, which for $k_{max} > 1$ is $\leq k_{max}^{3}$. And that's potentially great news, since this might mean that we can always just increase $k_{max}$ in order to prevent the vacuum boson solution to be interfered by the fact that the bosonic nature is lost once we get close enough to $N_{max} \propto k_{max}^{3}/\delta k^3$.\,.\,! (16:40)

(17:00) And about being able to derive a Hamiltonian on a Hilbert space without the vacuum bosons (and without the vacuum-perturbing terms as well, actually): Yes, that's pretty simple to do.\,. well, at least if we consider the case with no vacuum--vacuum interactions, or if the various vacuum bosons just aren't entangled.\,. And yeah, then I should think about what happens if they are entangled.\,. well, how \emph{can} they be?\,. When.\,. Well, maybe there's a solution where it's just a mix of vacuum bosons all in the perturbed ground state, and where each perturbation strength then just depends only on the previously calculated perturbed states.\,. That would be awesome.\,. But maybe it'll be more complicated.\,. (17:05)

(17:51) Oh, I think I can reduce the perturbed vacuum to a quite simple Hamiltonian (not necessarily easily solvable, but simple nonetheless).\,.(!\,.\,.) .\,.\,Well, or a simple equation.\,. (17:56)

(17:58) Oh!\,.\,. And perhaps there won't be any significant interference between the various vacuum bosons.\,.\,! That would be great, if that's the case.\,. .\,.\,I think we might be able to find some invariants where, if we make our bosons the sum of all parts within each invariant group, then there will only be significant transitions between exactly these bosons, and they also won't interfere with each other. That would really be great. I'm not sure though; these are just my.\,. ``initial thoughts'' (except that I have thought about this topic before, at least in some ways).\,. (18:05)
%..I'll need to do some drawing to check these premonitions. But I think I will hold feierabend (fyraften) (call it a day) now.. Maybe I'll do some drawing and such later, but I'll see.. (18:12)

(19:17) Hm, no, maybe I won't be quite so lucky.\,. .\,.\,Hm, but maybe that won't matter too much, though.\,. (19:23)

(19:28) Wait, maybe the state that comes from the vacuum bosons interaction with each other \emph{will} cause a drain.\,. Well, or a divergence, I'm not sure.\,. .\,.\,Ah, no, never mind.\,. (19:33)

(19:49) Oh, there might be a drain.\,.\,!\,.\,.

.\,.\,A principle that might keep all the (average) occupation numbers below $\sqrt{N_{max}}$ *(i.e.\ growing at a slower rate, and ending up vanishing compared to $\sqrt{N_{max}}$), which will then, in that case, mean that the vacuum \emph{does} decouple from the ``physical'' particles.\,. (19:53)

(20:12) Oh, no, never mind.\,. (almost).\,. .\,.\,Hm, but the equation might get simpler, then, after all.\,.

.\,.\,(20:21) Oh, it might even be solvable, if I'm lucky.\,. .\,.\,(Well, machine-solvable, perhaps, but that would also be great. Even just knowing that a solution exists would be enough for me, and if it's a tangible, machine-or-human-solvable problem, then that just makes it really great.\,.)

(03.11.24, 12:08) I went to bed pretty early (for me) (around ten), and then I of course lay and thought about this problem. And I actually had some great realizations, and thought I almost solved it. (Of course, I then lay wake for another three hours as well, on top of the first two where I got said realizations.\,.) After waking up, I got a but puzzled over the Lorentz covariance part of the argument. But here in the shower, just then, I realized that you probably just need to turn $q$ up slowly for the inner volume:

If you turn up $q$ slowly, then if the vacuum solution stays non-degenerate, then by the adiabatic approximation, it should turn into the bare vacuum state at the end and at the beginning. And what's more, the in- and outgoing ``physical'' momentum--energy eigenstates should also turn slowly (likely) into their unperturbed versions again.

But, I realized something else in the shower as well: I think I've made a mistake in my thought calculations, and I think there actually might indeed be a ``drain.'' By the way, I should say that my solution idea is to try coherent wave solutions for each of the vacuum bosons, essentially. And a great realization from the bed last night (in those first two hours before twelve), is that since the standard deviation of the curve should go as $\sqrt{n}$ these curves should be less and less perturbed by by e.g.\ multiplying by a factor proportional to $n$, and such. So I wanted to, more specifically, try solutions where we just have a bunch of un-entangled coherent waves.

However, when we look at transitions where two vacuum bosons interact, then the transition amplitude should then be proportional to $n^2$. I think I accidentally took the square root of this, thinking of $n \propto \sqrt{N_{max}}$, and then just thought: oh, just another factor proportional to $n$. But no, it should be $n^2$. And these are still quite loose thoughts at this point, but I think that ought to cause a ``drain,'' where the first coherent wave, of the vacuum bosons that are created from the bare vacuum, well never even reach up to $\sqrt{N_{max}}$.\,. (12:28)
.\,.\,(Or rather the rate: $\sim \sqrt{N_{max}}$, i.e.)

%..Thought the clock said 9:something when I woke, but it must have been more, or else I either slept without knowing, or just lay for much longer than I thought.. Can't really believe that it's 12:33 now, if it said 9:something back then..

.\,.\,Hm, but a factor of $n^2$ is just two factors of $n$; it still shouldn't alter the shape of the Poisson distribution.\,. .\,.\,Yeah, no, so maybe the vacuum bosons do play a part.\,. Okay, so I should get to trying to solve the problem.\,. (12:37)

.\,.\,Oh, I should mention, we do need the solution to be the (non-degenerate) ground state. But I don't intend to try to show that. If I can find a solution that is an (almost-)eigenstate, and that looks like it could be the ground state, then I'm satisfied with that.\,. (12:40)

(14:01) Nah, it's not ``just $n^2$,'' not when seeking the ground state. $\hat n^2$ should matter, i.e.\ since $\hat n$ also matters a lot on its own.\,. Interesting.\,. .\,.\,Hm, so maybe there will actually be some kind of ``drain''.\,.

(14:50) Hm, but couldn't we try a separable solution instead, where we solve for each occupation number of the first vacuum bosons individually.\,.\,? .\,.\,Hm, if the energy of each separate solution turns out to be proportional to said occupation number, yes.\,. (14:53) .\,.\,And if not, then we might also still be good.\,.\,? .\,.\,Nah, we prob.\ can't separate due to the self-interaction between these first-order vacuum bosons.\,. (14:57)

(14:59) Oh, by turning up $k_{max}$, don't we get a more and more powerful ``drain''.\,.\,?

(04.11.24, 9:58) I don't think there will be a ``drain'' after all. The way I see it, we have a problem which might be solvable with something that is close to a solution of unentangled coherent waves. But that $\hat n^2$ effect that I think we get seems to make it a not so easy problem.\,. I think I will look a little bit more into it this morning, but otherwise I might resume working on the semantic network project again, and just be happy that I now basically understand QED, finally.
That's actually really nice.(!) I do still think that the perturbed vacuum problem might very well be computer-solvable. And if that is indeed so, then it is just a matter of doing that, and then we can derive our Hamiltonian of QED. This result might then make the path integrals much quicker to convergence. (I hope so.) So it might even be at least borderline Nobel-price-worthy *(Nah, then it would probably, as a theoretical result, have to really be useful in solving some practical problems, I guess (not that I know much).\,.) to solve this problem (even if done numerically, I think). Now, if there exists an easy solution to the problem, well that's just awesome, but if it is a bit harder than that, that's also fine in the end.

I do have an idea about solving the problem that I want to share. But I also ought to look a bit more into it, I think, now that I have finally the way to get the full $\hat H_\mathrm{QED}$. So let me do that I guess (unless I want to just resume programming, and let this be evening work instead, but I'll see about that as well).\,. (10:09)

Oh, and although I thought I was so close with the $n$-dependent cutoff above (and considering $n - m$ and such), I just don't see how to get around that interference problem. So I no longer think that this is the way. Instead, if the vacuum is really decoupled with the ``physical'' particles, which I now doubt, then the solution to the vacuum (ground) state will show that in the end. So the next step for this project seems---apart from the whole SA aspect of it, and maybe seeing if my `regularizing bosons' idea works (well, after also first having extended my SA proposition in that case)---to be solving this perturbed vacuum problem (trying something close to coherent wave states). (10:15)

Oh, by the way, I think a reason why I didn't like to pursue this approach before was that I found the interference between the ``physical'' particles and the vacuum particles (and the ``vacuum bosons,'' you can say) to be troublesome. But as long as.\,. Wait, no, I might have been right.\,. Let me think for a bit about this (which I also already planned to do).\,. (10:18) .\,.\,Ah, no, I think that the fact that we can let $k_{max}$ increase is what then gives us less and less interference (and I do specifically mean `interference' (between fermions), not `interaction;' that I no longer think will vanish).\,. (10:22)

(10:37) Okay, I think I will indeed let it be something to potentially do in the evenings instead from now on (until/if I get a major breakthrough at some point) (if I have the energy to spare).

The thing I wanted to share is that you might try, when solving numerically, to use solutions to something like $c_1\hat n^2 + c_2 \hat n + c_3 \hat x$ (instead of just to $c_2 \hat n + c_3 \hat x$) in place of the coherent wave solutions. This gives some extra parameters to adjust (unfortunately), but it might still be possible to solve (numerically). Now, I should also say, this suggestion is just based on loose thoughts about the problem (I haven't done any actual paper (or latex) calculations anywhere yet about trying to solve the specific problem yet (only a few small calculations related to it).) .\,.\,Hm, and I should also mention, I'm not so used to the problem of finding exact eigenstates, let alone the exact ground state. So I've also had a hard time even seeing how one would prove that. But as I've said, just finding some solution that seems to minimize energy, at least locally might be satisfactory for now.\,. Hm, but that the thing I was really getting at: Maybe one could try to solve the equation directly, and maybe one could also instead try to just minimize the energy. It's not the the latter is necessarily easier, but if we consider a case where you just assume that there is a solution that is approximately equal (norm-topology-wise) to a state of unentangled coherent waves, well, then you \emph{could} just (``just'') solve the problem of minimizing the energy from there. And if that is doable, one might even obtain some knowledge of what the actual ground state could look like. But of course, as long as you have some proof/argument for why the ground state should be approximately that, then you would still get the desired results from this calculation, since all we need to know in order to get $\hat H_\mathrm{QED}$ is what the occupation number is for each of these almost-coherent waves (in relation to $\delta k$, as well as the $k_{max}$-dependent amplitude of the unnormalized vacuum bosons.\,.). When we have that, we'll get the strength of each of the interaction terms that we need to add to the Hamiltonian. (10:57)

%Let me also mention: In the past I have also sometimes wrongly assumed that an interaction with the perturbed vacuum would change the state of the perturbed vacuum as well. But this is not the case (when the solution is coherent(-wave)-like). I think I did manage to realize this when I worked on the project last this spring/early summer, though.. (11:00)

%..Oh, and let me also just underline, that I'm also just so happy to have reached this point: I really feel like I finally understand QED. Even though I don't yet know what those coefficients on the added interaction terms are, I know that they are most likely there (and likely non-zero, I now think), well-defined and finite.. How nice is that.:) (11:10)

(19:12) Oh, maybe it won't change the scattering computations much, I don't know. So yeah, maybe it won't be the helpful technology that I hope for. But it would still be a great theoretical result. And who knows, maybe it \emph{will} help the computations, so it's definitely worth solving (numerically or otherwise).\,.

(05.11.24, 16:47) Oh, it might help the computations, indeed. But I'm not sure. It remains to be seen. But yeah, it might.\,.\,:)

(21.11.24, 10:59) I (purposefully) haven't really touched this physics idea since last time I wrote here. But I woke lay awake for a while tonight and accidentally started thinking about it again. I think the problem might be even easier than I thought when I left it last time. (And I also actually think that there \emph{is} a decent change that the perturbed vacuum is decoupled from the ``physical'' stuff, as it happens, but that will wait to be seen.) I think the problem might be solvable by essentially just solving the $c_2 \hat n^2 + c_1 \hat n + g \hat x$ Hamiltonian for all coefficients. And then for the first vacuum boson, you.\,. Well, the coefficients are all determined for a given $\delta k$, so you'll get that solution. .\,.\,Now, the point is, when two level-one bosons interact (through absorption) and create one of the two second-level bosons (the other one being where a level-one boson emits a particle (or when a boson absorbs some of itself, I guess.\,. (Well, we'll see.\,.))), if this boson state is also in a coherent-like wave solution, unentangled with the first solution, then this should just give the same solution back, almost, and only with a small error, which I believe might go to zero as fast as $1/n$ norm-wise (and $1/n^2$ norm-squared-wise).\,! And the factor you get is also just 1 (the solution doesn't change size when you add a particle to the coherent-like wave (since you don't have the $\sqrt{n}$-dependency in this case that $\hat a^\dagger$ has)). And that's why $c_2$ should be determined just by knowing $\delta k$, and being independent on the solution of the level-two bosons!\,.\,. Now, I also think the same logic applies for the level-two boson that comes from emitting a particle. Only this time, the ($\delta k$-dependent) factor adds to the $\hat n^1$ part, adding to the free energy, to give us the ($\delta k$-dependent) $c_1$. So if only we keep choosing.\,. Oh, and I should say, for all other bosons, $g$ will be 0, but $c_2$ and $c_1$ will still generally be non-vanishing. (For we absorb the transitions \emph{to} the boson as part of the equation of the emitting boson.\,.) Hm, but I might have forgotten the.\,. Nah, maybe not.\,. Hm, yeah, there is a chance that this simple solution might work. I was about to say: So if only we keep choosing (coherent-like) solutions for.\,. Oh.\,. Hm, `for the other bosons where the energy matches that of the first solution,' was what would have said, but I guess these coherent-like solution won't be almost eigenvectors when $g$ is 0 (since we have already treated that part as part of the equation for the other boson that transitions into the given one.\,.).\,. Okay, so what does this mean.\,.\,? (11:27) .\,.\,Well, let me continue to think about this in my spare time.\,. (11:29)

(12:32) There still won't be a ``drain,'' meaning that, no, I don't expect the perturbed vacuum to be decoupled from the ``physical particles.'' .\,.\,We should expect that the solutions, well, at least for the level-one boson, has an occupation number that goes as $\delta k^{-3/2}$, which is exactly what will (potentially) make the interaction with the vacuum converge.\,. (12:35) .\,.\,(And I don't expect $k_{max} \to \infty$ to change things *(other than to make the ``vacuum bosons'' more and more bosonic), although I haven't calculated much in regards to this.\,.)

(12:49) Hm, here's a different idea: One could try to treat each occupation level of the level-one boson (state, i.e.) separately, finding solutions for each one, and then putting that together somehow.\,.

.\,.\,(12:53) Oh, won't the average occupation number for the higher-level boson( state)s actually be very small, and not grow as fast as $\delta k^{-3/2}$?\,.\,. Well, there's the fact that these also have the $c_2$ part, so I'm not sure.\,. (12:55) .\,.\,But intuitively, the $c_2$ should only come into play, \emph{if} the occupation number becomes large enough, and not the other way around, i.e.\ when you think about it in terms of the dynamics of what happens, if you start out in the level-one boson state (in some occupation number), and then let the particles time-evolve from there.\,. (12:59) .\,.\,Right, and you can say the same thing when thinking in terms of finding an almost-eigenvector: One might be able to find such for low occupation numbers for the higher-level boson states (if what I'm thinking about holds.\,.).\,. (13:01) .\,.\,Oh, but we can't look at the different occupation numbers of the level-one boson separately, since this changes when.\,. Well, that was not the point.\,. The point was to look at the space of all states that can be \emph{reached} from that initial level-one $n$-state separately.\,. (13:04) .\,.\,Hm, but that then means entanglement between the occupation numbers of the different boson states. But on the other hand, if the higher-level boson states has occupation numbers that doesn't grow with $\delta k \to 0$, then this entanglement could be okay, in terms of not preventing us from getting a final $\hat H_\mathrm{QED}$ from the solution, i.e.\ one that doesn't include the vacuum part of the Hilbert space.\,.\,!\,.\,. (13:08)

.\,.\,(13:14) Oh, and if the solutions doesn't really depend much on the $n$ of the initial level-one boson state (which is the starting point for defining the given subspace of states), save for perhaps an energy difference, then the thing could still perhaps be very much solvable.\,.\,!\,.\,. Definitely worth looking into.\,. (13:16)

\ldots\ (14:48) I haven't thought about the fact that the free energy is actually not a $c \hat n$ term, but it also includes an ``interaction'' turning one vacuum boson into another. But I guess one could separate the part of the free energy that turns the boson into itself, and call that the resulting free energy, and then take the other part and consider it as part of the interaction transitions, on par with the emission interaction transitions.\,.

Another thing: I kinda forgot that we in principle need to find \emph{the} ground energy eigenstate state, and not just some almost-eigenvector. So there we are, the thing is still quite a hard problem, it seems. Unless of course $k_\mathrm{max} \to \infty$ changes things and means that the vacuum does decouple. And come to think of it, why do I think that it won't?\,. I think I ought to do some calculations on this (either now or at a later time).\,. (14:53)

%(15:08) Hm, jeg kan ikke samle tankerne om at programmere, så lad mig i stedet bare prøve at se på det her med k_{max}..
%..Hm, hvordan griber jeg det an..? ..Hm, jeg er virkelig langsom i dag.. (15:19) ...(15:30) Hm, when the amplitude in k-space isn't changed.. wait.. ..Oh, the density isn't changed, rather.. (15:31) ..Hm, if we look at Diract triplets, what is the invariant density, again?.. ..I'm really not very useful today.. (15:35) ..Inefficient.. ..I should try to do something else, something more productive.. (15:39) ..(15:41) Hm, I don't think k_max changes things: I think the dynamics converge (and the interaction with the ``physical world'').. ..Yeah, I think so.. (15:42)

(15:42) Okay, I don't think $k_\mathrm{max} \to \infty$ changes things. The thing is: There should be an invariant in the density of $k$-space. And that should also mean that the transition flux to the higher-level vacuum bosons ought to converge for $k_\mathrm{max} \to \infty$, I think. So with this, let me leave the physics topic again. It's still axciting to now that solving the perturbed vacuum might be feasible, but I think it'll be quite hard still, and probably much too hard for me.\,. (15:46) .\,.\,So yeah, I'm just glad I've gotten to this point. (15:46)



















\chapter{Energy and climate ideas}

(17:23, 11.01.24) In my 2021 notes, I wrote about some green energy ideas, some of which are pretty far out. One of the ones that are pretty far out, but which I nonetheless think ought to be investigated more is my `grasshopper idea' (see those notes).

Then there were also some ideas about growing algae, some far out, some less so. I just wanted to quickly write (perhaps again) about a not-too-far-out version of this idea: Imagine laying out a large net on which algae/seaweed can grow (big) in the Pacific Ocean, or some other relatively quit part of the world's oceans. At certain points in this net could be buoys which also (at least some of them) had the task of pumping up nutritious water from deeper below. Algae could grow on the net, and once in a while, you could pull the net in and remove the algae, dry them, and burn them *(to get energy)---preferably in a way that collects and preserves their nutrients (nitrogen and phosphor). This would then be a source of energy, and it would also have the ``side effect'' of increasing the Earth's albedo. (17:33) *[(16:38, 22.03.24) And instead of burning the algae for energy, it could also potentially be used in food and/or fodder instead.]


(22.03.24, 10:07) I have thought a bit about an idea of mine about building ice mountains on the poles. It was also combined with thought that this might be a potential idea for a space elevator as well. But I guess the temperature would rise too high on the mountain slopes and melt the ice. And there's also a problem with using the idea to cool the planet, 'cause then you'd also have to get the frozen ice back into the ocean, which would be costly---not saying that pumping that much water, even slowly, wouldn't be very costly as well.

But then I also thought a bit more about the seaweed/alae idea, and I really think that I might be on to something very great there. Let me see.\,. .\,.\,Okay the combined agricultural area of Earth is almost 1 \% of the total surface. And then think of how costly it is to grow and maintain all those fields, and compare that to just having a net of (very spread out) tubes in the Pacific Ocean, say, where nutrients from the deep water can be pumped (slowly) up and through the tubes, and where the surface of the tubes is rough and fit for seaweed to grow from. Oh, and where the nutrients can also diffuse out (from holes) of the tubes. The point is since these tubes could potentially be spread out quite far, it maybe wouldn't require many meters of tubes to cover the area corresponding to a single field on the land. And the cost of these tubes, and the cost of maintaining them, ought to be much much lower when comparing to maintaining a whole field on land with all the tasks that that requires throughout the year. .\,.\,Maybe the costs aren't even really comparable: Maybe it would be incredibly more cheap to build and maintain the seaweed net. And from what I (think I) know, if we cover around a percent or two of the earth's surface, it should make a very significant change to the climate, and could prevent climate change, in part due to the captured carbon in the seaweed, and due to the increased albedo. I also thought, if the main tubes can be pretty much parallel to each other for large patches of the area, before any supporting tubes or wires crosses over them, then there could be these large rectangles where boats are free to sail unhindered (without having to cross over a tube or a wire). This could mean that boats/ships could sail in these rectangles and harvest seaweed/algae, and potentially also all the fish that would naturally seek shelter in and feed on the seaweed algae. *(And of course you don't fish all the fish at once; you fish only a fraction and let the others multiply to fill the space afterwards.) So their might even be a great yield for this climate idea: Who knows, it might even be profitable even without counting the good of increasing the albedo and capturing carbon. It might potentially be an incredibly great idea. Definitely worth investigating/researching more.(!!) (10:40)

.\,.\,Hm, and the idea might especially work if one could find a good type of algae (or several types) which can grow on top of each other, or better yet, grow together as one (and I know that there are one-celled seaweed/algae that can do this) and create a symbiosis such that the algae closer to the tubes share the nutrients with the algae further out, and the outer algae share their generated sugar (perhaps) with the whole symbiotic organism ``in return.'' (10:46) .\,.\,(Oh, and preferably one that floats, of course (if they don't all do that, I don't know).)


\ 

(11:11, 15.06.24) I have thought a little bit more about this idea yesterday and today. I think it might be a good idea to actually focus, at least initially, on the fact that we don't need to harvest the algae in this idea. A version of the idea is simply to distribute nutrients and make conditions right for the algae to grow on the water surface, let the algae grow and increase the Earth's albedo, let fish graze on the algae, and then catch some of all this increased (and gathered) mass of fish (and potentially turn a large profit). So instead of having to do all the work of gathering and processing the algae, for instance to make it into fodder (which would be a good use of it), the algae can just become `fodder' for the fish directly where they are on the surface of the water, without having to do anything. I think this is a good point which can help convince people that this idea is worth researching.

And just a quick note: The idea is really simply in this form. The only questions are how do you distribute the nutrients, and what other things do you do to make the conditions for the algae to grow?\,. About the first thing, my top ideas, I think, is to either spread them manually and continuously on the surface (from ships that sort of `fertilizes' the algae `fields'). Here one might perhaps use some oil and/or some emulsifier to make the nutrients stick to the surface of the water. Or, as another idea, one could perhaps make some kind of stations---or they could actually also be movable ships---that pumps up nutritious water from the deep. Yeah, so the `fertilizing' ships \emph{could} maybe, instead of carrying and spreading nutrients, just pump those nutrients up from below. About the question of what else to do for the conditions to make the algae grow, well, that's what we ought to research. Oh, and about \emph{what} kind of algae to use, that is also something we should research, but right now I'm personally thinking more along the lines of blue-green algae (cyanobacteria), rather than seaweed. But again: that is something to be researched. (11:29)

I think I might actually try to contact some engineers about this idea.\,. .\,.\,why not.\,.\,? (11:32)

.\,.\,(Oh, and let me just reiterate: I'm thinking mainly about the Pacific Ocean, by the way, since that seems like the optimal choice.\,.)

(16.06.24, 9:33) Let me also note that the fertilizing boats can just be small ones, and they can move very slowly, and pump up water quite slowly as well, enough that they are able to be powered by solar energy.

.\,.\,Oh, and I think it's worth noting the idea that if the nutrients are instead spread from the boats rather than pumped up, and if it turns out that it can be done via a thin film on the water surface, then perhaps this film itself could be part of increasing the albedo. And then the material used for creating and maintaining this film might not be wasted, as it goes to feed the algae, which feeds the grazing fish population, which in turn can be fished and sold for perhaps a greater profit than is used to maintain the film (one can hope). (9:40)

.\,.\,Hm, this version of the idea is actually also pretty exciting.\,. For it might be a more efficient way of increasing the albedo.\,. well, unless perhaps one were to breed high-albedo algae, and then have the fertilizing boats ``sow'' these algae as well while they are pumping up water.\,. .\,.\,So both ideas are pretty exciting.\,. (9:50)

(10:00) I also just realized: If the film reflects the light via `thin film reflection,' then the algae doesn't necessarily have to ``compete'' with the film for the light, as the film could perhaps.\,. oh, and this is perhaps even easier to implement if the light is reflected via pigments.\,. The point is that the film could possibly be made to reflect green-yellow-red part of the spectrum that plants do not absorb for photosynthesis.

(18.06.24, 10:31) I think the film thin reflection idea will probably costs too much material to be worth it. But the idea of spreading/pumping up nutrients and let the algae increase the albedo (potentially be breeding and seeding high-albedo algae) might not be to costly to be worth it.


%(16:50, 20.06.24) I had another idea, by the way, yesterday or foreyesterday, which is to great big lakes on the poles, far enough inland as to not spill out, and far enough away from the pole to melt in the summer. The idea is that the melting salt water (the salt will stay in the ice as crystal when it is frozen) will then take up heat (as a buffer) and cool its suroundings in the summer. A great idea if not for the fact that it would probably take too much water to really be feasible, I suspect. So I'll just keep it here in the source comments for now, I guess..

(11:14, 26.06.24) I've had an idea about creating saltwater lakes on the icecaps of the poles to create a temperature buffer and prevent ice from melting and flowing out into the ocean in the summer (by having the frozen saltwater lakes melt first and making sure that the water doesn't run out from them). I've just kept the idea in the source comments so far, but I thought some more about it yesterday, and I think it could in principle be possible. I think the amount of saltwater needed is comparable to.\,. Oh wait, no. It's a lot more than that. Okay, so maybe it would be more costly than I just thought. Oh well, it was an interesting idea, still.\,.



%(13:35, 19.07.24) I had a funny idea about iron rods (perhaps with some insulation around, perhaps not) hammred into the ground---or perhaps cables, but let's go with rods---down to where the iron is near its melting point. And then maybe you would just have an unending heat resource going forward from there as a consequence. I might write about this at some point and look a bit further into it, but now I've mentioned (and actually explained) the idea here. It might not be worth while, but it could then still perhaps serve a purpose in case of a nuclear vinter (or large volcano erruption or meteor creating the same effect), i.e. as an energy source for underground bunkers.. Oh well, now I've mentioned the idea.. (13:40)











%\chapter{Evolution}

%(14:38, 15.06.24) About evolution and kin selection: I believe I wrote somewhere in my 2021 notes (I think) about the fact that I thought it was weird to include ant workers in the the fitness calculation (of Hamilton's rule) when workers are sterile (are they not?). But I've just read that worker ants are in fact not sterile, and can reproduce. So that probably clears up that conundrum.
%(I am taking a very relaxed day today, before I start working on (quick-)fixing my vacuum paper, and now I'm actually reading a bit about group selection vs. kin selection again.. ..which feels kinda refreshing.:)..)
%(17:50) Okay, the whole discussion is actually a bit more `just semantic' (although not completely, it seems) than what I have thought before, I think..
%(18.06.24) I should maybe read The Selfish Gene again at some point, and see if I have soften up to the views. I definitely now think that the altruism theory of kin selection + mutalism is a correct theory, as long as one just remembers that you don't need social contracts of `if you do this, then I will do this in the future' for mutalism to work: A lot of times, animals (including humans) will just benefit from having more group members rather than fewer. This could be to divert predators (as in `the selfish herd') or because it increases the number of potential mates, both for the individual animal and also for its offspring in the future(!) And for social animals, the benefits (to the individual---and its next of kin) are often more strong of having a sizable group. And yeah, in general it might be important to note the future aspect of the calculation, as it might be relevant to some cases: How likely will it be for my offspring to find potential mates (and thereby to multiply rather than shrink in number)?. ..One can also point out: Developing sexual reproduction is an ultimate example of `what will increase the likelihood of my offspring, and my offspring's offsring, etc., of surviving. (10:49)









\chapter{Notes from 2022 (out-commented)} \label{notes_from_2022}




\begin{comment}

Disse noter er bare nogle korte ting, som jeg ikke har lyst til at skrive ind i mit nuværende "main-tex"-dokument (altså mit 2021-22-notesæt), og jeg gider heller ikke starte et nyt (2022-xx-)notesæt lige nu, bare for det.. Så dette dokument bliver altså et slags mellemled. (08.07.22, 12:19)



## Tanker fra i morges (08.07.22) omkring bl.a. børneopdragelse, men også meget mere

Jeg tænkte bare lidt på, at der sådan noget som børneopdragelse, og også sådan noget som hvordan man skruer en hverdag og et (sam-)liv sammen som et andet godt eksempel, at der kan jo være rigtigt mange forkellige parametre og stille på: rigtigt mange forskellige tilgange, man kunne eksperimentere med. Og min pointe, jeg har lyst til lige at notere, er, at med mine web 3.0-idéer så kan folk jo på globalt plan diskutere sådanne tilgange, og ikke mindst arbejde på at sætte omtalte parameterrum op --- og her kan man helt sikkert bruge ML som en stor hjælp. Så vi vil altså i fremtiden kunne få et meget bedre overblik over sådan et helt rum af forskellige tilgange. Dette kan man så diskutere omkring og analysere, og bl.a. prøve at gætte på, hvilke parametre, der kunne spille godt sammen, og hvad der kunne passe bedst til forskellige omstændigheder/forudsætninger. Så man vil altså i dette globale netværk meget bedre kunne opstille en masse forskellige muligheder, og derfra bruge dette til at komme med gæt og forudsigelser, som er værd at slå ned på og "undersøge." Og i de to tilfælde, i.e. børneopdragelse, samliv generelt, og også bare >>liv<< generelt, der betyder at "undersøge" jo så, at nogle mennesker og/eller nogle lokalsamfund prøver at teste nogle af disse hypoteser simpelthen ved at udleve dem (i en længere periode i det mindste). Og med sådan et globalt (videns)netværk, så vil man hurtigt kunne opnå det samme, og meget mere endda, end hvis man havde kreative teoretikere (eller hvad man skal kalde sådan en som mig) til selv at udtænke diverse tilgange, der kunne vise sig at bære frugt under diverse forudsætninger. Og en side-konklusion er så derfor også lidt, at selvom jeg tror, dette emne *(omkring børneopdragelse og sådan noget) lige præcis er et, hvor jeg kunne være god, og hvor jeg kunne lægge rigtigt meget arbejde potentielt set (i fremtiden), så vil dette altså være endnu en ting, hvor mine 3.0-web-idéer også bare (formentligt(7, 9, 13) og fohåbentligt!) vil komme og ændre billedet totalt (og i sådan en grad, at der vil være langt mindre behov for enkeltindivider, eller enkelte små grupper, til at designe sæt af gode tilgange fra bunden og op). (12:38)
%*(18.08.22, 13:39) Denne idé/tanke er jo meget en naturlig fortsættelse af mange af mine andre tanker. Det er jo lidt bare, at man kan skabe gode muligheder for diskussioner, og så vil befolkningen meget hurtigt og effektivt kunne udvikle en masse gode nye idéer, gode nok til at de er værd at afprøve. Denne del af det ligger senere end nogle af de andre forestillinger (f.eks. vil disse muligheder sikkert først komme rigtigt en del tid efter, at man får gode muligheder for debatter.. men ja, jeg tror helt sikkert at dette også vil blive et resultat af hele den udvikling på sigt). Og lad mig så også lige nævne en lille ting, som nok egentligt (også) burde stå i en "sektion" med et andet navn: Jeg vil bare gerne lige understrege, at ja, jeg tror virkeligt på, at vi med denne udvikling, som jeg forudsiger, vil blive gode til at diskutere ting godt. Og ja, faktisk tror jeg på, at vi i en ikke al for fjern (faktisk rimelig nær) fremtid også vil blive i stand til som befolkning(er) (globalt og lokalt), virkeligt at få diskuteret grundigt, hvordan f.eks. vore politik skal være (lokalt, men også mere globalt), og i det hele taget hvilke nogle retninger, vi skal bevæge os som samfund, og hvilke mål vi skal betræbe os --- og hvordan vi skal bære os ad med dette. Dette bliver dog ikke en ting vi opnår lige med det samme: Det ligger altså nok som en af de lidt fjernere muligheder. (Der er nemlig mange muligheder, som denne udvikling vil bringe, som vi hurtigt kan få gavn af, og så er der altså også nogen, som nok vil tage længere tid om at komme ordentligt skub i. Og ja, denne sidstnævnte ting er nok ikke en situation, man skal forvente vil opstå med det samme, men jeg tror altså som sagt, at der ikke vil gå mange mange år, før den beskrevne situation bliver en realitet.) (13:50)


## Fortsat omkring diskussioner og videndeling, som det fremtidige internet vil åbne op for

(05.09.22, 20:08) Jeg har sikkert nævnt dette hurtigt et sted i mine 2021--22-noter, men noget andet som virkeligt bare vil blive godt i fremtiden, er når vi kan få bygget en god ontologi / et godt kort over, hvilke personlige problemer og/eller klager og/eller ønsker folk har i samfundet. Det ville hjælpe samfund (altså vores nuværende store samfund, i.e. lande) gevaldigt, hvis forskellige befolkningsgrupper nemt kunne få langt større indblik i, hvordan de andre befolkningsgrupper har det, og hvilke problemer de slås med. Og så vil man jo i det hele taget også bare kunne overveje politiske beslutninger sammen meget mere effektivt, hvis man har tingene (altså ønsker/klager/problemer) kortlagt så godt på den måde. Selvfølgelig kan man komme ud for, at folk smørre tykt på med, hvor store deres problemer er i forhold til andres, men så skal man jo bare lige sørge for, at det hele først bliver diskuteret og analyseret endnu mere (hvor man bl.a. kan tage stikprøver især fra folk der ligger lidt på kanten mellem to befolkningsgrupper (og/eller på anden måde har en position, hvor de har indsigt i, hvad sandheden egentligt er, men ikke har en personlig bias for selv at lyve/smøre tykt på)), inden man begynder at behandle det som fakta, at så og så mange borgere har et så og så stort problem med det og det. Så ja, det kan vi altså også se meget frem til --- det er desværre nok en af de ting, der kommer til at ligge meget sent i hele udviklingen, desværre, men vi skal nok nå dertil på et tidspunkt som (global) civilisation. (20:19)



[...]

*(Jeg har skrevet noget på et tidspunkt her om børneopdaragelse, men jeg vil lige understrege igen: Bare glem det. Scratch that. Men som sagt var hele pointen også bare, at alle sådanne nogle tanker og forestillinger, om hvad end emne det lige skulle være, det bliver altsammen meget nemmere at diskutere og udvikle i fællesskab i fremtiden, når vi får det Semantiske Web..)



## Web 3.0-bevægelse og forretningsidé

(07.08.22, 20:51) Hvis jeg skulle starte en virksomhed for at komme i gang med at opnå de drømme, jeg har om dette emne, så ville jeg fokusere på at starte med at lancere en web 2.0-side, det opfylder kravende fra min "forretningsidé" (om at kunder skal blive til medejere, og at det hele skal gå på omgang osv. osv. (se noterne i main.tex fra i januars og/eller februar, eller hvornår jeg helt præcist skrev dem)), og så vil jeg også virkeligt prøve på hurtigt at få indført, at betalende brugere for stemmemagt over en rigtig stor del af.. ja, af hvad jeg vist har kaldt skaber-aktierne, men nærmere bestemt, så vil jeg sørge for, at disse starter med at udgøre en rigtig stor andel af de samlede kunde/skaber-aktier, og så vil/ville jeg altså sørge for ret hurtigt at få indført, at det er brugerne/kunderne, der har høj stemmemagt over, hvordan diverse skaber-bidrag belønnes (med de skaber-aktier, som virksomheden uanset hvad alligevel er kontraktbunden til at udstede til nogen). (20:59) ..Og ja, så vil jeg bestemt også sørge for, at det hele er open source (og jeg ville bestemt prøve at få eksisterende open source programmører med på bølgen (som "skabere")). ..Og ja, derfra må man sige, at jo hurtigere brugerne kan begynde at føle, at der er flere muligheder på siden, bl.a. ved at der kommer flere og flere (open source) algoritme-muligheder, jo bedre, for det er jo så der, man kan begynde at tiltrække brugere/kundere på baggrund af selve indholdet/rammerne(/mulighederne).. (..og altså ikke bare på baggrund af hele forretnings- og open source-idéen ved det; pga.\ nutidige muligheder for brugerne, og ikke bare på baggrund af fremtidige visioner.) (21:04)

(17:42, 19.09.22) En god måde at starte et web 2.0-til-3.0-firma (med min forretningsidé), kunne bare være at starte et firma og en kickstarter, og så bare love, at alle donationer hurtigst muligt vil blive omdannet til kunde-aktier, så snart papirarbejdet er gjort. Angående iværksætterenes og arbejdernes egen aktie-gevinst, så kunne man jo bare sige, at der lige i starten gælder, at.. Tja, eller man kunne faktisk sørge for hurtigst muligt at brugere kan uploade og stemme om vedtægter på en hjemmeside over, hvordan lønnen skal fordeles. Og så kan der bare være en fast klausul fra starten om, at en vis andel af al denne løn i denne indledende fase skal gå som løn til iværksætterne, og/eller at disse så også for nogle kunde-aktier genereret herved, svarende til en lille procentdel af denne løn. Måske kan man endda også vedtage, at lønmodtagerne i denne indledende fase også skal have nogle procentdele af deres løn i form af kundeaktier. Og i starten vil alt dette så bare baseres på løfter (men hvor iværksætterne muligvis alligevel kan retsforfølges, hvis de bryder disse løfter, fordi de så har handlet falskt og har fået betaling for en vare, som de så har valgt ikke at levere --- hvilket kun er godt, hvis de kan det, også for iværksætterne selv, fordi dette så vil få flere kunder til at stole på opstarten). Men hurtigst muligt skal man altså have udarbejdet kontrakter osv., så man kan gå ind i en ny fase, bl.a. hvor folks stemmeret omkring løn m.m. er mere konkret og detaljeret udarbejdet og sikret.. (17:53) 
%..Og hvad skal firmaet så starte med at lave? Jo, det skal såmen, udover at få styr på kontrakterne til fase 2, planlægge og give løn for programmeringsbidrag til en open source web 2.0-side (gerne én der både kan fungere som YouTube, Twitter og Reddit (m.m.) på én gang (og også gerne Wikipedia, men det kan godt komme lidt senere), men hvor man altså bare kan starte ét sted (f.eks. som en Reddit- eller en Youtube-agtig side)), og så må man så regne med, at tingene bare kan rulle derfra --- det tror \emph{jeg} i hvert fald helt bestemt på, at de kan. ..(For man bevarer jo selvfølgelig bare et system, hvor aktionærerne (og dermed kunderne!) kan stemme på vedtægter omkring lønfordelingen.) :) ..Og så kunne man jo oplagt have endnu en faseovergang efter lidt tid, hvor man også får de sidste ting på plads, bl.a. om hvordan fissioner af firmaet (og måske fussioner med andre, hvis det virker realistisk) skal kunne foregå, plus hvad jeg ellers må have glemt her fra mine noter (i januars/februars)..:) (18:02)
%..(18:07) Hm, og selvom det godt må være open source lige i starten, så kan det godt være, at man hurtigst muligt vil lave et system, så det er rimeligt åbent at se, hvem har gjort hvad, og hvor det måske ikke kræver særligt meget at få adgang til selve kildekoden også, men hvor kildekoden alligevel er eget af firmaet og ikke må tages/stjæles af andre. (18:09)

(21.09.22, 11:09) Jeg kan ikke huske, om jeg har skrevet om dette før, men jeg kom i tanke om i går aftes, at det jo er ret vigtigt, at normale kunder ikke ligestilles med f.eks. andre firmaer. Et firma/"underfirma" skal altså gøre det klart, om dets services er til private kunder eller til andre firmaer (for hver service i det mindste). Man må f.eks. ikke komme ud i en situation, hvor en instans bare kan købe og videresælge produkter, og så få de samme kunde-aktier for det, som de kunder, der køber til eget forbrug. Så derfor skal man altså generelt kun sælge produkter beregnet til eget forbrug eksklusivt til eget forbrug (hvilket i øvrigt sikkert også er meget normalt for firmaer allerede her i nutiden). (11:13)

(27.09.22, 17:47) Okay, jeg har tænkt en hel del mere over forretningsidéen i dag (og også lidt i går aftes), og nu kan jeg se at: Never mind den der forestilling om at lave en kickstarter eller lignende og så forvente, at firmaet så kan brede sig videre og videre derfra. Det er jo for nemt for alle andre firmaer bare at konverterer over, hvilket jo i bund og grund er godt, men det gør jo altså, at der slet ikke bliver noget (BitCoin-agtigt)venture-hype omkring idéen, sådan som jeg ellers kom til at tænke det nu her, hvor jeg er begyndt at tænke over denne idé igen. Så never mind alt det med (som jeg sikkert har nævnt under "Planer" nedenfor) at idéen kan blive den nye "helt store ting" i den forstand. 
Tvært imod vil det nok ikke kunne betale sig at investere helt vildt i normale firmear, der begynder at konvertere til forretningsidéen, medmindre man på en eller anden måde kan mærke, at de konkurrerende firmaer ikke vil have evnen eller viljen til at hoppe med på bølgen, og derfor altså vil blive udkonkurreret (formentligt, hvis man tror på idéen) af det firma, man så investerer i. Så medmindre der kun er nogle gangske få firmaer, der formår at brande sig godt på at være med på den nye bølge, så vil det nok mere bare være en situation, hvor flere og flere firmaer langsomt vil konvertere til de nye forretningsprincipper. 
Der er så også lige den undtagelse, at nogen brancher jo netop kunne få rigtig meget god synergi med denne idé, hvor kunderne/forbrugerne/brugerne kommer til at bestemme meget, og her tænker jeg jo så særligt lige præcis på min idé til en ny web-forretning/bevægelse. Så lige akkurat her vil der altså muligvis være gode investeringsmuligheder, men bid så mærke i, at dette så ikke vil skyldes.. hvad der svarer lidt en pyramide eller boble, hvor de første investorer altså kan tjene kassen på baggrund af, at de kom lidt før de andre. I stedet vil det simpelthen bare skyldes, som jo er normen omkring investeringer, at den nye forretningsløsning har potentiale til at tilfredstille kunderne meget mere --- ikke bare fordi disse også er investorer (og i og med at de så får en pengesum i vente), men altså lige præcis bare i forhold til det produkt de bliver leveret som kunder! Fordi der altså er mulighed for at sådanne hjemmesider m.m. kan komme til at levere et meget bedre produkt (altså bedre funktionalitet, bedre tilpasningsmuligheder, større udvalg og bedre kvalitet af indhold osv.), så vil det altså være værd at investere i, og kun ligesom af den grund.. Tja altså, medmindre selvfølgelig at man også regner med et vist hype omkring det, men det er jeg nu slet ikke sikker på, vil komme, hvis man netop ikke har nogen grund til at tro, at virksomheden vil brede sig til andre brancher derfra (fordi dem med aktiver i forvejen der også bare selv kan joine den nye bevæglese til hver en tid). (18:07)

(02.10.22, 16:52) Det kan faktisk godt være, at der kan lægge en stor investerings/forretningsmulighed i, hvis nogen kan finde på et rigtigt godt brand og en tilhørende rigtig god (offentlig) plan for, hvordan upstarts-firmaet skal være forbrugernes falgskib for at få bragt liv i den nye forretningsbølge: Hvis man kan overbevise en stor gruppe kunder til, at "det er her det sker," og at firmaet er hvad, man bør "investere" i som kunde, hvis man gerne vil sikre sig, at bevægelsen bliver til noget.. Så ja, dette kunne altså potentielt være en mulighed, især hvis man altså kan finde på en godt sted at starte (måske med en supermarkedkæde, eller en eller anden stor og alsidig handels/salgs-forretning), som virkeligt har mulighed for at brede sig meget ud, og som dermed kan vokse sig kæmpe stor, hvis bare alle kunder pludselig begynder at priotere handler med denne i høj grad.
Men ja, dette er nu ikke ligefrem noget jeg forudsiger, bliver en mulighed; jeg siger bare, at der måske kunne være et potentiale. Og ellers så tror jeg altså på, at man hellere skal tænke i firmaer/brancher, hvor firmaet vil have direkte gavn af (ift. det produkt, de ender med at levere!), hvis kunderne kommer til at bestemme mere, og hvis det også er sikret, at det bliver de ved med. 
Og i den forbindelse, så tror jeg altså faktisk på, at dette kunne være tilfældet for nærmest alle brancher, der handler med noget digitalt på en eller anden måde, enten med indhold, film, spil, læsestof, nyheder, bruger-til-bruger-indhold.. you name it.. Alle sådanne brancher, hvor det enten er sådan, at brugere selv i høj grad til at bidrage til værdien af det digitale, man nu end snakker om, og/eller hvis bare vi snakker kreative ting som kan konsumeres digitalt, hvor brugerne samlet set vil drage gavn af, hvis der kommer bedre forhold for skabere/kunstere, og også ikke mindst at alting bliver mere åbent (uden at folk behøver at bekymre sig om, hvis andre stjæler). For ift. sidstnævnte, så tror jeg jo på, at man, ved at kunderne styrer, kan nå en situation, hvor skabere/kunstere kan "bagud-belønnes" for deres arbejde. Og derfor kan alt sådan noget blive meget mere open source. Hvis vi så f.eks. tager spilindustrien (som et rigtigt godt eksempel), så er det ret nemt at se, at den samlede brugerskare kunne drage kæmpe fordel, hvis skabere ikke var nødsaget til at gøre alting så lukket. 
Ok. :) (17:11)

(06.10.22, 9:29) Lad mig lige præcisere noget i den tekst i 21--22-noterne, som jeg skrev d. 12/02-22: Jeg skriver noget med at "opkræve penge" fra aktionærerne. Her mener jeg selvfølgelig ikke, at man sender dem en regning, men altså at man bare nedjusterer det afkast, de har i vente. (Det fremgår sikkert et sted, men nu synes jeg lige, jeg ville kommentere og rette det her.) ...Hm, vi kunne da sagtens snakke 40 år i stedet (20 virker da ikke vildt langt..), appropos samme tekst.. *(Ah, det var for ikke at gøre udsigten for lang til en ægte kd.v., så tja.. ..Ah, men i princippet kan den jo blive "ægte kundedrevet" efter en ret kort periode alligevel, for det handler jo bare om, hvor stor en stemmemagt aktionærerne giver til sig selv i starten og i hvor lang en periode den stemmemagt varer.! Så der er faktisk ingen grund rigtigt til at sætte en kortere kundeaktie-periode.!:) (10:12) ..Nå nej, det passer så ikke helt alligevel, for hvis perioden er for lang, så kan der blive et demografisk (m.m.) skel imellem gamle og nye kunder. Så ja, hvilken periode man skal vælge fra starten er lidt et åbent spørgsmål, men man skal så huske, at denne dog stadig skal justeres (langsomt) løbende, således at den kun er en vis faktor større end, hvad anlagsaktiv-størrelsen som minimum kræver..) ..Og appropos samme tekst, bemærk så at det der med at have en instans, der vurderer firmaets samlede værdi til forskellige tidspunkter (ved at se lidt tilbage i tiden, så måske et år eller to efter), det skal ikke forstås som et væsentligt krav. Det er bare godt at have, bl.a. fordi det altså så gør det mere fair overfor aktionærer, hvis aktier udløber over en periode, hvor firmaet gjorde mange nye investeringer, og at de samlede mere direkte penge-omregnelige aktiver faldt i perioden, på trods af at værdien steg. (Men måske kan sådan en instans også bare se på aktiernes værdi i handler som en god kilde, man lad mig lige genopfriske, hvad jeg endte med at beslutte omkring aktiehandler..) 
...(11:04) Det var måske ikke så tydeligt, da jeg skrev om at opdele virksomheden, sådan at IP(/IM)-skaberne lidt fik deres egen "undervirksomhed".. tja.. Tjo, tja, giver det ikke lidt sig selv, selvom jeg ikke lige fik formuleret det tydeligt? Tanken er bare, at de så kan komme til at tilhøre og sælge deres bidrag til en "undervirksomhed," som så kan have andre "undervirksomheder" som kunder, der så bruger IM-bidragene til at implementere f.eks. en Web 2.x/3.0-side. Ja, det var nok rimeligt selvsagt, men nu har jeg også sagt det her. 
(11:21) Det kan i øvrigt godt være, at jeg her ovenfor på et tidspunkt har glemt lidt igen, at der skal være klare sætninger for, hvem der er de primære kunder (som skal have kundeaktier), og hvem man ellers bare handler med. Men ja, dog kan man jo sagtens starte med at have "donorer" eller "investorer," som altså kun giver rene pengebeløb.. med som kunder.. tja, men det kommer ikke rigtigt til at fungere. ..Nej, i stedet skal sådanne investorer jo bare købe aktier med deres "pengebidrag," hvilke jo så i høj grad naturligvis vil være "start-aktierne," eller hvad jeg nu har kaldt dem (dem til de indledende iværksættere og investorer). 

(11:42, 06.10.22) Jeg bliver nødt til lige at slå følgende fast, for jeg har jo snakket lidt om, her for nyligt, at der "ikke er den helt store investeringsdrøm." Men det passer ikke, eller rettere: sætningen skal i hvert i så fald bare forstås relativt til, hvis nu situationen var, at en enkelt kundedrevet virksomhed ville kunne udbrede sig til det meste af markedet. Jeg siger som sagt ikke, at der ikke er en vis sandsynlighed for, at ikke-så-web-baserede kd.v.'er kan udbrede sig rigtigt meget, men det er nok lidt for stor en drøm at forvente, at en sådan kan udbrede til stor mængde af markede, for som sagt kan andre firmaer jo altid bare følge trop.. Tjo tja.. Whatever, det giver ikke mening for mig at sidde her og prøve at forudsige den ene eller den anden vej på det punkt. Det jeg i stedet ville nævne var bare, at man jo (og det havde jeg måske kortvarigt glemt ovenfor, det ved jeg ikke..) skal huske, at de web-relaterede bracher jo også er \emph{kæmpe} store i sig selv. Så never mind, "at der ikke er en stor investeringsdrøm i det," for hvis jeg har ret, og at mine idéer omkring en stor web 2.0--3.0-virksomhed virkeligt vil kunne udkonkurrere gængse web-virksomheder, jamen så vil der jo potentielt set være en kæmpe investeringsdrøm i det, det er klart. Selvfølgelig er intet sikkert, hvorfor det er vigtigt at understrege 'potentielt' i den sætning. Men ja, følte bare, det var ret vigtigt lige at pointere. :) Det ville være lidt ærgerligt, hvis jeg unødvendigt kom til at ende på, at "der ikke er så stor en investeringsdrøm i idéen." Det kan der jo nemlig selvsagt meget vel gå hen og blive. :)
Lad mig også bare lige gentage, at hvis nu det var mig, der skulle starte sådan en "kundedrevet" web 2.0--3.0-virksomhed, så ville jeg altså virkeligt prøve at gøre virksomheden tiltrækkende for ""open source"-programmører" og andre skabere som muligt, nemlig ved meget hurtigt at prøve at implementere et "bagud-belønning"-system, rigtigt gerne hvor kunderne (og måske også gerne tidligere skabere/programmører) hurtigt for stemmemagt ift. bagud-belønningen også (og i øvrigt gerne hvor man også prøver at opstætte retningslinjer omkring, hvem fortjener hvad for hvad). Og her skal "open source" altså forstås meget i gåseøjne: Vi snakker nemlig slet ikke open source bidrag, for IP-rettighederne skal meget gerne gå til en fælles pulje som eges af virksomheden (eller endnu bedre: en mere uafhængig instans/"undervirksomhed" som virksomeheden så er kunde hos..). Men når jeg alligevel kalder dem ""open-soruce"-programmører," så er det altså bare for at pointere/hentyde til, at deres arbejde i høj grad så kommer til at minde om open source-arbejde, fordi tanken altså netop er, at bidragsyderne bare kan bidrage rimeligt frit og altså uden at være ansat og/eller have underskrevet en masse kontrakter, men hvor de så alligevel kan få løn for arbejdet via bagud-belønnings-systemet. Så ja, det ville jeg sandsynligvis nok prøve at sigte efter, hvis det var mig, der skulle starte sådan en ("kundedreven") virksomhed. (12:07) ..Ah, og vigtigt: Jeg ville også bestemt sørge for, at denne "bagud-belønning" også i høj grad (i starten især) ville komme i form af "IM-skaber-aktier," det er klart, for så kan man jo dermed belønne dem (hvis alt går godt) meget mere fra starten, også selvom man ikke har de store indtægter (fra kunder) endnu, og samtidigt så også gøre alle disse programmører/skabere mere investerede (også altså i overført betydning) i projektet. Det kan godt være, man lige skal se denne sidstnævnte ting efter i sømmene og regne efter på det hele først.. men ja, det ville jeg jo så gøre, hvis det var mig, der skulle være med til at opstarte en kd.v., for umiddelbart ser det ud til, at denne sidstnævnte ting også kunne gå hen og blive rigtigt smart at gøre. Nå. :) Følte lige for at gentage/understrege disse ting. :) (12:17, 06.10.22)


(19.10.22, 10:29) Jeg har fået tænkt noget mere over min forretnings(bevægelse)idé. Jeg har skrevet lidt ny brainstorm i et andet dokument, hvor jeg i går overvejde igen at tage mere udgangspunkt i omsætningen, men det går ikke. Og nu er jeg faktisk kommet frem til, at jeg nok bør ændre idéen til en mere simpel udgave (overordnet set, for jeg har også nogle nye tilføjelser, som jeg fandt på i går, om frit at kunne købe en vis størrelse kundeaktie oven i sin egen som kunde).
I bund og grund tror jeg nu på (bl.a. fordi jeg har indset, at mange af mine tidligere bekymringer skyldtes en tanke om, at virksomheden skulle være den eneste af sin slags, men det skal den slet ikke.. hm, ikke på nær måske hvis man tænker en web 3.0-virksomhed.. det må jeg lige tænke over, men lad mig her bare skrive om idéen med tankerne rettet mod normal industri og handel).. Jeg tror nu på, at idéen faktisk er bedst, hvis bare man simpelthen har start-aktierne og kunde-aktierne som før beskrevet, begge med en vis fast udløbskurve, således at aktiernes "størrelse" starter på et punkt og efterfølgende aftager efter hver lille salgsperiode. Nu mener jeg så, at det så bare skal være frit op til den samlede mængde aktionære (via deres stemmemagt) at beslutte løbende, hvor stort et afkast skal betales pr. aktiestørrelse efter disse salgsperioder. Virksomheden skal så bare have åbne regnskaber, så alle kan følge med i, inklusiv fremtidige kunder, hvad virksomheden har af reelle omkostninger, og dermed hvad pris-markup'en er for hvert produkt over tid (hvor man så selv kan vælge som iagttager, hvordan man vil regne udviklingsomkostninger ind sammen med "produktionsomkostningerne"). Det er så fornuftigt at forvente som kunde, og fornuftigt at drive virksomheden som aktionær, således at markup'en er rimelig konstant, når man midler over en periode, f.eks. over et år eller to. Og den skal i hvert fald helst gøres så stor, at kunderne er mere investerede til hver en tid, end hvad de samlede aktiver er hver, hvis man skulle sælge dem. Og desuden er det også smart at have en højere markup, hvis man gerne vil give større encitament for aktionærerne til at træffe gode beslutninger frem for dårlige --- plus dette giver også en vis investeringsbuffer, så virksomheden ikke hele tiden teknisk set er på randen af konkurs, altså fordi den "kun lige løber rundt," kan man sige. Men en al for høj markup er dog heller ikke at fortrække, for det kan skræmme nye kunder væk, som ikke rigtigt har nogen kundeaktier i forvejen. Dette vil så gøre virksomheden sårbar over for, at en konkurrent kan melde sig på banen og tiltrække alle disse kunder. Så disse tanker bør man altså gøre sig, når markup'en og det løbende afkast skal udregnes (i forhold til produktions og udviklingsomkostningerne). Men det er selvfølgelig rart lige at huske, at i sidte ende så kommer virksomheden jo meget hurtigt til at være styret af en stor (og i mange tilfælde almen) gruppe mennesker, som dermed ikke vil have meget ud af at prøve at presse citronen over for nye kunder, da disse jo ofte i høj grad vil være dem selv. Og i de få tilfælde, hvor der kan være en anseelig forskel på gamle og nye kunder, jamen så må man også forvente, at hvis en (\emph{stor}) gruppe mennesker vil presse en anden (\emph{stor}) gruppe mennesker som forbrugere, så er der jo stor chance for, at den anden gruppe vil gøre gengæld. Hvis der altså vil ende med at være visse store grupperinger af forbrugere, så er det altså naturligt at forudse, at disse bare vil indgå aftaler med hinanden i stedet for at prøve at presse citronen og skrabe til sig.. Ja, og al denne snak er jo stort set ligegyldig, for det vil jo være meget sjældent, at der vil være stor forskel på gamle og nye forbrugere af en virksomhed, og hvis der er, jamen så vil det jo kun lige være midlertidigt, må man regne med. Ja, så never mind al denne snak i bund og grund. (11:00) 
Så ja, afkaststørrelsen pr aktiestørrelse, eller rettere aktiestørrelse der udløber, hvis nu kurverne ikke er lineære, skal altså bare bestemmes rimeligt frit af aktionærerne via deres stemmemagt, og det samme gælder alle priserne. "proportionalitetsfaktoren," som jeg har snakket om, hvad der også svarer ret meget til "markup'en," den er altså nu bare en implicit størrelse, som folk selv kan regne ud hver især (idet alle regnskaber skal være offentlige (samt i øvrigt også alt muligt andet i virksomheden, f.eks. også hele beslutningsprocessen, når det kommer til den overordnede ledelse af virksomheden)). Så nu skal kunderne altså bare foholde sig til en enkelt pris, og så kan de selv regne ud, hvad markup'en er på denne. I denne version af idéen tænker jeg så også bare, at aktierne udstedes i slutningen af enhver lille salgsperiode, således at den altså er propertionel med prisen divideres med det samlede salg i den pågældende periode. Det vil sige, at kunderne ikke ved eksakt hvor stor en aktie de får ved købet ned til hvert decimal, men de kan stadig regne det ud tilnærmelsesvist eksakt i de fleste tilfælde, for man må jo formode, at salget vil være rimeligt konstant. Og hvis det lige tager et hop op på et tidspunkt, så vil det jo ikke gøre det vildt store. Det virker altså ikke som om, at det vil være værd at indføre et buffersystem eller tilsvarende, bare for den mikro lille generelle usikkerhed omkring aktiestørrelsen, man får med i købet, slet ikke.. (11:09)
Så det er altså den store nye ændring. Jeg har i øvrigt så også lige nævnt, at virksomhedens regnskaber og ledelses-beslutningsprocess gerne skal være offentlige (for det vil helt klart være det værd fra kundernes synspunkt, frem for den lille makedsfordel det vil kunne være, at holde visse ting hemmelige). Så det vil jeg også fremføre som en del af idéen. Og så har jeg altså nogle nye tanker om, hvordan kundeaktionærerne skal kunne sælge og ikke sælge deres aktier.
Jeg har som sagt fundet på, at kunder jo gerne må kunne sælge deres kundeaktier til andre kunder, i hvert fald så længe en specifik kunde bare ikke kan købe mere en en vis aktiestørrelse i forhold til dennes originale kundeaktie(størrelse), altså den mængde aktier, vedkomne har tilegnet sig via eget forbrug. Hm, jeg skal egentligt lige tænke over, hvad man gør, hvis.. Hm.. (11:14)
(11:24) Hm, der er ingen grund til at give så mange restriktioner. Man kan bare sige, at man altså kun må sælge sine kundeaktier til andre kundeaktionærer, og at enhver kunde ikke må købe en aktie(størrelse), således at deres samlede aktie bliver større end, vi kunne jo sige det dobbelte, af deres nuværende del af deres aktie, som de har tilegnet sig via forbrug. Fordi nykøbte aktier dog godt kan aftage langsommere i størrelsen (f.eks. hvis man bruger en lineær forskrift), så kan man godt komme ud for, at forholdet overstiger det dobbelte i den efterfølgende fremtid, men det er også fint nok; man kan bare sige, at det ikke må overskride det dobbelte i selve handelstidspunktet. Denne forordning bør være tilstrækkelig til at sikre, at folk kan finde købere, hvis de nu gerne vil sælge deres kundeaktier (måske til en anesle lavere end, hvad de er værd), men systemet forhindrer stadigvæk tredjeparter i at komme udefra og opkøbe en majoritet i virksomheden, så denne ryger væk fra kundernes hænder. (Det gør også, at afdøde kunder, kan få "solgt" deres aktier videre (og dette bør man altså sikre sig, at de kan, selvfølgelig ved at deres arvtagere får lov at styre handlen). 
Jeg har vist i øvrigt også en lille note om fissioner, lad mig lige se, og var der så ellers andet, jeg lige skulle nævne?.. (11:34) ..Hm, nogle ændringer i mine planer, men var der ellers andet?.. ..Nej, det var der vist ikke, og ellers kommer jeg i tanke om dem. ..Ah jo, lad mig lige berøre amnet omkring, hvem der er kunder kort.. Virksomheden skal stadig have en beskrivelse og, hvad der er dets "servicer og produkter," og hvad der ikke er, og dette skal det kunne ændre i løbende, men gerne hvor der så er en vetoret, således at enhver stor nok mængde aktionærer (også alt efter hvilken type) kan vetoe enhver ændring. ..Ja. Mere er der sådan set ikke at sige om dette..
Min tanke angående fission var bare, (hvilket jeg også har tænkt på før) at man jo også potentielt set kunne forstille sig virksomhedssplittelser, hvor virksomheden bare dels i flere afdelinger, som administrerer (og har magt over) forskellige ting. Og så kan det så altså stadig være sådan, at kunder, der er mere kunde det ene sted, så vil få mere aktie og stemmemagt hos denne afdeling. Vi kunne altså forestille os en form for "blød fission," hvor den "hårde fission" så altså vil være, at virksomheden skilte sig totalt ad i to (eller flere). Dette er helt klart værd at have med i tankerne, når man skal overveje fissioner. Hm, i øvrigt tror jeg da egentligt sagtens, man bare kan planlægge fissionsreglerne efter at virksomheden er i sving; man behøver vel ikke nødvendigvis at planlægge disse ting fra starten af? Nej, det må man jo ikke behøve, for hvis kunderne har træng og lyst til, at sådanne fissionsregler skal være en realitet, så kan de jo også indføre dem.. Hm tjo, men det kunne dog måske være smart nok, hvis man som startaktionær lovede sine kunder, måske med en kontrakt indblandet, hvis det giver mening.. at man rimeligt hurtigt vil forsøge at udforme gode fissionsregler for virksomheden.. Hm.. ..Tja, det allerførste kd.v.'er på markedet kan jo nok sagtens bare give dette som et løfte (hvis de vil), og når så teknologien bliver mere udviklet, så kan eventuelle nye kd.v.'er jo bare adoptere andres fissionsregler. Og det vil betale sig for en virksomhed (der har udviklingspotentiale) på et tidspunkt at lave sådanne regler, da dette sandsynligvis vil behage den brede kundebase, og dermed vil det sænke risikoen for, at en konkurrent med gode fissionsregler melder sig på banen (hvis vi altså tænker på en virksomhed, hvor startaktionærerne stadig har magt og så overvejer, om det kan betale sig at indføre fissionsregler på et halvtidligt tidspunkt).. Ok. Lad mig bare lade det være det for nu. Og i det dokument, jeg har tænkt mig at skrive nu her i de kommende dage, regner jeg så bare med, at jeg nævner "virksomhedsfission" kun ultrakort, måske bare i en liste over ting, jeg kan uddybe på et senere tidspunkt. Ok. Nu vil jeg så lige opdatere mine "planer" nedenfor.. (11:58)
...(12:13) Nå ja, jeg vil også lige nævne, at der jo er stor forretnings- og investeringsmulighed (for start-aktionærerne), for den positive kunde-feedback, nemlig i form af at kunderne i højere grad vil fravælgekonkurrenter, jo mere de selv er investerede, gør at kd.v.'er vill udkonkurrere ikke-kd.v.'er i samme branche, hvorved der jo vil være et stort vækst- og fortjeneste-potentiale. ..Og denne indsigt, hvis folk kan forstå den, gør også, at idéen vil sælge lidt sig selv, når først bare nogen vil have forstået den (tror jeg, 7, 9, 13). (12:17)

(25.10.22, 16:17) Okay, jeg har nogle nyheder. Nej jeg tror ikke længere, at idéen vil sælge sig selv helt så meget, som jeg ellers sluttede den sidste paragraf af med at sige. Min forretningsbevægelse vil nok i høj grad skulles gennemføres på baggrund af en stor politisk vilje i folk til at opnå de fremtidsudsigter, den handler om at opnå. ..Hm, jeg kan forresten også lige nævne, at jeg nu mener, at det nok kunne være en god idé at gøre, så at kunder i starten er frie til at handle med hvem som helst, og at det så bare er efter et vist tidspunkt, at nye kundeaktier kun må sælges til andre kunder. For det kræver jo, at mængden af kunder (for heri tæller vi jo ikke start-aktionærerne) er stor nok, før at det kan fungere, at de kun må sælge til andre kunder. Jeg mener så stadig, at man kunne gøre det sådan, at ingen kunde må få mere end dobbelt ved tidspunktet af sælget, end hvad denne ville have haft, hvis denne aldrig havde solgt eller købt kundeaktier. Og det skal så i øvrigt siges, at disse restriktioner måske er ret skrappe ift., hvad der måske er nødvendigt, men ja, om ikke andet så tror jeg altså, at de er gode nok, hvis ikke man kan finde på et mildere system, der også stadig klarer ærterne. Nå, det var lidt et sidespring: Det var egentligt ikke med i de 'nyheder,' jeg tænkte på.. ..Ja, for at fortsætte de egentlige nyheder, jeg tænkte på, så tror jeg altså, at forretningsidéen nu skal sælges meget mere på baggrund af den 'gode bevægelse' ligesom, mere end at 'de første kunder også bliver belønnede'.. Og nu tænker jeg så at skrive mine "bright future"-noter om, så at afsnittet om denne idé så mere bare kommer til at sige: Jamen hvis nu det går endnu mere ned ad bakke, jamen så må man jo på et tidspunkt nå et punkt, hvor der bliver grobund for sådan en bevægelse, hvis altså den ikke bare finder god nok grobund med det samme. Så afsnittet bliver så ikke så meget: "Nu skal I se denne sikre plan, som vi kan gå i gang med med det samme," men mere: "Okay, selvom kapitalismen for nogen kan se lidt sort ud pt., så kan det altså ikke gå helt galt; det skal nok gå den rigtige vej overordnet set." Og jeg kan se på, om jeg så vil prøve at vinkle afsnittet mere som et "fix af" kapitalismen eller som et "forsvar for" kapitalismen, det kan godt være, at jeg så vælger det sidste i stedet.. ..Nå, men på en god eftermiddagsgåtur her kom jeg så også på en masse nye ting. Jeg skal faktisk udbygge mit e-demokrati-afsnit en del: Det er ikke nok bare at lægge op til det majoritetsenevælde, som den gør; der kan være meget mere komplicerede og spændende forhold, som gør, at folk gerne vil kunne oprette grupper, hvor de så kan handle med de andre grupper med deres stemmer til diverse ting ("hvis vi stemmer for det og det, så stemmer I for det og det"), og også i øvrigt handle omkring, om gruppen overhovedet vil bakke op om den samlede enhed (hvilket f.eks. kunne være et parti eller et firma), hvis ikke de får sådan og sådan. Så det skal jeg altså også lige skrive om, inkl. at skrive om, at det digitale system skal indrettes, så brugere altså kan oprette disse grupper og sådan. Og noget helt andet er så, at.. Ja, eller der er faktisk tre ting mere, og måske skulle jeg starte med denne i stedet: Jeg har tænkt på, at.. (16:42) ..Hm nå, der gik jeg lige lidt død, så lad mig i stedet starte med: At jeg fik tænkt over, hvad der egentligt lidt er en gammel version af en anden idé (nemlig min "donationskæde-idé"), nemlig at man også kunne igangsætte en bevægelse, hvor folk simpelthen lover dusører for, hvis virksomheder eller organisationer m.m. opnår et eller andet specifikt i fremtiden. ..Nå, nu gik jeg også lidt død i det igen.. Hm, og den tredje ting var så omkring.. Hm, den første/anden ting var bare noget omkring at give de nuværende kunder mere magt, hvilket jeg faktisk tror kan være ret vigtigt især for en Web 2.0--3.0-virksomhed. Hm, men mere er der vel ikke nødvendigvis at sige/nævne om den ting her, så det var én ud af tre.. Og den tredje ting var så.. ..Nå jo, det var at jeg også nu har tænkt mig at hive "forbrugerforeninger" m.m. (også samt hvad jeg lidt har kaldt "civilforeninger" på et tidspunkt) mere i forfronten nu. Jeg kan så skrive om dette, efter jeg har skrevet om e-demokrati og det. Og så kan jeg altså kort gøre rede for, hvad man kan opnå med sådanne "forbruger"-/"civil"-foreninger, eller hvad vi skal kalde dem, og så også at man jo kan gøre brug af et e-demokrati her. I øvrigt kan jeg lige gentage/præcisere, at min idé om "civilforeninger" i høj grad bare handlede om, at man går sammen i grupper til at representere sig som borger og/eller forbruger i et samfund (gerne altså sammen med nogen i en lignende båd), og hvor man så begynder at samarbejde og opføre sig meget som en slags forrestning, både idet man så begynder at lave handler samlet og betale nogen for at stå for at indgå (handels- etc.)aftaler med andre grupper/instanser, og også idet man så begynder at betale folk der kommer med nye smarte idéer til, hvad man kan gøre som gruppe. Hm, det lyder godt nok løst, når jeg skriver det her (og det er lidt blandet sammen med "forbrugerforeninger"), men på den anden side er jeg også ret træt nu, kan jeg mærke, og ja, det er lang tid siden, jeg har tænkt så meget over det, så det skal nok give god mening, når lige jeg får støvet det af.. ..Men ja, så nummer tre ting er altså bare, at jeg også vil skrive om disse forbrugerforeninger, og altså sikkert også "m.m.".. Hm, og lad mig så vende tilbage og skrive om donations-/dusør-idéen på at senere tidspunkt, når jeg er mere frisk i hovedet igen.. (17:06, 25.10.22)
%(26.10.22, 10:58) Okay, jeg har tænkt lidt mere over min nye version af denne dusør-idé, og jo, det er lidt kød på den, men ikke nok til, at jeg vil skrive om det, og måske ingen gang nok til, at jeg vil bruge tid nu på at forklare den. ..Nej, lad mig bare forklare den på et senere tidspunkt ved lejlighed, for idéen er altså ikke så vigtig (den handler bare om, at investorer, der øjner nogle gode fremtidsmuligheder i en idé, men ikke synes investeringsmulighederne/afkastudsigterne er helt gode nok, de kan så bede folk om at søtte idéen ved at udlove dusører, betinget af at idéen går godt og at disse folk vil drage nytte af den; så kan de så love at give lidt retur for dette. (Så ja, ikke den vildt store idé, når man tænker over det..)).. Hm, nu fik jeg faktisk næsten forklaret den her i den parentes.. Og ellers tilføjer idéen bare lidt om, hvordan disse dusørløfter kunne gøres.. men det gider jeg ikke skrive om nu.. Ok. Jeg har så også tænkt mere over nogle af de andre ting, jeg vil skrive om, og nu tror jeg planen er, at skrive en lidt mere simpel version af min business movement-idé, at skrive e-demokrati-sektionen færdig (med de nye tilføjelser), og så også bare lige inkludere et afsnit om forbruger-/civilforeninger, inden jeg når til Web 2.0--3.0-sektionerne (og så videre derfra). Og jeg kan så forklare, at denne idé altså i bund og grund handler om, at gå sammen i store grupper og ansætte agenter til at finde gode tilbud til gruppen og til at finde frem til mulige gavnlige aftaler for gruppen i det hele taget.. (11:08) Ok.. Mere er der vist ikke at sige her. Nu tror jeg så derfor, jeg vil fortsætte skriveriet, hvor jeg så nok lige begynder på en version 2 af "bright future"-dokumentet i øvrigt. (11:09)
(12:34) Okay, nu har jeg godt nok lige tænkt lidt mere, og.. Ja, det korte af det lange er bare, at jeg faktisk virkeligt tror, det kunne blive en stor bevægelse. Nærmere bestemt så skal jeg vist bare lige fokusere på, at det især er detail- (retail på engelsk) forretninger, man nok bør starte med virkeligt at fokusere bevægelsen på. Alle kd.v.'er kan så have et bestemt mærke, som folk kan gå efter. Fordelen er her, at denne branche indeholder mange små virksomheder, og generelt kræver det ikke så meget kapital at starte en butik m.m., og hvis vi tænker butiks\emph{kæder}, så er der også alligevel så mange, så man kunne godt forestille sig at mindst én vil konvertere. Nå ja, og webshops / web stores (det første er muligvis et begreb vi mest bruger Danmark, men det ved jeg ikke helt) er selvfølgig også en rigtigt vigtig mulighed at satse på også. Så herved kan bevægelsen altså starte og begynde at få mere og mere opmærksomhed og kapital til rådighed. Og når man så har fået samlet kapital nok, så kan man så også begynde at brede sig ud til andre brancher, hvis altså ikke der ellers er nogen virksomheder, der har konverteret endnu her. :) Okay, så min idé kan altså muligvis stadig rykke en hel del, selv her i nutiden; det er nok alligevel ikke bare en idé, der ligesom kan være et sikkerhedsnet i fremtiden, den er mere end det.. :) (12:44) 




## Energi, ressourcer, klima

(18.08.22, 13:30) Ja, vi bør helt klart fremelske en slags tang- og/eller vandplante, der kan fungere som en slags hvede (eller lignende), men på havoverfladen. Og hvis vi så kunne opdyrke f.eks. dele af stillehavet, så kunne man jo i teorien få en KÆMPE ny ressource, som kan bruges til alt muligt (energi, føde, og potentielt set også til at grave ned for at indkapsle CO_2, hvis man altså \emph{[virkeligt} får overskud..). Man kunne jo så evt. gøde planterne bare ved at suge næringsholdningt vand op fra dybet. Kunne have potentiale til at blive kæmpe stort, hvis det kan fungere.. 

(08.09.22, 9:57) Kom lige til at tænke på: Gad vide, hvad der ville ske, hvis man bare konstruerede en masse rev i havet, måske bare ved at lægge et netværk a flydende slanger/rør ud eller lignende. Tanken er lidt, at der måske så ville dannes en hel masse tang/alger og måske andet fiskeliv. Og en follow-up-tanke er jo så lige, om man eventuelt så skulle pumpe næringsholdigt vand op og igennem slangerne/rørene.. Jeg tror, jeg vil give dette emne nogle flere tanker i min fritid, og så vende tilbage hertil, hvis jeg skulle få yderligere idéer omkring det, der er værd at nævne. ..Men ja, den helt simple version er disse tanker er bare: Gad vide, om man kunne skabe en masse plantevækst, og vækst i betanden af diverse havdyr, hvis man bare lagde flydende rev ud. Jeg ved jo godt, at alt bliver vildt dyrt at udføre, hvis det skal skaleres meget op, før det får en effekt, men idéen er da nogle flere tanker værd.. (10:05)

(26.12.22, 11:06) Idéen med at køle planeten med "aske" eller lignende er selvfølgelig fundet på, og det er faktisk en rigtig populær idé endda. Den går under navnet "stratospheric aerosol injection," og den virker nemlig faktisk rigtigt lovende (rent teoretisk altså)..:)



## Andre ting, der relaterer sig lidt web- og forretnings-idéer m.m., men som er lidt uden for kategori

(18:28, 27.09.22) Jeg kom til at tænke lidt over, i går eller i forgårs, retsystemer i fremtiden. Jeg vil så bare lige nævne, at jeg jo allerede har skrevet om (i mine 21--22-noter), hvordan man kan få klare fælles etiske retningslinjer i fremtiden i diverse samfund. Og hertil kunne man jo så lige tilføje, at man så også kunne forestille sig et retssystem, som baserer jeg lidt mindre på lovskrifter og deres fortolkninger, men lidt mere på en.. ontologi/model/.. mængde.. af retningslinjer for, hvordan man skal dømme diverse forseelser og andre sager/konflikter osv. Og så kunne man jo naturligvis have et rangsystem af en slags dommere, men hvor hele befolkningen i det samlede samfund ligesom sidder i toppen (i praksis) og som fællesskab så har ret til.. måske hvor forskellige mennesker får ret til forskelligt data.. ret til at udtage stikprøver af under-dommerinstansernes bedømmelser, og hvor man derfor kan rette op på.. Ja og/eller man kan selvfølgelig også bare have et anke-system, som jeg også tænkte på, hvor parterne --- og måske også vedrørende til disse (eller måske bare folk, der har fulgt med i retsagen) --- jo så kan få lov at anke til en højere instans.. Så ja, men den primære pointe er altså, at man kunne forestille sig et alternativ til et lovsystem baseret på en masse paragraffer, hvor man måske i stedet bare havde en stor mængde (hierarkisk ordnede) retningslinjer og eksempler at gå ud fra.. (18:40) 




## Evolutionsspykologi

(18.08.22, 13:35) Jeg tror ikke, jeg har understreget dette i mine 2021-22-noter, men ét punkt, hvor jeg virkeligt tror man kan komme langt med evo.-spyk., er til at analysere og prøve at forstå, hvad der bringer os lykke som mennesker. Jeg tror virkeligt man kan skubbe meget til den analyse, hvis man tager evo.-psyk. godt i betragtning.


## Lykke

(18.08.22, 13:38) Angående lykke, så har jeg også lige nogle flere ting, jeg vil skrive om det emne, men det kan være, at jeg lige udsætter det en gang.. 

(31.08.22, 20:02) Okay, lad mig lige prøve at forklare de her tanker lidt. Jeg havde/har for det første lyst til lige at kommentere noget omkring, hvorfor man bl.a. har så mange glæder som barn (hvis man altså er heldig nok). Ja, der er vel mange mange grunde, men jeg har bare lige lyst til at fremhæve, at man (måske især som drengebarn) kan have en virkeligt stor lyst til eventyr --- i hvert fald til tanken om dem, men man kan alligevel godt på en måde få udlevet sin eventyr lyst (også selvom man ikke rent faktisk tager på et eventyr) via lege og via bøger, tegneserier og film m.m. Jeg kan personligt huske at tanken om et eventyr (og nu tænker jeg faktisk selv meget på One Piece som et eksempel på et "eventyr") bare var "helt oppe og ringe" dengang. Samtidigt havde man også en anden ting som barn, som virkeligt gjorde mange glæder mere tilgængelige: Man blev så nemt awestruck af ting. Jeg kan huske at en af de helt store øjeblikke i mit liv, var da vi fik vores første pokemon (blå) gameboy-spil. Vi var bare Helt oppe at køre, og det var bare sådan en lykke; det var sådan en fed følelse; så spændende. Nå, hvorfor har jeg så lyst til at nævne disse ting (for de er jo rimeligt velkendte)?. Jo, min pointe er så, at vi jo altså lidt kan tabe nogen af disse lykke-givende faktorer --- eller "parametre" kunne man kalde det --- i vores sind, når vi bliver ældre. Eller de dæmpes i hvert fald lidt. Men man kan dog have disse ting for øje, når man skal overveje, hvordan man konstruerer et lykkeligt liv sammen i et fælleskab. Særligt den der awe: man kan gøre mange til for at give ting mere mening.. Ah, jeg kunne også nævne det her med, at gamle ting, man er så nostalgisk omkring (hvis man altså har sådan nogle ting), der er ofte en god grund til, at man synes de var/er så store, og faktisk en ting som ikke nødvendigvis har rent med alder at gøre: Når mange mennesker går og er hypet omkring det samme, f.eks. når et nyt spil (eller en ny bog, eller hvad har vi) udkommer, jamen så ligger der bare så meget mere.. underbevist prestige omkring at klare sig godt i det, og selv for middelmådige spillere vil der stadig bare være.. en følelse af at ting "vejer" meget mere (end hvis man går i gang med samme spil mange år efter, eller nu hvor der er sådan et kæmpe udvalg så ingen spiller det samme (medmindre det er et rigtigt populært og nyt spil)). Det er f.eks. også bl.a. derfor Pokemon GO var så stort, fordi så mange interesserede sig for det, og fordi man derfor blev grebet meget af, at avancere i spillet. Og igen, hvorfor er det her så interessant og nævne i forbindelse med emnet om "lykkelige fælleskaber?" jo, fordi man jo så kan prøve at begrænse adgangen til hobbyer og/eller gøre tiltag for at skabe nogle store "diller" (som det hedder; det er bare ikke vildt tit man bruger det ord mere) løbende, som folk i høj grad kan blive grebet af. Og ja, jeg synes det er interessant, for det er jo en helt anden dimension end bare at sørge for, at man har nogle gode muligheder og nogle gode traditioner (og kreative mennesker til at finde på events): Der er også hele den dimension omkring, at folk også tit skal \emph{gribes} af en lyst til at deltage i en ny dille(/sport / kunstnerrisk/udfoldningsmæssig/literær strømning osv.). Så man skal altså ikke nødvendigvis se på bare at lave gode events som folk kan deltage i; man må også gerne overveje, hvordan man får folk grebet af ting i fælleskab. 
Nå, så det var så ligesom tankerne omkring awe og generel begejstring (som også relaterer sig til, hvad man går og har nostalgifølelse omkring). ..Nå ja, og jeg kan forresten også lige hurtigt nævne, at disse betragtninger altsammen er noget, der kan forklares med evolutionspsykologi (hvis man lige udvikler den gren, så den handler meget mere om at se på, hvordan evolutionsprincippet spiller aktivt og dynamisk ind i vores nutid *(og i vores helt nære fortid osv.), og hvordan vi i høj grad stadig fungerer som skabninger af evolutionen, hvad vi er, *(således at vores følelser og handlinger stadig i høj (men dog ikke fuldstændig) grad kan.. "forklares," eller man kan i hvert fald gøre sig meget mere vis på emnet, ved at se det i lyset af, at vi er formet af evolutionen) i stedet for (*himler og tager hånden op til panden* (for effekt)) at fokusere på, hvordan vi er "fortidsmennesker" med basale instinkter, der ikke længere passer til den morderne verden (*himler*)). Nå, det var et lille sidespring..
Og lad mig så lige prøve at vende tilbage til det med eventyrlyst (hvis der altså er noget her jeg mangler at sige..?).. ..Nej, her er der faktisk ikke så meget at sige, andet end at det er lidt ærgerligt, at vi mister nogle af disse barnlige trænge/lyster.. hvad jeg en gang ofte ville kalde "behov" (og måske også "værdier" nogen gange, det kan jeg ikke lige huske (og gider ikke lige prøve)) (hvilket handlede om, at vi jo fra naturens side har behov, og mange af disse behov er vi så så heldige, at naturen har udviklet en "gulerod" til, således at vi føler lykke, når det lykkes os at opfylde behovet.). Det kan være at vi i fremtiden kan opfinde en terapi eller andet, så man kan forstærke disse "behov," i.e. disse lyster, men indtil da må vi jo bare nyde det imens vi er børn, og så ellers prøve i nogen grad ikke at slippe de "behov"/lyster, når vi bliver ældre. Så ja, det var vist rimeligt meget de tanker (plus lidt sidespring), jeg havde lyst til at nævne/notere. :) (20:50)

(05.09.22, 19:55) Mon ikke jeg har været lidt inde på dette, men lad mig lige nævne, at et rigtigt godt råd, og en rigtig god ting at bestræbe sig på, er: Vær gavmild med komplimenter! Giv dem ofte! Igen: Vi har jo i bund og grund mest bare den glæde, vi får fra andre. Og det giver bare SÅ meget mere lykke, hvis man hele tiden sørger for at sætte udtrykkeligt pris på sine nærmeste --- og komplimenter til alle de ikke-nærmeste er også en god idé at bestræbe sig på at være gavmild med: det bringer alt sammen lykke. Selvfølgelig er det mere og mere vigtigt, jo nærmere folk kommer på en, at give komplimenter (og ros og andre tegn på værdsættelse), men ja, så længe rosen/komplimenterne kan gives oprigtigt, så er der ingen grund til at holde igen med dem. Bare en lille ting, jeg lige ville nævne (måske igen), og som virkeligt kan være værd at leve efter --- det må jeg også selv gøre meget for at huske på. 




## Eksistens

(06.09.22, 10:31) Jeg har i øvrigt også tænkt nogle små tanker omkring, hvis man antager at der er en overordnet skaber-gud. Hvis man bare antager, at han ikke er \emph{uendeligt} potents, men måske bare potent langt ud over, hvad vi kan forestille os, og hvis man også antager at han faktisk er god --- ikke bare sådan god som i: "Åh, hvor er du god, gud! ..Vær nådig ikke at sende mig i helvede og brænde..!" eller som i: "Definition af god er hvad gud er, for gud er den største og bedste,"  men som i at han rent faktisk ønsker så mange sjæle som muligt at opleve så meget lykke (og så lidt ulykke, selvfølgelig, der trækker fra af den samlede lykke) som muligt --- jamen så ville det jo egentligt give god mening, at han ville vælge at bruge sine skaberkræfter på at skabe en afsindig stor mængde af universer, som er nemme at opstille lovene for, og som ikke kræver nogen kræfter at styre, således at han ikke hele tiden skal gå tilbage og passe sine gamle kreationer men hele tiden bare kan fokusere sine kræfter på at skabe flere. Jeg synes dette er ret oplagt at forestille sig, at det ville kræve mere energi, hvis man hele tiden skulle overvåge og indgibe i alle de universer, man har lavet --- især hvis man sætter sig selv den umilge opgave for at sørge for at "intet ondt sker imod gode mennesker"---ja, for så skal han jo endda gå ind og forudsige, hvad der sker, og så skal han jo basalt set "køre simulationen flere gange alligvel, indtil han får det resultat, han ønsker, og det må jo tage mange mange kræfter og meget fokus, relativt til bare at fokusere på at skabe det næste uhyrligt store batch af universer, som skal sættes i gang. ..Hm, der er egentligt også andre antagelser, man kunne tænke over, selvfølgelig er der det, men lad os bare begrænse os til dette her.. Og ja, det skal så understreges, at denne analyse ikke handler om, at konkludere på, hvad der må gælde for sådan en gud. I stedet handler det bare om at forklare, hvorfor det ikke er langt ude i hampen, hvis man gør disse antagelser, at nå til en teori/hypotese, hvor guds ikke-indgriben og tings tilsyneladende tilfældighed faktisk kan forklares ret godt, på trods af guds godhed (for en af antagelserne er jo faktisk, at han \emph{rent faktisk} er \emph{god}, altså i en forstand der passer meget bedre til, hvad vi almindeligvis vil betegne (på trods af at der kan være mindre variationer af, hvad folk ser som 'godhed') som 'godt,' når vi snakker om \emph{menneskers} handlinger). (10:52) ..(10:56) Hm, jeg kom lige i tanke om, at gud pr. den kristne (og jødiske --- og sikkert også den muslimske) tro jo skabte verden på seks/syv dage, så en antagelse om at "uendeligt potent" bare skal ses lidt metaforisk må jo egentligt være ret oplagt.. 

(05.10.22, 15:10) Jeg havde tænkt mig at forberede en lille teaser udgivelse, som jeg vill lægge ud på GitHub her som noget af det første, men nu har jeg lidt fortrudt. Her er mine tanker (noter) nu her fra et andet dokument, jeg skrev i:
\# Existence theory

%Let me begin this introduction/teaser on a small personal note.
%
%When I was younger, in my teens, I was quite interested in 

"How and why was the universe created?" "What constitutes consciousness?" and "how does matter, in particular brains, gain consciousness?" 
These are questions that many people have asked themselves, probably often with the same open-ended conclusion: These questions are perhaps just to big for us "mortals" to answer. 

Indeed this seems to be the case: Even if we found a good answer, how would we ever know whether it is actually correct or not? And furthermore, we might not even be able to understand the correct answer if a god/oracle could tell it to us; it might be too complicated, and it even might include some otherworldly logic that we can never comprehend.

So if we look at the %...(10:44, 05.10.22) Jeg søgte lige lidt på filosofi (har bare læst wiki-artikler), og jeg har lige set, at hypotesen/antagelsen om at "alle mulige verdener eksisterer" også hedder "modal realism." Spændende. Jeg har vist hidtil kun læst om "mathematical universe theory." Der står på wiki-siden, at nogle modstandere mener, at hypotesen er i konflikt med Occam's razor, hvilket jo er rigtigt interessant, for det er den nemlig ikke; det kræver bare lige lidt omtanke og analyse for at komme frem til det resultat. (Og altså også en antagelse om at verdener/universer har en naturlig, fundamental ordning i multiverset, nemlig ud fra, hvilken information de indeholder/bygger på.) ..Lad mig lige læse videre om det, og også omkring de andre emner, der relaterer sig til spørgsmålene ovenfor.. (10:51) ...(11:13) Hm, det virker alligevel til at associationerne omkring "modal realism," inklusiv hovedproponentens egne holdninger, alligevel er for forskellige fra, hvad jeg tænker på. Og nu læser jeg lidt om MUH, og det virker helt klart til, at det passer mine tanker. Jeg skal lige finde ud af, og der er forskel på CUH og så det, der også er nævnt i wiki-artiklen, nemlig MUH eksklusivt med kontruerbare universer, det må jeg lige finde ud af. Men ja, hovedparten af mine tanker lægger sig altså rigtig meget op ad MUH/CUH, og så er det altså muligvis bare mine tanker omkring bevisthed (som muligvis lægger sig op ad "idealism" og/eller "Platonism," *(nej Platon var vist "realist," ser det ud til, som så er det modsatte..) men det skal jeg lige have læst op på igen), og så måske også bare min tilgang med ikke at lede efter \emph{den} rigtige eksistensteori, men i stedet bare forgrene analysen, hver gang man støder på et spørgsmål, der med fornuft både kan antages, i nogen grad, at være sandt eller at være falsk. (11:21) ..Wow! "Virtually all  historically successful theories of physics violate the CUH"!! Helt ærligt. Så svært kan det altså heller ikke være at forstå Gödels ufuldstændighedsprincip..! Det ser ud til, at den originale (hvilket jeg på en måde også kan siges at være, mener jeg, men der var altså andre, der kom først..) opfinder/opdager af teorien tror (som i øvrigt ser ud til selv at foreslå CUH som modsvar på en vis kritik (om så end det ham, der fandt på det først, det fangede jeg ikke lige)), at f.eks. mængdelære og andre matematiske teorier, hvor ufuldstændighedsprincippet gælder, at det ikke har fuldstændige modeller.. Suk suk. Jeg ville ønske at Gödels fuldstandighedsprincip ikke var kommet så meget i skyggen af det andet princip; der virker til at være meget forvirring omkring det.. ..Hm, folk burde bare blive undervist mere i Gödels kontruerbare univers, og i hvad det betyder: at al matematik kan deles op i to mængder: matematik over objekter kontrueret af en endelig mængde information, og matematik over (filosofisk questionable) objekter dannet af uendeligt meget information.. (11:34)
%...(12:11) Hm, jeg tror hellere lige, jeg må summe lidt over, om det overhovedet kan betale sig for mig at tease mine idéer på dette område nu; MUH er jo ret gammel --- den går faktisk mindst helt tilbage til 1998, kan jeg se (jeg troede den var lidt nyere, selvom den jo stadig er ret ny overordnet set).. Og hvis man tager MUH, eller rettere CUH, som afsæt, så vil det måske blive svært for mig, at få mine idéer teaset, så de lyder interessante (altså hvis man kender til CUH i forvejen).. Ja, lad mig summe lidt over det.. (12:15)
%... (14:56) Ja, jeg er bange for, at der ikke bliver nok kød på det til ligesom at tease det.. Det er for inviklet at forklare, hvad jeg tror, jeg kan bidrage med til emnet. ..Det kan nok ikke rigtigt gøres kortfattet særligt godt. Og ift. at jeg jo havde tænkt mig primært bare at fortælle den lille hurtige redegørelse for, hvorfor man med meget normale antagelser (udbredt blandt ateister og lignende især) hurtigt kommer frem til, at vi så i bund og grund lever uendeligt og i alle afskygninger, nemlig i og med at der så også vil findes alle (og man kan i princippet blive ved med at zoome ind) mellemtrin imellem to forskellige personligheder og tilhørende liv ("oplevelse," i.e.), så vi dermed i praksis alle er den samme, bare i forskellige udgaver. Og jo, mange af mellemtrinene er vildt usandsynlige, men selv "vildt usandsynlig" er forekommer stadig groteskt "ofte" sat op i mod "uendelighed." Og budskabet er jo så, at vi alle er den samme i praksis, og at alt vi gør mod andre mennesker, det bliver så gjort mod os selv i et fremtidigt liv, i praksis, altså.. Men ja, selvom dette resultat ikke behøver så meget teori i forvejen, så føler jeg stadig, at det er altså bart i sig selv til, at det giver mening at tease/forklare det. Hvis det kunne være en del af en teaser til "eksistensteori" generelt, så ville det give god mening, men jeg tror ikke, det vil blive modtaget med meget begejstring, hvis det bare står helt alene.. ..Også fordi, hvis man skal være lidt streng ved sig selv, så kan det jo i bund og grund reduceres bare til at sige: "hey, har I tænkt over, at multiversets uendelighed vil medføre, at alle afskygninger af "liv"/"oplevelser" vil førekomme?" Og det vil sgu nok ikke skabe særligt mange bølger i sig selv.. (15:07) ..Så ja, jeg venter med at udgive (og brygge videre på) mine eksistenstanker (som jeg dog stadig tror virkeligt kan noget, \emph{selvom} meget af det jo dog er tæt på noget kendt)..
slut. 
Så ja, som sagt, jeg føler altså, at jeg virkeligt har noget at byde på, men jeg tror ikke jeg kan finde på noget kortfattet, der kan skabe meget interesse i sig selv.. (15:13)
... (17:13) Åh, jeg kan også lige nævne, at min intension var efter "Indeed this seems to be the case ..." paragrafen (i readme-filen) så at lægge op til: Men hvad med at droppe målet om at finde \emph{det} korrekte svar, men i stedet bare prøve at overveje/analysere (gerne i fællesskab, btw), det samlede træ, ligesom, over de mulige svar der kan være til de grundlæggende spørgsmål. (Og her jeg jeg i øvrigt lige nævne, at der dog ikke bare vil være ét træ, for man kan godt stille spørgsmålene i forskellige rækkefølger, hvor analysen godt kan have karakter heraf. Særligt kan det vist være betydende hvilket spørgsmål (eller hvilke få spørgsmål), man starter med. Jeg mener dog stadig, at det ikke er sådan, at vi så skal analysere en hel skov af træer på en gang; jeg tror på, at det nok bare bliver en lille gruppe af træer, der vil være interessante for de fleste..) (17:19) 



(05.01.23, 11:33) Jeg har nogle tilføjelser til dette emne, og så har jeg også nogle idéer til, hvordan jeg nok vil strukturere en artikel om det. Lad mig se.. I virkeligheden har jeg nok skrevet meget af det før. ..Hm, jeg tror lidt, at jeg har en ny måde at tænke på det mulige fænomen med at Oplevelser bliver vagt til live, hvor jeg altså nu tænker meget, at man nærmest kan sige, at det er [...] At man nærmest kan sige, at det er "universet" --- og her snakker vi altså om det idealistiske univers: et univers der beskriver Oplevelser (hvilket også bare kan være hele multiverset selv) --- der oplever Oplevelserne. Og ja, mine yndlingsteorier har multiverset som selve dét (eneste) idealistiske univers, så jeg fortolker det altså nu meget som at "multiverset oplever Oplevelserne." (Og da en af mine klart yndlingsteorier nu er den hvor multiverset "udregner" al logik, og at Eksistens dermed ligesom er den fundamentale logik om alt, der så at sige udleder sig selv (eller rettere alle "sætninger" i logikken), så ser jeg det altså meget sådan at "multiverset udleder alle Oplevelser, og dermed også udlever/oplever dem"). [...] Nå, men dette var jo en lidt mindre ting. 

En større ting er så, at jeg er gået lidt væk fra at forestille mig den fundamentale logik som et sprog; altså som noget med en syntaks eller tilsvarende. Nu tænker jeg altså mere, at den fundamentale Logik ligesom er "rent semantisk." ..Vi kunne snakke om "Pure Reasoning".. "Pure ..." Hm.. ..Ja, "pure and fundamental logical reasoning." Og denne opfattelse betyder faktisk rigtigt meget, for det gør det nemlig pludselig meget nemmere at forestille sig, at der bare er én "fundamental logik for alt." ..Så ja, det er altså derfor, at jeg nu hælder rigtigt meget til, at multiverset ligesom bare er en fundamental og "ren" "logik," der forstår mere og mere "af sig selv," så at sige, og dermed også forstår, hvordan diverse forskellige sammenhængende Oplevelser må føles, og idet "den" forstår dette, så vil den også opleve disse Oplevelser, enten netop idet den forstår det, eller for alt tid igen og igen efter den ("den") har forstået det. (Så altså med andre ord: Enten sker udlevnigen af Oplevelserne på kanten af den forståelses-kulge/-mængde, der udvider sig mere og mere, eller også sker udlevningen konstant indenfor kuglen/mængden (hvor der så bare kommer flere og flere Oplevelser til denne mængde).) 

...Nå, og nu kunne jeg så fortsætte med at sammenligne "den fundamentale logik" i denne teori med, hvad man næsten kunne kalde en gud (og den sammenligning er hurtig at lave), men nu vil jeg i stedet prøve at følge den struktur, jeg har i tankerne for en artikel om det. Jeg forestiller mig nemlig at starte med at liste nogle gode kandidater til Eksistens-teorier, hvoraf den ene så skal være den, jeg lige har beskrevet (som egentligt er to teorier, alt efter om Oplevelserne udleves på kanten eller inden i kuglen/mængden af Oplevelser).

Hm, så lad mig prøve at skrive, hvad den første sektion i den artikel kunne indeholde.. (12:56)

Nå ja, jeg skal jo starte med at redegøre for hypotesen om, at "alt hvad der kan eksistere, eksisterer." Med andre ord er hypotesen, at der er en komplet symmetri ift. hvad der kan eksistere, og hvad der rent faktisk eksisterer af denne mængde; hvis ét univers eller ét delmultivers indeholder nogle specifikke "valg," jamen så må der bare eksistere modsvarende universer/delmultiverser i lige mængde. Men denne tanke leder så til at spørge: Hvordan defineres den underliggende teori for det samlede multivers så, for for at man kan afgøre, at multiverset er symmetrisk eller ej, så må man jo have et udgangspunkt for at definere, hvad der er symmetrisk og ikke symmetrik --- hvad vil det sige, at et univers er "modsvarende" til et andet univers for eksempel? Ja, og svaret på det er... Ej, det var bare for sjov; det kan vi selvfølgelig ikke svare på. Men vi kan hypotisere, at der kunne finde en fundamental "teori" for multiverset, selvom "teori" dog i så fald vil være et dårligt ord at bruge for det, fordi det indebærer, at der findes andre teorier. Så lad os hellere kalde det "en fundamental logik for alt," og her skal "logik" altså ikke forstås som en formel logik, men i en meget mere løs forstand, nemlig som det fundamentale koncept om, at visse ting kan være sande og visse ting kan være falske, og ting kan følge logisk af andre ting. Så lad os hypotisere, at der eksisterer en fundamental logik under den samlede eksistens, for hvad er alternativet? At der ikke er en samlet eksistens? At der ikke er nogen "logik" (i ordets meget løse forstand) bag? Nej, det går ikke rigtigt, så det virker som et fornuftigt aksiom. Og lad os forresten også bare benævne den samlede eksistens for 'multiverset,' da det er lettere at sige. (Og så må man bare endeligt ikke antage, at 'multiverset' består af en mængde af universer i den forstand som 'universer' ofte betegner, nemlig en samling love, noget rum og noget tid. Lad os endeligt ikke antage, at det er den eneste form for selvstændige eksistenser, der findes.) ..Hm, faktisk tror jeg, jeg vil bruge et nyt begreb om.. Hm, eller..?.. ..Nå, det vender jeg tilbage til. Men, hvad dælen beskriver denne fundamentale logik så? Ja, det er så her det store spørgsmål virkeligt ligger. Hvad beskriver den fundamentale logik? Den kunne f.eks. beskrive objekter, så som strygejern, computere, jordkloder og hele universer, og så ville vores princip om, at "alt hvad der kan eksistere, må eksistere (i symmetrisk forhold)," føre til at alle "ting" eksisterer. Så det vil særligt sige, at alle mulige universer må eksistere. Jamen det lyder da meget godt, især hvis man er materialist i forhold til spørgsmålet om bevidsthed. Men selv da kunne man så også spørge, eksistere strygejern virkeligt side om side og på lige fod med hele universer? Er multiverset ikke bare en samling af universer? I så fald må man jo hypotisere, at den samlede fundametale logik om alt har en særlig "klausul" om, at det der kan eksisterer, er "universer," hvordan man så lige skal definere det begreb helt præcist. Nå, men det er detaljer: Overordnet set har vi altså bare en mulighed for, at genstandene for eksistens i multiverset --- med andre ord de ting, der kan eksistere --- er, ja, "ting." "Genstande." Fysiske objekter med andre ord, og muligvis altså yderligere begrænset til kun at indebære, hvad vi kan tænke på som 'universer' i en ret gængs forstand af ordet (altså samlinger af love, materie, rum og tid). Der er dog også en anden vigtig mulighed, især hvis man mere er såkaldt idealist frem for materialist, og der er, at genstandene for eksistens i multiverset er: Bevidste (sammenhængende) oplevelser. I denne hypotese er universerne i multiverset altså ikke en samling af rum, tid, materie og tilhørende love for, hvordan dette forløber, men af bevidste oplevelser (jeg vil skrive Oplevelser med stort fra nu af), der så også har nogle love for, hvordan de forløber. Da sanseinput er en stor del af Oplevelser, så vil sådanne Oplevelser jo også skulle indeholde beskrivelser/"love" for, hvad der sanses i Oplevelsen, og disse "love" kan jo så indebære de fysiske love i vores universer. Så hvis vi tænker på vores eget univers, så er der tydeligvis nogle love for, hvordan objekter bevæger sig og udvikler sig i tid. Med den.. objektorienterede hypotese om genstandene for eksistens, så vil disse love være "indskrevet" i multiverset direkte om objekterne, hvorimod i den bevidstheds-/Oplevelse-orienterede hypotese, der findes de samme love også for et univers, men de hører så i stedet bare ind under, hvor jeg-personen/erne oplever. To sider af samme sag. Ingen af os kan empirisk afgøre, om vores univers har love, der tager udgangspunkt i objekter eller om det tager udgangspunkt i de bevidste oplevelser i det. Hardcore materialister vil nok hælde mest til, at universer er orienteret omkring love, da disse per definition ikke ser noget problem i, at bevidste oplevelser bare opstår af sig selv, men alle os andre, der synes, at der ligger noget mærkeligt i tanken om, at bevidste oplevelser bare kan opstå af genstandes bevægelser (og denne forundring bliver kun forstærket, når man dykker grundigt ned i kvantemekanikkens verden, skulle jeg hilse og sige) *(Det skal så dog siges, at jeg hele mit liv selv har hældt mest til materialisme, selvom jeg kunne se nogle store spørgsmål ved det, som er svære at svare på, og at jeg først opdagede det elegante ved idealismen (som jeg slet ikke vidste, det hed på det tidspunkt) der i sommeren 2019, hvor jeg pludselig fik en række åbenbaringer om dette emne, bl.a. også omkring hvad der så svarer til den eksisterende teori om CUH.), jamen vi vil naturligvis være så meget mere desto åbne over for et oplevelse-/bevidsthedsorienteret multivers, fordi dette løser hele den problematik automatisk: I et oplevelseorienteret multivers skal oplevelserne ikke opstå fra noget andet, men i stedet er de der fra starten af, og man kan så nærmere sige, at objekter "opstår" ud fra dem (fordi objekterne altså så kun eksisterer i det omfang, at de bliver oplevet af en bevidsthed). (13:42)

Man kan så selvfølgelig også hypotisere, at multiverset har begge ting som genstand for eksistens, fysiske objekter og bevidste oplevelser, og så vil det så bare være spørgsmålet, om man er materialist eller ej, der afgør om førstnævnte så også fører bevidste oplevelser med sig indirekte eller ej.

Okay, så det var en ret vigtig og grundlæggende opdeling i mulighederne ved, hvad den fundamentale logik om alt har som genstand for, hvad kan eksistere i det samlede multivers. Et andet vigtigt spørgsmål omhandler så subjekterne for Oplevelserne. (13:46) ...(14:10) Hvem (eller hvad) er jeg-personerne i fortællingen med andre ord. En hardcore materialist tror som bekendt ikke på, at der er en sjæl som er genstand for de oplevelser, som objekterne producerer, og en sådan vil derfor nok sige, at der simpelthen ikke er nogen.. hm, vi kunne sige, at der ikke er noget "modul," der indgår i oplevelsesskabelsen, som oplever Oplevelsen; en oplevelse oplever bare sig selv. Hm, jeg vil meget nødigt kalde disse moduler for "sjæle," for vi har desværre nogle associationer til dette begreb, der er uhensigtsmæssige i visse sammenhænge (altså med visse hypoteseantagelser).. ..Hm, lad os kalde det en "oplevergenstand" her.. ..Nej, et "oplevelsessubjekt." Ok. Nå, og det er så slet ikke kun materialister, der kan have denne opfattelse. I hypotesen, hvor den fundamentale logik beskriver Oplevelser som genstande for eksistens er det jo også ret unødvendigt at have eksistensen af en helt trejde ting, nemlig et oplevelsessubjekt, for at Oplevelserne kan udleves. En måske mere naturlig opfattelse (det synes jeg i hvert fald) er nok, at det bare ligesom er multiverset selv, der udgør det samlede "opevelsessubjekt," og at alle eksisterende oplevelser derfor bare opleves af.. ja, af muliverset, eller af den "fundamentale logik for alt," kunne man også tolke det som. Men der findes altså også en mellemvej, hvor at multiverset indeholder mere end ét oplevelsessubjekt, som vi altså ofte kan tænke på som "sjæle" (men ikke i alle henseender). Og hvis multiverset indeholder en samling (meget vel en uendelig samling) OS'er.. Hm, lad mig bare kalde det Subjekter med stort S fra nu af.. Hvis multiverset indeholder sådan en samling Subjekter, så afhænger det så af, hvorvidt multiverset er oplevelses- eller objektorienteret (eller en blanding), om hvert Subjekt så tilknytter sig en Oplevelse i multiverset, eller om de tilknytter sig en "hjerne" (lad mig skrive Hjerne fra nu af), som altså skal forstås i en meget bred forstand af ordet (maskiner kan f.eks. også være Hjerner, og det kan alt muligt andet også (medmindre man specifikt begrænser sin hypotese for multiverset herom)), i et specifikt fysisk univers af objekter. I denne todelte hypotese vil vores oplevelser altså hver især være et produkt af, at der "sidder" et specifik Subjekt et eller andet "sted" i multiverset --- enten i et abstrakt rum eller i et fysisk rum, muligvis lige oven i din Hjerne, som den oplever fra (i hvilket tilfælde "Subjekt" netop bliver helt ækvivalent med vores normale forståelse af begrebet "sjæl") --- og er så i færd med at opleve, det vi oplever ligenu, og nærmere bestemt er vi hver især det Subjekt og den er os. Vi skal senere diskutere noget mere om, hvad dette betyder for os. Men lad os bare her påpege, at det sjove ved denne hypotese, hvis vi kan sige det sådan, er, at hvis man har to universer, der indeholder samme Hjerne med same tidsudvikling, eller hvis man i et oplevelsesorienteret multivers har to oplevelser, der i en vis periode er helt identiske med hinanden, så vil helt den samme personlighed, med de samme tanker og helt den samme selvforståelse, opleves af to forskellige entiteter.. Tja, det kan man jo sige alligevel.. Nå, men.. Ja, lad mig bare lige nævne her i stedet, at lige netop denne (todelte) hypotese kan føre til, at man kan bekymre sig om døden, for hvad skal der så ske med ens "sjæl" (ens Subjekt) bagefter? Dette problem har materialisterne og idealisterne, der tror på at alle Oplevelser udleves af et stort samlet Subjekt (som man meget vel kunne tænke på som multiverset selv), ikke; for dem er to identiske (del-)oplevelser, der udleves i multiverset, også identiske i forhold til, hvad de betyder for den samlede mængde af oplevelser, nemlig fordi der ikke i disse hypoteser vil findes nogen skjult variabel, så at sige, der afgør om Oplevelsen i givet fald opleves af det ene eller det andet Subjekt i multiverset. For dem, eller rettere for os, for jeg er selv idealist med tro på ét samlet Subjekt) er alle oplevelser ligeværdige, for vi vil så mene, at vores egen nuværende oplevelse er en del af ét samlet hele, og at vi altså ligeså meget er en del af alle andre Oplevelser i multiverset, som den Oplevelse, vi selv føler at vi lever lige nu (hvad "nu" så end betyder helt præcist, men det kommer vi til).

Nå, og nu fik jeg så lige akkurat teaset den næste store opdeling, man kan have i hypotesen for multiverset, og det er nemlig i forhold til, om der findes en form for en global tid eller ej. (14:47) ...For materialister er det meget naturligt at antage, hvis ikke en global tid for det samlede multivers, så i det mindste lokal tid for hvert enkle univers --- ja, det er nærmest uundgåeligt. Og herfra er der så ikke meget i vejen for videre at antage, at der også er en global tid. For de idealistiske hypoteser er spørgsmålet derimod en smule mere indviklet. Her er der nemlig ikke et behov for "tid" andet end som noget, der er subjektivt for hver Oplevelse. En oplevelsesorienteret multivershypotese kan således godt bare antage, at "tid" er et rent subjektivt begreb, og at alle Oplvelser (hvad end de bliver oplevet af individuelle, adskildte Subjekter eller af et stort samlet Subjekt) bare er, og at de via deres væren (som altså så er konstant så at sige, i og med der ikke findes nogen egentlig tid (eller måske bare ikke en "tid" som svarer til, hvad vi normalt forstår ved begrebet)) bare resulterer i at de ligesom konstant udlever sig selv, så at sige, eller bliver udlevet af de Subjekter, der har knyttet sig til dem. Dette er faktisk ret dejligt, for konceptet om Tid kan også i sig selv godt virke lidt mærkeligt, lidt ligesom da vi snakkede om, at konceptet om, at bevidsthed skulle opstå af genstandes bevægelser, også kan virke mærkeligt. Så det er dejligt, at der findes Eksistens-hypoteser, hvor tid er et rent "subjektivt" fænomen, så at sige. Nå, men idealistiske hypoteser kan nu også godt indeholde koncepter om tid. For eksempel kunne man have en specifik idealistisk hypotese, der sagde at alle oplevelser udleves samtidigt i multiverset --- enten af et stort samlet Subjekt eller af adskilte, individuelle Subjekter hver især --- og at den globale Tid i multiverset så dermed bare måles i den subjektive tid som hver Oplevelse har. Med andre ord kunne man fortolke dette som, at multiverset indeholder en (uendelig) række af Oplevelser, som multiverset, hvis vi personificerer dette i denne metafor, så ligesom "trykker play på" i Tidens begyndelse, og så kører de ellers hver især samtidigt i henhold til deres subjektive tidsopfattelse. En anden mulighed kan være, at der til hver Oplevelse også er tilknyttet en vis "regnekraft" så at sige, og at man metaforisk set så kan sammenligne hver af de "afspilne" Oplevelser som en slags computer, der regner på, hvordan Oplevelsen forløber. En oplevelse, der foregår i et stort univers med meget materie i og med "regnetunge" fysiske love, vil så "afspilles" langsommere end en oplevelse, der foregår i et mindre "regnetungt" univers. (I øvrigt kunne man også have den hypotese, når det kommer til at objektorienteret univers: Her kunne man også stille alle universerne på række og så sige, at den globale tid ikke svarer til de lokale tider, men i stedet afhænger af, hvor "mange udregner skal klares," så at sige.) Disse tanker svarer altså til en antagelse om at den fundamentale logik ligesom skal bruge tid på at udregne sig selv, og så at sige opdage flere og flere sandheder om sig selv, hvilket, når vi siger det på den måde (og ikke snakker om det, som om hvert univers/Oplevelse udregnes af en computer), så lyder det jo faktisk pludselig slet ikke helt så dumt. Og med den grundlæggende fortolkning, så hører dette faktisk også med til min egen yndlingshypotese, nemlig at den fundamentale logik om alt ligesom fra Tidens begyndelse opdager flere og flere sætninger om sig selv, og at det er i takt med, at den opdager (og nu tillader vi os altså lige at personificere den her) disse sætninger, så udlever den så også de Oplevelser, som sætningerne omhandler. *(Det skal dog siges, at jeg også synes rigtigt godt om flere andre hypoteser.) Nå, men det vender vi tilbage til. Ellers skulle man ikke tro, at denne forskel gør så meget, nemlig om den globale Tid, hvis der er en, afhænger af "udrengernes" kompleksitet eller ej, men faktisk så giver det meget muligt en forskel i den samlede 'prior'-sandsynlighed, som det hedder. Som et sjovt lille eksempel på dette, så kan man faktisk teoretisere omkring, om det faktum, at vores univers er relativistisk, måske ligesom kunne skyldes, at det hermed så faktisk kan have uendelig størrelse, uden at det er uendeligt komplekst at "regne på"/"simulere," fordi man i et relativistisk univers kan tillade sig at regne på/simulere alt ved at starte i et enkelt punkt og så regne på alt med udgangspunkt i en lyskegle derfra. Hermed bliver et uendeligt tung simulering faktisk til en endelig tung simulering. Man kunne også nævne mange andre sjove ting i denne sammenhæng, men lad os bare stoppe her, for det er lidt et sidespor ift. det overordnede tema her. (15:37)

Nå, det næste man så kunne tage fat på, det er så sprøgsmålet, hvis vi specifikt snakker de oplevelsesorienterede multivershypoteser, og det er hvordan.. hm, hvordan de beskrives, men det bør næsten komme i et helt nyt afsnit, for nu bevæger vi os så videre til noget helt nyt, og det er at fundere over, ..ja, over "strukturen" af den fundamentale logik så at sige.. Hm, det er lidt en stor mundfuld, men jeg tror muligvis, der er en god, hurtig vej igennem det, så lad mig lige tænke mig om først... (15:42) ..(Okay, men hjerne skal også lige bruge en god pause, tror jeg...) (15:47) ...(16:02) Ah jo, jeg tror godt nogenlunde, jeg ved, hvad jeg vil sige.. ..Ja.. Men jeg synes næsten, emnet fortjener, at jeg skriver det færdigt i morgen, når jeg er mere frisk igen --- hvilket jeg i øvrigt godt tror, jeg kan; de næste "afsnit" behøver nok ikke at blive så lange..:) 

(06.01.23, 11:45) Okay, det var rigtigt godt, at jeg lige tog aftnen til at tænke mere over emnet, for nu kom jeg i tanke om nogle andre vigtige ting. I forbindelse med de objektorienterede multivershypoteser, så fik jeg kun snakket om genstande/objekter og universer, og fik så også snakket om Subjekter, altså hvad vi nærmest kan tænke på som en slags "sjæle," selvom der dog følger flere antagelser med, hvis vi kalder det 'sjæle,' som vi ikke ønsker at antage om Subjekter. Men ja, jeg fik jo så udeladt den mulighed, at det kun er Subjekter, der eksisterer, og at fysiske universer og objekter bare er noget som de ligesom "tænker frem," så at sige. Sådanne hypoteser indeholder jo også de hypoteser, der siger at multiverset består af en mængde guder, som hver især står for at skabe fysiske universer, samt udleve de tilhørende oplevelser i de universer. Så altså også en rigtig vigtig gruppe af hypoteser at få med. Og i sidste ende bør det også nævnes, at når vi kommer til at antage, at "alt hvad der kan eksistere i den fundamentale logik, gør det," så får vi jo faktisk et samlet multivers hvor en "logik" ligesom skaber ting spontant. Aha, men kunne man så ikke også i stedet forestille sig et multivers af flere end én "logik," hvor hver "logik" hver især så skaber universer/delmultiverser og skaber og udlever de tilhørende Oplevelser (..eller skaber Subjekter, som så udlever dem)? Jo, det kunne man selvfølgelig godt, men i så fald så falder disse hypoteser jo faktisk sammen med de hypoteser, der siger at Subjekter er de fundamentale genstande for eksistens.. Nå nej, ikke helt, vent lidt.. ..Hm, hvis vi ser på hypoteserne, hvor en fundamental logik skaber alt og også udelver alt selv, og hvor der altså ikke er individuelle adskildte Subjekter, men hvor alt opleves af multiverset/"logikken" selv.. Hvis vi tager de hypoteser og omdanner dem, så der nu er flere "logikker" i stedet, så svarer denne mængde af hypoteser ret meget til.. Nej, den indgår i mængden af hypoteser, hvor kun Subjekter er genstande for eksistens, nemlig hvis man tillader sig at bruge en bred definition af, hvad Subjekter kan være (og hvorfor ikke, for det er et super abstrakt begreb i forvejen), således at det også inkluderer "logikker." Hm, og hvad så med de hypoteser, hvor den fundamentale logik også skaber individuelle Subjekter, der er adskildt fra alt andet..? ..Tjo, men her kunne man udvide.. Tja, never mind, det er også lige meget; lad os bare inkludere den mulighed som en selvstændig ting, og lad os bare notere os, at det under visse antagelser også kan svare til en gruppe af hypoteser som hører til den mængde, hvor Subjekter er de fundamentale genstande for eksistens (altså de hypoteser, hvor "guder" (hvis man fortolker dem sådan) er de fundamentale genstande for eksistens, nemlig fordi man her kan omfortolke "logikkerne" til at være det samme som "guder;" at en "gud" er en "logik"). 

Okay, så det var rigtig godt lige at få de former for mulige hypoteser med. 

Nu kommer vi så til at tale om, hvad man så får ud af at antage at "alt hvad der kan eksistere, eksisterer" ovenpå de beskrevne hypoteser om, hvad kan eksistere. Og dette bliver så faktisk et relativt kort afsnit, for det korte af det lange er, at det ville kræve en forståelse af, hvordan den fundamentale logik om alt er.. "struktureret"/"ordnet," før man ville kunne sige noget præcist om vores univers-/Oplevelse-prior-sandsynligheder (og her må man altså lige læse lidt sandsynlighedsregning og statistik for at forstå, hvad menes med 'prior-sandsynligheder'). Og det kan vi jo aldrig komme til. Men! Vi kan teoretisere os frem til nogle ting, bl.a., og dette er rigtigt vigtigt, at man med nogle ordninger vil opnå det, der (desværre allerede er opfundet af en anden;)) kaldes 'Mathematical Universe Hypothesis' (MUH), eller hvis man skal være mere præcis (for selv ham, der postulerede idéen er vist gået over til at fokusere på følgende også): 'Computable Universe Hypothesis' (CUH). Begge teorier (altså teorierne omhandlende hypoteserne) handler så om, at man ved at antage, at alt hvad der kan beskrives i en (ordnet) matematisk teori eksisterer, faktisk nok for en høj frekvens af ikke-kaotiske universer som vores eget i multiverset. (Mere specifikt en høj frekvens af universer, som kan beskrives med relativt lidt information.) Og CUH præciserer så bare og siger: Vi er ligeglade med ikke-konstruerbare matematiske objekter (og hvorfor skulle nogen også kære sig om dem, andet som en filosofisk beskæftigelse? (jeg er matematisk konstruktivist, kan man høre)). 

Men for at nå CUH, så kræver det altså, at der er en vis ordning i den fundamentale logik, samt en ordning i.. ja, i hvilken rækkefølge at Oplevelserne bliver udlevet (men hvor man dog godt i princippet kan have, at et endeligt antal Oplevelser kan udleves på én gang). I mange hypoteser kan dette skabe nogle store problemer. Men det gode er så, at man altid kan sige, at, jamen, bare fordi vi med vores jordlige (er det et ord?..) matematik ikke har mulighed for at definere et fornuftigt sandsynlighedsrum, hvis ikke alle Oplevelser i multiverset er ordnet på en vis måde, så er det jo ikke ensbetydende med, at multiverset ikke selv kan.. ja.. se ud på en fornuftig måde. ..Bare fordi vi vil opnå logiske paradokser, hvis vi prøver at tildele sandsynligheder til noget, der er udvalgt fra en uendelig mænge, så betyder det ikke at multiverset behøver at indeholde paradokser, hvis det nu f.eks. indebærer, at uendeligt mange Oplevelser udleves "samtidigt" --- eller hvis de udleves i rækkefølge, forresten, med hvor prior-sandsynlighederne bare aldrig konvergerer.. Så ja, selv hvis man ikke lige kan finde, eller ikke lige synes om de hypoteser, hvor alle Oplevelser er ordnet pænt, så betyder det ikke at multiverset ikke godt kan følge de hypoteser, uden at det bryder sammen. Vi kan så selv pålægge nogle antagelser til de hypoteser, der får prior-sandsynlighederne til at konvergere alligevel, og her er det så bestemt værd at nævne, at man herved alt andet end lige sikkert også vil komme frem til CUH i sidste ende. 

Der er dog også et problem til den anden side, og det er, at nogle hypoteser fører til et komplet kaotisk univers. Disse problemer er dog helt anderledes, for der kan man bare sige, at fordi vores eget univers/vores egen Oplevelse ikke er komplet kaotisk, så må man forkaste de hypoteser, der siger, at det/den/de bør være det. Dette forklares nemmest, hvis vi ser på et eksempel. ...(13:04) Hvis vi ser på et idealistisk multivers, hvor det er (bevidste) Oplevelser, der er genstand for eksistens, så er det betydende for Oplevelsernes prior-sandsynlighed, hvordan Oplevelserne er "beskrevet" i den fundamentale logik, så at sige. Hvis en signifikant delmængde af alle Oplevelser er beskrevet med udgangspunkt i en Hjerne, hvor man altså ser på de fysiske bevægesler i en Hjerne (som dog med idealistiske antagelser kun eksisterer i kraft af den bevidste Oplevelse og ikke omvendt), og hvor Oplevelsens forløb så afhænger af disse bevægelser.. Hvis en signifikant delmænge af Oplevelserne i multiverset er beskrevet på den måde, så vil vi ikke opnå komplet kaos i multiverset, og hypotesen kan således ikke forkastes. Men hvis vi i stedet antager til vores hypotese, at alle Oplevelser i multiverset er beskrevet lidt som et slags computerprogram, hvor hver linje beskriver en ny følelse i rækken, som Oplevelserne følger, så vil der jo herved blive komplet koas, når man så antager (\emph{hvis} man altså antager), at "alt hvad kan eksistere, eksisterer." Så ville der være 0 orden i alle Oplevelser og alt ville være koas og tilfældigt. Enhvert udsnit af en Oplevelse, hvor denne indebærer en følelse af orden, vil så med al sandsynlighed hurtigt erstattes af noget komplet kaotisk igen. Og selv hvis man prøver at pålægge, at kun Oplevelser, hvor der er en sammenhængende selvforståelse, der gennemgår Oplevelsen, er gyldige, så vil dette stadig ikke kunne forklare, hvorfor vores omgivelser ikke går amok omkring os. Så alle sådanne hypoteser kan vi altså udelukke.

Dette er i øvrigt også interessant i en anden henseende, for nogle af modargumenterne mod materialisme går bl.a. ud på, hvis vi forestiller os.. Ja, der findes en vis xkcd, hvor en mand går i en ørken og flytter sten for at simulere vores univers. ..To sek.. ..Nummer 505, A Bunch of Rocks, hedder den. Så kan man så spørge, hvad ville der ske, hvis han gjorde det to gange? Hvad ville der ske, hvis han gjorde det to steder samtidigt, måske forskudt med en lille tidsforskel eller ej? Og slutteligt, hvad hvis han bare havde to sten hvert sted, som han mere eller mindre flyttede samtidigt? Nå ja, og helt slutteligt, hvad hvis det i stedet var bunker af sand, han flyttede rundt på, måske endda hvor nogle sandkorn faldt fra og nogle kom til i bunkerne, når han flyttede dem. Disse spørgsmål klarer de Oplevelses-orienterede multivershypoteser jo nemt, hvor der definerer hver Oplevelse jo bare selv, som en del af dens "naturlove," hvordan dens Hjerne defineres, samt hvordan denne bevæger sig og udvikler sig i tid. Hm, jeg kan dog nævne, at jeg lige her i går kom til at tænke på, at man måske kunne slippe af sted hvs man prøvede at definere en materialistisk hypotese, hvor man gør brug af entropi og koncepter om, hvad definerer information, hvornår information er unikt, og.. ja, og ting i den stil, men hvem ved? måske løber man bare ind i andre paradokser/svære spørgsmål herved.. Anyway, det jeg egentligt ville hen til, det var at jeg kan huske, at vi på et tidspunkt snakkede om dette i forbindelse med VT (videnskabsteori og etik (for fysikere)) på fysik, hvor en af mine venner fra fysik sagde, at han så (vist nok; sådan husker jeg det i hvert fald) troede på, at to identiske.. ja, "Hjerner" med identisk udvikling bare producerer netop én bevidst Oplevelse i multiverset. Elegant svar. Men nu kan jeg jo så se, at der faktisk er et stort problem med dette svar, for medmindre vi begrænser multiverset til noget meget endeligt, så vil alle mulige Hjerner jo forekomme, hvilket vil sige at alle mulige Oplevelser, der afviger fra hinanden vil forekomme netop én gang i multiverset/den samlede Eksistens. Men dette vil jo derfor medføre en komplet kaotisk prior-sadsynlighed for alle Oplevelser, og denne hypotese går derfor faktisk ikke, interessant nok. (13:46)

(15:10) Hov, jeg har også helt haft glemt noget andet virkeligt vigtigt. Når jeg har skrevet om de objektorienterede/materialistiske hypoteser ovenfor, så har det måske lydt som om, at materialismen har nogle ting, den ikke kan forklare, som Oplevelsesorienterede hypoteser kan forklare, men sådan er det nu slet ikke. Jeg synes personligt, at de Oplevelses-orienterede hypoteser gør det en anelse mere elegant, men det er bare en personlig holdning. For hvis vi nu starter med at se på den her hypotese, som jeg beskrev, med at hver Oplevelse har i/med sig en beskrivelse af/nogle love for, hvordan Oplevelsen starter og udviler sig i tid, eksempelvis ved at definere en Hjerne (muligvis sammen med en større samling af objekter, som Hjernen er en del af, nemlig et fysisk univers) samt nogle love for, hvordan bevægelsen af information i den Hjerne (hvor 'Hjerne' altså er et fuldstændigt abstrakt begreb, og kan endda indebære et helt univers f.eks.) fører til en (eller flere) bevidst(e) oplevelser, så kan man i de objektorienterede hypoteser jo i stedet bare have nogle love, ved siden af lovene der beskriver, hvordan materie i universet bevæger og udvikler sig, som så beskriver, hvordan bevidste oplevelser kan opstå ud fra disse fysiske genstande. Hvis vi så tænker på xkcd-eksemplet (A Bunch of Rocks), så kunne der altså bare være nogle universer, hvor to af hver sten vil føre til to adskildte Oplevelser, nogle hvor de kun vil føre til én, osv (men hvor alle de fysiske love måske er de samme, og hvor startkonfigurationen af universet også er det samme; bare hvor lovene for de resulterende Oplevelser produceret af den fysiske materie er forskellige). Så ja, det kan sagtens lade sig gøre at give et klart svar på, hvorfor fysiske objekter kan føre til bevidsthed i et ellers overvejende objektorienteret multivers, og som altså ikke bare antager hardcore materialisme og siger: "jamen det sker bare helt automatisk, nemlig at når man har en fysisk Hjerne et sted, der kan have en bevidst oplevelse, så har den det også." Men ja, jeg synes så dog, at de Oplevelses-orienterede hypoteser klarer denne del mere elegant, end de overvejende objektorienterede multiversehypoteser, hvor man så indfører Oplevelses-love oveni, ved siden af de "fysiske love" i de indeholdte universer. (15:29)

Okay, nu når vi så til et afsnit, hvor jeg bare lige siger et par ting om, hvilke af de hypoteser, vi har set på, som jeg selv synes er ret nice, og som jeg tror mange sikkert vil kunne finde fornuftige i større eller mindre grad, og derefter kommer så det sidste afsnit, hvor vi ser på konsekvenserne ift. multiversets Subjekter (du og jeg og vi), og også på nogle pointer omkring moral.

Lad mig starte med at pointere, at en hypotese, hvor der er én grundlæggende (og "ren") logik om alt, og hvor alt så forekommer i takt med at denne logik ligesom "opdager flere og flere sætninger om sig selv," så at sige, og dermed også forstår hvordan flere og flere samlede oplevelser må føles.. At denne hypotese faktisk muligvis kunne give et matematisk regnestykke for prioren, ikke som vi kan finde frem til nøjagtigt, selvfølgelig, men hvor vi kan sige, at dette regnestykke faktisk på fornuftig vis godt kunne indeholde en ordning af alle Oplevelser, således at vi faktisk (med vores "jordlige" matematik) ville kunne tillægge en prior-sandsynlighed til hver Oplevelse i princippet. Lad mig prøve at omformulere dette.. ..Det er ikke ufornuftigt med en sådan hypotese, at teoretisere, at multiverset i princippet kunne indeholde en orden, en rækkefølge, kunne vi også sige, hvor alle Oplevelser (og jeg kan som man måske kan gætte sig til godt lide at antage, at hver Oplevelse er endelig --- det er i hvert fald en god antagelse, hvis man gerne vil nå frem til, at der må være en i princippet udregnelig prior i multiverset på denne måde..).. hvor alle Oplevelser udleves mere eller mindre én efter hinanden. Okay, kan jeg sige dette endnu mere klart..? ..Whatever, måske er dette underemne bare for komplekst, således at vi må gemme det til endnu senere (og at jeg altså ikke vil tale så meget om det i min første artikel om emnet). Men det korte af det lange er bare, at jeg altså tror, at der findes hypoteser, der (i hver fald for mig, og sikkert for mange) lyder ret fornuftige, og som kan føre til en fornuftig antagelse om, at multiverset har en pæn ordning af alle dets Oplevelser (også selvom rækken er uendelig), som gør at man matematisk (vores vores "jordlige" matematik) kan tillægge en prior-sandsynlighed til hvert univers/hver Oplevelse. Ok. Jeg vil ikke sige meget mere om, hvorfor jeg tror dette, men jeg bliver dog nødt til lige at nævne her, at jeg i går kom lidt i tvivl om fornuften ved dette, for hvordan skal en stor, samlet, "Ren" logik om alt lige vægte forholdet imellem, hvor lang tid sætninger "tager" at udlede (altså hvor mange logiske skridt, der går til udledningen), og hvor meget information sætningerne indeholder, når de skal ordnes. Nu ved jeg godt jeg "vrøvler" igen, så lad mig lige se på, om ikke jeg kan omformulere dette mere klart..  ..Hm, jo: Hvis vi ordner alle matematiske sætninger i en teori, f.eks. mængdelære, ud fra, hvor mange logiske skridt det tager for at udlede dem, så går det ikke ift., hvad vi ønsker a opnå, for så vil der (så vidt jeg lige kan se) blive uendeligt mange sætninger i hver (skridtantal-)kasse. Men hvis man så til gengæld indfører, at det også koster nogle skridt at læse lange sætninger, f.eks. hvis man har en lang antecedent som skal sammenlignes med en vist sætning i et modus ponens-skridt --- ret meget som om det foregik på en Turing-maskine (eller anden maskine), jamen så vil der pludselig blive endeligt mange sætninger i hver (skridtantal-)kasse. Problemet bliver så, at dette giver noget arbitrært til den "Rene" fundamentale logik, men ja.. Hm.. ..Okay, lad mig bare stoppe her, for det bliver hurtigt vildt kompliceret.. ..Det næste man så kunne tage hul på, det er at sige: "jamen hvad så hvis der så bare er en undelig mængde af fundamentale logikker," hvorved hver "logik" så kan tildeles en matematisk veldefineret prior for dets Oplevelserne, men så render man så bare ind i spørgsmålet: "Hvad hjælper det at der er lokale eldefinerede prior-sandsynligheder for hver 'logik,' hvis de samlede Oplvelser i multiverset, når man sætter det hele sammen, så stadig giver en svært-definerbar samlet prior for hver Oplevelse".. ..Hm.. ..Hm, måske skal man bare give op på, at få en matematisk veldefineret prior (ikke at vi nogensinde ville kunne regne den ud alligevel). I så fald kan man dog godt måske sige, at hypotesen, som jeg beskrev her lige ovenfor, nemlig med en helt fundamental "logik," der ligesom udleder (og "forstår"/"føler"/"oplever") sætninger omkring sig selv, "kommer tæt på," hvis det giver mening.. Det synes jeg i hvert fald lidt det gør..

Nå, men ellers har vi altså ogå bare rigtigt mange andre gode kandidater, må man sige. Jeg kan personligt faktisk også rigtigt godt lide den, hvor det ligesom er en masse "logikker"/"guder," der ikke har et mål med deres tankevirksomhed, og nok ikke har en selvbevidsthed på samme måde som, hvad vi forstår ved selvbevidsthed, som bare fremtænker universer, nærmest som en konsekvens af, hvad man kunne kalde en simpel nysgærrighed --- eller hvis man tænker mere "logikker" frem for "guder," så bare fordi at, jamen det er bare det fundamentale logikker gør; udvikler sig selv og "opdager" (og oplever) sætninger i sig selv. ..Og i sidste ende, så kan jeg egentligt også ok godt lide den hypotese, der bare siger: Alle mulige "objekter" eksisterer, og så forholder det sig i øvrigt bare sådan, at 'objekter' ikke bare indebærer dumme genstande, der flyver rundt og passer sig selv, men at 'objekter' i vores multivers også kan indeholde nogle definitioner af Oplevelser, som så bliver udlevet, enten ved at multiverset indeholder Subjekter samt nogle love for, hvordan disse Subjekter kan opleve ting, eller fordi der i hvert univers simpelthen bare er plads til, at der ved siden af de fysiske love også står nogle love, der simpelthen bare definerer, hvornår og hvordan diverse Oplevelser bliver udlevet i universet (altså et sæt love, som vi nærmest kunne kalde "sjæle-love"). En todelt objektorienteret multivershypotese, som også bestemt lyder ret fornuftig i mine ører, selvom jeg dog selv hælder mere til de første, jeg har nævnt her, som ikke er ligeså "objektorienterede." 

Ok. Jeg synes, vi slap nogenlunde godt igennem det. Så når vi til det sidste afsit, som i høj grad handler om spørgsmålet: Hvordan skal vi forholde os til "døden?" (Ikke at vi vil besvare dette spørgsmål eksakt, men det er altså i høj grad temaet for afsnittet.) Og et andet spørgsmål for afsnittet er også: Hvordan skal vi forholde os til moralspørgsmålet. (16:16)

Det korte af det lange, hvis vi snakker omkring "døden," det er at langt de fleste af de hypoteser, inklusiv alle dem, der garanteret er/vil være mest udbredte hos folk, er at, hvad vi normalt betegner som "døden" ikke rigtigt har nogen betydning. For hvis multiverset er uendeligt, så vil vi leve igen og igen og igen i alle mulige afskygninger af os selv, og dette gælder så f.eks. både hvis man er hardcore eller semi-materialist, eller hvis man tror at ens eget Subjekt, som jeg har kaldt det ovenfor, ikke er fundamentalt adskildt fra andre Subjekter i universet, men bare er en del af det store hele, hvad end "det store hele" så er en "gud" eller en "logik" for ens univers. Så hvis man altså ikke er tilbøjelig til at tro, at vi har en "sjæl," jamen så når man den konklusion (at vi skal leve alle afskygninger af vores liv --- og af alle andres liv, men det kommer jeg til om lidt), og hvis man tror på, at vi har en slags sjæl, men at den sjæl enten bare er en del af Gud (eller en del af noget andet meget grundlæggende i universet og/eller multiverset som helhed), eller at vi returnerer og blive en del af Gud efter døden, og således også blandes sammen med alle andre nuæevende sjæle, jamen så når man også samme konklusion. I sidstnævnte tilfælde (især hvis man også forestiller sig at der er en global Tid i multiverset --- eller bare en lokal til, som den lokale "gud" eller "logik" også følger) så vil det jo nemlig være naturligt videre at antage, at når en ny person fødes i universet, jamen så tages der lidt af "Gud" igen til at danne en sjæl igen, og således vil ens nuværende sjæl altså fordeles ud på alle andre personer/Oplevelser, som leves efter en selv. Og medmindre Gud/"logikken" der foresager det univers, vi lever i, er utroligt begrænset, så vil der være mange universer, som dette væsen foresager, og der vil altså aldrig ophøre med at være liv. Jeg tror allerede disse antagelser dækker, hvad rigtig mange mennesker ville synes giver god mening. En anden antagelse, som ikke har så meget med "alt hvad der kan eksistere, eksisterer"-hypotesen at gøre, men som måske også ville være udbredt hos folk, det er at sige: Multiverset er faktisk ret begrænset, men jeg tror på en Gud, og at man sjæl når op til ham og bliver passet på ham efter døden. Og ja, denne antagelse gør jo selvsagt også frygten for døden ret irrelevant --- ja, medmindre man i stedet tror på, at man skal i helvede, men det er nu nok de færreste, der slås for alvor med den tanke, for hvis man er en person, der tager den tanke seriøst om sit eget "efterliv," så vil man nok bare prøve at leve mere fromt og så håbe på det bedste. Så ja, det virker virkeligt ikke som om, at nogle af de hypoteser, der nok vil være mest udbredte hos folk, vil føre til andet end, at man ikke behøver at frygte dødens komme. Nu mangler jeg så bare lige at forklare mere om, hvorfor at vi ikke bare skal lave alle afskygninger af vores egne liv men også alle andres med de første hypoteser nævnt i denne paragraf, og så mangler jeg også at dykke ned i de få hypoteser, der antager "alt hvad der kan eksistere, eksisterer"-hypotesen, men hvor man stadig kan være urolig for "efterlivet".. Nå ja, og så mangler jeg også lige at sige: Der vil måske også være nogle få helt- eller semi-materialister, som af en eller anden grund tror på, at multiverset er ret begrænset. Men hvis bare multiverset indeholder universer som vores, og hvis det nu indeholder bare ét unvers, som bliver ved med at Big Crunch'e og udvide sig igen i en uendelighed, jamen så er det også med al sandsynlighed det vi lever i *(og alle andre døende universer vil med al sandsynlighed allerede være døde for en fantasilliard år siden), og så vil vi dermed også leve igen i alle afskygninger. (16:42)

Ok, nu til den der pointe om, at vi ikke bare skal leve "vores eget liv" i alle afskygninger, men også alle andres, og det er simpelthen fordi, at hvis vi antager at multiverset er uendeligt, så vil hver eneste mulige liv man kan forestille sig (og også sikkert vildt mange, som vi ikke kan forestille os;)) forekomme, nogle bare med virkelig lav frekvens i forhold til andre. Så hvis vi opstiller to liv overfor hinanden, så vil vi kunne finde udgaver af liv midt imellem de to liv på en kontinuer linje, hvor hvert liv vi plotter ind på linjen har en vis større eller mindre frekvens ift. hvor ofter det forekommer i multiverset. Og vi kan sågar finde uendeligt mange forbindelser på denne måde mellem to liv. Hvis man så spørger, hvad hvis den ene er en mand og den anden en kvinde, hvad hvis de bor på to forskellige planeter, hvad hvis de er af to helt forskellige arter (f.eks. hvis den ene eller de begge er en eller anden alien)? Jamen selv i alle disse tilfælde vil man kunne finde en glidende overgang, hvis altså man bare dykker dybt nok ned og tager fra de lavfrekvente livsforekomster i multiverset. Og målt op mod uendeligt vil selv ufatteligt lav frekvente livsforekomster forekomme, ja, uendeligt mange gange. Så på den måde indebærer "alle afskygninger af vores eget liv" simpelthen bare "alle afskygninger af mulige liv." 

Og hvis man altså dermed tror på, at multiverset ikke er begrænset, men er uendeligt ift. dets muligheder og dets forekomster, så når man altså ret nemt til, at "vi skal leve alle afskygninger af vores eget og alle andres liv igen og igen." Dette er dog medmindre man altså antager nogle ret specifikke ting, nemlig: At vi har en sjæl hver især, som er adskilte fra hinanden, og som aldrig smelter sammen igen på noget tidspunkt. At disse sjæle af en eller anden mærkelig grund også oveni købet er dødelige --- eller at de bare ligesom lever den samme meget begrænsede mængde liv igen og igen. Hvis man mener nogle af disse ting, så når man altså ikke nødvendigvis den konklusion. Jeg tror dog, at førstnævnte udgave, nemlig at vi alle har for altid adskildte sjæle, som dog er dødelige, vil være vildt sjælden at finde hos folk. At ens sjæl f.eks. lever det samme liv igen og igen, enten helt uden eller måske med nogle få variationer, den vil måske være lidt mere udbredt, men på den anden side kun slem, hvis man har haft et ligefremt dårligt liv. Men selv da, så vil mange nok hælde til den version, hvor der dog sker nogle få variationer gang på gang, og hvis man så dykker ned hypotesen herfra, så vil mange nok ende med at erkende, at hvis dette sker i al uendelighed, så vil variantionerne også ende med at blive uendeligt store, og så når man samme konklusion igen. Det kan man om ikke andet håbe *(at folk vil erkende, altså).

Men ja, så det korte af det lange er altså, at hvis man tænker i dybden over multiversets afgrænsning, eller nærmere bestemt mangel på afgrænsning, så vil de fleste nok kunne blive ret afklaret med døden heraf. (Ikke at de fleste ikke allerede er afklaret med døden, men der findes dog alligevel også mange der frygter den på nuværende tidspunkt). 

Og så kan vi så slutte af med spørgsmålet om moral *(hov, jeg mener 'etik,' rettere), for det smukke ved disse teorier er, at når man når omtalte konklusion med at vi skal leve alle afskygninger af alle liv igen og igen, jamen så når man dermed også frem til en utrolig bogstavelig udgave af "what comes around goes around." Man udvisker altså herved helt forskellen på, hvad en filosofisk egoist vil mene er korrekt etik, og hvad en utilitarist (eller andre etikker, der fremhæver altruisme og "godhed") vil mene! Jeg vil altså påstå, at man, uanset hvordan man vender og drejer det (stort set), når frem til en etik der siger: Lev dit liv som om, at alt hvad du foresager af godt og ondt med andre (inklusiv andre arter og livsformer), det vil du selv opleve (med rollerne byttet om) i et efterfølgende liv, og bak om om at andre i dit samfund bør følge den samme etik. Og svaret på, hvorfor denne etik bør følges, er så både, at, jamen, dette er en god etik at følge for et samfund, men også at, jamen, antagelsen i den etiksætningen er sikkert også sand for all intends and purposes. ..Kortere sagt kan man sige: Lev dit liv ud fra en antagelse om, at du og alle andre skal leve hinandens liv i lige forhold i alle jeres efterliv.

Det skal så lige siges, at nogen vil pege på fri vijle og sige, at jamen, bare fordi jeg med min frie vilje gør skade/ondt på en anden person, jamen så betyder det ikke, at den/en anden person vil gøre skade/ondt på mig i efterlivet i de udgaver, hvor rollerne er byttet om. Denne opfattelse fordrer, at man tror at ens handlinger ikke kan forklares alene ved hjernens fysiske bevægelser, men at sjælen på en måde også sidder der med en slags joystick i overført betydning og påvirker, hvad hjernen gør. Hm, tja, det har jeg vel egentligt ikke så meget at sige til, når det kommer til stykket, for det er muligvis så langt væk fra min egen opfattelse af, hvordan virkeligheden fungerer, at jeg nok aldrig har tænkt så meget på at argumentere omkring de antagelser.. Hm.. Oh well, lad mig så bare slutte for nu, og så lade den diskussion stå åbent.. ..Hm, ah øv, den holdning kunne godt gå hen at blive problematisk, når det kommer til at enes om etiksprørgsmålet.. ..Hm, og måske også når det kommer til at trøste folk, der har levet et direkte dårligt liv, og er bekymret for, hvis de skal gøre det igen og igen i al uendelighed.. Hm.. Nå, men jeg lader det være for nu. I de ovenstående noter har jeg bare antaget, at alle Subjekter bare oplever deres Oplevelser, og at Subjekterne altså ikke selv går ind og påvirker de Hjerner, de har tilknyttet sig.. Hm, men det er da egentligt en hypotese-mængde, der er værd at have med også. Ja, ok. Så når jeg på et tidspunkt skriver dette som en artikel, så må jeg lige huske, at inkludere sådanne hypoteser også, og så må jeg også lige sørger for inden da at tænke lidt mere over, hvad man så kan sige om døden og om etik, hvis man antager sådanne hypoteser.

Men slut for nu.:) Det var rigtigt dejligt lige at få gennemgået det hele i en nogenlunde sammenhængende tekst, for jeg tror at alle mine tidligere noter omkring emnet alt i alt har været ret usammenhængede. Så rigtigt dejligt lige at få gået det hele (eller rettere det meste af det) igennem igen i store træk --- og dejligt også at få tænkt nogle nye tanker omkring emnet!:) Og jeg tror altså umiddelbart godt, jeg kunne skrive denne gannemgang her om, så det kunne blive en god lille (eller knap så lille, alt efter hvor kortfattet jeg kan gøre det..) artikel. Det vil jeg se frem til.:) (17:36, 06.01.23)

(07.01.23, 9:13) Okay, der er lige nogle få ting, jeg skal huske at nævne også, og så mangler jeg også at diskutere den mulighed, at vi har dødelige "sjæle"/Subjekter noget mere. Lad mig lige starte med at uddybe, at i de Oplevelse-orienterede hypoteser er hvert "univers" i multiverset ikke et fysisk univers, men et idealistisk univers, der indeholder én eller flere Subjekter, der udlever en eller flere Oplevelser. Jeg mener endda, at det mest oplagte for sådanne hypoteser bare er at have ét univers pr. Oplevelse. Men så skal det altså ikke forstås sådan, at vi er.. alene om at være bevidst i vores fysiske univers --- jo, det er vi på en måde, for det fysiske univers findes kun i kraft af vores egen Oplevelses eksistens i så fald, og ikke af personer omkring os, men man skal så huske på (og dette er så selvfølgelig antaget, at "sjælen"/Subjektet ikke har indflydelse på Oplevelsen, og at hver Oplevelse der fastlagt ud fra nogle love, ligesom også jeg har antaget i resten af denne tekst), at alt hvad man gør i ens "eget" univers så bare bliver spejlet i et tilsvarende univers, hvor "sjælen"/Subjektet har tilknyttet sig en anden hjerne (med lille 'h,' fordi vi her snakker om vores egne "kød-hjerner"). Det var det første, jeg lige ville uddybe.

Det næste, jeg vil uddybe handler om hvordan Oplevelser mon kan defineres i hypotesen. Her kan vi starte med at se på en sjov lille idé om, at alle Oplevelser kunne være meget korte, altså i subjektiv tid, og at vores egen opfattelse af en lang, sammenhængende Oplevelse bare er.. ja.. er subjektivt skabt.. Men nej, vi kan faktisk forkaste sådanne hypoteser, eller i hvert fald givet den indledende antagelse om at "alt hvad kan eksistere, eksisterer," for så vil en vilkårlig sammenhængende Oplevelse jo ikke behøve at følge nogen lovsætninger rigtigt. For eksempel kunne vi have en sammenhængende oplevelse af, at en person træder ud af en dør og med det samme kommer ind ad en helt anden dør; oplevelser der isoleret set måske følger nogle lovmæssigheder, men ikke når man sætter dem sammen. Alt ville derfor blive totalt kaotisk (sandsynlighedsmæssigt), mere eller mindre, og derfor kan vi altså forkaste det. Ok. Så Oplevelser i sådanne Oplevelse-orienterede multivershypoteser skal altså være længerevarende. Men hvor lang tid skal de så vare? Jo, det ville jo være underligt, hvis de alle var en meget specifik længde; hvorfor skulle det store samlede multivers være så specifik? Så de tre eneste fornuftige muligheder er nok bare, at de enten er uendelige alle sammen, at de er endelige men med vilkårlige længder, og som den tredje mulighed at der både findes uendelige og endelige Oplevelser. Nu er det så oplagt at spørge: Jamen slutter en Oplevelse ikke bare ved døden, mere specifikt altså når den pågældende Hjerne ikke længere opfylder de krav der skal gælde for den (pr. de love som "universet" (som i dette specifikke tilfælde er defineret som led i definitionen af Oplevelsen) påskriver)? Tjo, det kunne de sikkert sagtens gøre i mange tilfælde. Men man kunne også sagtens forestille sig, at Oplevelsens/"universets" definition af den iboende Hjerne tillader, at Oplevelsen kan hoppe fra fysisk hjerne/Hjerne til en anden fysisk hjerne/Hjerne. Desuden kunne man også have Oplevelser, der bare simpelthen er sammensatte, i den forstand at de er defineret med en "lovtekst" noget i retning af: "Først skal du hoppe sådan og sådan fra fysisk hjerne/Hjerne i det her fysiske univers for så og så lang tid (eller indtil det og det sker), og efter det skal du så leve i det og det fysiske univers (med de og de fysiske love) og hoppe sådan og sådan fra Hjerne til Hjerne indtil sådan og sådan, og efter det..." På den måde kan man altså definere nogle oplevelser, der er virkeligt lange. Og både hvad angår endelige og uendelige Oplevelser kunne man endda have definitioner, der definerede hver del-Oplevelse i sekvensen ud fra en mere abtrakt formel (eller (Turing-)maskine-starttilstand, eller hvad man nu kan tænke sig), og så iterere over alle de individuelle udgaver som følger den formel (eller hvad man tænker sig). Og hermed kan man så nå nogle virkeligt lange Oplevelser. Og hvis vi videre tillader, at Oplevelsernes definitioner også kan sige noget så som: "Gentag disse iterationer et antal gange lig Grahams tal" (eller TREE(3) eller TREE(TREE(3)) og hvad vi ellers kan finde på), jamen så kan man have (bogstavelig talt) helt ufatteligt lange Oplevelser. Ok. Hertil skal det så pointeres, at hvis vi antager at, "alt hvad kan eksistere, eksisterer," og hvis vi kan danne vilkårligt lange af sådan nogle Oplevelses-beskrivelser, jamen hvis vi så prøver at spå om, hvor lang vores egen Oplevelse-beskrivelse må være, så vil vi jo ikke kunne sige andet end: Dens længde må i gennemsnit være uendeligt. Det virker vildt, men det er faktisk konsekvensen.. Nå, og nu er det så her teorien om CUH kommer ind, for hvis man så overvejer hvilke nogle del-Oplevelser (altså dem med en nogenlunde konstant selvforståelse), der må være flest af i sådan en uendelig mængde af Oplevelser med vilkårligt lange beskrivelser, jamen så kommer man vist rimeligt nemt frem til (tror jeg/mener jeg), at de del-Oplevelser med tilsyneladende relativt lav information vil være mere frekvente.. Tja, eller det giver faktisk lidt sig selv: Der vil være en høj frekvens af del-Oplevelser, hvor det iagtagende univers tilsyneladende følger ret simple principper. Så det er altså sådan, at CUH kommer ind i billedet, når vi snakker Oplevelses-orienterede multiverser. *(Der er kan være lidt forskel på, hvad (prior-)sandsynlighedsfordelingen er i forskellige Oplevelse-orienterede hypoteser, eksempelvis afhængende af hvorvidt multiverset ordner Oplevelser og udlever dem ud fra, hvad der svarer til en "regnekraft" og sådan. Men det er nu ikke fordi, vi alligevel kan regne os frem til den faktiske sandsynlighedsfordeling overhovedet. Så for os behøver vi bare at vide, at vi nok får noget, der svarer til CUH, hvilket det vil gøre hvis det opfører sig pænt, og altså ikke giver os komplet kaos, som vi har set på.)

Nå, og nu kan jeg så slutteligt vende tilbage til den mulighed, at multiverset indeholder endelige ("dødelige") Oplevelser, og hvad det bør betyde for vores tilgang til døden. Og det korte af det lange her er så bare, at hvis der både er endelige og uendelige Oplevelser, jamen så vil der med al sandsynlighed, for dig der læser dette, allerede været gået en fantasiliard år (altså TREE(TREE(TREE(...))) år; find selv på hvor mange TREE der skal stå i rækken) og størstedelen af alle endelige Oplevelser vil allerede være døde, og du vil med al sandsynlighed være en af de uendelige. Og faktisk kan man næsten sige noget tilsavarende, når det kommer til de hypoteser, der kun indeholder endeligt varende Oplevelser. ..Tja, eller det afhænger godt nok af den specifikke hypotese, men under antagelse af at Oplevelsers levetider godt kan være defineret ud fra koncepter så som TREE(TREE(...)) osv., så vil de afsindigt lange Oplevelser også lynhurtigt.. tja, det var et forkert ord at bruge, men set i forhold til uendeligt, så jo, så vil det "lynhurtigt" blive kun dem, der er tilbage. Og ikke nok med det, de vil også veje utroligt meget mere end alle de knap så lange Oplevelser. Hvis man f.eks. ser specifikt på antallet af gange, hvor et Subjekt spørger sig selv (eller rettere har oplevelsen af at spørge sig selv): "Hvor lang mon min Subjektive levetid er endnu?" Hvis vi ser på statestikken omkring det antal for hver Oplevelse, så stort set alle.. nej, basalt set alle (som i: alle for all intents and purposes) forekomster af disse spørgsmål findes i Oplevelser, der er længere end T=TREE(TREE(...)) tid, efter vi den globale tid er lig TREE(TREE(...)). Altså når det globale ur slår TREE(TREE(...)), så vil forekomsten af alle sådanne spørgsmål til sig selv have fundet sted på ligeledes langvarige (eller længere) Oplevelser. Ok, dette er sikkert vrøvl for mange, og desuden så har jeg også her antaget, at der findes en global tid, eller i det mindste at der findes noget der svarer til en sådan. ..Okay, lad mig bare sige det sådan her: Hver gang et Subjekt spørger sig selv (eller rettere har oplevelsen af at spørge sig selv): "hvor lang mon min Subjektive tid har varet indtil nu?" så vil denne tid i gennemsnit være uendeligt (så i praksis, når vi spørger os selv, så vil svaret være: så ufattelig stor at du ikke vil kunne skelne det fra uendeligt). Og når man så spørger: "hvor lang mon min Subjektive levetid er endnu?" så vil svaret være det samme. Forklaringen på, at jeg mener at dette er tilfældet, den er lidt indviklet, og den kræver også lige nogle antagelser, må jeg indrømme, men under rimelige antagelser når man altså ret nemt hen til det samme svar: Hvis alle Oplevelser er dødelige, så vil alle korte og mellemlange Oplevelser "lynhurtigt" dø ud, når man opvejer dem mod uendeligt, og kun de ektremt lange (ja faktisk ufatteligt lange) Oplevelser vil være tilbage, hvilket vil sige at du selv, i det øjeblik du læser dette, i så fald så med al sandsynlighed vil tilhøre mængden af de ufatteligt lange Oplevelser. Okay, så er det vist godt med det for nu.xD^^ 

Det var, hvad jeg havde at tilføje om dette emne.:) (10:31, 07.01.23)







## Blockchain

(31.08.22, 10:25) Jeg havde egentligt lidt tænkt mig alligevel at udgive min angrebsvektor (med tilhørende forsvar) hurtigt på min GitHub, men jeg tænkte lidt over det i går, og det er lige før, at jeg faktisk ikke gør det alligvel; ikke i nogen stor fart.. Jeg kan lige tænke lidt mere over det, men jeg tror faktisk ikke helt, der er kød nok på.. tjo, tja, jeg ved det ikke; jeg skal nok lige tænke lidt mere over det. Men umiddelbart tænker jeg altså ikke at bekymre mig om at skynde mig at få det ud.. 
(02.09.22, 11:01) Nej, jeg tror simpelthen ikke der rigtigt er noget guf på denne idé. Så ja, alle mine blockchain-tanker er nu ret meget ude af vinduet.. Selvfølgelig vil jeg dele idéen om angrebet på et tidspunkt, men jeg tror altså ikke rigtigt, den kan få nogen til at spærre øjnene op (og være vildt interesseret, i.e.).. 


*((18.09.22, 11:51) Okay, glem stort set alt, hvad jeg har skrevet her i går:)
(17.09.22, 12:27) Jeg havde en ret vild (som i 'ude af normen'; ikke vild som i ' fest-vild') nat, hvor jeg gik i seng lidt efter tolv og så var vågen helt til omkring seks. I lang tid kunne jeg bare ikke sove (selvom jeg ikke synes, jeg gjorde noget som helt galt, eller havde det for varmt/koldt; det eneste jeg kan tænke på var, at jeg måske var en lille smule sulten, men kun en meget lille smule og ikke noget, jeg synes forstyrrede mig!..), men på et tidspunkt begyndte jeg også at tænke lidt over fysik, så lidt om mine "planer" (jævnfør sektionen nedenfor), og så fik jeg så også tænkt på blockchain, hvor jeg mellem fem og seks synes jeg fik et hel væld af gode idéer.. Så ja, dem vel jeg så skrive om her (og genoverveje dem)..:).. 
..Hm, lad mig bare prøve at forklare, og så kan jeg overveje imens:
..Hm, eller lad mig egentligt lige starte med at skrive om, at jeg i første omgang fik nogle tanker, hvor jeg bare tænkte: ah, måske kan jeg alligevel godt skrive om blockchain (og mit angreb) i min GitHub-mappe, hvis jeg bare indleder den grundlæggende del af det/hovedparten med at sige, at det altså bare er et argument, hvorfor en kryptovaluta (KV) ikke kan overtage og blive en meget almindelig valuta på lige fod med normale penge. Så fik jeg tænkt på, at pointen i sig selv om, at et angreb kan være mere attraktivt at udføre, fordi man nemt kan ende med ikke at skulle betale alle de mønter, man satte på højkant i replay-angrebet, jo er en vigtig pointe i sig selv, som er værd at dele.. Nå, og der ved femtiden (måske lidt efter; måske tyve over) kom jeg så til at tænke på, at man jo faktisk ingen gang behøver at sætte penge på højkant, i bund og grund, fordi man bare kan forke og lave en gren, hvor man har (brugte!) penge, og så sætte dem på "højkant." I løbet af den næste halve time derfra (omkring tyve over fem altså) fik jeg så en masse gode supplerende idéer (nogen af dem, som jeg har fået før i andre sammenhænge), nemlig om at man jo kan snyde med tiden og dermed sørge for at booste mining-farten en smule til at starte med (ved at skrue tiden frem en smule, nemlig til.. tja, eller også kunne man egentligt bare starte fra nutidspunktet, det ville måske være det nemmeste..), hvilket både gør at man hurtigere kan danne blokke trods stærkt formindsket minin-kapacitet (og man bør i øvrigt helst forke lige inden den blok, hvor "målet" (the target) bliver sat ned) og altså hrutigere kan få sine malicious kontrakter ud, og også gør at det bliver noget nemmere at tiltrække minere (for lønnen er jo den samme pr. blok). Derfra tænkte jeg så også, at man bare kon offentliggøre kontrakterne forud for at blokkende bliver minet. Og så tænkte jeg på, at man jo så også kan rekrutere alle, der har solgt KV siden fork-punktet, for man kan jo gentage alle de kontrakter, medmindre de har betingelser, der nævner tidligere blokke i kæden (som jo nu vil blive erstattet). At sælge sine mønter med smartkontrakter, der nævner, hvad tidligere blokke er, kan så i øvrigt være en måde at sikre sig mod sådanne angreb *(nå nej, man sikrer sig jo ikke herved, så never mind den del (her til venstre for denne kommentar)), hvilket på en måde faktisk bare er godt for angrebet, for det gør det jo bare nemmere at rekrutere folk, nemlig som har sikret sig, at de ikke selv kan udnyttes. Nå, men man kan også gøre endnu mere, tænkte jeg på: man kan også starte med at gøre så den ekstra miningløn, som angreberne sætter minerne i vente på angrebsforken, ligesom bliver administreret på en sidechain af angrebsforken. Her skal folk så kunne melde sig til rimeligt frit, og man kan så implementere, at man kan udlove dusører på betingelse af, at den endelige blok overholder nogle ting, hvorved man altså kan gøre så dusører bliver betinget af, hvilke kontrakter der kommer med, og hvilke ikke kommer med (når man arbejder sig hen imod nutidsdatoen). I øvrigt (tænkte jeg på lige nu her) kan man jo potentielt set, hvis man skal gøre det virkeligt sofistikeret, sørge for at angriberne har en vis frihed til at lave bestemmelser over de dusører, de allerede har udlovet, hvor de altså så får en vis frihed til at vælge, hvilke eksisterende kontrakter skal med og ikke med, men hvor det så skal sikres, at de ikke har mulighed for at skabe nogen modstride og gøre deres dusør-kontrakter ugyldige herved. Og ja, ellers kan man jo også bare have det sådan, at de betingede dusører udstedes, når der er behov for dem, for angriberne kan jo sagtens give flere og flere dusører løbende (og hvor "angriberne" altså også med tiden kan indbefatte flere og flere).. (13:04) ..Nå ja, og prikken over i'et, som jeg kom til at tænke på omkring ti i seks (ja, det gik ret hurtigt, kan man sige ..tja, men mange af "idéerne" minder jo trodsalt rigtig meget om tidligere idéer..), er jo så denne pointe, nemlig at angrebskæden faktisk har en fordel over for den "uskyldige kæde"/den originale ("rigtige") kæde, fordi angriberne jo, i modsætning til folk på den "rigtige" kæde, der skal prøve at gå til modangreb, ikke "mister" noget som sådan, når de udlover dusører, for de penge har de jo allerede brugt!. (13:08)
..(13:09) Okay, nu ser min angrebsidé jo faktisk ud til virkeligt at virke, hvilket ville være kæmpe stort, for det kommer bare til at booste interessen for mit andet arbejde \emph{så} meget mere, kan jeg forestille mig. Så hvis jeg ikke tager fejl i det her, så har jeg altså nu potentialet til, med en rimelig kortfattet tekst (jeg vil bare forklare det simpelt; i kortfattede, måske to-tre-sætnings-lange, paragrafer), at opnå, hvad der ift. mit fysik projekt vil svare til vildt meget arbejde (hvis vi tænker på sådan noget som at arbejde på "future work"-emnerne, og også bare sådan noget som at rette min artikel godt igennem, så den kommer til at fremstå skarp --- det bliver pludesligt ikke nær så vigtigt, hvis jeg også har denne kæmpe nyhed om blockchain (der endeligt kan bane vejen for mere "grønne" blockchains(/soft forks)))..! :D:) (13:15) ..Hm, det er i øvrigt sikkert også det \emph{helt} rigtige tidspunkt at komme med sådan en idé som denne..!.. 
...(13:33) Hm, og det er jo klart, at dette stadig mere er et argument for, hvorfor nuværende KV'er ikke kan fungere som konventionelle betalingsmilder, for det vil aldrig være attraktivt at lave et konspirationelt replay-angreb på en kæde, der så bare mister al dens værdi (hvilket jo er rigtig godt; det gør jeg jeg ikke behøver at have skrubler over at udgive det). Men en etableret kæde, hvor store dele af samfundet pludselig har stake i kæden, det er en anden snak, for så kan værdien nemlig holdes oppe af denne stake. 

(18.09.22, 11:53) Glem alt hvad jeg har skrevet her ovenfor. Nu hvor jeg har fået mere hjerne igen (har sovet godt i nat, men kom allerede frem til i går aftes, at mine tanker her ovenfor ikke kunne bruges), kan jeg se, at det ikke holder. Især ikke den del med, at angriberne "har en fordel".. ..Og de andre ting holder bare heller ikke rigtig; gider næsten ingen gang forklare hvorfor.. Nej.. ..Nej, lad mig bare strege hele den.. hvad skal vi kalde det? undersektion?.. fra i går.. Ok. 




## Planer

(02.09.22) Nu hvor blockchain-idéen lidt er ude af vinduet, så tror jeg let det kan blive en langtrukken proces om at slå igennem. Jeg er så ved lige at planlægge, hvad jeg skal gøre efter min udgivelse. Jeg er lidt kommet frem til, at jeg skal starte med at give nogle korte ("less is more"-agtige) udgivelser (i min GitHub-mappe) omkring mine web 3.0-idéer --- hvor jeg i øvrigt måske kan fokusere lidt på "ratings" i det semantiske web, som en af de gannemgående idéer, men så ellers også bare det at starte det ud fra web 2.0-sider og wiki-sider; ting som virker allerede, men som kan forbedret af brugerdrevet semantik.. ..Nå, men jeg har lige nogle få ting, jeg lige skal planlægge færdigt omkring det.. (11:10) ...Ah, jeg tror måske, jeg ved hvad jeg gør.. Måske lader jeg bare være med sige, at der er mere ved idéerne end hvad jeg skriver --- ah, og måske også skriver om dem som om de er meget nye idéer (hvad de på én led også er, kan man sige, selvom jeg dog har gennemarbejdet dem lidt (men ja, jeg kan så lidt lægge skjul på, at jeg har gennemarbejdet dem overhovedet, og præsentere dem bare som nogle idéer, jeg gerne vil arbejde videre på.:))) --- og så tænker jeg nemlig særligt også at præsentere min forretningsidé som en helt ny idé, jeg godt kunne tænke mig at arbejde videre på.. ..Hm, men jeg tænker nu dog lige at vente en ekstra omgang stadigvæk med at udgive denne idé-skitse af min forretningsidé. ..Ah, eller endnu bedre ift. at kalde dem nye idéer: Jeg kan bare sige, at de alle er ret nye; at jeg har haft lidt tid til at overvejet dem hver især, men at de dog stadigvæk er på design-stadiet (og på et stadie, hvor de bør overvejes endnu mere for at finde fejl og mangler i dem)..:) Nice.!.. For så har jeg dækket ryggen på en fin måde, og så kan jeg bare lade pitchene tale for sig selv. ..Nå ja, og jeg skal så heller ikke reklamere med, at jeg har en forretningsidé til at starte med: Pointen er lidt \emph{ikke} at give folk opfattelsen af, at jeg muligvis sidder og gemmer på en guldgrube --- ikke før det kan betale sig. (11:42) (For på den måde tror jeg, jeg vil få meget mere positiv energi, og nemlig forhindret en masse potentiel negativ energi (bl.a. fordi folk kan få en negativ reaktion til en, de mener, har munden for fuld).) (11:43)
(03.09.22, 11:09) Tror faktisk endda lige jeg venter en omgang med at nævne min wiki-idé og dabatside-idé også. Til gengæld kan jeg måske nævne ret hurtigt, at der er en/nogle ekistensteori(er) på vej også. Men det kan jeg lige se på. Det er forresten lige før, at jeg vil arbejde på at skrive en uddybende tekst (på engelsk) omkring min forretningsidé først, før jeg begynder at skrive om mit selvadjungeretheds-bevis.. 

(04.09.22, 10:11) Jeg skal faktisk lige overveje noget mere, om det nu også er klogt at offentliggøre sine idéer, så de gængse sider faktisk nærmest får et forspring. Jeg tror muligvis, det går, men det må jeg altså lige tænke mere over i mine pauser.
(14:58) Ja, det går; jeg skal helt sikkert bare offentliggøre de idéer med det samme. Jeg skal i øvrigt også huske at snakke om brugergrupper og anonymitet i disse, men ja, jeg vil jo bare gå igennem alle mine punkter, npr jeg når dertil, og så udgive om alle dem, der kan forklares rimeligt kortfattet, nok på nær dem der har med vidensdeling og debat lige i første omgang, dog. (15:00)


(13:17, 17.09.22) Jeg tror muligvis jeg snart vil begynde at oplaude ting til min note-mappe, og måske vil jeg faktisk også allerde begynde at lave nogle små tidslåse (bare over et par måneder), f.eks. til min blockchain-angrebs-idé (hvor jeg så på den anden side vil udgive løsningsforslaget med det samme). 

(11:58, 18.09.22) Hm, nu overvejer jeg faktisk bare at give hints til koden, og så måske lave en kode (også) bare af nogle danske ord sat sammen (og måske gentaget tre gange, hvis det ellers er kort). ..Det virker på en måde sjovere, end det andet, og jeg tænker alligevel faktisk at åbne det hele før snarere end senere.. ..Og jeg overvejer også faktisk lidt at pitche min forretningsidé i samme omgang, som jeg pitcher mine idéer til en ny web 3.0-bølge.. ..Men det må jeg jo lige tænke nærmere over i de kommende dage (i min "fritid").. (12:03)

(19.09.22, 16:11) Okay, nu har jeg nogle meget bedre planer..! Jeg er kommet frem til her i dag (i en pause fra at tænke på fysik her på min seneste gåtur). Det korte af det lange er, at jeg bare skal forklare om min forrestningsidé med det samme, og så ikke være bleg for at sige: jamen, jeg er sikker på, at dette bliver den næste vildt store ting; langt langt større end BitCoin osv. endda (hvad jeg jo helt klart også tror på selv). ..Jeg tror lige, jeg vil vende tilb.. nå nej, det hører jo alligevel til ovenfor. Ok, så jeg har også nogle idéer til, hvordan man kunne opfordre til at starte det meget simpelt, nemlig med en kickstarter og med nogle løfter (som man så hurtigst muligt skal underbygge med kontrakter). Og dette vil jeg så altså også bare lægge op til, når jeg (meget hurtigt efter min fysik-udgivelse) skriver om idéen i min GitHub-mappe.. ..Ok, lad mig vende tilbage hertil, og uddybe lidt mere, hvis jeg synes, jeg bør sige noget mere...
%..(16:21) Nå jo, lad mig også lige nævne, at jeg nu også har tænkt mig at give en kort note om, hvordan der ikke skal "særligt meget uendelighed til" i multiverset for at man når frem til, at vi basalt set genfødes som alle levende væsner igen og igen --- og at man derfor kan tage "what comes around goes around" fuldstændig bogstaveligt, for vi skal på den måde alle opleve de samme glæder og smerter. 
%(17:38) Jeg tænker så også bare at lægge alle mine noter ud med det samme, bare til \emph{hvis nu}, nogen skulle være interesseret i at sneak-peak'e (og ofre noget tid), inden jeg får skrevet mere sammenhængende noter over emnerne (de vigtige af dem i hvert fald). Og så tænker jeg i øvrigt også bare ikke at lægge skjul på, at jeg tror min forretningsidé, hvis den udbreder sig til andre områder, kan blive lidt en kur til de negative sider af kapitalismen (men endda uden at bryde med kapitalismen; forretnings baserer sig helt på et frit markede osv.!). (17:41) 


(18:07, 27.09.22) Jeg tænker nu lidt faktisk at starte med at skrive om min forretningsidé i sig selv (og som jeg lige har skrevet ovenfor, bliver dette altså uden at hype den som den næste store investeringsdrøm, for det er jeg kommet i tanke om, at den jo nok ikke vil være som sådan..). Jeg vil så forklare om, hvordan den fungerer, hvad den lover for fremtiden, hvad hver part vil få ud af den, hvorfor forbrugere i det hele taget burde have magten i et kapitalistisk ("forbruger-")samfund, hvem det kan komme til at gå ud over, og, også rimeligt vigtigt, hvorfor det egentligt ikke behøver at gå ud over de rige som sådan (og hertil hører også en lille idé om, at man kan slå to fluer med ét smæk og gøre idéen mere attraktiv for de rige, og forhindre, at boligmarkedet eksploderer i fremtiden, når bevægelsen har slået rod og de velhavende naturligvis vil begynde at lede efter steder, hvor de alligevel kan få deres penge til at yngle..) (For hele pointen med idéen er jo netop, at man når til et ret lige samfund, hvor der stadig er masser af plads til rigdom, men hvor rigdom i sig selv ikke bare kan yngle; hvis rige mennesker skal gøre sig selv rigere, skal de gøre dette ved at bruge deres (eventuelle) talenter til at forbedre samfundet, i.e. de skal gøre et stykke arbejde (medmindre selvfølgelig, de har haft held med at lave og opretholde en privat virksomhed, der ikke følger princippet i min forretningsidé, for sådanne skal bestemt ikke forbydes --- der skal jo ikke laves nogen som helst nye regler/love i samfundet i princippet i forbindelse med min forretningsidé).) (18:18) ..Og så vil jeg så forklare, hvad jeg lige har nævnt ovenfor under den relevante sektion, nemlig at man kun i visse tilfælde kan forvente, at der kan blive en stor investeringsdrøm i det, og at én (vigtig) mulighed her lige præcis er (nogen af) mine web-idéer, fordi sådanne virkeligt kan have gavn af, at brugerne har magten (og ved at brugerne af systemerne altid vil have dette). Og det vil jeg altså så nok slutte af med at referere til, og så må jeg jo se, hvornår jeg så får skrevet om de emner efterfølgende.. (18:26) 


(11:59, 19.10.22) Okay, der er ændringer i mine planer. Min fysikartikel ser faktisk næsten ud til at være lidt et flop muligvis.. Jeg skal lige have overvejet alle tingene og læst godt op på literaturen, men ja, der er altså muligvis ikke så meget nyt i den. Mine nye planer er så, at jeg skriver et samlet dokument om lidt af hvert: min forretningsidé, e-demokrati, og nogle af mine web-idéer/forudsigelser, samt også om eksistensteori, og så regner jeg med at gøre den samlede overskrift til noget a la "idéer og forudsigelser..".. Hm, eller jeg har også tænkt bare "En lys fremtid".. Men ja, pointen er: Jeg har nogle forudsigelser om den nære og fjerne fremtid, som fortæller, at denne er rigtig lys (og så har jeg også lige nogle eksistensbetragtninger, som gør det hele endnu mere lyst at tænke på). Så altså en meget positiv tekst om den nære fremtid (og idéer til at gøre denne bedre), om den fjerne fremtid også (og det vil jeg i øvrigt også skrive lidt om (at vi nok får et godt post-scarcity-samfund på et tidspunkt)), samt også hvorfor vi sagtens kan gå at glæde os over en lys fjern fremtid, også selvom vi ikke selv kommer til at leve iden (hvor vi altså snakker mine eksistensteori-betragtninger). Det vil jeg altså gå i gang med nu, og så vil jeg prøve også at arbejde lidt på at reparere min fysik-artikel om aftenerne. (12:07)   




## Diverse tilføjelser her i starten af 2023 (muligvis afrundig på dokument)

(04.01.23, 16:36) Okay, jeg har lige fået ig en ny computer, efter at min forhenværende gik i stykker i julen, og har lige fået installeret Linux. Her under nytårsræset (nærmere bestemt natten til d. 31.) fik jeg tænkt lidt over Eksistens igen, og det har jeg også brugt de følgende dage på indtil nu, imens jeg ikke havde en funktionel computer. Jeg har et par nye tanker om emnet, og har også en nogenlunde idé til, hvordan jeg ville strukturere en lille artikel om det. Det vil jeg derfor lige skrive om ovenfor i 'Eksistens'-sektionen, efter (..uh ha, det er et lidt drilsk tastatur, fordi man godt kan trykke ned på siden af en tast uden at den registrerer et ryk.. he, meget passende: jeg skulle have skrevet 'tryk'..) at jeg har skrevet denne sektion. 

Jeg kan starte med at skrive, at selvom jeg tror på at mine idéer omkring tag-ratings, kommentarkategorier, fanetræer og alt det omkring en web 2.0--3.0-side virkeligt indeholder et kæmpe potentiale (også rent forretningsmæssigt), så er det altså ikke sikkert, at de idéer bare sådan lige vil føre et semantisk web med sig med det samme.. Måske vil det hjælpe, at folk bliver vent til at diskutere i træer/diskussionsgrafer, når det kommer til kommentarer, men det kommer nok ikke til at accelerere overgangen til en videns-/diskussionsgraf (med sandsynligheder på, altså en "p-ontologi," som jeg før har kaldt det) super meget. Det tror jeg til gengæld nu mere mine debatside-idéer vil. Og også mine e-demokrati-applikations-idéer. For e-demokrati-idéerne kommer jo måske nok til at give incitamenter til folk om at deltage i (graf-)diskussionerne, og mine debatside idéer vil i øvrigt også åbne op for, at folk vil være meget mere villige til at lytte til hinanden på tværs af grupper, nemlig fordi man gør det til en udfordring: "Hvis du tror denne anden gruppe er helt henne i vejret, hvorfor så ikke bevise det via en grundig diskussion med en saglig og upartisk 'dommer' (eller flere) for diskussionerne?" Så det tror jeg altså mere kunne være dét, der kan sætte skub i den udvikling. 

Nå ja, og så kan jeg også sige, at jeg i går eller i forgårs kom til at tænke på, at (semantisk struktureret) NLP måske kan gå hen at blive en virkelig vigtig kilde til, at vi en dag får videnskabet semantisk struktureret (hvad ville være SÅ (SSSÅÅ!!) godt), hvis nu ikke semantic web-løsninger når at komme inden om først og tilveje bringe den realitet på en mere direkte måde (altså mere via menneskeligt arbejde frem for hjælp fra en A.I.). For når først den tekologi bliver udviklet tilstrækkeligt, hvad den sikkert gør i en ikke alt for fjern fremtid, så vil man ret nemt kunne opnå et kæmpe spring fremad simpelthen ved at få en A.I. til at lave et stort forarbejde med at analysere al eksisterende videnskabeligt arbejde og fylde det ind i en stor semantisk struktureret graf. Og når først man så har sådan en graf, så kan man derfra ved hjælp af menneskearbejde og v.h.a. yderligere forbedriger af A.I.'en (og altså ikke mindst ved en synergisk sammenblanding af disse) relativt nemt få udbygget denne store graf, så man får struktureret al viden semantisk. Så NLP-teknologien kan altså sagtens komme til, hvis ikke sem-web-løsninger kommer indenom først, at blive utroligt vigtige for den samlede videnskab (og dermed hele vores teknologiske udvikling). Og det virker som om, at den allerede er godt på rette spor, for der tænkes vist allerede godt i, hvordan man kan gøre NLP-algoritmer mere semantisk funderet (hvilket jo er klart, for så må man jo i sidste ende ende ud med et mere "intelligent" produkt). Så lad mig da endeligt huske mig selv på at følge noget mere med i den (spændende) udvikling. 

Udover min e-demokrati- og debatside-idéer, som jeg på et tidspunkt vil prøve at iværksætte (er planen), så har jeg altså nogle forskellige politiske og økonomiske idéer. Nå ja, og én af de politiske idéer handler jo så om at bruge en e-demokrati-app, nemlig til at lave et e-demokrati-parti.. ..Hm, jeg skal forresten nævne, at man også kunne forestille sig e-demokrati-appen mere som et socialt medie, hvorved altså alle personer og grupper altså enten direkte eller inddirekte har adgang til hinanden, og mere eller mindre deltager i det samme "digitale rum," kan man kalde det, når de bruger appen. Og så kan alle mulige befolkningsgrupper, og alle andre typer grupper (hvis ikke befolkningsgrupper er et altomfattende udtryk), altså finde sammen på appen og diskutere og forhadle med alle andre grupper på mediet. ..Og ja, så tænker jeg altså også, at man kunne blande appen sammen med debat-appen, således at e-demokrati-grupperne også kan oprette diskussioner/debatter med hindanden ovre i det andet ben af appen, der så altså er "debatside"-delen. (17:18)

Men ja, og udover disse ting, så har jeg altså mine idéer til nye typer af forbrugerforeninger (som forklaret i mit nye dokument ('consumer unions.pdf')) og mine idéer omkring "share-redistributing companies," hvilke jeg begge tror kan blive virkeligt store, ja, og sådan set ligefrem neutralisere alt (ikke ALT, men ~alt) hvad der er skidt i den nuværende udgave af kapitalismen, således at vi kan få en meget bedre form for det. Så planen er altså, at jeg vil prøve at diskutere SRC, CU, og.. EDP (e-demokrati-parti) med andre politik-interesserede, og så prøve at fokusere mit ejet faglige arbejde i retning mod at kunne iværksætte ED- og debatside-app --- bl.a. faktisk også ved simpelthen at læse open source kodebaser i min fritid (når jeg får sådan en igen). (17:25)

Hm, var der mere jeg skulle sige her (som ikke omhandler Eksistens)..? Der ver lige en lille ting, og det er bare, at jeg lige tænkte på igen, at hvis man som befolkning skulle åbne mere op for overvågning, så skulle det være via personlige kamerare, som alle folk helt selv er frie (ikke bare lovligt men også fysisk) til at slå fra og til, hvornår det skal være, og hvor det alt sammen sendes til datacentre, som er offentligt overvåget via kamerarer og lydoptagelser --- og digitale diagnosticeringer --- hvor man så bare altså har alt indholdet krypteret hele vejen, også hvis nu det skal bruges af ejeren og altså dermed sendes tilbage igen.. Men ja, jeg har sikkert sagt (skrevet) noget tilsvarende en gang.. ..Men jeg kom fra: var der andet, jeg skulle skrive her..? 

..Det tror jeg ikke, og ellers vil jeg bare lige vende tilbage. Så nu vil jeg altså skrive nogle tilføjelser til Eksistens ovenfor. Derefter vil jeg gå i gang med at se på at skrive (/ om jeg skal skrive) min SRC-artikel om og/eller skrive en ny version. Samtidigt med dette vil jeg så også tage kontakt til flere omkring det, og her tænker jeg så særligt på en vis tænketank, jeg er blevet fortalt om: Demokratisk Erhverv. Og når jeg så endelig når til et holdt/en pause i det, så vil jeg faktisk se på at få skrevet en Eksistens-artikel. Og ja, derefter snakker vi så at arbejde mig frem mod at kunne realisere mine e-demokrati- og debatside-idéer. Nå jo, jeg skulle forresten også nævne, at min wiki-idé nok heller ikke i sig selv bliver sindsygt accelererende for sem-web-udviklingen, mener jeg nu, selvom den nok dog vil kunne blive en rigtig god ting med tiden. Og lad mig så lige påpege, hvis det giver mening, at det nok er for meget at håbe på, at en videnskabelig semantisk ontologi vil kunne indeholde de helt grundlæggende argumenter til at starte med. I stedet bør man nok huske kun at sigte efter i starten, at den bare kan indeholde samlede videnskabelige værker som dens grundsten, hvilke så godt nok dog kan få noget semantisk data omkring sig om, hvad de siger, men som man dog altså ikke kan forvente skal splittes ad til atomer i den semantiske struktur. Jeg håber, det giver mening.. ..Man må altså forvente, at videnskabelige værker, der belyser et emne, må indgå so mere eller mindre atomare grundsten i den videnskabelige ontologi/semantiske graf til at starte med, og så er det nok først på meget længere sigt --- medmindre NLP-/AI-teknologien virkeligt kommer til at sparke røv på det punkt ---  at man kan forvente at disse værker i sig selv bliver splittet ad til semantiske atomer. ..Og når vi f.eks. særligt snakker e-demokrati-og-debatside-diskussioner, så må man også i høj grad forvente, at folk skal gøre brug af tredjepartsinstanser (altså forskere/forskergrupper, tænketanker, fagfolk, eksperter, ordfører, osv.) og hvad de siger/skriver om en ting, mere end at man skal forvente at hver del af alle disse analyser også bliver uploadet til mediet og behandlet online af brugerne (og analyseret semantisk). Selvfølgelig er det sundt jo mere der kommer online og bliver behandlet der, nemlig således at alle kan få indblik i detaljerne, og så der altså er høj gennemsigtighed, og så det er nemt at finde og udpege fejlslutninger m.m., men i sidste ende må man altså nok regne med, at dette ikke er muligt, og at man er nødt til bare at inkludere værker og udsagn fra andre som grundlæggende, atomart materiale for diskussionerne, uden at dette materiale bliver taget ind og splittet ad (og analyseret) på selve siden/i selve appen. (17:52)

..Men ja, nu fik jeg vist skrevet alt, hvad jeg ville sige, lige inden jeg kom med den sidste tangent her, så jeg kan vist bare afslutte her.:) Det er altså planerne, som de er nu, og for det første vil jeg altså så lige prøve at tilføje nogle ting om Eksistens ovenfor. (17:54, 04.01.23)


\end{comment}




































































%
%
%
%\chapter{E-democracies} \label{E_democracies}
%
%The concept of a so-called `e-democracy' is not a new one. Wikipedia thus has (in the moment of writing) a whole article about the overall concept that one can read. (That article, in its current form, defines the concept perhaps a bit more abstractly than what we need for our purposes here, but it might still be helpful to glance at.) In this section, I will therefore not introduce the overall concept, but simply give some short notes on how one might implement such an e-democracy, which can for instance be used to govern a company like the ones described in the previous chapter (as its shareholders), or a political party, etc. 
%
%
%\section{A basic digital application where voters can build proposition graphs}
%
%Imagine a digital application where all voters in a given democracy (concerning e.g.\ a company, a union, a political party, etc.) can log on and build a proposition graph together, which can then define the policies of the body governed by the given democracy. We are here talking about the `graphs' of mathematical graph theory. (One can make a brief search the internet for `graph theory' to see what this is about, and one might then also want to search for `directed graphs' and `connected graphs' at the same time.) 
%
%Each node of the graph holds a proposition, which is simply expressed in plain text of whatever natural language (such as English) is appropriate for the case. 
%
%When adding a new node to the graph, one can add it by itself (i.e.\ not connected to any other nodes) or add it with at least one of two kinds of (directed) edges to an existing node. The two types of edges then represents whether the node's proposition is an elaboration on the parent node, or if it is a self-contained proposition that should, however, only apply conditioned on the parent node being active.
%
%A node becomes active if it has enough votes and if a majority of those votes are positive rather than negative. Whether `votes' are counted simply by number (such that all voters have equal power) or if the votes are weighted (meaning that some voters have more power than others) of course depends on the case. 
%
%The point of being able to `elaborate' the proposition nodes rather than having to replace it with a more detailed note instead is simply to make the work easier for everyone, and also to make the graphs easier to read. It means that the policies can be defined somewhat loosely at first (and therefore much more quickly and easily), and whenever some vagueness of the propositions is discovered subsequently, either by people studying them or because of a relevant case that reveals it, the voters can then work to specify and mend the propositions. 
%
%The point of being able to add proposition nodes that are conditional on their parent nodes being active is of course some propositions might only be beneficial to implement given that certain other ones are already in place. If a somewhat fundamental proposition node is voted inactive again, it is thus convenient that such `conditional child nodes' follow along. If the parent node is then voted active once again (or perhaps for the first time) at a later time, all the child notes that has retained a positive voting score in the meantime will then become active one again, as well as any child node whose score has become positive in that time. 
%
%The application might also allow these `conditional child notes' to have several parent nodes for convenience. 
%And the same could also apply for the `elaboration child nodes' since there might be case where it could be beneficial to be able to elaborate the interpretation when two propositions nodes are active at the same time, for instance if these two proposition have a slight conflict with each other, or if the create some other issue that needs to be handled when they are both active together.
%
%`Elaboration child nodes' should of course also depend on their parents being active. The difference between a `conditional child node' and an `elaboration child node' is therefore essentially only in the interpretation: The propositions of `conditional child notes' and those of their parents are meant to be independent of each other as statements, whereas `elaboration child nodes' are free to correct and override parts of the statements contained in their parent nodes, thus allowing these to not necessarily be absolutely precise and self-contained. 
%
%Every user should be able to add new proposition nodes and each node should also have a separate `interest score' that users can rate (with the same weights on the votes as for the first score in the case where these are weighted). A proposition node whose `interest score' exceeds a certain threshold becomes visible to all users in the main graph, and people will then have to give their votes to it, if they want to influence whether it is applied or not. 
%
%Users should thus also be able to view nodes in the graph that has not yet exceeded said threshold, perhaps by being able to select various ranges of interest scores to look at. It might also be beneficial to let such nodes expire after a certain time if their `interest score' has been low enough for too long. 
%
%Users should also preferably have their own `workbench' with enough storage capacity to hold a number of propositions nodes. If a proposition node expires, they can thus make sure that the work is not lost as long as they keep said proposition on their own `workbench.' It would probably be beneficial also if users could then have shared `workbenches' as well, where they can collaborate on making new proposition nodes. 
%
%Anonymity is of course generally very important for democracies. So it is naturally very important that no one can see which user has added what nodes, unless of course they have collaborated on it from the same `workbench.' Users should also not (for most cases of democracies) be able to see which users has voted for what. 
%%
%For cases with weighted voting, either with very few voters or with very precise weights, this might be helped further by making sure that the exact voting scores are not visible to the users, and that the can thus only see a number that is rounded to a less precise floating point number. One might also implement intervals such that new votes are always declared together in groups, some time after they have been cast individually. 
%
%
%\section{How the proposition graphs are used to govern a body}
%
%The point of building these proposition graphs is then that the leadership of the body you are governing should to some extent be required to follow the active propositions, at least within some basic limitations of they can be required to do. 
%
%When the proposition graph changes, they leadership should be required to implement these, but here it might of course be a good idea to implement some delays on when new changes are supposed to be carried out. One might thus rule that a change should only be implemented after a certain period from when happened, and only if that change has remained active in the proposition graph during that period. 
%
%If the proposition graph gets some contradictions and/or ambiguities that makes it hard for the leadership to know what to do, they should also be allowed to postpone implementing the relevant changes until the voters sorts out the issues (by which they make some new changes which restarts the acceptance process). 
%
%How to make sure that the leadership follows the democratic decisions of the proposition graph? Well, by making sure that the voters also have enough direct power over the governed body to enforce their will. This might typically be ensured by the group of voters having the power to fire leaderships and/or decrease or increase salaries, thus giving this group ``sticks'' and potentially ``carrots'' that they can use to make sure that the hired leadership does what it is supposed to.
%
%
%It has to be mentioned that a high level of transparency is an all-important part of an effective e-democracy when it comes to the body that is being governed. Luckily, one can say that as long as the voters have the aforementioned ``carrots'' and ``sticks'' at their disposal, they should at least be able to make the body more and more transparent, even if it not very much so from the beginning.
%
%
%
%%Husk:
%	% Fortolkningspolitikker (inkl. hvad man gør, hvis der er modstride) og delays. (tjek)
%	% The point with having conditional propositions.. (tjek)
%
%
%\section{A more advanced application}
%
%A basic system like the one described above is good enough for very simple cases where it is okay to just have a majority rule. But for more complex cases where there are a lot of groupings of voters with different interests when it comes to various topics, to have such a majority rule is not really sufficient. If we for instance think of the policies of a whole country, this is a good example of such a complex case, where most people probably have \emph{some} special interests that are only shared with a fraction of the society. In such a system, it is important for people to be able to \emph{negotiate} with their voting power in order to get what they want, not just to always vote for exactly what they want as individuals. One group might thus want to meet with another to make a deal where they say: ``If you vote for this and this, even though you might not be particularly interested in that, we will vote for this and this which you do have a particular interest in (even though we might not).'' 
%
%In order to accommodate these realistic needs of its users, the digital application in question should therefore also first of all make it possible for users to form groups in the system. On a technical note, having such groups can of course be implemented in a lot of ways, but I can suggest an implementation where the creators of a group start out with some divisible `moderation tokens' that give them power to decide who can join the group and who gets kicked out, and where they are then free to transfer parts of (or all of) these tokens to other users within the group at any time. This moderation system is open enough that the users can implement any other moderation system on top of this, if only they trust some central party (which can then control a user profile in the system) to enforce the results of this external moderation system.
%
%And in order for such groups to be able to start negotiating with their voting power, it is then first of all important that some overall statistics (perhaps where numbers are rounded to ones with less precision for the sake of anonymity) of how a group votes on average is made public at all times. Otherwise a group who has made a deal with another group would not be able to check that this other group holds its promises. 
%
%With this addition to the application, groups can now in principle make all the deals in private that they want to. But of course, a good implementation of an `e-democracy application' would also afford its users with ways to make these deals within the digital application, online. 
%A way to do this might be to add what we could call `conditional votes' to the system. A `conditional vote' is then a vote on a proposition, whose sign can depend on other factors. In particular, a conditional vote should be able to depend on parameters regarding the voting statistics of groups. A group that want to make a deal with another one can then decide to make a conditional vote for something the other group wants and then make the condition such that this latter group has to vote for something the former group wants to unlock the conditional votes. 
%
%On another technical note: Depending on how the system is implemented, such conditional votes might be able to cause deadlocks, where two or more conditional votes all wait for the other to turn the other way in order to turn themselves. But a way to mitigate this is to give a direction to all conditional votes which denotes the sign expected from a successful deal. The system can then continuously refresh the proposition graph by turning all conditional votes in their positive direction and then see if they fall back to the same state or if they settle on a new state, which will mean that some deadlock has been conquered. 
%
%And on a design-related note: The conditional votes can be visualized/rendered as leaves in the graphs, each one attached to a certain proposition node. The users can then create and add these `conditional vote nodes' to the system in the same way as proposition nodes are added, and then all users can decide to cast their vote for the given proposition either by casting it (unconditionally) on the proposition itself or casting it instead on one of the conditional vote nodes (meaning that their vote will now be automatically conditioned on some parameters of the voting statistics in the system, continuously, until they change their vote again). 
%
%%Another thing that a more advanced system might account for, is the fact that the power of the voters might not just have different weights but might also be dependent the area that a given proposition deals with. This could for instance be in a company or a government where there are many departments/ministries in charge of different areas. If such a company/government decided to go for a more democratic leadership, it might still want to keep some division of power within the democracy. This example is perhaps not the most realistic one so here is one that is more so:  
%
%
%Another very important thing that an advanced application should afford its users is to make sure that the voters can choose representatives. It might seem odd to want to implement a direct democracy only for people to end up choosing representatives once again, but is indeed exactly what a direct democracy should aim for. It is nowhere near feasible if the system requires all users to engage in all discussions and decision making in order for the democracy to work, not unless we have a simple case with relatively few voters who are all quite engaged. But in most cases that one could think of, being able to choose representatives and trust these with looking into specific and/or complicated matters and vote on the person's behalf is all-important. The problem with representative democracies that a direct democracy aims to fix should thus not be to get rid of representatives, but simply to ensure that people can change these much more rapidly should they want to, and also that any voter can always choose to look into specific matters themselves and choose to vote differently on those than how their representative has voted.
%
%An advanced e-democracy application should therefore also allow users to choose representatives. With `conditional votes' implemented, users can of course in principle just cast conditional votes on all propositions that they want representatives to decide for them, but this is too cumbersome and we can do much better than that. The application could thus first of all allow the voters to give their votes to others. But it is likely that some users will only trust a certain representative to decide for them in a certain area of concern. And in general, users will therefore probably want to be able to have multiple representatives at a time, each responsible for making decisions for the voters in specific areas. 
%
%I therefore propose that an advanced e-democracy application also implement what we could simply call `areas of concern,' which are then essentially groupings of propositions regarding a certain subject. Whenever a new proposition is added by itself (as what we could think of as the ``root'' of a connected graph) it should thus be given an area of concern such that the application can group it with proposition graphs with the same area of concern. And whenever a child node is added, it should of course get the same area of concern as its parent. With this implemented, users should then be able to give their votes to another user (i.e.\ representatives) when it comes to any specific area, which should then effectively mean that the user will automatically cast the same vote as their representative, at least when it comes to propositions that the user has not voted on themselves. (And one might then implement different settings to this, such that a user for instance might be able to even let a representative override the user's own previous votes.) 
%%(If someone creates an otherwise relevant proposition node but adds the wrong area of concern, one can expect that it will then simply not be voted forth (over the aforementioned threshold), not until the author gives it the right area of concern.)
%
%It now almost goes without saying that each `group' in the system can then potentially choose to have their own specific representatives that the members are recommended (or perhaps required in some special instances) to use. 
%
%The system might also implement `subareas,' such that any user can try to add one such to any proposition node. Users should then be able to vote such subareas in and out, and if one is voted in, the proposition node and all its children will then get this extra area, that users can then also choose to sign representatives to. With these `subareas' implemented, this then allows users to delegate different representatives to these, even if they are also part of the same overall area of concern. **(This paragraph will probably need some rephrasing.)
%%(One could also implement `subareas' simply by requiring that these are also added from the beginning when the relevant proposition nodes are created, but it might make it easier for the users if they can just change the subareas by vote at any later time (instead of having to recreate and substitute the whole subgraph, with similar nodes but with updated subareas).)
%
%
%%These `areas of concern' also allows for something else that might be very useful, namely that the same community of users/voters can govern a variety of bodies with the same overall (potentially disconnected) proposition graph. 
%%Because when the contracts and/or agreements concerning the various bodies' commitment to follow the proposition graph are external to the digital system, a body might as well agree to only be ruled ...
%%The usefulness in this, apart from maybe having everything gathered in one place, is that this will mean that voters
%%(16:06, 06.11.22) Hm, dette kan nok godt blive mere kompliceret, fordi flere foreninger så skal blive enige om, hvordan stemmerne fordeler sig, og så skal det lige pludseligt topstyres på en helt anden måde.. Så lad mig lige se en gang... ..Hm, men handler det så ikke bare om, at forskellige grupper skal kunne "genbruge" de samme propositionsgrafer, og også om at de andre gruppers aktivitet så også godt må kunne gøres synlig i samme propositionsgraf (altså for en vis gruppe, der bruger denne)..? (16:09) ...(16:29) Jo, men så har det så ikke rigtigt så meget med subareas at gøre.. ..Nej, for så skal man også simpelthen gøre, så at graferne.. er helt adskilte, ja, så måske giver det altså slet ikke mening.. hm, andet end at man stadigvæk kunne have graferne side om side, og mere vigtigt, at conditional votes så også kan komme til at afhænge af eksterne grafer.. Ja, er det ikke bare det..?:) 
%
%
%%Furthermore, each group should have its own page and/or own `area of concern,' where the members of that group have all the voting power. This is useful since it means that each group can then build their own proposition graph over its policies and opinions. A group might then also signal external actions via this proposition graph. For instance, a group the represents a workers union might create conditional votes in their own proposition graph that depends on some statistics regarding the main proposition graph.. 
%%say that: ``On these
%
%
%And lastly, it might be beneficial for various groups in society who govern different bodies, e.g.\ political parties, unions, organizations, companies, to also be able to negotiate with each other online and to view each other's policies. For instance, a company might want to say to a governing political party that: ``if you implement certain laws, we will move out company elsewhere.'' And a trade union might then say to a company: ``if you do not give us higher salaries, we will go on strike.'' These are thus examples where a group in society can use their power over one body (including simply themselves as a group) to negotiate concessions from another group with power over a different body. 
%So if the advanced e-democracy application really wants to afford its users with all that they could want for negotiating effectively with each others, it should allow different voter groups to come together in the same space. First of all, each `group' in a e-democracy should have their own proposition graph that only they have voting power over. This local proposition graph can then be used to signal the groups policies, opinions and potential actions. And when it comes to the `conditional votes' in this local proposition graph, these should also be allowed to depend on statistical parameters in the main proposition graph, outside of the local one. 
%And furthermore, different e-democracies (governing over different bodies) that uses this same digital application should also be able to invite another group to join together, such that the two e-democracies can have their proposition graphs shown side by side (but with the same distribution of voting power in each of these graphs), and more importantly, that one e-democracy can then start making conditional votes that depends on statistics regarding the other e-democracy and vice versa. 
%
%
%\section{An e-democracy party}
%
%There are of course a lot of different examples where an e-democracy such as this could be useful; political parties, companies, unions, organizations and other communities. %In this section, I will, however, only give some points when it comes to political parties and companies of the type that I described in the previous chapter.
%%If we ..
%And when it comes to political parties, there is the natural option that these are run only by their members. We could thus imagine two or more parties competing for power, each being run e-democratically by its members. But since politically parties are typically inclusive anyway, why not just strive to have one political party where every person in the society gets an equal vote? 
%
%I believe that such a party could gain massive support over time. It might start out as a small party, especially in the early days where people are still getting used to working with the proposition graphs, and when the technology is perhaps at an early stage. And then as the technology to matures and the userbase grows, more and more people would trust the new system enough that they would want to give their vote to this `e-democracy party.' That party might then, at least in multi-party systems, get some representatives in government and by that point, the interest in the party would grow further, since all registered users would then be able to have a say in the policies of those representatives. And if the technology works, more and more people would then see the potential in an e-democracy. This is especially true in countries where the people in general do not always feel heard by there politicians: When they then see that the resulting proposition graph for most people will fit their interest better than what the traditional political parties offer, they will want for the e-democracy party to be voted in as a ruling party. 
%
%Now, if the party thus lets member of society have an equal vote in it, this might then be problematic at this stage when the party want to take over from the traditional parties, since people might then be tempted to make their vote count twice, essentially, by voting for their favorite traditional party and then also using their vote in the e-democracy party. And since voting is anonymous, there is no real way that the e-democracy party stop this. That is, apart from taking steps to balance out this effect. The e-democracy party might thus choose to temporarily break its commitment to giving all people an equal vote in this phase, and instead promise that it will commit itself to try to counter all representatives in government that are not part of the e-democracy party by giving more votes internally to a group of representatives whom it deems are exactly at the mirrored end of the spectrum than the group of non-e-democracy representatives in government. (The chosen counter-group can, however, be much larger then the group of representatives it is supposed to counter.) This way, a voter who wants the e-democracy party to take over would not be tempted to cast their votes to a traditional party instead. It also means that once the party sits on a majority of the power in government, other representatives are more likely to join it while in power (if the relevant constitution permits such migrations of representatives while in power) if they see that the e-democracy is more practical since the e-democracy party can then just remove the appropriate amount of counterbalance as these former outsiders join. 
%
%
%E-democracies as governments of countries might thus be a much closer reality in the near future, than a lot of literature on the topic seems to suggest, at least in countries governed by a multi-party system. In two- or one-party systems, the development might of course be much slower. But then again, once some multi-party governments successfully switches to an e-democracy, the two- and one-party governments would then be able to analyze and copy the technology, at least giving them a much easier route to an e-democracy, should their voters want one. 
%
%
%
%\section{Anonymity}
%
%
%As mentioned, anonymity is often very important for a democracy, especially if we think about the case of governing a country. Therefore, the digital application should allow the users to vote anonymously. This can be achieved letting each user control an anonymous profile, but if information about which user has which anonymous profile is stored on a server, that server might be hacked. 
%
%So the question is, can an e-democracy system be as safe and anonymous as going to a box, drawing a cross in a field on a piece of paper and putting that paper into a box? Yes, actually: there are ways to ensure complete anonymity of the users where the anonymity is preserved even if the servers of the application is hacked.
%
%The following protocol allows a set of clients to each provide a server with a set of public keys such that each client knows the private key of exactly one of the keys in the set (and no one else but them knows this private key) but where no one knows which public key belongs to which client apart from the clients knowing their own key. The protocol is furthermore resistant to DoS attacks. 
%
%It works by having the clients take turns building blocks in a block chain, which we can think of as a `block spiral,' where the clients form a circle and where the turn to provide a new block to the spiral goes around in the circle. 
%
%\ldots\ \textit{Okay, jeg tror lige, jeg venter med at forklare om min idé her, for det kan godt være, at der findes en lidt nemmere måde. Det vil jeg lige tænke over. Men ellers er det en god idé, altså den hvor hver klient sender nogle nøgler videre til en tilfældig anden klient i kredsen (hvor hver blok krypteres med den næste klients offentlige nøgle (fra begyndelsen) og sendes til denne), og hvor klienter, der modtager nøgler gerne skal sende dem videre og slette dem fra hukommelsen. Herved vil man meget sjællendt kunne se, hvem var den oprindelige sender af en nøgle (medmindre både modtagerklienten og klienterne for og bag brugeren er ondsindede), og selv hvis den bliver sporet tilbage kan pågældende klient bare sige, at ``den nøgle kom fra en tidligere omgang og altså fra en helt anden bruger, men jeg har altså slettet data om, hvor den kom fra, som jeg burde.'' Men ja, jeg tænker nu lige lidt mere over det, inden jeg skriver denne sektion færdig. .\,.\,Jeg har i øvrigt også tænkt mig at sige, at man efter at have brugt denne protokol så bare kan bruge et VPN herfra, men hvis man vil være endnu mere sikker, så kan man endda bruge helt den samme protokol til at indsende data om, hvordan man vil stemme med sin profil, hvor man så altså bare erstatter de (tilfældigt) genererede nøgler i protokollen med tilfældigt genereret data samt det faktiske data, man vil indsende, og til sidst så offentliggør man så bare, hvilket skrald, man har sendt ind, men ikke den faktisk data, man så lader serveren beholde. (.\,.\,Så kan det dog godt være, at man skal ændre protokollen lidt, så man lige sørger for, at hver mængde data også vil nå slutningen af protokollen, så at ingen data-klumper bliver tabt i protokollen --- medmindre der altså er sket en synlig fejl i protokollen.)}
%
%\ldots\ \textit{Nej, der er vist en nemmere protokol, hvor man vist nok også kan finde frem til en DoS attacker. Man kan vist bare have et VPN, hvor klienterne sender beskeder frem og tilbage, og hvor de så kan pakke en nøgle ind i flere krypteringer med forskellige nøgler, hvor beskeden så skal sendes til alle de klienter i rækkefølge, som kan dekryptere beskeden en efter en. Og hvis så man gør det tilfældigt, hvor mange krypteringer, der skal til, så kan ingen igen vide, om en nøgle kom fra en person, bare fordi de får opsnappet, at beskeden på et tidspunkt blev sendt fra denne, for vedkommende kunne jo sagtens have fået den fra andre og så bare have sendt den videre. Og hvis man så har nogle få DoS'ere i netværket, så kan brugere der har sendt en nøgle der aldrig nåede frem jo pege på, hvem der kan have været de skyldige (af den række af brugere).\,. Hm, ja, men hvis man nu vil bevise det også, så kunne disse brugere.\,. Nå nej, man kan ikke bevise det på et VPN, men det gør vist heller ikke noget. For brugere skal jo stadig gerne sende flere nøgler pr.\ protokol, og hvor de så bare opsiger alle på nær én til sidst. Og hvis der så er en DoS'er i netværket, jamen da det ikke vil være fatalt, så må det være fint nok, at brugerne bare kan page dem ud nogenlunde. (Og hvis det så bliver et større problem, så kan man altid bare bruge den mere krævende blok-spiral-protokol, jeg har haft tænkt på.)} %(08.11.22, 10:27)
%
%
%
%
%
%
%
%%Hm, jeg har fået tænkt lidt over anonymitet, men det kan godt være, at jeg lige skal tænke lidt mere. Men jeg har altså fundet på nogen fine systemer til at skjule stemmeres identitet, og jeg tænker, at stemmere generelt skal kunne vælge enhver tredjepartsbruger til at videreformidle deres stemme anonymt. Sådanne kunne så med fordel få lov at give floating point værdier (i stedet for bare 0 eller 1) med deres stemmer, eller de kunne bare råde over et antal stemmer, således at de både kan give et antal positive og et antal negative stemmer til hver proposition (men jeg tænker at det første næsten er nemmest..). Og ja, så kunne én form for sådan en tredjepart så være en organisation med fysiske lokationer, hvor medlemmerne så kan møde op personligt og ændre deres stemmer og/eller repræsentanter, og hvorved organisationen kan opdatere deres stemmer herefter med en frekvans, der kan afhænge af, hvor mange ændrer deres stemmer ad gangen over en gennemsnitlig periode. Og en anden, meget smartere;), måde at have en videreformidlingsrepræsentant på, kunne så også være.. hm, lad mig lige se.. (13:50) ...(14:30) (ordner også vask) Jo, man kan også have en videreformidlingsrepresentant, der fungerer via mindst to tredjeparter, som klienten selv kan vælge. Først er der en trejdpart, eller instans bør vi nok hellere kalde det, bare.. som via asymmetrisk krypering får en nøgle fra hver bruger, som kun denne instans og hver enkelt relevant bruger må kende. ..Ja, eller på nær at de også så skal sende alle disse nøgler til en anden instans, der heller ikke må offentliggøre dem, og som så i øvrigt ikke ved hvor hver enkelt nøgle stammer fra (og må ikke få dette af vide af første instans). ..Hm, vent, giver dette mening..? ..Ah, jo, jeg kan få det til at give mening, men lad mig nu lige se.. (14:36) ..Hm jo, denne instans nr. 2 kan så også få en offentlig nøgle med fra brugeren til hver enkelt nøgle af første instans, sådan at denne altså bare får et sæt af nøgle par, hvor den ene er en offentlig nøgle. Denne instans kan så kryptere.. Hov, nej, så behøver vi faktisk ikke den første nøgle; instans nr. 1 sender altså bare et sæt af offentlige nøgler videre (gennem en krypteret kanal) til instans nr. 2. Denne offentliggør aldrig disse, men bruger dem hver især til at kryptere en besked med en ny nøgle i, og offentliggør alle disse krypterede beskeder. Brugerne prøver så at dekryptere dem hver især, indtil de finder deres egen.. Hm, er dette får ressourcekrævende, eller skal denne instans også lige tilknytte et meget lille hash a hver offentlig nøgle med beskeden, så hver bruger ikke skal igennem så mange..? ..Det kunne man sige.. ..although.. ..Tjo, men brugerne kan så stadig downloade alle beskeder i rækkefølge og så bare nøjes med at beholde dem, de skal tjekke.. Hm, lad mig lige tænke, om ikke der er en smartere løsning.. ..Hm, men ellers var pointen så, at enhver bruger, som ikke får en passende besked, bør så anråbe dette, hvorefter alle nøgler så skal indgives, sådan så man kan finde ud af, hvilken part var synderen (inkl. anråberen, hvis dette var en fejl), hvorefter man så kan starte forfra, muligvis uden synderen. Men når hver bruger så har fået en ny nøgle, som kan kan spores hen til dem, hvis alle de involverede instanser (for man kan godt have flere nr.-2-instanser her) bryder deres løfter og offentliggør deres data (og ikke bare sletter det kort tid efter). Nu kan man så være sikker på, at alle brugere i gruppen har netop én anonym nøgle, som nu kan bruges til at oprette en anonym bruger profil for hver bruger, selvfølgelig med VPNs involveret, hvormed denne frit kan afgive sine stemmer og ændre dem, hvornår det skal være, uden at det kan spores tilbage til dem. (14:52) .. ..Og disse anonyme brugere kan så udløbe således at de skal opdateres en gang imellem, således at hvis nu nogen for lækket deres bruger, så vil det allerhøjest kun være den seneste aktivitet, der bliver lækket (og derudover kan man selvfølgelig også dele brugeren op i flere (der ikke kan kædes sammen af andre), hvis man synes, der er besværet værd, men ja, og sådan vil der selvfølgelig altid være ting, man kan tilføje, hvis man finder frem til, at det giver mening..). Nå, men selv hvis der findes et bedre system end dette, så kan jeg jo bare skrive, at det f.eks. ikke er svært at finde på systemer, hvor man via flere instanser, der hver især holder på sin del af en samlet hemmelighed (hvor alle stykker skal bruges, hvis man vil spore tilbage), kan opnå at hver bruger i en gruppe får netop én anonym bruger. Og ja, hvis man så sørger for at de udløber med jævne mellemrum.. Og at brugerne skifter.. Hm.. ..Hm, men det er nu ikke perfekt anonymitet, hvis man sammenligner med valg, hvor ingen data bliver gemt til at starte med, således at ingen nogensinde kan spore det tilbage.. Hm.. ..Hm, men kunne man ikke bare bruge en teknik, som jeg vist også har tænkt på før, hvor en instans bare offentliggør en mængde af.. Hm.. ..Hm jo, en mængde af dens egne offentlige nøgler, nemlig med et antal svarende til antallet af klient-deltagere i øvelsen, og hvor hver klient så vælger et hemmeligt ID, krypterer.. Hm, nej, lad mig lige se... ..Hm, hvad med at alle klienter bare opretter et VPN kun med demselv som noder, og så begynder at sende data rundt. På et tilfældigt tidspunkt sender hver bruger så et ID videre til en naboknude, som modtager, sender ID'et videre til én naboknude, og noterer også ID'et og modtagelsestidspunktet.. nej.. Hm, dette virker vist næsten, men ikke helt.. ..(15:21) Ah, nu har jeg det måske. Man kunne lave en kæde af krypterede blokke, hvor hver blok offentligt hører til en klient, og hver blok rummer data, som brugeren fik tilsendt af ejeren af den tidligere blok, og data som brugeren har sendt videre til næste klient. Denne blokkæde kører så på omgang i en ring, således at den tager flere runder. Og på et tilfældigt tidspunkt tilføjer hver bruger så et offentlig nøgle, som de sender videre. ..Hm, nej det er endnu ikke helt vandtæt.. ..Ah, men måske hvis man tilføjer sin nøgle i krypteret tilstand, så den først kan lukkes op, når den når til en (tilfældigt udvalgt) anden bruger.. Hm, spændende idé.. (15:27) ..Ja, man må næsten kunne lave sådan et system, hvor klienterne billedligt talt danner sådan en rundkreds, hvisker data videre til hinanden én ad gangen i rundkredsen, og hvor klienter i kredsen så kan kryptere en hemmelighed, som en klient et andet sted så kan forstå. Denne bør så med det samme kryptere en ny besked, hvormed denne hemmelighed kastes videre til en anden person. Hemmeligheden er så en offentlig krypteringsnøgle. Man slutter så, efter et vist tidspunkt, når man er næsten 100 \% sikker på, at alle brugere for længst vil have kastet deres nøgle ind i rundkredsen, og at denne er læst af modtageren. Alle brugere offentliggør så de nøgler, der har været sendt frem til dem. Herefter skal alle brugere/klienter (jeg kan ikke lade være med at skrive "brugere" i stedet for klienter, men det er vel også næsten ligeså godt..) så sige, om deres nøgle er iblandt de offentliggjorte (men selvfølgelig ikke udpege dem). Hvis antallet af nøgler passer og alle brugere/klienter siger, at deres er med i mængden, så stopper "legen" succesfuldt. ..Eller rettere, det gør den, efter at man så beder alle brugere om at slette de nøgler, de ville have brugt til at dekryptere deres egne blokke med. Og sikkerheden i systemet handler så om, at man har tillid til, at størstedelen af klienter vil gøre dette (selvfølgelig fordi de bare bruger det udleverede software til det, og ikke har bygget eller tilegnet sig en malicious kopi af denne software).. Men hvis der er for mange nølger, eller at en klient mangler en nøgle, jamen så må man så bede alle brugere om at dekryptere alle deres blokke. Og så er pointen, at man kun ved at have alle disse blokke dekrypteret, kan finde frem til, hvem der er synderen, fordi man så både vil kunne se, hvis de ikke har opfundet netop én nøgle selv, og fordi man kan se, hvis de ikke har videresendt den rette nøgle hver gang.. Nå ja, og hver bruger skal så også bare i det hele taget indsende deres private nølger, så man kan finde frem til synderen. Og hvis enten en klient nægter at indsende den private nølge i dette tilfælde, eller hvis man finder synderen ved at dekryptere alle nøglerne, så må man så udelukke denne bruger i næste tur (altså give denne karantæne). Men ja, som sagt, hvis legen derimod ender succesfuldt, så skal brugerne endeligt ikke offentliggøre deres private nøgler, nej faktisk skal de slette alle deres nøgler, der blev brugt under selve legen og kun beholde den private nøgle, som passer til den offentlige nøgle, de herved fik indsendt anonymt via legen. Og ja, så længe de fleste brugere bare gør dette, så er man ikke i fare for, at det bliver afsløret, hvilke nøgler i slutmængden hører til hvilke klienter. :) (15:53) ..(16:02) Hm, der er faktisk en lille smule hangman's paradox tilstede i denne løsning, men det kan man vist gøre bod på ved bare at sige, at hver knude.. Hm.. ..Hm, eller hvad i stedet med bare at gøre sådan, at klienter i kredsen generelt skal vente et tilfældigt antal omgange, inden.. hm, men det løser dog ikke problemet eksakt.. (16:07) ..Hm, men jo, man kunne vel også bare sørge for, at sandsynligheden for at ens software sender en nøgle starter virkeligt lille og kun vokser over mange runder, og så kunne man gøre sådan, at hvis en bruger bagefter kan se, at deres software har sendt.. Hov, vent, dette er da slet ikke et problem, netop fordi man kaster hemmeligheden frem i rækken.. hm.. ..Hm, der skal kun tre (specifikke) andre brugere til at afsløre en i denne løsning, men de kan det kun hvis man har været uheldig at softwaren har sendt ens nøgle tidligt.. ..Hm, man kunne også bare give hver bruger mulighed for at afbryde legen, hvis deres sofware har sendt deres nøgle tilstrækkeligt tidligt.. Hm.. ..Hm, i øvrigt kan man hurtiggøre processen, hvis kredesn har mange kæder i gang på én gang, så alle klienter kan bygge en blok i hver runde (nemlig hvis der er ligeså mange kæder i gang, osm der er klienter i kredsen).. ..Hm, men kan man ikke bare generere flere nølger, end der er behov for..? (16:18) ..Jo, og så kan brugerne/klienterne til sidst bare vælge, hvilken nøgle af dem, de har fået genereret i legen, de vil beholde, ved at.. Hm.. ..Ah, ved selvfølgelig bare at bekende offentligt bagefter, at "disse nølger var mine, men jeg skal ikke bruge dem alligevel."!:) Og hvis så der lige præcis bliver det samme antal efterfølgende, som der er klienter, og hvis alle meddeler, at de har en nøgle iblandt de endelige, så når man i mål, og ellers må man så bare til at optrævle kæden, for at finde DoS-synderen, hvis ikke legen ender som den burde. :) (16:25) Og ja, det skal så bare anbefales, at hver bruger ikke vælger en nøgle, der blev genereret helt i starten af systemet, men ved at det stadig er brugerens beslutning at udvælge den ønskede nøgle, så eliminerer man altså hangman's paradox.:) (16:26) ..Nå ja, og lad mig lige præcisere, at hver blok så skal indeholde en liste af krypterede nøgler (som hver er kryperet med en tilfældig andens offentlige nøgle), og denne lister vokser altså bare.. tja, eller man kan måske begynde at fjerne ting fra bunden af listen efter et vist stykke tid, når det er sikkert, at samme nøgle er blevet indsat igen i ny version (nemlig ved at en knude har dekrypteret og re-krypteret nøglen og sat den på). Og man kan så kræve, at hver knude tilføjer netop én ting til listen i hver runde.. how, "runde" er et dårligt term at bruge for hvert enkelt lille step, når vi har en rundkreds, så lad os kalde.. tja, lad os bare kalde det enten hver 'step'/'skridt' eller hver tur.. nej, lad os udelukkende kalde det 'skridt'/'step.' Og hvis en bruger så modtager flere beskeder på én gang i et step.. ..Hm, nej vi kan også godt kalde det turn i stedet (for så tænker man jo bare på et lille turn af hjulet).. Så må denne bruger så altså gerne vente en omgang med at sende nummer 2 besked (osv., hvis der modtages flere end to), og altså så kun videresende én af nøglerne i den første tur, hvor nøglerne modtages. Ok, så det var vist bare det, jeg lige skulle præcisere.. (16:40)
%
%%(16:42) Nå, men der er også et andet issue, jeg skal tænke over, og det handler om: Vil det ikke være for fristende for folk at stemme på deres vante repræsentanter i et regeringsvalg, hvor et e-demokrati kæmper, og ser ud til at kunne vinde? For hvis man gør dette, så vil man vel kunne få dobbelt magt, medmindre e-demokratiet kan se, hvem der ikke stemte på det.. hvad de jo ikke vil kunne.. Hm, måske er dette et ret stort problem, men ja, nu vil jeg altså give mig til at tænke godt over det... (16:44)
%
%%(31.10.22, 9:21) Kort efter, jeg klappede i i går kom jeg frem til, hvad vist også havde været oppe at vende i periferien af mine tanker tidligere på dagen, at den simple og måske eneste løsning nok bare er, at sørge for, at e-demokrati-partiet i starten også har til opgave at booste stemmevægte inde i systemet (på en helt transparant måde selvfølgelig), således, at alle repræsentanter, der ellers har fået mandater udover partiet, de får en modvægt til sig inde i partiet. På den måde kan det ikke betale sig at stemme uden for partiet for at pågældende mening skal få mere magt, for så vil den pågældende mening bare blive countered. Og ja, det er så partiets opgave at finde frem til og være ærlig omkring, hvad der er midten af det politiske spektrum i henhold til forskellige punkter, således at man kan counter'e et vist mandat ved at give mere magt til en (eller flere) fra den modsatte (i.e. spejlede) ende af spektrummet. Når partiet så er i regering, så kan man så også bede de repræsentanter, der ikke er med, om at joine, for så vil e-demokratiet bare fjerne magt igen fra dem, der står for at counter'e/udbalancere magtbalancen.. (9:29)
%
%
%%\section{A note on transparency}
%
%
%\section{E-democracies in companies}
%
%%To finish this chapter, let me just make a small point about how an e-democracy application like this might also be incredibly useful when it comes to democratically run companies, or indeed the almost-democratically run `Economically Sustainable' Companies (ESCs.\,. hm, that looks a lot like `Escape(s)'.\,.) that was described in Chapter \ref{MSE}. 
%%
%%If the company in question has a goal of expansion, such as should be the case for the 
%
%To finish this chapter, let me just make a small point about how an e-democracy application like this might also be incredibly useful when it comes to democratically run companies, or indeed the almost-democratically run `economically sustainable' companies that was described in Chapter \ref{MSE}. 
%
%If the company in question has a goal of expansion, such as should be the case in general for the `economically sustainable' companies as described, I envision that this venture will be all the more exciting for the participants if there is a vibrant online community that engages in discussing and finding what strategies to go ahead and try in order to expand the company. 
%
%And if this e-democracy application can be as useful a tool for this as I believe it can, it could thus accelerate the interest in taking part and supporting such a company immensely. 
%
%%Hm, skal jeg så bare stoppe her for nu? (Eller skal jeg skrive videre på denne sektion, og var der i øvrigt andet, jeg har glemt at nævne..?) (15:21) ..Hm, jeg har glemt at nævne min pointe omkring gennemsigtighed ved at sørge for, at folk med jævne mellemrum bliver udtaget til at sætte sig ind i detaljerne og så rapportere tilbage til den interessegruppe, der udvalgte vedkommende, men måske jeg bare skal gemme denne pointe til en anden gang.. (15:23)
%%...(16:01) Nej, jeg tror ikke, jeg behøver at tilføje mere nu. Når jeg så lige får tænkt lidt mere over spiral-protokollen, så kan jeg skrive om den, og ellers er det nok bare lige at redigere teksten. (16:02)
%
%
%
%%Husk:
%	%Jeg havde tænkt mig her at nævne det med, at det kan være smart at udvælge nogen (som så jo kan vælges til at være upartisk og/eller repræsentativ (men måske smart/intelligent nok)) fra en gruppe til at studere og gennemgå systemet i nærmere detaljer og så rapportere tilbage..
%	%Transparancy.
%	%
%	%You never have to waste a vote (and never have to fear wasting a vote). And never have to be fearful, that who you voted for does something you didn't expect (since this system requires no trust in representatives, at least not except in cases why you don't feel like you have the time (or interest) to go through the details of a matter).
%	%..(16:47, 29.10.22) Hm, og husk det her med at man kan have flere områder, hvor forskellige bestemmer, og at dette så også gør, at andre grupper kan logge sig på i systemet, hvor vi snakker om at styre et land. I et sådant e-demokrati kan grupper altså også tilføje områder. På den måde kan de gøre det offentligt for alle, hvad de har tænkt sig. Hermed kan vi altså få en stor markedsplads, der handler om at lave aftaler og bestemmelser, både i regeringen, men også i andre instanser (det kunne f.eks. være såsom fagforeninger, hvilket jo vil være meget relevant i den sammenhæng). ..Og ja, det kan også være grupper, der egentligt ikke har nogen anden magt over noget, men som alligevel vil oprette et område, der hedder "vi mener sådan og sådan, og vil vil gøre sådan og sådan," altså et område, hvor de kan signalerer til omverdnen, hvad deres interesser er, og hvad de gerne vil / er parate til at gøre. (16:54) ..Og ja, det kan så nævnes, at dette så også kan være sådan noget som at trække sig fra den overordnede gruppe (f.eks. e-regeringspartiet eller trække sig som kunde og/eller investor i et firma). 
%	%Jeg kunne godt nævne muligheden i "forbrugerforeninger" kort også (som et eksempel på anden form for magt), men så tilføje, at min kd.v.-idé så netop nok ville være endnu bedre her, for så kan man undgå sådanne reprimanter (eller hvordan det staves). Men om ikke andet kunne det så blive en måde at tvinge gang i en kd.v., hvis nu virksomhederne indenfor en branche er tøvende med det. 
%	%Jeg skal forresten huske at have område-repræsentanter med under avancerede punkter, sammen selvfølgelig med områderne selv. Jeg kan således nok godt nævne "områderne" først, også selvom det egentligt er vigtigere, det med at kunne vælge repræsentanter.. 
%	%"Det handler om at det bliver: meget lettere at samle sig i små grupper, og meget lettere at sætte i gang i en proces, hvor man overvejer, om ikke der kan gøres noget ved et forhold, netop fordi man bare kan starte denne diskussion i nogle små grupper (som så kan kontakte andre grupper, små eller store, når de har fået samlet en oversigt over, hvad problemet er, og hvad man kunne gøre for at løse det m.m.). Så altså langt større tilgængelighed for den enkelte og dermed mange mange flere mennesker aktiveret ad gangen (som så overvejer og finder på løsningsforslag til problemer i samfundet (ofte særlige problemer for nogen specifikke i samfundet, men det kan jo også være mere almene)). Og så vil der så derefter også kunne være meget kortere tid til, fra løsningsforslag til løsning i sådan et direkte demokrati, der er klart. Og ikke mindst vil folk (i grupper) få langt nemmere mulighed for at indgå selv komplicerede politiske aftaler med andre folk (i grupper (ikke nødvendigvis disjunkte med de første, btw)), således at man får et meget bedre og hurtigere kan få handlet sig til at få opfyldt sine behov som en gruppe af mennesker, og således at smafundet derfor vil blivet meget bedre fintunet, så at sige, til at opfylde så mange menneskers forskellige behov som muligt på en gang."
%	%(15:01, 01.11.22) Jeg skal huske noget, jeg lige fik tænkt på, og det er, at et sådant demokrati kan få en meget meget fladere struktur, hvor at man, når man har en ny idé til forandring, lad os sige som lille gruppe, i stedet for så at skulle indsende og ansøge om idéen til en central, så kunne man i første omgang dele den, med den/de mest relevante nabogruppe(r). Hvis de så også er med på den, så kunne man så brede det til endnu flere. Og når idéen så har samlet nok opbakning, så kan man melde det til det brede fællesskab, hvor idéen så allerede har opbakning, når den ansøges om. Jeg ved godt, at sådanne måder at fremføre idéer på allerede finder sted mange steder, men jeg tror, at man i et e-demokrati kunne gøre den fremgangsmåde endnu nemmere og endnu mere hyppig.. Hm, måske vil jeg skrive om dette, men om ikke andet er det da bare rart at tænke på, at der kunne blive sådan en rigtig flad struktur, hvor relaterede grupper selvstændigt kan diskutere og handle om, hvilke idéer og forslag, man vil gå videre med..:).. (15:08)
%	%Man kan også bruge min blok-spral-idé til når stemmerne skal kastes..!
%
%
%
%
%
%
%%(09.11.22, 9:46):
%\section{(I'm considering adding something like:) A similar application for scientific discussion}
%
%\textit{I have now realized that this application could also be used for scientific discussion graphs, which goes hand in hand with decision making since facts are of course important when deciding policies. In a discussion graph, on would just not really need the `conditional node' edges, but would instead just use the `conditional votes' instead --- which could then be drawn as edges between notes for this type of application. %(This all of a sudden make this idea quite a bit more interesting for me in terms of what I would like to work on myself.\,. .\,.\,Hm, hvilket er relevant for mig at have i tankerne i denne stund, for jeg skal nemlig snart til jobsøgeningsmøde med A-kassen. Og ja, med denne indsigt, så må det da næsten være denne idé, jeg vil prøve at gå videre med (og sige jeg vil iværksætte), det tænker jeg.. (..Altså i stedet for Web 2.0--3.0-idéen/erne.))
%*And it should then be very much recommended (as a key part of the idea), that users try to commit themselves to continuously update their votes for propositions as conditional ones, once more fundamental propositions are added to the system. A scientist might for instance be an expert on drugs and say (or actually ``vote'') that: ``this drug is so and so addictive,'' but then once propositions are added about the existence of relevant studies are added, as well as propositions about trust, then that scientist (along with everyone) are then strongly recommended to change the vote into a conditional vote such that the vote now depends on the study existence proposition and the trust proposition. This way (if the community follows this (strong) recommendation), every proposition can slowly become more and more founded in the basis empirical propositions/data, plus trust propositions (which are essentially propositions about how the users want to apply epistemology, i.e.\ when these propositions are also boiled down to their roots). This both has the advantage of the system being more flexible, when new studies turn up or if old ones come into question at some point, and also, importantly, it makes it easier to browse and find out what fundamental facts our more abstract facts in society are built on, i.e.\ to find the sources, and it also gives a better and easier understanding of what is interesting to research, since it shows were the ``gaps'' are, so to speak, or more precisely: where the research is thin and could use bolstering. 
%}
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%%\chapter{A possible road towards Web 3.0}
%%
%%
%%**(Lad mig bare lige skrive, hvad jeg tænker nu at skrive i dette afsnit/kapitel bare ud i én køre, og så kan jeg altid redigere bagefter..) %(06.11.22, 9:46) (Jeg fik nemlig lige tænkt en del over emnet igen i går aftes, og nu synes jeg alligevel, at jeg bør kunne forklare meget af det ret kortfattet..:))
%%*(Okay, jeg har alligevel ikke tænkt mig at beholde dette kapitel, men lad mig bare lige skrive denne køre færdig, også fordi jeg har nogle små nye gode tilføjelser, mener jeg..)
%%
%%
%%\section{Everything section}
%%
%%
%%My idea for how we can reach the promises of Web 3.0, and specifically the Semantic Web, is to first implement a Web 2.0 site with an underlying semantic structure and then really try to give the users a lot of power to redesign things on the site and to program algorithms themselves. This implementation of the Semantic Web then does \emph{not} rely on XML/HTML. Instead, all semantic sentences should be recorded in relational database. 
%%
%%This is radically different from the first implementations of the Semantic Web, where metadata is simply added to various sites and resources on the web, and then algorithms in the Semantic Web would simply work by querying the web (i.e.\ the World Wide Web) and finding the necessary information online. But when all the semantic sentences (what is also called triplets in the current conventional implementations) are stored in one database, the algorithms can run way faster. 
%%
%%Essentially, one can say that the idea is to start out with a Web 2.0 site as we know them, e.g.\ such as YouTube, Reddit, Twitter, etc., and then implement the Semantic Web there. But hold on, you might say, it can hardly be called the Semantic \emph{Web}, if it is controlled by a private company. No, but if it is instead controlled by a open source organization (similar to how the web is run today, e.g.) it is another matter. Hereby it can be ensured that no one owns all the user contribution, save perhaps for the relevant user, and that any other organization can always come and take up the mantle at any time, should it be needed (just how it also is with Wikipedia).
%%
%%Alternatively, if starting this idea as a non-profit organization is slow and lacks investment, one could also start it as the type of company as described in Chapter \ref{MSE}, such that the ``organization'' can start out as a private, commercial company, but where it is guaranteed that the users will slowly become the owners. 
%%
%%But let us move on from this topic for now and assume that the organization will have plenty of funding (just like Wikipedia has). 
%%
%%
%%Let me now try to explain the overall design of a Web 2.0 site that I envision, which has an underlying semantic structure. Some of the details here are more important than others (and some are less), but it is nice to see a good example that could work (and attract many users), and then from there, I can explain why the underlying semantic structure becomes important. 
%%
%%If I were to design such a Web 2.0 site, where the intention is that it can grow into a Web 3.0 site over time, I would probably give it this following initial design:
%%
%%A main feature of the site should be a page with a category tree, which I would implement basically as a structure of tabs, i.e.\ the kind of tab system we see everywhere in the interfaces of Windows and Mac applications and so on. Whenever a new tab is selected, it will then potentially open a new list of sub-tabs. The user might thus have selected the category `movies' as a tab, and then a submenu of movie categories should open. Thus, we get a category tree (which hopefully should be pretty quick and easy to navigate as a user). Whenever a new list of (sub-)tabs is in focus, it should be expanded as a whole box of selection, in fact one might even implement it as a whole HTML page at some point (instead of just a box containing a lot of tabs). But when a tab/subcategory is selected, the box/page should nevertheless collapse into just a single bar of a horizontally adjacent tabs, where one tab is then selected. And underneath, the new subcategory selection should then automatically expand. Also, if a user has navigated down into a category tree, but want to go to a different super-category, the user can either just click on some of the visible tabs in the one of the above tab bars (which are aligned vertically adjacent, each with tabs aligned horizontally adjacent), or he/she can expand that given tab box once again.
%%
%%Okay, that was a lot of details to explain a very simple design, but it is nice to have an example to hold on to, and one that does the job. But of course, there could be many other types of design that this Web 2.0 site might start with.
%%
%%Moving on, now that we have a category tree, we should also have some resources in it, of course. So at the same time as the user selects these categories and subcategories, there should be a list of resources at the bottom that is updated in principle whenever the user chooses a new category (although the user might want to click a button manually to make the resource list refresh such that it doesn't refresh al the time while the user is navigating the category tree).
%%
%%When the user then selects a resource from the list, the user is led to a the page of that resource. That resource is then displayed pretty much at the top of that page. And how the resource is displayed then depends on what kind of resource it is, i.e.\ whether it a video or a HTML page, and so on. And each resource should then also have a list of comments below, but similarly to the all the main resources of the site, if we can call them that, these commant should also be ordered in a similar category tree. Examples of different categories of comment could be `related resources,' `user reactions,' `related discussions,' `links to source material,' and so on and so forth. 
%%
%%And to finish up this description of this basic design, there should also be a homepage where each user can see one or more lists of the user's favorite categories and resources, such that the user can quickly navigate to some of their favorite spots in the category tree (without having to start from the root and navigate down).
%%
%%Okay, that was a quick sketch of a quite basic site design. 
%%%
%%%...Jeg har fået tænkt lidt mere. Nu ved jeg faktisk bedre, hvad man skal sige gælder for de rating-tal, der skal følge med sætningerne/tripletterne. Jeg tror ikke, det vil tage mig lang tid at færdiggøre denne hurtige udredning herfra, men lad mig lige se, om jeg lige vil skifte emne lidt og skrive på noget andet, eller om jeg vil holde en lille pause.. 
%%%...Okay, jeg prøver at skrive færdigt..
%%%
%%Now I can get on to some of the stuff that is actually interesting.
%%
%%Assuming that the reader knows about the Semantic Web (and about triplets and so on), the reader might have already guessed that the categorization of the resources should then of course be user-driven. The users should thus be able to say for instance: ``this resource belongs to this category,'' and thereby be able to vote resources into various categories. Note that ``this resource belongs to this category'' can be implemented as a triplet. The users should also be able to say ``this category is a relevant subcategory to show under this other category.'' The users should thus also be in control of the category tree --- and of the category trees under each resource (where one can reuse resource category trees for similar types of resources).
%%
%%So far so good: One thus get a Web 2.0 site where the categorization structure is semantic and user-driven. And because it is semantic, all the user data can easily be reused in for other similar sites, and specifically also for other implementations of the site in question.
%%
%%If such a Web 2.0 site can become more and more popular, and if it is run by an open source organization (and/or community) as mentioned, this site might thus effectively become all that people hope for in terms of what Web 3.0 might bring.
%%
%%Okay, at this point I have explained the overall idea, and also explained an overall type of implementation that could be the starting point for a Web 2.0 site that thus aims to become, what we could call af Web 3.0 site (bringing forth the features that people hope for in Web 3.0). Now I will move on to the \emph{really} intersting stuff, because I actually have a few idea that i believe can make such a ``Web 2.0--3.0 site,'' as I like to call it, really take off! 
%%
%%I actually believe that \emph{triplet} system will not be enough to carry forth a really useful Semantic Web (which is a big part of people associate with Web 3.0)! %(12:44)...
%%
%%First of all, it is important that the ``triplets,'' but let us actually just call them `relations' or `sentences' instead, should contain the user ID of who uploaded it, as well as a timestamp for the upload. So they should not just have the three entries. Second of all, I think it is \emph{so} important for the usability of the system, that each relation/sentence can also include a number (with whatever precision is appropriate for the case) that signifies a rating of \emph{how much} the user believes the sentance to be true. 
%%
%%This makes it possible to \ldots \textit{Okay, jeg har skrevet så meget af det her allerede, så lad mig ikke gentage alle pointerne her, nemlig da jeg nu igen har besluttet mig, at jeg alligevel bare børe vente med at fokusere på dette emne. Så lad mig i stedet bare lige ridse mine nye tilføjelser op.\,.}
%%
%%Okay, let me make this short and just mention the new thoughts that a had about this idea. The rest of the ideas, as well as the explanation of why they will be so good, can be found in my 21--22 notes (in \texttt{main.tex}, as the document is still called in the moment of writing).
%%
%%My big idea for making it easy and attractive to rate the resources on their lists, is that they can simply drag them up and down on the lists to rate them (according to the proposition that they are viewing). So when the user moves the resources in the list around, it should generate sentences/relation to the database, where the rating number in these relations are determined by where the user drops the resource in the list (and where only the most recent adjustment applies (which is what the timestamp is useful for determining)). The number might run from 0 to 1, or from -1 to 1, or whatever; that does not matter much (and the site can always change the conventions and then simply convert the previous data to such that it is scaled to the new convention). And the scale should only be very vaguely defined. The real precision that the user should worry about is how the number related to the neighboring resources on the list. So if the user believes, say, a movie to be incredible, and it has a low rating, the user might want to pull it up closer to 1. But if the question is, should it have a 0.6 rating or a 0.9 rating (assuming 1 is the highest score), that should actually only depend on the existing scores of what movies have received scores in about that interval. So the underlying rating should thus be primarily defined in relation to what resources are already rated. And then! If one wants to turn the resulting rating into one where points on the rating axis is more precisely defined, one can then just (and should be able to), upload a translation of that rating, which is basically a conversion function that takes the primary rating and converts all the numbers to the new rating. This `translation' function can then simply be defined by taking a bunch of resources, plotting them in on the list, and then use statistics after that to plot in all the other resources on that new axis (with an updated metric). And by putting a Gaussian ``error'' on all the ``fixed'' resources on this new axis, one can make sure that this process does not run into contradictions. So in short: The normal rating axis that is used when users drag and drop resources to rate them should have very vaguely defined semantics to begin with, and then the users can always translate the resulting axis from all the user activity into something with a more precise meaning, simply by defining a new metric for the axis that moves the resources into new positions on the axis.
%%
%%I also want to mention that when users drag and drop resources, they should be able to dial up and down the number of resources shown on the list as the drag and drop. Here, a setting to show few items in the list could thus only show the most `popular' items, i.e.\ such that an external predicate can be used for defining this setting, other than the predicate that orders the list. The lists should thus also have `filters,' and these filters should have different settings. And if the user can change these settings by hitting some keys, they can basically ``zoom in and out'' in the lists, namely by changing the filter dial to show fewer or more `popular' items, e.g. The user can then be ``zoomed in'' and chose a resource to rate. The user would then start draging it up or down, but since the list is long when ``zoomed in,'' the users might then hit the key to ``zoom out'' while dragging the resource. And when the user find the desired spot to drop it, the user might even not drop it right away, but ``zoom in'' a bit first to find a more precise spot for the resource. 
%%
%%Okay, I think that was it.\,. no, wait, maybe I also want to quickly reiterate something about the subcategories actually being implemented via `compound predicates,' and also that when rating such predicates, the user actually have to rate each atomic predicate individually (such that the `compound predicates' are not meant for rating, but only showing resources in a list.\,. oh, and then if the user wants to filter the list such that only resources of the one category/predicate is shown while rating resources in terms of another predicate, the user then just has to use the `filter' that I just mentioned.\,.).\,. Hm, no, I think that I have already covered these point in my 21--22 notes (in \texttt{main.tex}). So let me just stop again with this subject for now.\,. 
%%
%%\ldots\ Well, let me just mention another thing quickly, namely that the site might also use some automatically generated meta-sentences/relation, such as: ``this resource was uploaded by this user'' or ``this resource was uploaded as a comment to this resource.'' These are thus automatic sentences/relations that the site itself is responsible for applying to all uploads.
%%
%%.\,.\,Oh, and I also intended to mention something else, by the way: I wanted to mention that certain resources, e.g.\ HTML resources (or other markup), might actually get access to the database themselves. For instance, a HTML document might say ``insert a list of the top resources in this category here'' or something like that. More generally these resources might thus be able to query the database when they are viewed and change their appearance after what is contained in the database. (And this way, the site can thus also implement my so-called ``wiki-idea'' from the 21--22 notes.) 
%%
%%\ldots And yeah, all that jazz about `user groups' to distribute trust, and about the user-driven filter algorithms, about rating tags, and about so on and so forth, all that is written about in my 21--22 note collection, I don't want to try to repeat these things now.\,:) %(14:29)
%%
%%%(07.11.22, 10:32):
%%\ldots\ Oh, and I of course also need to mention an important point, and that is: Sure the idea could work as an organization, but if we think about e.g.\ YouTube and Twitch, the commercial part is a big part of what makes those sites work. And by using my ``Economically Sustainable'' (ES) companies instead, the Web 2.0--3.0 site would still be able to give big rewards to the users who create popular content. And on that topic, if I were in charge of such a company, I would try to bring the average user on as soon as possible, given them some vote and some say in how the creators should get paid (how much in total, perhaps, and more importantly: how it should be distributed). I thus envision an e-democracy using the system described above for all the users, where their decisions in this e-democracy will be heard by the company, at least if it is reasonable (and in fact, the company could also be a part of the e-democracy, giving it self a significant weight on its vote, which would then make it easier for the company to keep to its promise of listening to that e-democracy). And of course, since it is an ES company, this e-democracy will be more a representation of the true power over the company, not just power that is ``lend out'' to the users, as long as their decisions does not stray to much from the company's wishes. *(This last sentence does not seem to make a whole lot of sense, but I guess I just needed to point out that if the company is what I am currently calling an 'SRC,' the users will also eventually get that power \emph{within} the company..)
%
%
%%*(26.11.22, 9:01) I mentioned movies at some point above as an example of a category of resources. That does not mean that the site then has to contain all the movies themselves; the resources can simply be reference-type resources (such as kind of movie ID, etc.). And since users should be able to control how resources in different categories are generally viewed, meaning that they can add HTML-wrappers to the resources, they can thus make it so that all movie refernce resources are viewed with potential links to site where they can be viewed or what not (the HTML code can fetch anything that is desired from the database (and/or from the web)). 
%
%
%*(20.12.22) I should also mention, that I imagine that all sentences/relations (formerly known as triplets) should be signed by a private key of the user, which is publicly associated with the (or \emph{is} the) user ID. This way you don't have to trust the particular server when it comes to who uploaded what and when. 






















\end{document}